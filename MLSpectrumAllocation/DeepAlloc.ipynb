{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input,optimizers, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers, Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datetime, time\n",
    "import os, sys\n",
    "import tqdm\n",
    "import gc\n",
    "from keras import callbacks\n",
    "from multiprocessing import Process\n",
    "Point = namedtuple('Point', ('x', 'y'))\n",
    "Circle = namedtuple('Circle', ('r'))\n",
    "Square = namedtuple('Square', ('side'))\n",
    "Rectangle = namedtuple('Rectangle', ('length', 'width'))\n",
    "PointWithDistance = namedtuple('PointWithDistance', ('p', 'dist'))\n",
    "float_memory_used = 'float32'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT\n",
    "# PART 1\n",
    "number_samples = [128, 256, 512, 1024, 4096] \n",
    "validation_size, noise_floor = 0.2, -90.0\n",
    "su_power = 0 # this is not actually su power just a number to show there is an SU in its image\n",
    "max_x, max_y, number_image_channels, su_szie = 299, 299, 8, 60  # su_size:30 for 1000, 10 for 100\n",
    "cell_size, pixel_expansion = 1000 / max_x, max_x / 100\n",
    "pu_shape, su_shape = 'circle', 'circle' # shape = {'circle', 'square', 'point'}\n",
    "style = \"raw_power_min_max_norm\"  # {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "intensity_degradation, slope, su_slope = 'log', 5, 5  # 'log', 'linear', slope 3 for 1000, 5 for 100\n",
    "max_pus_num, max_sus_num = 20, 10\n",
    "propagation_model = 'splat' # 'splat', 'log', 'testbed'\n",
    "noise, std = False, 1 # False for splat\n",
    "if su_shape == 'circle':\n",
    "    su_param = Circle(su_szie)\n",
    "elif su_shape == 'square':\n",
    "    su_param = Square(su_szie)\n",
    "else:\n",
    "    su_param = None\n",
    "    \n",
    "sensors = False\n",
    "if sensors:\n",
    "    sensors_num = 225\n",
    "    sensors_file_path = f\"data/sensors/square{100}/{sensors_num}/sensors.txt\"\n",
    "\n",
    "# PART 2\n",
    "number_of_proccessors = 12\n",
    "memory_size_allowed = 4 # in Gigabyte\n",
    "float_size = 0\n",
    "if float_memory_used == \"float16\":\n",
    "    float_size = 16\n",
    "elif float_memory_used == \"float\" or \"float32\":\n",
    "    float_size = 32\n",
    "elif float_memory_used == \"float8\":\n",
    "    float_size = 8\n",
    "\n",
    "\n",
    "batch_size = int(memory_size_allowed / (max_x * max_y * number_image_channels * float_size/(8 * 1024 ** 3)))\n",
    "\n",
    "\n",
    "dtime = datetime.datetime.now().strftime('_%Y%m_%d%H_%M')\n",
    "color = \"color\" if number_image_channels > 1 else \"gray\"\n",
    "image_dir = 'ML/data/pictures_' + str(max_x) + '_' + str(max_y) + '_transfer/' + propagation_model + (\n",
    "    \"/noisy_std_\" + str(std) if noise else \"\") + '/pu_' + pu_shape + '_su_' + su_shape + '_' + (\n",
    "    \"\" if su_shape == 'point' else str(su_szie)) + \"/\" + style + \"/\" + color +'/' + (\n",
    "    \"\" if pu_shape == 'point' and su_shape == 'point' else (f\"{intensity_degradation}_pu{slope}_su{su_slope}\")) + (\n",
    "    \"/\" + str(sensors_num) + \"sensors\" if sensors else f\"/{max_pus_num}pus\") + \\\n",
    "        f\"_{max_sus_num}sus_{number_image_channels}channels\" + \"/images\"\n",
    "\n",
    "if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"/home/shahrokh/projects/MLSpectrumAllocation/ML/data/pictures_299_299_transfer/log/\" +\\\n",
    "            \"noisy_std_1/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su6/\" +\\\n",
    "            \"variable_sensors_10_20_pus_5_sus_8_channels/images\"\n",
    "sensors_location = {}\n",
    "for sensor_num in [49, 100, 225, 400, 625]:\n",
    "    sensors_location[sensor_num] = []\n",
    "    with open(f\"data/sensors/square{100}/{sensor_num}/sensors.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.split(',')\n",
    "            sensors_location[sensor_num].append(Point(int(float(line[0])), int(float(line[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "MAX_SU_TOTAL = False\n",
    "num_columns = (sensors_num + 1 if sensors else max_pus_num * 3 + 1) + max_sus_num * 3 + 2\n",
    "# num_columns = max_pus_num * 3 + 1 + max_sus_num * 3 + 2\n",
    "cols = [i for i in range(num_columns)]\n",
    "dataset_name = \"dynamic_pus_using_pus_70000_min10_max20PUs_1SUs_square100grid_splat_2021_10_16_04_46.txt\"\n",
    "max_dataset_name = \"dynamic_pus_max_power_70000_min10_max20PUs_1SUs_square100grid_splat_2021_10_16_04_46.txt\"\n",
    "if MAX_SU_TOTAL:\n",
    "    max_su_total_dataset_name = \"dynamic_pus_maximum_total_sus50000_min10_max20PUs_5SUs_square100grid_splat_2022_06_09_13_24.txt\"\n",
    "with open('/'.join(image_dir.split('/')[:-1]) + '/datasets' + dtime + '.txt', 'w') as set_file:\n",
    "    set_file.write(dataset_name + \"\\n\")\n",
    "    set_file.write(max_dataset_name)\n",
    "    if MAX_SU_TOTAL:\n",
    "        set_file.write(max_su_total_dataset_name)\n",
    "\n",
    "dataframe = pd.read_csv('data/' \n",
    "                        + dataset_name, delimiter=',', header=None, names=cols)\n",
    "dataframe_max = pd.read_csv('data/' \n",
    "                            + max_dataset_name, delimiter=',', header=None)\n",
    "if MAX_SU_TOTAL:\n",
    "    dataframe_max_su_total = pd.read_csv('data/' + max_su_total_dataset_name, delimiter=\",\", header=None,\n",
    "                                        names=[i for i in range(max_sus_num * 3 + 1)])\n",
    "\n",
    "dataframe.reset_index(drop=True, inplace=True)\n",
    "dataframe_max.reset_index(drop=True, inplace=True)\n",
    "if MAX_SU_TOTAL:\n",
    "    dataframe_max_su_total.reset_index(drop=True, inplace=True)\n",
    "dataframe_max[dataframe_max.shape[1] - 1] = dataframe_max[dataframe_max.shape[1] - 1].astype(float)\n",
    "\n",
    "dataframe_tot = pd.concat([dataframe, dataframe_max.iloc[:, dataframe_max.columns.values[-1:]]], axis=1,\n",
    "                        ignore_index=True)\n",
    "\n",
    "idx = dataframe_tot[dataframe_tot[dataframe_tot.columns[-1]] == -float('inf')].index\n",
    "dataframe_tot.drop(idx, inplace=True)\n",
    "if MAX_SU_TOTAL:\n",
    "    dataframe_max_su_total.drop(idx, inplace=True)\n",
    "\n",
    "data_reg = dataframe_tot.values\n",
    "data_reg[data_reg < noise_floor] = noise_floor\n",
    "if MAX_SU_TOTAL:\n",
    "    data_max_su_tot = dataframe_max_su_total.values\n",
    "\n",
    "if sensors:\n",
    "    sensors_location = []\n",
    "    with open(sensors_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.split(',')\n",
    "            sensors_location.append(Point(int(float(line[0])), int(float(line[1]))))\n",
    "del dataframe, dataframe_tot, dataframe_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(p1: Point, p2: Point):\n",
    "    return ((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2) ** 0.5 * cell_size\n",
    "\n",
    "def calculate_mu_sigma(data, num_pus):\n",
    "    sum_non_noise = 0\n",
    "    for pu_n in range(num_pus): # calculate mu\n",
    "        sum_non_noise += data[pu_n*3+2]\n",
    "    mu = ((max_x * max_y - num_pus) * noise_floor + sum_non_noise)/(max_x * max_y)\n",
    "    sum_square = 0\n",
    "    for pu_n in range(num_pus): # calculate sigma\n",
    "        sum_square += (data[pu_n*3+2]-mu)**2\n",
    "    sum_square += (max_x * max_y - num_pus) * (noise_floor - mu)**2\n",
    "    sigma = math.sqrt(sum_square/(max_x * max_y))\n",
    "    return mu, sigma\n",
    "\n",
    "def get_pu_param(pu_shape: str, intensity_degradation: str, pu_p: float, noise_floor: float, slope: float):\n",
    "    pu_param = None\n",
    "    if pu_shape == 'circle':\n",
    "        if intensity_degradation == \"linear\":\n",
    "            pu_param = Circle(int((pu_p - noise_floor) / slope)) # linear\n",
    "        elif intensity_degradation == \"log\":\n",
    "            pu_param = Circle(int(10 ** ((pu_p - noise_floor) / (10 *slope)))) # log_based\n",
    "    elif pu_shape == 'square':\n",
    "        if intensity_degradation == \"linear\":\n",
    "            pu_param = Square(int(2 ** 0.5 * (pu_p - noise_floor) / slope)) # linear\n",
    "        elif intensity_degradation == \"log\":\n",
    "            pu_param = Square(int(2 ** 0.5 * 10 ** ((pu_p - noise_floor) / (10 *slope)))) # log_based\n",
    "    elif pu_shape == 'point':\n",
    "        pu_param = None\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported PU shape(create_image)! \", pu_shape)\n",
    "    return pu_param\n",
    "\n",
    "def create_image(data, slope, sensors_num, style=\"raw_power_z_score\", noise_floor=-90, pu_shape= 'circle',\n",
    "                 pu_param=None, su_shape='circle', su_param=None, intensity_degradation=\"log\", \n",
    "                 max_pu_power: float=0, max_su_power: float=0):  \n",
    "    # style = {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "    # intensity_degradation= {\"log\", \"linear\"}\n",
    "    # if param is None, it's automatically calculated. Highest brightness(or power value) (255 or 1.) would\n",
    "    # assigned to the center(PU location) and radius(side) would be calculated based on its power, slope, and noise floor.\n",
    "    # If it is given, intensity(power) of pixel beside center would be calculated in the same fashin with an exception that \n",
    "    # intensity below zero(noise_floor) would be replaced by zero(noise_floor)\n",
    "    if style == \"raw_power_min_max_norm\":\n",
    "        # In this way, PUs' location are replaced with their power(dBm) and the power would fade with \n",
    "        # slope till gets noise_floor(in circle shape)\n",
    "        \n",
    "        # creating pu matrix\n",
    "        image = np.zeros((max_x, max_y, number_image_channels), dtype=float_memory_used)\n",
    "        if not sensors:\n",
    "            pus_num = int(data[0])\n",
    "#             print(pus_num)\n",
    "            for pu_i in range(pus_num):\n",
    "                pu_x = max(0, min(max_x-1, int(data[pu_i * 3 + 1] * pixel_expansion))) \n",
    "                pu_y = max(0, min(max_x-1, int(data[pu_i * 3 + 2] * pixel_expansion)))\n",
    "                pu_p = data[pu_i * 3 + 3]\n",
    "                pu_channel = int(abs(pu_p)//5) if number_image_channels > 3 else 0\n",
    "#                 print(pu_x, pu_y, pu_p)\n",
    "                if pu_param is None:\n",
    "                    pu_param_p = get_pu_param(pu_shape, intensity_degradation, pu_p, noise_floor, slope)\n",
    "                else:\n",
    "                    pu_param_p = pu_param\n",
    "                points = points_inside_shape(center=Point(pu_x, pu_y),\n",
    "                                             shape=pu_shape, param=pu_param_p)\n",
    "                for point in points:\n",
    "                    if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                        if intensity_degradation == \"linear\":\n",
    "                            image[int(abs(pu_p))//10][point.p.x][point.p.y] += (pu_p - slope * point.dist - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                        elif intensity_degradation == \"log\":\n",
    "                            if point.dist < 1:\n",
    "                                image[point.p.x][point.p.y][pu_channel] += (pu_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                            else:\n",
    "                                image[point.p.x][point.p.y][pu_channel] += (pu_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                    max_pu_power - noise_floor)\n",
    "        else:\n",
    "            ss_channels_num = number_image_channels - 2\n",
    "            ss_param, ss_shape = pu_param, pu_shape\n",
    "            sensors_num = int(data[0])\n",
    "            sensors_num_at_each_row = int(sensors_num ** 0.5)\n",
    "            for ss_i in range(sensors_num):\n",
    "                ss_x = max(0, min(max_x-1, int(sensors_location[sensors_num][ss_i].x * pixel_expansion)))\n",
    "                ss_y =  max(0, min(max_x-1, int(sensors_location[sensors_num][ss_i].y * pixel_expansion)))\n",
    "                ss_p = max(noise_floor, data[ss_i+1])\n",
    "                ss_row, ss_cols = ss_i // sensors_num_at_each_row, ss_i % sensors_num_at_each_row\n",
    "                ss_channel = ss_cols % ss_channels_num\n",
    "                if ss_row % 2:\n",
    "                    ss_channel = (ss_channel + ss_channels_num // 2) % ss_channels_num\n",
    "                if ss_param is None:\n",
    "                    ss_param_p = get_pu_param(ss_shape, intensity_degradation, ss_p, noise_floor, slope)\n",
    "                else:\n",
    "                    ss_param_p = ss_param\n",
    "                points = points_inside_shape(center=Point(ss_x, ss_y), shape=ss_shape, param=ss_param_p)\n",
    "                for point in points:\n",
    "                    if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                        if intensity_degradation == \"linear\":\n",
    "                            image[point.p.x][point.p.y][ss_channel] += (ss_p - slope * point.dist - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                        elif intensity_degradation == \"log\":\n",
    "                            if point.dist < 1:\n",
    "                                image[point.p.x][point.p.y][ss_channel] += (ss_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                            else:\n",
    "                                image[point.p.x][point.p.y][ss_channel] += (ss_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                    max_pu_power - noise_floor)\n",
    "        del points\n",
    "        # creating su matrix\n",
    "        su_num_idx = sensors_num + 1 if sensors else (pus_num * 3 + 1)\n",
    "        su_num = int(data[su_num_idx])\n",
    "#         print(su_num)\n",
    "#         su_num = (len(data) - pus_num * (3 if not sensors else 1)) // 2\n",
    "#         if not (len(data) - pus_num * (3 if not sensors else 1)) % 2:\n",
    "#             raise ValueError(\"Data provided is not correct; can't get SUs' information(create_image)\")\n",
    "        if su_param is None:\n",
    "            # if su_param is unavailable, a circle(square) with radius(side) 1 is created\n",
    "            if su_shape == 'circle':\n",
    "                su_param = Circle(1)\n",
    "            elif su_shape == 'square':\n",
    "                su_param = Square(1)\n",
    "            elif su_shape == 'point':\n",
    "                su_param = None\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported SU shape(create_image)! \", su_shape)\n",
    "        \n",
    "        for su_i in range(su_num - 1):\n",
    "            su_x = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 1] * pixel_expansion)))\n",
    "            su_y = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 2] * pixel_expansion)))\n",
    "            su_p = data[su_num_idx + su_i * 3 + 3]\n",
    "#             su_p = su_intensity\n",
    "            su_param_p = get_pu_param(su_shape, intensity_degradation, su_p, noise_floor, su_slope)\n",
    "            points = points_inside_shape(center=Point(su_x, su_y),\n",
    "                                         param=su_param_p, shape=su_shape)\n",
    "            su_channel = 0 if number_image_channels == 1 else -2\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                            su_val = (su_p - su_slope * point.dist - noise_floor)/(max_su_power - noise_floor)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "                            su_val = (su_p - noise_floor) / (max_su_power - noise_floor)\n",
    "                        else:\n",
    "                            su_val = (su_p - su_slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                max_su_power - noise_floor)\n",
    "                    image[point.p.x][point.p.y][su_channel] += su_val\n",
    "            del points\n",
    "        # the last and  target SU\n",
    "        su_intensity = 1.\n",
    "        su_x = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 1] * pixel_expansion)))\n",
    "        su_y = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 2] * pixel_expansion)))\n",
    "        points = points_inside_shape(center=Point(su_x, su_y),\n",
    "                                     param=su_param, shape=su_shape)\n",
    "        su_channel = -1\n",
    "        for point in points:\n",
    "            if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                image[point.p.x][point.p.y][su_channel] += su_intensity\n",
    "        del points\n",
    "        return image\n",
    "        \n",
    "    elif style == \"image_intensity\":\n",
    "        # creating PU image\n",
    "        image = np.zeros((1,number_image_channels,max_x, max_y), dtype=float_memory_used)\n",
    "        pus_num = int(data[0])\n",
    "        for pu_i in range(pus_num):\n",
    "            pu_x = max(0, min(max_x-1, int(data[pu_i * 3 + 1] * pixel_expansion))) \n",
    "            pu_y = max(0, min(max_x-1, int(data[pu_i * 3 + 2] * pixel_expansion)))\n",
    "            pu_p = data[pu_i * 3 + 3]\n",
    "            \n",
    "            if pu_param is None:\n",
    "                pu_param_p = get_pu_param(pu_shape, intensity_degradation, pu_p, noise_floor, slope)\n",
    "            else:\n",
    "                pu_param_p = pu_param\n",
    "            \n",
    "            points = points_inside_shape(center=Point(pu_x, pu_y), shape=pu_shape, param=pu_param_p)\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                        image[0][0][point.p.x][point.p.y] += (pu_p - slope * point.dist - noise_floor)/(\n",
    "                            max_pu_power - noise_floor)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "                            image[0][0][point.p.x][point.p.y] += (pu_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                        else:\n",
    "                            image[0][0][point.p.x][point.p.y] += (pu_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                        \n",
    "        # creating SU image\n",
    "        del points\n",
    "        # creating su matrix\n",
    "        su_num_idx = sensors_num if sensors else (pus_num * 3 + 1)\n",
    "        su_num = int(data[su_num_idx])\n",
    "        if su_param is None:\n",
    "            # if su_param is unavailable, a circle(square) with radius(side) 1 is created\n",
    "            if su_shape == 'circle':\n",
    "                su_param = Circle(1)\n",
    "            elif su_shape == 'square':\n",
    "                su_param = Square(1)\n",
    "            elif su_shape == 'point':\n",
    "                su_param = None\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported SU shape(create_image)! \", su_shape)\n",
    "        \n",
    "        for su_i in range(su_num - 1):\n",
    "            su_x = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 1])))\n",
    "            su_y = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 2])))\n",
    "            su_p = data[su_num_idx + su_i * 3 + 3]\n",
    "            \n",
    "#             su_p = su_intensity\n",
    "            points = points_inside_shape(center=Point(su_x, su_y), param=su_param, shape=su_shape)\n",
    "            su_channel = 0 if number_image_channels == 1 else -1\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                            su_val = (su_p - slope * point.dist - noise_floor)/(max_pu_power - noise_floor)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "                            su_val = (su_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                        else:\n",
    "                            su_val = (su_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                    image[0][su_channel][point.p.x][point.p.y] += su_val\n",
    "            del points\n",
    "        # the last and  target SU\n",
    "        su_intensity = 1.\n",
    "        su_x = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 1])))\n",
    "        su_y = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 2])))\n",
    "#         print(su_x, su_y)\n",
    "        points = points_inside_shape(center=Point(su_x, su_y), param=su_param, shape=su_shape)\n",
    "        su_channel = 0 if number_image_channels == 1 else -1\n",
    "        for point in points:\n",
    "            if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                image[0][su_channel][point.p.x][point.p.y] += su_intensity\n",
    "        del points\n",
    "        return image       \n",
    "            \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported style(create_image)! \", style)\n",
    "        \n",
    "def points_inside_shape(center: Point, shape: str, param)-> list:\n",
    "    # This function returns points+distance around center with defined shape\n",
    "    if shape == 'circle':\n",
    "        # First creates points inside a square(around orgigin) with 2*r side and then remove those with distance > r.\n",
    "        # Shift all remaining around center. O(4r^2)\n",
    "        r, origin = param.r, Point(0, 0)\n",
    "        square_points = set((Point(x, y) for x in range(max(-int(r/cell_size), -max_x), \n",
    "                             min(int(r/cell_size), max_x) + 1) \n",
    "                             for y in range(max(-int(r/cell_size), -max_y), min(int(r/cell_size), max_y) + 1)))\n",
    "        points = []\n",
    "        while square_points:\n",
    "            p = square_points.pop()\n",
    "            dist = euclidian_distance(p, origin)\n",
    "            if dist <= r:\n",
    "                points.append(PointWithDistance(Point(p.x + center.x, p.y + center.y), dist))\n",
    "                if p.x != 0:\n",
    "                    points.append(PointWithDistance(Point(-p.x + center.x, p.y + center.y), dist))\n",
    "                    square_points.remove(Point(-p.x, p.y))\n",
    "                if p.y != 0:\n",
    "                    points.append(PointWithDistance(Point(p.x + center.x, -p.y + center.y), dist))\n",
    "                    square_points.remove(Point(p.x, -p.y))\n",
    "                if p.x != 0 and p.y != 0:\n",
    "                    points.append(PointWithDistance(Point(-p.x + center.x, -p.y + center.y), dist))\n",
    "                    square_points.remove(Point(-p.x, -p.y))\n",
    "        del square_points\n",
    "        return points\n",
    "    elif shape == 'square':\n",
    "        half_side = param.side // 2\n",
    "        return [PointWithDistance(Point(x, y), euclidian_distance(Point(x, y), center)) for x in range(-half_side + center.x,\n",
    "                                                                                               half_side + center.x+1) \n",
    "                         for y in range(-half_side + center.y, half_side + center.y + 1)]\n",
    "    elif shape == 'point':\n",
    "        return [PointWithDistance(center, 0)]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported shape(points_inside_shape)! \", shape)\n",
    "        \n",
    "def read_image(image_num, image_dir=image_dir):\n",
    "    if False and style == \"image_intensity\":\n",
    "        image = plt.imread(image_dir + '/image' + str(image_num)+'.png')\n",
    "        image = np.swapaxes(image, 0, 2)\n",
    "        image = np.array(image[:number_image_channels], dtype=float_memory_used).reshape(1, number_image_channels, max_x, max_y)\n",
    "    elif  style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\" or style == \"image_intensity\":\n",
    "        suffix = 'npz'  # npy, npz\n",
    "#         image = np.load(f\"{image_dir}/images{image_num//100000}/image{image_num}.{suffix}\") \n",
    "        image = np.load(f\"{image_dir}/image{image_num}.{suffix}\") \n",
    "        if type(image) == np.lib.npyio.NpzFile:\n",
    "            image = image['a']\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating synthesized images\n",
    "def creating_image(start, end):\n",
    "    for image_num in tqdm.tqdm(range(start, end+1)):\n",
    "        image = create_image(data=data_reg_train[image_num], slope=slope, style=style, \n",
    "                             noise_floor=noise_floor,\n",
    "                             pu_shape=pu_shape, su_shape=su_shape, su_param=su_param, \n",
    "                             sensors_num=(sensors_num if sensors else 0), \n",
    "                             intensity_degradation=intensity_degradation, \n",
    "                             max_pu_power=0.0 if not sensors else -60)\n",
    "        if new_image_state[image_num] == \"rot\":\n",
    "            image = np.rot90(image, 2)\n",
    "        elif new_image_state[image_num] == \"lr\":\n",
    "            image = np.fliplr(image)\n",
    "        elif new_image_state[image_num] == \"ud\":\n",
    "            image = np.flipud(image)\n",
    "        np.savez_compressed(image_dir + '/aug/image' + str(image_num), a=np.expand_dims(image,0 ))\n",
    "        \n",
    "        del image\n",
    "        \n",
    "train_size = 2048\n",
    "data_reg_train = np.repeat(data_reg[:train_size], 4, axis=0)\n",
    "image_state = [\"\", \"rot\", \"lr\", \"ud\"] * train_size\n",
    "p = np.random.permutation(train_size*4)\n",
    "data_reg_train = data_reg_train[p]\n",
    "new_image_state = []\n",
    "for idx in range(train_size*4):\n",
    "    new_image_state.append(image_state[p[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = []\n",
    "proc_sizes = [data_reg_train.shape[0]//number_of_proccessors] * (number_of_proccessors)\n",
    "proc_sizes[-1] += data_reg_train.shape[0]%number_of_proccessors\n",
    "proc_idx = [(sum(proc_sizes[:i]), sum(proc_sizes[:i+1])-1) for i in range(number_of_proccessors)]\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    p = Process(target=creating_image, args=(proc_idx[i][0], proc_idx[i][1]))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].join()\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].terminate()\n",
    "    jobs[i].close()\n",
    "del jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving images once to save time\n",
    "# run this cell just for creating images\n",
    "def creating_image(start, end):\n",
    "    for image_num in tqdm.tqdm(range(start, end+1)):\n",
    "        image = create_image(data=data_reg[image_num], slope=slope, style=style, \n",
    "                             noise_floor=noise_floor,\n",
    "                             pu_shape=pu_shape, su_shape=su_shape, su_param=su_param, \n",
    "                             sensors_num=(sensors_num if sensors else 0), \n",
    "                             intensity_degradation=intensity_degradation, \n",
    "                             max_pu_power=0.0 if not sensors else -60,\n",
    "                             max_su_power=40.0)\n",
    "        if False and style == \"image_intensity\":\n",
    "            if number_image_channels != 3:\n",
    "                image = np.append(np.array(image[0]), np.zeros((3-number_image_channels,max_x, max_y), \n",
    "                                                               dtype=float_memory_used), axis=0)\n",
    "            image_save = np.swapaxes(image, 0, 2)\n",
    "            plt.imsave(image_dir + '/image' + str(image_num)+'.png', image_save)\n",
    "        elif style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\" or style == \"image_intensity\":\n",
    "            np.savez_compressed(f\"{image_dir}/images{image_num//100000}/image{image_num}\",\n",
    "                                a=np.expand_dims(image,0))\n",
    "        del image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = []\n",
    "proc_sizes = [data_reg.shape[0]//number_of_proccessors] * (number_of_proccessors)\n",
    "proc_sizes[-1] += data_reg.shape[0]%number_of_proccessors\n",
    "proc_idx = [(sum(proc_sizes[:i]), sum(proc_sizes[:i+1])-1) for i in range(number_of_proccessors)]\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    p = Process(target=creating_image, args=(proc_idx[i][0], proc_idx[i][1]))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].join()\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].terminate()\n",
    "    jobs[i].close()\n",
    "del jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xception model\n",
    "model_name = \"ResNet50V2\"\n",
    "base_model = applications.ResNet50V2(include_top=False, weights=None,\n",
    "                                      input_shape=(max_x, max_y, number_image_channels))\n",
    "base_model.trainable = True\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-vgg pretrained - pu-setting\n",
    "model_name = \"log_vgg16\"\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "model_path = \"/home/shahrokh/projects/MLSpectrumAllocation/ML/data/pictures_299_299_transfer/log/noisy_std_1/\" + \\\n",
    "             \"pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su6/20pus_5sus_8channels/models/vgg16/\" + \\\n",
    "             \"700000/best_model_lambda_0.1.h5\"\n",
    "model_path = \"ML/data/pictures_299_299_transfer/log/noisy_std_1/pu_circle_su_circle_60/\" + \\\n",
    "             \"raw_power_min_max_norm/color/log_5/pus_1_sus_3_channels_700k/models/700000/\" + \\\n",
    "             \"best_model_lambda_0_fit.h5\"\n",
    "base_model = models.load_model(model_path, \n",
    "                              custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                               'fp_mae': fp_mae,\n",
    "                                               'mae':'mae', 'mse':'mse'})\n",
    "base_model.trainable = False\n",
    "base_model = base_model.layers[1]\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log-vgg pretrained - ss-setting\n",
    "model_name = \"ResNet50V2\"\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "model_path = \"/home/shahrokh/projects/MLSpectrumAllocation/ML/data/\" +\\\n",
    "             \"pictures_299_299_transfer/log/noisy_std_1/pu_circle_su_circle_60/\" + \\\n",
    "             \"raw_power_min_max_norm/color/log_pu5_su6/\" + \\\n",
    "             \"variable_sensors_10_20_pus_5_sus_8_channels/models/ResNet152V2/\" + \\\n",
    "             \"700000/best_model_lambda_0_1.h5\"\n",
    "model_path = \"/home/shahrokh/projects/MLSpectrumAllocation/ML/data/pictures_299_299_transfer/\" +\\\n",
    "             \"log/noisy_std_1/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su6/\"+\\\n",
    "             \"20pus_5sus_8channels/models/ResNet50V2/700000/best_model_lambda_0_lr0.01_mae10.725.h5\"\n",
    "base_model = models.load_model(model_path, \n",
    "                              custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                               'fp_mae': fp_mae,\n",
    "                                               'mae':'mae', 'mse':'mse'})\n",
    "base_model = base_model.layers[1]\n",
    "base_model.trainable = False\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEEP = True\n",
    "def cnn_model(kernel_lam, bias_lam):\n",
    "    inputs = Input(shape=(max_x, max_y, number_image_channels))\n",
    "    convolution_filter, dense_filter = 'relu', 'linear'\n",
    "    convolution_init, dense_init = \"lecun_normal\", \"RandomNormal\"\n",
    "    \n",
    "    \n",
    "    if True:\n",
    "        # We make sure that the base_model is running in inference mode here,\n",
    "        # by passing `training=False`. This is important for fine-tuning, as you will\n",
    "        # learn in a few paragraphs.\n",
    "        cnn = base_model(inputs, training=False)\n",
    "        cnn = layers.GlobalAveragePooling2D()(cnn)\n",
    "        cnn = layers.Dense(2048, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                             bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Dropout(0.5)(cnn)\n",
    "        outputs = layers.Dense(1, activation=dense_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                             bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=dense_init)(cnn)\n",
    "        return Model(inputs, outputs)\n",
    "    else:\n",
    "        data_format=\"channels_last\"\n",
    "        filter_shape, pool_size = (1, 1), (2,2)\n",
    "        cnn = layers.Conv2D(512, filter_shape, padding='same', \n",
    "                            activation=convolution_filter, data_format=data_format, \n",
    "                            kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                            bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(cnn)\n",
    "        \n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter,\n",
    "                            data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(cnn)\n",
    "        \n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter,\n",
    "                            data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(cnn)\n",
    "        cnn = layers.Conv2D(512, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(128, filter_shape,padding='same', activation=convolution_filter,\n",
    "                            data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(32, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(8, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.Conv2D(1, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.GlobalAveragePooling2D()(cnn)\n",
    "        return Model(inputs, cnn)\n",
    "    \n",
    "\n",
    "class DataBatchGenerator(Sequence):\n",
    "    def __init__(self, dataset:np.ndarray, batch_size:int, start_idx:int,\n",
    "                 number_image_channels:int,\n",
    "                 max_x, max_y, float_memory_used, image_dir = image_dir, conserve=0):\n",
    "        self.dataset, self.batch_size, self.start_idx = dataset, batch_size, start_idx\n",
    "        self.number_image_channels, self.max_x, self.max_y = number_image_channels, max_x, max_y\n",
    "        self.float_memory_used = float_memory_used\n",
    "        self.conserve = conserve\n",
    "        self._image_dir = image_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.ceil(self.dataset.shape[0] / self.batch_size).astype(int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        size = min(self.dataset.shape[0] - idx * self.batch_size, self.batch_size)\n",
    "        batch_x = np.empty((size, self.max_x, self.max_y, self.number_image_channels), \n",
    "                           dtype=self.float_memory_used)\n",
    "        batch_y = np.empty((size), dtype=self.float_memory_used)\n",
    "        for i in range(size):\n",
    "            batch_x[i] = read_image(self.start_idx + idx * self.batch_size + i, self._image_dir)\n",
    "            batch_y[i] = self.dataset[idx * self.batch_size + i][- 1 - self.conserve]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "class PredictBatchGenerator(Sequence):\n",
    "    def __init__(self, dataset_size: int, batch_size: int, start_idx: int,\n",
    "                 number_image_channels: int, max_x: int,\n",
    "                 max_y: int, float_memory_used,\n",
    "                 image_dir: str):\n",
    "        self.dataset_size = dataset_size\n",
    "        self.batch_size, self.start_idx = batch_size, start_idx\n",
    "        self.number_image_channels, self.max_x, self.max_y = number_image_channels, max_x, max_y\n",
    "        self.float_memory_used = float_memory_used\n",
    "        self.image_dir = image_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.ceil(self.dataset_size / self.batch_size).astype(int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        size = min(self.dataset_size - idx * self.batch_size, self.batch_size)\n",
    "        batch_x = np.empty((size, self.max_x, self.max_y, self.number_image_channels),\n",
    "                           dtype=self.float_memory_used)\n",
    "        for i in range(size):\n",
    "            batch_x[i] = read_image(self.start_idx + idx * self.batch_size + i, self.image_dir)\n",
    "        return batch_x\n",
    "        \n",
    "    \n",
    "def custom_loss(fp_penalty_coef, fn_penalty_coef):\n",
    "    # custom loss function that penalize false positive and negative differently\n",
    "    def loss(y_true, y_pred):\n",
    "        res = y_pred - y_true\n",
    "        res = tf.where(res > 0, res * fp_penalty_coef, res * fn_penalty_coef)\n",
    "        return K.mean(K.square(res))\n",
    "    return loss\n",
    "\n",
    "def fp_mae(y_true, y_pred):\n",
    "    # custom metric that replace false negative with zero and return the mean of new vector\n",
    "    res = y_pred - y_true\n",
    "    res = tf.nn.relu(res)\n",
    "    return K.mean(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.test.is_gpu_available()\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN: support batching\n",
    "AUGMENTED = True\n",
    "TEST, CONSERVE, FINE_TUNING = True, False, True\n",
    "mini_batch = 32 if max(max_x, max_y) == 1000 else 64\n",
    "epochs = 35 if max(max_x, max_y) == 1000 else 130\n",
    "MAX_QUEUE_SIZE, WORKERS = 28, 4\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "hyper_metric, mode = \"val_mae\", 'min'  # the metric that hyper parameters are tuned with\n",
    "prev_sample = 0\n",
    "lambda_vec = [0, 0.01, 0.1, 1] \n",
    "# average_diff_power, fp_mean_power = [],[] #[7.177, 8.088, 8.183], [3.438, 3.506, 2.662]\n",
    "# best_lambda = []\n",
    "average_diff_power_conserve, fp_mean_power_conserve = [], []\n",
    "all_cnns = []\n",
    "if CONSERVE: # for conservative\n",
    "    prev_number_samples = [0] + number_samples[:-1]\n",
    "\n",
    "for num_sample_idx, number_sample in enumerate(number_samples):\n",
    "    if CONSERVE:\n",
    "        data_reg[prev_number_samples[num_sample_idx]:number_sample, -1] = data_reg[\n",
    "            prev_number_samples[num_sample_idx]:number_sample, -1] - 1.5 # conserv value\n",
    "    MODEL_PATH = '/'.join(image_dir.split('/')[:-1]) + '/models/' + (\"aug/\" if AUGMENTED else \"\") \\\n",
    "    + model_name + \"/\" + (\"conservative/\" if CONSERVE else \"\") + str(number_sample)\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "    MODEL_PATH += \"/best_model_lambda_\"\n",
    "    if True:\n",
    "        cnns = [cnn_model(lamb, 0) for lamb in lambda_vec]\n",
    "        for cnn in cnns:\n",
    "            cnn.compile(loss=custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                        optimizer=optimizers.Adam(), \n",
    "                        metrics=['mse', 'mae', fp_mae])\n",
    "        checkpointers = [ModelCheckpoint(filepath=MODEL_PATH + str(lamb)+ '.h5',\n",
    "                                         verbose=2, save_best_only=True, \n",
    "                                         monitor=hyper_metric,\n",
    "                                         mode=mode)\n",
    "                         for lamb in lambda_vec]\n",
    "    else:\n",
    "        cnns = []\n",
    "        cnns = [models.load_model(MODEL_PATH + str(lamb) + '.h5', \n",
    "                                  custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                  'fp_mae': fp_mae }) \n",
    "                for lamb in lambda_vec]\n",
    "    number_start = time.time()\n",
    "    train_generator = DataBatchGenerator(\n",
    "        dataset=data_reg_train[:number_sample * 4] if AUGMENTED else data_reg[:number_sample],\n",
    "        batch_size=mini_batch,\n",
    "        start_idx=prev_sample,\n",
    "        number_image_channels=number_image_channels,\n",
    "        max_x=max_x, max_y=max_y, float_memory_used=float_memory_used,\n",
    "        image_dir=image_dir + (\"/aug\" if AUGMENTED else \"\"))\n",
    "    \n",
    "\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "#     val_size = data_reg.shape[0] - number_sample\n",
    "    val_generator = DataBatchGenerator(dataset=data_reg[number_sample:number_sample+val_size], \n",
    "                                       batch_size=mini_batch,\n",
    "                                       start_idx=number_sample,\n",
    "                                       number_image_channels=number_image_channels,\n",
    "                                       max_x=max_x, max_y=max_y, \n",
    "                                       float_memory_used=float_memory_used,\n",
    "                                      image_dir=\"/\".join(image_dir.split(\"/\")[:-1]) + \"/images\")\n",
    "  \n",
    "    print('number_samples:', number_sample, \", New samples:\", number_sample - prev_sample)\n",
    "    print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", number_sample + val_size - 1)\n",
    "    \n",
    "    for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "        lambda_start = time.time()\n",
    "        cnns[lamb_idx].fit(train_generator, epochs=epochs, verbose=2,\n",
    "                           validation_data=val_generator, \n",
    "                           shuffle=True, callbacks=[checkpointers[lamb_idx], \n",
    "                                                    callbacks.EarlyStopping(monitor=hyper_metric, min_delta=1e-2,\n",
    "                                                                           patience=10,\n",
    "                                                                           mode=mode)],\n",
    "#                                                    callbacks.ReduceLROnPlateau(monitor=hyper_metric,\n",
    "#                                                                               factor=0.2,\n",
    "#                                                                               patience=10,\n",
    "#                                                                               mode=mode)], \n",
    "                           workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                           use_multiprocessing=False)\n",
    "        \n",
    "        print(\"\\nLambda:\", lamb, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - lambda_start))))\n",
    "        print(\"Train Error(all epochs):\", min(cnns[lamb_idx].history.history['mae']), '\\n', \n",
    "              [round(val, 3) for val in cnns[lamb_idx].history.history['mae']])\n",
    "        print(\"Train FP Error(all epochs):\", min(cnns[lamb_idx].history.history['fp_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['fp_mae']])\n",
    "        print(\"Val Error(all epochs):\", min(cnns[lamb_idx].history.history['val_mae']), '\\n', \n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_mae']])\n",
    "        print(\"Val FP Error(all epochs):\", min(cnns[lamb_idx].history.history['val_fp_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_fp_mae']])\n",
    "    if FINE_TUNING:\n",
    "    # ******************** fine-tunning *******\n",
    "        print(\"******FINE TUNNING ******\")\n",
    "        # reloading the best\n",
    "        cnns = [models.load_model(MODEL_PATH + str(lamb) + '.h5', \n",
    "                                  custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                   'fp_mae': fp_mae,\n",
    "                                                   'mae':'mae', 'mse':'mse'}) for lamb in lambda_vec]\n",
    "        for cnn in cnns:\n",
    "            cnn.trainable = True\n",
    "            cnn.compile(loss=custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                        optimizer=optimizers.Adam(1e-5), \n",
    "                        metrics=['mse', 'mae', fp_mae])\n",
    "        train_generator = DataBatchGenerator(\n",
    "            dataset=data_reg_train[:number_sample * 4] if AUGMENTED else data_reg[:number_sample],\n",
    "            batch_size=mini_batch//2,\n",
    "            start_idx=prev_sample,\n",
    "            number_image_channels=number_image_channels,\n",
    "            max_x=max_x, max_y=max_y, float_memory_used=float_memory_used,\n",
    "            image_dir=image_dir + (\"/aug\" if AUGMENTED else \"\"))\n",
    "        val_generator = DataBatchGenerator(dataset=data_reg[number_sample:number_sample+val_size], \n",
    "                                           batch_size=mini_batch,\n",
    "                                           start_idx=number_sample,\n",
    "                                           number_image_channels=number_image_channels,\n",
    "                                           max_x=max_x, max_y=max_y, \n",
    "                                           float_memory_used=float_memory_used,\n",
    "                                          image_dir=\"/\".join(image_dir.split(\"/\")[:-1]) + \"/images\")\n",
    "        for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "    #     for lamb_idx, lamb in enumerate(lambda_vec[:len(lambda_vec) - num_sample_idx//2]):\n",
    "    #         if num_sample_idx == 3 and lamb_idx < 4:\n",
    "    #             continue\n",
    "            lambda_start = time.time()\n",
    "            cnns[lamb_idx].fit(train_generator, epochs=int(epochs//2), verbose=2,\n",
    "                               validation_data=val_generator, \n",
    "                               shuffle=True, callbacks=[checkpointers[lamb_idx],\n",
    "                                                        callbacks.EarlyStopping(\n",
    "                                                            monitor=hyper_metric, min_delta=1e-2,\n",
    "                                                            patience=7, mode=mode)], \n",
    "                               workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                               use_multiprocessing=False)\n",
    "\n",
    "            print(\"\\nLambda:\", lamb, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - lambda_start))))\n",
    "            print(\"Train Error(all epochs):\", min(cnns[lamb_idx].history.history['mae']), '\\n', \n",
    "                  [round(val, 3) for val in cnns[lamb_idx].history.history['mae']])\n",
    "            print(\"Train FP Error(all epochs):\", min(cnns[lamb_idx].history.history['fp_mae']), '\\n',\n",
    "                  [round(val,3) for val in cnns[lamb_idx].history.history['fp_mae']])\n",
    "            print(\"Val Error(all epochs):\", min(cnns[lamb_idx].history.history['val_mae']), '\\n', \n",
    "                  [round(val,3) for val in cnns[lamb_idx].history.history['val_mae']])\n",
    "            print(\"Val FP Error(all epochs):\", min(cnns[lamb_idx].history.history['val_fp_mae']), '\\n',\n",
    "                  [round(val,3) for val in cnns[lamb_idx].history.history['val_fp_mae']])\n",
    "    \n",
    "    models_min_mae = [min(cnns[lam_idx].history.history[hyper_metric]) for\n",
    "                      lam_idx,_ in enumerate(lambda_vec)]\n",
    "    best_lamb_idx = models_min_mae.index(min(models_min_mae))\n",
    "    best_lambda.append(lambda_vec[best_lamb_idx])\n",
    "    print(\"\\nTrainig set size:\", number_sample, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - \n",
    "                                                                                              number_start))),\n",
    "          \", best_lambda:\", lambda_vec[best_lamb_idx], \", min_\" , (\"fp_\" if hyper_metric == \"val_fp_mae\" else \"\"),\n",
    "          \"error:\", round(min(models_min_mae), 3))\n",
    "    all_cnns.append(cnns)\n",
    "    del cnns, train_generator, val_generator\n",
    "    \n",
    "    if TEST:\n",
    "        # evaluating test images\n",
    "        best_model = None\n",
    "        best_model = models.load_model(MODEL_PATH + str(lambda_vec[best_lamb_idx]) + '.h5', \n",
    "                                       custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                       'fp_mae': fp_mae,\n",
    "                                                      'mae':'mae', 'mse':'mse'})\n",
    "        test_generator = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                            batch_size=mini_batch * 2,\n",
    "                                            start_idx=number_sample + val_size, \n",
    "                                            number_image_channels=number_image_channels,\n",
    "                                            max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "\n",
    "        print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "        time.sleep(1)\n",
    "        test_res = best_model.evaluate(test_generator, verbose=1, \n",
    "                                       workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)\n",
    "        \n",
    "        test_mae_idx, test_fp_mae_idx = [best_model.metrics_names.index(mtrc) \n",
    "                                         for mtrc in ['mae','fp_mae']]\n",
    "        test_mae, test_fp_mae = test_res[test_mae_idx], test_res[test_fp_mae_idx]\n",
    "        average_diff_power.append(round(test_mae, 3))\n",
    "        fp_mean_power.append(round(test_fp_mae, 3))\n",
    "        print('average_error: ', average_diff_power[-1], ', fp_average_error: ', \n",
    "              fp_mean_power[-1])\n",
    "        \n",
    "        if False:\n",
    "            test_generator_conserve = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                                         batch_size=mini_batch,\n",
    "                                                         start_idx=number_sample + val_size, \n",
    "                                                         number_image_channels=number_image_channels,\n",
    "                                                         max_x=max_x, max_y=max_y, \n",
    "                                                         float_memory_used=float_memory_used, \n",
    "                                                         conserve=1)\n",
    "            test_res_conserve = best_model.evaluate(test_generator_conserve, verbose=1, \n",
    "                                                    workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                                                    use_multiprocessing=False)\n",
    "            test_mae_cons, test_fp_mae_cons = test_res_conserve[test_mae_idx], test_res_conserve[test_fp_mae_idx]\n",
    "            average_diff_power_conserve.append(round(test_mae_cons, 3))\n",
    "            fp_mean_power_conserve.append(round(test_fp_mae_cons, 3))\n",
    "            print('Conserve, average_error: ', average_diff_power_conserve[-1], ', fp_average_error: ',\n",
    "                 fp_mean_power_conserve[-1])\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        \n",
    "        var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/models/' + model_name + \"/\"\n",
    "                     + intensity_degradation + '_' + str(slope) + '_' + \n",
    "                     dtime + \".dat\", \"wb\") # file for saving results\n",
    "        pickle.dump([average_diff_power, fp_mean_power, number_samples, best_lambda, \n",
    "                     dataset_name, max_dataset_name, average_diff_power_conserve, fp_mean_power_conserve,\n",
    "                     checkpointers],\n",
    "                    file=var_f)\n",
    "        var_f.close()\n",
    "        del best_model, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"average_diff_power:\", [round(x, 3) for x in average_diff_power])\n",
    "print(\"fp_error:\", [round(x, 3) for x in fp_mean_power])\n",
    "print(\"best_lambda:\", best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving images once to save time\n",
    "# run this cell just for creating images\n",
    "def creating_image_max_su_tot(start, end):\n",
    "    # for image_num in range(115, data_reg.shape[0]):\n",
    "    # for image_num in range(1625, 5000):\n",
    "    for row_idx in tqdm.tqdm(range(start, end+1)):  #4463, data_reg.shape[0]\n",
    "        row_sample = data_reg[row_idx]\n",
    "        row_sample[int(row_sample[0]) * 3 + 1] = 1\n",
    "        for su_idx in range(int(data_max_su_tot[row_idx][0])):\n",
    "            row_sample[int(row_sample[0]) * 3 + 2 : \n",
    "                       int(row_sample[0]) * 3 + 4] = data_max_su_tot[row_idx][1 + su_idx * 3: \n",
    "                                                                              1 + su_idx * 3 + 2]\n",
    "            image = create_image(data=row_sample, \n",
    "                                 slope=slope, style=style, noise_floor=noise_floor,\n",
    "                                 pu_shape=pu_shape, su_shape=su_shape, su_param=su_param, \n",
    "                                 sensors_num=(sensors_num if sensors else 0), \n",
    "                                 intensity_degradation=intensity_degradation, \n",
    "                                 max_pu_power=0.0 if not sensors else -60,\n",
    "                                 max_su_power=40.0)\n",
    "            if False and style == \"image_intensity\":\n",
    "                if number_image_channels != 3:\n",
    "                    image = np.append(np.array(image[0]), np.zeros((3-number_image_channels,max_x, max_y), \n",
    "                                                                   dtype=float_memory_used), axis=0)\n",
    "                image_save = np.swapaxes(image, 0, 2)\n",
    "                plt.imsave(max_su_image_dir + '/image' + str(image_num)+'.png', image_save)\n",
    "            elif style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\" or style == \"image_intensity\":\n",
    "        #         np.save(max_su_image_dir + '/image' + str(image_num), image)\n",
    "    #             np.savez_compressed(f\"{image_dir}{(600000 + image_num)//100000}/image{600000 + image_num}\",\n",
    "    #                                 a=np.expand_dims(image,0))\n",
    "                np.savez_compressed(f\"{max_su_image_dir}/image{row_idx * 5 + su_idx}\",\n",
    "                                    a=np.expand_dims(image,0))\n",
    "            del image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-SU using single deep-alloc and another NN\n",
    "# Create dataset\n",
    "\n",
    "number_samples = [128, 256, 512, 1024, 2048, 4096]\n",
    "MAX_QUEUE_SIZE, WORKERS = 28, 4\n",
    "mini_batch = 128\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "dataset_path = '/'.join(image_dir.split('/')[:-1]) + \"/\" + model_name\n",
    "\n",
    "model_path = \"/home/shahrokh/projects/MLSpectrumAllocation/ML/data/pictures_299_299_transfer/\" + \\\n",
    "             \"splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/\" + \\\n",
    "             \"pus_1_sus_3_channels/models/log_vgg16/\"\n",
    "y = np.copy(data_max_su_tot[:,3::3])\n",
    "\n",
    "for number_sample in number_samples:\n",
    "    model = models.load_model(f\"{model_path}/{number_sample}/best_model_lambda_0.h5\",\n",
    "                              custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef),\n",
    "                                              'fp_mae': fp_mae,\n",
    "                                              'mae':'mae', 'mse':'mse'})\n",
    "    model.trainable = False\n",
    "    predic_batch_generator = PredictBatchGenerator(dataset_size=data_reg.shape[0] * max_sus_num,\n",
    "                                                   batch_size=mini_batch,\n",
    "                                                   image_dir=max_su_image_dir,\n",
    "                                                   max_x=max_x, max_y=max_y, \n",
    "                                                   number_image_channels=number_image_channels,\n",
    "                                                   start_idx=0, float_memory_used=float_memory_used)\n",
    "    number_dataset_path = f\"{dataset_path}/{number_sample}\"\n",
    "    if not os.path.exists(number_dataset_path):\n",
    "        os.makedirs(number_dataset_path)\n",
    "    predict_power = model.predict(predic_batch_generator, verbose=1, \n",
    "                                  workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE,\n",
    "                                  use_multiprocessing=False)\n",
    "    X = np.copy(data_max_su_tot[:, 1:])\n",
    "    X[:,2::3] = predict_power.reshape(data_reg.shape[0], max_sus_num)\n",
    "    \n",
    "    np.savetxt(f\"{number_dataset_path}/X.txt\", X, delimiter=\",\")\n",
    "    np.savetxt(f\"{number_dataset_path}/y.txt\", y, delimiter=\",\")\n",
    "    print(f\"{number_sample} finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(n_inputs: int, n_outputs:int, kernel_lam, bias_lam, num_hidden_layers = 2, num_neurons = 100):\n",
    "    hidden_filter, last_layer_filter = 'relu', 'linear'\n",
    "    hidden_init, last_layer_init = \"lecun_normal\", \"RandomNormal\"  #he_uniform\n",
    "    model = Sequential()\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(layers.Dense(num_neurons,\n",
    "                              input_dim=n_inputs, \n",
    "                               kernel_initializer=hidden_init,\n",
    "                               activation=hidden_filter,\n",
    "                               kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                               bias_regularizer=regularizers.l2(bias_lam)))\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(layers.Dropout(0.8))\n",
    "    model.add(layers.Dense(n_outputs, \n",
    "                           kernel_initializer=last_layer_init, \n",
    "                           activation=last_layer_filter,\n",
    "                           kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                           bias_regularizer=regularizers.l2(bias_lam)))\n",
    "    return model\n",
    "        \n",
    "    \n",
    "def custom_loss(fp_penalty_coef, fn_penalty_coef):\n",
    "    # custom loss function that penalize false positive and negative differently\n",
    "    def loss(y_true, y_pred):\n",
    "        res = y_pred - y_true\n",
    "        res = tf.where(res > 0, res * fp_penalty_coef, res * fn_penalty_coef)\n",
    "        return K.mean(K.square(res))\n",
    "    return loss\n",
    "\n",
    "def fp_mae(y_true, y_pred):\n",
    "    # custom metric that replace false negative with zero and return the mean of new vector\n",
    "    res = y_pred - y_true\n",
    "    res = tf.nn.relu(res)\n",
    "#     res = tf.where(res <= 0, 0, res)\n",
    "    return K.mean(res)\n",
    "\n",
    "def cus_mae(y_true, y_pred):\n",
    "    def log10(x):\n",
    "        numerator = K.log(x)\n",
    "        denominator = K.log(K.constant(10, dtype=numerator.dtype))\n",
    "        return numerator / denominator\n",
    "    p_true = K.sum(K.pow(10.0, y_true/10))\n",
    "    p_pred = K.sum(K.pow(10.0, y_pred/10))\n",
    "    return K.mean(K.abs(10 * log10(p_true) - 10 * log10(p_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN + NN(multi-SUs): support batching\n",
    "TEST, CONSERVE = True, False\n",
    "mini_batch = 64 if max(max_x, max_y) == 1000 else 16\n",
    "epochs = 35 if max(max_x, max_y) == 1000 else 300\n",
    "MAX_QUEUE_SIZE, WORKERS = 28, 4\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "hyper_metric, mode = \"val_cus_mae\", 'min'  # the metric that hyper parameters are tuned with\n",
    "lambda_vec = [0, 0.01, 0.1, 1, 10] \n",
    "prev_sample = 0\n",
    "average_diff_power, fp_mean_power = [],[] #[7.177, 8.088, 8.183], [3.438, 3.506, 2.662]\n",
    "best_lambda = []\n",
    "average_diff_power_conserve, fp_mean_power_conserve = [], []\n",
    "total_power, max_min_ratio = [], []\n",
    "number_samples = [128, 256, 512, 1024, 2048, 4096]\n",
    "\n",
    "for num_sample_idx, number_sample in enumerate(number_samples):\n",
    "    MODEL_PATH = f\"{dataset_path}/{number_sample}/models\"\n",
    "    X = np.loadtxt(f\"{dataset_path}/{number_sample}/X.txt\", delimiter=\",\")\n",
    "    y = np.loadtxt(f\"{dataset_path}/{number_sample}/y.txt\", delimiter=\",\")\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "    MODEL_PATH += \"/best_model_lambda_\"\n",
    "    if True:\n",
    "        nns = [nn_model(max_sus_num * 3, max_sus_num, kernel_lam=lamb, bias_lam=0,\n",
    "                        num_hidden_layers=4, num_neurons=200)\n",
    "               for lamb in lambda_vec]\n",
    "        for nn in nns:\n",
    "#             cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae', fp_mean])\n",
    "            # =optimizers.SGD(lr=0.1, momentum=0.9, decay=0.1/epochs, nesterov=False)\n",
    "            nn.compile(loss=\"mse\", \n",
    "                        optimizer=optimizers.Adam(), \n",
    "                        metrics=['mse', 'mae', cus_mae])\n",
    "        checkpointers = [ModelCheckpoint(filepath=MODEL_PATH + str(lamb)+ '.h5',\n",
    "                                         verbose=0, save_best_only=True, \n",
    "                                         monitor=hyper_metric,\n",
    "                                         mode=mode)\n",
    "                         for lamb in lambda_vec]\n",
    "    else:\n",
    "        cnns = []\n",
    "        cnns = [models.load_model(MODEL_PATH + str(lamb) + '.h5', \n",
    "                                  custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                  'fp_mae': fp_mae }) \n",
    "                for lamb in lambda_vec]\n",
    "    number_start = time.time()\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "  \n",
    "    print('number_samples:', number_sample, \", New samples:\", number_sample - prev_sample)\n",
    "    print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", number_sample + val_size - 1)\n",
    "    \n",
    "    for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "#     for lamb_idx, lamb in enumerate(lambda_vec[:len(lambda_vec) - num_sample_idx//2]):\n",
    "#         if num_sample_idx == 3 and lamb_idx < 4:\n",
    "#             continue\n",
    "        lambda_start = time.time()\n",
    "        nns[lamb_idx].fit(x=X[:number_sample+val_size,:],\n",
    "                           y=y[:number_sample+val_size,:],\n",
    "                           epochs=epochs, verbose=0,\n",
    "                           validation_split=validation_size/(1 + validation_size), \n",
    "                           shuffle=True, callbacks=[checkpointers[lamb_idx]], \n",
    "                           workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                           use_multiprocessing=False)\n",
    "        \n",
    "        print(\"\\nLambda:\", lamb, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - lambda_start))))\n",
    "        print(\"Train Error(all epochs):\", min(nns[lamb_idx].history.history['mae']), '\\n', \n",
    "              [round(val, 3) for val in nns[lamb_idx].history.history['mae']])\n",
    "#         print(\"Train FP Error(all epochs):\", min(cnns[lamb_idx].history.history['fp_mae']), '\\n',\n",
    "#               [round(val,3) for val in cnns[lamb_idx].history.history['fp_mae']])\n",
    "        print(\"Val Error(all epochs):\", min(nns[lamb_idx].history.history['val_mae']), '\\n', \n",
    "              [round(val,3) for val in nns[lamb_idx].history.history['val_mae']])\n",
    "        print(\"Val custom mae Error(all epochs):\", min(nns[lamb_idx].history.history['val_cus_mae']), '\\n',\n",
    "              [round(val,3) for val in nns[lamb_idx].history.history['val_cus_mae']])\n",
    "    \n",
    "    \n",
    "    models_min_mae = [min(nns[lam_idx].history.history[hyper_metric]) for\n",
    "                      lam_idx,_ in enumerate(lambda_vec)]\n",
    "    best_lamb_idx = models_min_mae.index(min(models_min_mae))\n",
    "    best_lambda.append(lambda_vec[best_lamb_idx])\n",
    "    print(\"\\nTrainig set size:\", number_sample, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - \n",
    "                                                                                              number_start))),\n",
    "          \", best_lambda:\", lambda_vec[best_lamb_idx], \", min_\" , (\"fp_\" if hyper_metric == \"val_fp_mae\" else \"\"),\n",
    "          \"error:\", round(min(models_min_mae), 3))\n",
    "    all_cnns.append(nns)\n",
    "    del nns\n",
    "    \n",
    "    if TEST:\n",
    "        # evaluating test images\n",
    "        best_model = None\n",
    "        best_model = models.load_model(MODEL_PATH + str(lambda_vec[best_lamb_idx]) + '.h5', \n",
    "                                       custom_objects={'mae': 'mae', 'mse': 'mse', 'cus_mae': cus_mae})\n",
    "#                                        custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "#                                                        'fp_mae': fp_mae,\n",
    "#                                                       'mae':'mae', 'mse':'mse'})\n",
    "\n",
    "        print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "        time.sleep(1)\n",
    "#         test_res = best_model.evaluate(test_generator, verbose=1, \n",
    "#                                        workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)\n",
    "        y_hat = best_model.predict(X[number_sample + val_size:, :], verbose=1, \n",
    "                                       workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)\n",
    "        total_power.append(np.mean(10 * np.log10(np.sum(10**(y_hat/10), axis=1))))\n",
    "        max_min_ratio.append(np.mean(np.max(y_hat, axis=1) - np.min(y_hat, axis=1)))\n",
    "        average_diff_power.append(np.mean(np.abs(\n",
    "            10 * np.log10(np.sum(10 ** (y_hat/10), axis=1)) -\n",
    "            10 * np.log10(np.sum(10 ** (y[number_sample + val_size:]/10), axis=1)))))\n",
    "        \n",
    "#         test_mae_idx, test_fp_mae_idx = [best_model.metrics_names.index(mtrc) \n",
    "#                                          for mtrc in ['mae','fp_mae']]\n",
    "#         test_mae, test_fp_mae = test_res[test_mae_idx], test_res[test_fp_mae_idx]\n",
    "#         average_diff_power.append(round(test_mae, 3))\n",
    "#         fp_mean_power.append(round(test_fp_mae, 3))\n",
    "        print('total_power: ', total_power[-1], ', average_difference: ', average_diff_power[-1],\n",
    "              'max_min_ratio:', max_min_ratio[-1])\n",
    "        \n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        \n",
    "#         var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/models/' + model_name + \"/\"\n",
    "#                      + intensity_degradation + '_' + str(slope) + '_' + \n",
    "#                      dtime + \".dat\", \"wb\") # file for saving results\n",
    "#         pickle.dump([average_diff_power, fp_mean_power, number_samples, best_lambda, \n",
    "#                      dataset_name, max_dataset_name, average_diff_power_conserve, fp_mean_power_conserve,\n",
    "#                      checkpointers],\n",
    "#                     file=var_f)\n",
    "#         var_f.close()\n",
    "        del best_model\n",
    "#     prev_sample = number_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"average_diff_power:\", [round(x, 3) for x in average_diff_power])\n",
    "print(\"total_power:\", [round(x, 3) for x in total_power])\n",
    "print(\"max_min_ratio:\", [round(x, 3) for x in max_min_ratio])\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-SU using single deep-alloc and Bi-LSTM\n",
    "# DataBatchGenerator\n",
    "\n",
    "class LSTMBatchGenerator(Sequence):\n",
    "    def __init__(self, dataset, batch_size: int, start_idx: int, float_memory_used,\n",
    "                 image_dir: str, cnn_feature_model, num_sus, cnn_output_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size, self.start_idx = batch_size, start_idx\n",
    "#         self.number_image_channels, self.max_x, self.max_y = number_image_channels, max_x, max_y\n",
    "        self.float_memory_used = float_memory_used\n",
    "        self.image_dir = image_dir\n",
    "        self.cnn_feature_model = cnn_feature_model\n",
    "        self.num_sus = num_sus\n",
    "        self.cnn_output_size = cnn_output_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.ceil(self.dataset.shape[0] / self.batch_size).astype(int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        size = min(self.dataset.shape[0] - idx * self.batch_size, self.batch_size)\n",
    "        batch_x = np.empty((size, self.num_sus, self.cnn_output_size),\n",
    "                           dtype=self.float_memory_used)\n",
    "        batch_y = np.copy(self.dataset[idx * self.batch_size: idx * self.batch_size + size, 3::3])\n",
    "        batch_x2 = np.copy(batch_y)\n",
    "        batch_x2[:, 1:] = batch_x2[:, :-1]\n",
    "        batch_x2[:,0] = 0\n",
    "        for i in range(size):\n",
    "            for su_idx in range(self.num_sus):\n",
    "                imm = read_image((self.start_idx + idx * self.batch_size + i) * self.num_sus + su_idx,\n",
    "                                 self.image_dir)\n",
    "                batch_x[i][su_idx] = self.cnn_feature_model.predict(imm)[0]\n",
    "        return ([batch_x, batch_x2.reshape(size, self.num_sus, 1)], batch_y.reshape(size, self.num_sus, 1))\n",
    "\n",
    "def create_bilstm_model(units, num_sus, cnn_output_size):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Bidirectional(layers.LSTM(units, return_sequences=True), \n",
    "                                   input_shape=(num_sus, cnn_output_size)))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(units)))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    return model\n",
    "\n",
    "def create_rnn_encoder_decoder(units, num_sus, cnn_output_size):\n",
    "    # define bi-lstm encoder\n",
    "    encoder_inputs = Input(shape=(None, cnn_output_size))\n",
    "    encoder = layers.Bidirectional(layers.LSTM(units, return_state=True))\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(encoder_inputs)\n",
    "    state_h = layers.Concatenate()([forward_h, backward_h])\n",
    "    state_c = layers.Concatenate()([forward_c, backward_c])\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # define training decoder\n",
    "    decoder_inputs = Input(shape=(None, 1))\n",
    "    decoder_lstm = layers.LSTM(units * 2, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    decoder_dense = layers.Dense(1, activation='linear')\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_inference_model(model, units):\n",
    "    #inference encoder\n",
    "    encoder_inputs = model.input[0] #input_1\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = model.layers[1].output # lstm_1\n",
    "    state_h = layers.Concatenate()([forward_h, backward_h])\n",
    "    state_c = layers.Concatenate()([forward_c, backward_c])\n",
    "    encoder_states = [state_h, state_c]\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    #inference decoder\n",
    "    decoder_inputs = model.input[1] #input_2\n",
    "    decoder_state_input_h = Input(shape=(units * 2,),name='input_3')\n",
    "    decoder_state_input_c = Input(shape=(units * 2,),name='input_4')\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_lstm = model.layers[5]\n",
    "    decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h_dec, state_c_dec]\n",
    "    decoder_dense = model.layers[6]\n",
    "    decoder_outputs=decoder_dense(decoder_outputs)\n",
    "\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                          [decoder_outputs] + decoder_states)\n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "def predict_su_adjusted_power(inf_enc, inf_dec, num_sus, data):\n",
    "    # encode\n",
    "    state = inf_enc.predict(data)\n",
    "    # start of sequence input\n",
    "    target_seq = np.array([0.0]).reshape(1, 1, 1)\n",
    "    # collect predictions\n",
    "    output = list()\n",
    "    for _ in range(num_sus):\n",
    "        # predict next char\n",
    "        yhat, h, c = inf_dec.predict([target_seq] + state)\n",
    "        # store prediction\n",
    "        output.append(yhat[0,0,:])\n",
    "        # update state\n",
    "        state = [h, c]\n",
    "        # update target sequence\n",
    "        target_seq = yhat\n",
    "    return np.array(output)\n",
    "\n",
    "model_name = \"log_vgg16_max_su_total\"\n",
    "max_su_image_dir = '/'.join(image_dir.split('/')[:-1]) + \"/\" + model_name + \"/images\"\n",
    "number_samples = [2048, 4096]\n",
    "hyper_metric, mode = \"val_cus_mae\", 'min'\n",
    "MAX_QUEUE_SIZE, WORKERS = 28, 4\n",
    "lstm_units = 128\n",
    "mini_batch, epochs = 32, 100\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "dataset_path = '/'.join(image_dir.split('/')[:-1]) + \"/\" + model_name\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "# total_power, avg_diff_power, max_min_ratio = [], [], []\n",
    "cnn_output_size = 512\n",
    "dataset_path = '/'.join(image_dir.split('/')[:-1]) + \"/\" + model_name\n",
    "\n",
    "\n",
    "model_path = \"/home/shahrokh/projects/MLSpectrumAllocation/ML/data/pictures_299_299_transfer/\" + \\\n",
    "             \"splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_pu5_su5/\" + \\\n",
    "             \"pus_1_sus_3_channels/models/log_vgg16/\"\n",
    "MODEL_PATH = f\"{dataset_path}/bilstm/{lstm_units}_units\"\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "for number_sample in number_samples:\n",
    "    cnn_model = models.load_model(f\"{model_path}/{number_sample}/best_model_lambda_0.h5\",\n",
    "                                  custom_objects={ \n",
    "                                      'loss': custom_loss(fp_penalty_coef, fn_penalty_coef),\n",
    "                                      'fp_mae': fp_mae,\n",
    "                                      'mae':'mae', 'mse':'mse'})\n",
    "    feature_output = Model(inputs=cnn_model.layers[0].input, outputs=cnn_model.layers[2].output)\n",
    "    time_start = time.time()\n",
    "    train_generator = LSTMBatchGenerator(\n",
    "        dataset=data_max_su_tot[:number_sample],\n",
    "        batch_size=mini_batch,\n",
    "        start_idx=prev_sample, \n",
    "        float_memory_used=float_memory_used,\n",
    "        cnn_feature_model=feature_output,\n",
    "        num_sus=max_sus_num,\n",
    "        cnn_output_size=cnn_output_size,\n",
    "        image_dir=max_su_image_dir)\n",
    "    \n",
    "\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "#     val_size = data_reg.shape[0] - number_sample\n",
    "    val_generator = LSTMBatchGenerator(\n",
    "        dataset=data_max_su_tot[number_sample:number_sample+val_size],\n",
    "        batch_size=mini_batch,\n",
    "        start_idx=number_sample, \n",
    "        float_memory_used=float_memory_used,\n",
    "        cnn_feature_model=feature_output,\n",
    "        num_sus=max_sus_num,\n",
    "        cnn_output_size=cnn_output_size,\n",
    "        image_dir=max_su_image_dir)\n",
    "  \n",
    "    print('number_samples:', number_sample, \", New samples:\", number_sample - prev_sample)\n",
    "    print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", number_sample + val_size - 1)\n",
    "#     bilstm_model = create_bilstm_model(128, max_sus_num, cnn_output_size)\n",
    "    train_model = create_rnn_encoder_decoder(units=lstm_units, num_sus=max_sus_num,\n",
    "                                             cnn_output_size=cnn_output_size)\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath=MODEL_PATH + f\"/best_model_{number_sample}.h5\",\n",
    "                                   verbose=1, save_best_only=True, \n",
    "                                   monitor=hyper_metric,\n",
    "                                   mode=mode)\n",
    "    train_model.compile(loss=\"mse\", \n",
    "                        optimizer=optimizers.Adam(), \n",
    "                        metrics=['mse', 'mae', cus_mae])\n",
    "    train_model.fit(train_generator, epochs=epochs, verbose=2,\n",
    "                     validation_data=val_generator, \n",
    "                     shuffle=True, callbacks=[checkpointer], \n",
    "                     workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                     use_multiprocessing=False)\n",
    "    # re-loading best model\n",
    "    best_lstm_mode = models.load_model(MODEL_PATH + f\"/best_model_{number_sample}.h5\",\n",
    "                                       custom_objects={'mse':'mse', 'mae': 'mae',\n",
    "                                                       'cus_mae': cus_mae})\n",
    "    inf_enc, inf_dec = create_inference_model(model=best_lstm_mode, units=lstm_units)\n",
    "    \n",
    "    # predicting test-sample one-by-one\n",
    "    test_generator = LSTMBatchGenerator(\n",
    "        dataset=data_max_su_tot[number_sample+val_size:],\n",
    "        batch_size=1,\n",
    "        start_idx=number_sample+val_size, \n",
    "        float_memory_used=float_memory_used,\n",
    "        cnn_feature_model=feature_output,\n",
    "        num_sus=max_sus_num,\n",
    "        cnn_output_size=cnn_output_size,\n",
    "        image_dir=max_su_image_dir)\n",
    "    tot_power_tmp, max_min_ratio_tmp, avg_diff_power_tmp = [], [], []\n",
    "    \n",
    "    for test_idx in range(number_sample+val_size, data_max_su_tot.shape[0]):\n",
    "        X, y = test_generator.__getitem__(test_idx - (number_sample+val_size))\n",
    "        y_hat = predict_su_adjusted_power(inf_enc, inf_dec, max_sus_num, X[0])\n",
    "        \n",
    "        y_hat_tot_power = 10 * np.log10((10 ** (y_hat/10)).sum())\n",
    "        y_tot_power = 10 * np.log10((10 ** (y/10)).sum())\n",
    "        \n",
    "        tot_power_tmp.append(y_hat_tot_power)\n",
    "        max_min_ratio_tmp.append(y_hat.max() - y_hat.min())\n",
    "        avg_diff_power_tmp.append(abs(y_hat_tot_power - y_tot_power))\n",
    "    \n",
    "    total_power.append(sum(tot_power_tmp) / len(tot_power_tmp))\n",
    "    avg_diff_power.append(sum(avg_diff_power_tmp) / len(avg_diff_power_tmp))\n",
    "    max_min_ratio.append(sum(max_min_ratio_tmp) / len(max_min_ratio_tmp))\n",
    "    \n",
    "    print(f\"number_sample: {number_sample}, total_power: {total_power[-1]}, avg_diff_power: {avg_diff_power[-1]}\"\n",
    "          f\"max_min_ratio: {max_min_ratio[-1]}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilstm_inference(inf_enc, inf_dec, test_generator, start_idx, end_idx, results):\n",
    "    inf_enc.summary()\n",
    "    tot_power_tmp, max_min_ratio_tmp, avg_diff_tmp = [], [], []\n",
    "    for test_idx in tqdm.tqdm(range(start_idx, end_idx)):\n",
    "        X, y = test_generator.__getitem__(test_idx)\n",
    "        y_hat = predict_su_adjusted_power(inf_enc, inf_dec, max_sus_num, X[0])\n",
    "        \n",
    "        y_hat_tot_power = 10 * np.log10((10 ** (y_hat/10)).sum())\n",
    "        y_tot_power = 10 * np.log10((10 ** (y/10)).sum())\n",
    "        \n",
    "        tot_power_tmp.append(y_hat_tot_power)\n",
    "        max_min_ratio_tmp.append(y_hat.max() - y_hat.min())\n",
    "        avg_diff_power_tmp.append(abs(y_hat_tot_power - y_tot_power))\n",
    "    results.append((tot_power_tmp, max_min_ratio_tmp, avg_diff_tmp))\n",
    "jobs = []\n",
    "test_size = data_max_su_tot.shape[0] - 25905 # number_sample - val_size\n",
    "proc_sizes = [test_size//number_of_proccessors] * (number_of_proccessors)\n",
    "proc_sizes[-1] += test_size % number_of_proccessors\n",
    "proc_idx = [(sum(proc_sizes[:i])+ 25905 - (number_sample+val_size), \n",
    "             sum(proc_sizes[:i+1]) - (number_sample+val_size)+ 25905 -1) for i in range(number_of_proccessors)]\n",
    "results = []\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    p = Process(target=bilstm_inference, \n",
    "                args=(inf_enc, inf_dec, test_generator, proc_idx[i][0], proc_idx[i][1], results))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].join()\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].terminate()\n",
    "    jobs[i].close()\n",
    "del jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=model.layers[0].input, outputs=model.layers[2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_calc(data_arr):\n",
    "    min_data, tot_power = [], []\n",
    "    for data in data_arr:\n",
    "        min_arr = [(sorted(val, key=lambda x:x[1])[-1][1] - sorted(val, key=lambda x:x[1])[0][1]) for val in data]\n",
    "        min_data.append(sum(min_arr)/len(min_arr))\n",
    "        sum_arr = [dec_to_db(sum([db_to_dec(val[1]) for val in fair_res_sng]))for fair_res_sng in data]\n",
    "        tot_power.append(sum(sum_arr)/len(sum_arr))\n",
    "    return [round(min_, 2) for min_ in min_data], [round(tot,2) for tot in tot_power]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MULTIPLE - SUS\n",
    "def db_to_dec(db: float):\n",
    "    return 10 ** (db/10)\n",
    "def dec_to_db(dec: float):\n",
    "    return -float('inf') if dec <= 0 else 10 * math.log10(dec)\n",
    "\n",
    "def greedy_sus(pus, requesting_sus, model, pl_info, number_channel = 1):\n",
    "    # pus: (x, y, p), requesting_sus: (x, y)\n",
    "    def best_su_candidate(pus, active_sus, requesting_sus):\n",
    "        if len(requesting_sus) == 1:\n",
    "            return requesting_sus[0][0]\n",
    "        # active_sus: (x, y, allocated_power), requesting_sus: (req_id, x, y)\n",
    "        power_map_cell_size, area_size = 50, 1000\n",
    "        cell_weight, neighbour_weight = 0.5, 0.5\n",
    "        power_map = [[0] * int(area_size // power_map_cell_size) for _ in range(int(area_size // power_map_cell_size))]\n",
    "        \n",
    "        for pu_num in range(int(len(pus)//3)):\n",
    "            x, y, dec_p = pus[pu_num*3], pus[pu_num * 3 + 1], db_to_dec(pus[pu_num*3 + 2])\n",
    "            power_map[int(x * cell_size // power_map_cell_size)][int(y * cell_size // power_map_cell_size)] += dec_p\n",
    "        for su in active_sus:\n",
    "            x, y, dec_p = su[0], su[1], db_to_dec(su[2])\n",
    "            power_map[int(x * cell_size // power_map_cell_size)][int(y * cell_size // power_map_cell_size)] += dec_p\n",
    "        \n",
    "        power_score = [[0] * len(power_map[0]) for _ in range(len(power_map))]\n",
    "        # updating power score\n",
    "        for i in range(len(power_map)):\n",
    "            for j in range(len(power_map[0])):\n",
    "                power_score[i][j] = cell_weight * power_map[i][j] + neighbour_weight * sum(\n",
    "                [power_map[ii][jj] for ii in range(max(0, i - 1), min(i + 1, len(power_map))) \n",
    "                 for jj in range(max(0, j - 1), min(j + 1, len(power_map[0]))) if not (i == ii and j == jj)])\n",
    "        # find su with lowest weight\n",
    "        best_su_req, min_score = -1, float('inf')\n",
    "        for req_id, x, y in requesting_sus:\n",
    "            if power_score[int(x * cell_size // power_map_cell_size)][int(x * cell_size // power_map_cell_size)] < min_score:\n",
    "                min_score = power_score[int(x * cell_size // power_map_cell_size)][int(x * cell_size // power_map_cell_size)]\n",
    "                best_su_req = req_id\n",
    "        return best_su_req         \n",
    "    active_sus = [{} for _ in range(number_channel)] # (request_id: power_allocated)\n",
    "    assigned_req = set()\n",
    "    for _ in range(len(requesting_sus)):\n",
    "        max_allocated_power, best_channel, best_req_id = -float('inf'), -1, -1\n",
    "        for ch in range(number_channel):\n",
    "            nex_req_id = best_su_candidate(pus, [(*requesting_sus[i], active_sus[ch][i]) for i in active_sus[ch]],\n",
    "                                          [(idd, *requesting_sus[idd]) for idd in range(len(requesting_sus))\n",
    "                                           if idd not in assigned_req])\n",
    "            su_lst = []\n",
    "            for i in active_sus[ch]:\n",
    "                su_lst += [*requesting_sus[i]]\n",
    "                su_lst.append(active_sus[ch][i])\n",
    "            requesting_data = [len(pus)//3] + pus + [len(active_sus[ch]) + 1] + su_lst + requesting_sus[nex_req_id]\n",
    "            requesting_image = np.expand_dims(create_image(requesting_data, slope=slope, style=style, \n",
    "                                                           noise_floor=noise_floor, pu_shape=pu_shape, su_shape=su_shape,\n",
    "                                                           su_param=su_param, sensors_num=(sensors_num if sensors else 0), \n",
    "                                                           intensity_degradation=intensity_degradation, \n",
    "                                                           max_pu_power=0.0 if not sensors else -30,\n",
    "                                                           max_su_power=0.0), 0)\n",
    "            predicted_power = model.predict(requesting_image)\n",
    "            if predicted_power[0][0] > max_allocated_power:\n",
    "                max_allocated_power = predicted_power[0][0]\n",
    "                best_channel, best_req_id = ch, nex_req_id\n",
    "        active_sus[best_channel][best_req_id] = max_allocated_power\n",
    "        assigned_req.add(best_req_id)\n",
    "    res = []\n",
    "    for ch in range(number_channel):\n",
    "        for req_id in active_sus[ch]:\n",
    "            res.append((req_id, active_sus[ch][req_id], ch))\n",
    "    return res\n",
    "\n",
    "def fair_sus(pus, requesting_sus, model, pl_info, number_channel = 1):\n",
    "    # pus: (x, y, p), requesting_sus: (x, y)\n",
    "    def best_su_candidate(pus, active_sus, requesting_sus):\n",
    "        if len(requesting_sus) == 1:\n",
    "            return requesting_sus[0][0]\n",
    "        # active_sus: (x, y, allocated_power), requesting_sus: (req_id, x, y)\n",
    "        power_map_cell_size, area_size = 50, 1000\n",
    "        cell_weight, neighbour_weight = 0.5, 0.5\n",
    "        power_map = [[0] * int(area_size // power_map_cell_size) for _ in range(int(area_size // power_map_cell_size))]\n",
    "        \n",
    "        for pu_num in range(int(len(pus)//3)):\n",
    "            x, y, dec_p = pus[pu_num*3], pus[pu_num * 3 + 1], db_to_dec(pus[pu_num*3 + 2])\n",
    "            power_map[int(x * cell_size // power_map_cell_size)][int(y * cell_size // power_map_cell_size)] += dec_p\n",
    "        for su in active_sus:\n",
    "            x, y, dec_p = su[0], su[1], db_to_dec(su[2])\n",
    "            power_map[int(x * cell_size // power_map_cell_size)][int(y * cell_size // power_map_cell_size)] += dec_p\n",
    "        \n",
    "        power_score = [[0] * len(power_map[0]) for _ in range(len(power_map))]\n",
    "        # updating power score\n",
    "        for i in range(len(power_map)):\n",
    "            for j in range(len(power_map[0])):\n",
    "                power_score[i][j] = cell_weight * power_map[i][j] + neighbour_weight * sum(\n",
    "                [power_map[ii][jj] for ii in range(max(0, i - 1), min(i + 1, len(power_map))) \n",
    "                 for jj in range(max(0, j - 1), min(j + 1, len(power_map[0]))) if not (i == ii and j == jj)])\n",
    "        # find su with lowest weight\n",
    "        best_su_req, min_score = -1, float('inf')\n",
    "        for req_id, x, y in requesting_sus:\n",
    "            if power_score[int(x * cell_size // power_map_cell_size)][int(x * cell_size // power_map_cell_size)] < min_score:\n",
    "                min_score = power_score[int(x * cell_size // power_map_cell_size)][int(x * cell_size // power_map_cell_size)]\n",
    "                best_su_req = req_id\n",
    "        return best_su_req         \n",
    "    active_sus = [{} for _ in range(number_channel)] # (request_id: power_allocated)\n",
    "    assigned_req = set()\n",
    "    for _ in range(len(requesting_sus)):\n",
    "        max_allocated_power, best_channel, best_req_id = -float('inf'), -1, -1\n",
    "        for ch in range(number_channel):\n",
    "            nex_req_id = best_su_candidate(pus, [(*requesting_sus[i], active_sus[ch][i]) for i in active_sus[ch]],\n",
    "                                          [(idd, *requesting_sus[idd]) for idd in range(len(requesting_sus))\n",
    "                                           if idd not in assigned_req])\n",
    "            su_lst = []\n",
    "            for i in active_sus[ch]:\n",
    "                su_lst += [*requesting_sus[i]]\n",
    "                su_lst.append(active_sus[ch][i])\n",
    "            requesting_data = [len(pus)//3] + pus + [len(active_sus[ch]) + 1] + su_lst + requesting_sus[nex_req_id]\n",
    "            requesting_image = np.expand_dims(create_image(requesting_data, slope=slope, style=style, \n",
    "                                                           noise_floor=noise_floor, pu_shape=pu_shape, su_shape=su_shape,\n",
    "                                                           su_param=su_param, sensors_num=(sensors_num if sensors else 0), \n",
    "                                                           intensity_degradation=intensity_degradation, \n",
    "                                                           max_pu_power=0.0 if not sensors else -30,\n",
    "                                                           max_su_power=0.0), 0)\n",
    "            predicted_power = model.predict(requesting_image)\n",
    "            if predicted_power[0][0] > max_allocated_power:\n",
    "                max_allocated_power = predicted_power[0][0]\n",
    "                best_channel, best_req_id = ch, nex_req_id\n",
    "        if max_allocated_power < -20:\n",
    "            # it's less than threshold. sort active su w.r.t to best_req_id and try to decrease their power\n",
    "            def dist(p1, p2):\n",
    "                return ((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2) ** 0.5\n",
    "            dist_info = []\n",
    "            for ch in range(number_channel):\n",
    "                for i in active_sus[ch]:\n",
    "                    dist_info.append((ch, i, dist(requesting_sus[i], requesting_sus[best_req_id])))\n",
    "            dist_info.sort(key=lambda x:x[2])\n",
    "            SATISFIED = False\n",
    "            for i in range(len(dist_info)):\n",
    "                candid_ch, candid_su_id  = dist_info[i][0], dist_info[i][1]\n",
    "                candid_old_pow = active_sus[candid_ch][candid_su_id]\n",
    "                if  candid_old_pow > -10:\n",
    "                    candid_new_pow = candid_old_pow\n",
    "                    while candid_new_pow - 5 > -20.0:\n",
    "                        candid_new_pow -= 5\n",
    "                        # try this new power\n",
    "                        su_lst = []\n",
    "                        for ii in active_sus[candid_ch]:\n",
    "                            su_lst += [*requesting_sus[ii]]\n",
    "                            if ii == candid_su_id:\n",
    "                                su_lst.append(candid_new_pow)\n",
    "                            else:\n",
    "                                su_lst.append(active_sus[candid_ch][ii])\n",
    "                        requesting_data = [len(pus)//3] + pus + [len(active_sus[candid_ch]) + 1] + su_lst + requesting_sus[best_req_id]\n",
    "                        requesting_image = np.expand_dims(create_image(requesting_data, slope=slope, style=style, \n",
    "                                                                       noise_floor=noise_floor, pu_shape=pu_shape, su_shape=su_shape,\n",
    "                                                                       su_param=su_param, sensors_num=(sensors_num if sensors else 0), \n",
    "                                                                       intensity_degradation=intensity_degradation, \n",
    "                                                                       max_pu_power=0.0 if not sensors else -30,\n",
    "                                                                       max_su_power=0.0), 0)\n",
    "                        predicted_power = model.predict(requesting_image)[0][0]\n",
    "                        if predicted_power > -20.0:\n",
    "                            SATISFIED = True\n",
    "                            break\n",
    "                    if SATISFIED:\n",
    "                        break\n",
    "            if SATISFIED:\n",
    "                best_channel, max_allocated_power = candid_ch, predicted_power\n",
    "                active_sus[candid_ch][candid_su_id] = candid_new_pow  #update the candid su\n",
    "                    \n",
    "        active_sus[best_channel][best_req_id] = max_allocated_power\n",
    "        assigned_req.add(best_req_id)\n",
    "    res = []\n",
    "    for ch in range(number_channel):\n",
    "        for req_id in active_sus[ch]:\n",
    "            res.append((req_id, active_sus[ch][req_id], ch))\n",
    "    return res\n",
    "\n",
    "def random_sus(pus, requesting_sus, model, pl_info, number_channel = 1):\n",
    "    active_sus = [{}  for _ in range(number_channel)] # (request_id: power_allocated)\n",
    "    assigned_req = set()\n",
    "    for nex_req_id in range(len(requesting_sus)):\n",
    "        max_allocated_power, best_channel, best_req_id = -float('inf'), -1, -1\n",
    "        for ch in range(number_channel):\n",
    "            su_lst = []\n",
    "            for i in active_sus[ch]:\n",
    "                su_lst += [*requesting_sus[i]]\n",
    "                su_lst.append(active_sus[ch][i])\n",
    "            requesting_data = [len(pus)//3] + pus + [len(active_sus[ch]) + 1] + su_lst + requesting_sus[nex_req_id]\n",
    "            requesting_image = np.expand_dims(create_image(requesting_data, slope=slope, style=style, \n",
    "                                                           noise_floor=noise_floor, pu_shape=pu_shape, su_shape=su_shape,\n",
    "                                                           su_param=su_param, sensors_num=(sensors_num if sensors else 0), \n",
    "                                                           intensity_degradation=intensity_degradation, \n",
    "                                                           max_pu_power=0.0 if not sensors else -30,\n",
    "                                                           max_su_power=0.0), 0)\n",
    "            predicted_power = model.predict(requesting_image)\n",
    "            if predicted_power[0][0] > max_allocated_power:\n",
    "                max_allocated_power = predicted_power[0][0]\n",
    "                best_channel, best_req_id = ch, nex_req_id\n",
    "        active_sus[best_channel][best_req_id] = max_allocated_power\n",
    "        assigned_req.add(best_req_id)\n",
    "    res = []\n",
    "    for ch in range(number_channel):\n",
    "        for req_id in active_sus[ch]:\n",
    "            res.append((req_id, active_sus[ch][req_id], ch))\n",
    "    return res\n",
    "    \n",
    "def multiple_sus(data_reg, train_set_size, pl_info, model_path, number_channel):\n",
    "    random_res, greedy_res = [[] for _ in range(len(train_set_size))], [[] for _ in range(len(train_set_size))]\n",
    "    fair_res = [[] for _ in range(len(train_set_size))]\n",
    "    for ind, train_size in enumerate(train_set_size):\n",
    "        print(f\"Train size: {train_size}\")\n",
    "        model = models.load_model(f\"{model_path}/{train_size}/best_model_lambda_0.1.h5\",\n",
    "                                 custom_objects={ 'loss': custom_loss(1, 1), \n",
    "                                               'fp_mae': fp_mae,\n",
    "                                               'mae':'mae', 'mse':'mse'})\n",
    "        for i in tqdm.tqdm(range(len(data_reg))):\n",
    "            pu_num = int(data_reg[i][0])\n",
    "            pus = np.ndarray.tolist(data_reg[i][1:1 + pu_num * 3])\n",
    "            su_num = int(data_reg[i][1 + pu_num * 3])\n",
    "            if su_num < 4:\n",
    "                continue\n",
    "            sus = []\n",
    "            for su_ind in range(su_num):\n",
    "                sus.append(np.ndarray.tolist(data_reg[i][2 + pu_num * 3 + su_ind * 3:4 + pu_num * 3 + su_ind * 3]))\n",
    "\n",
    "            fair_res[ind].append(fair_sus(pus, sus, model, pl_info, number_channel))\n",
    "            random_res[ind].append(random_sus(pus, sus, model, pl_info, number_channel))\n",
    "            greedy_res[ind].append(greedy_sus(pus, sus, model, pl_info, number_channel))\n",
    "            var_f = open('/'.join(image_dir.split('/')[:-1]) + \"/multi_res_num_channel\"\n",
    "                     + str(number_channel) + \"_\" + intensity_degradation + '_' + str(slope) + '_' + \n",
    "                     dtime + \".dat\", \"wb\") # file for saving results\n",
    "            pickle.dump([random_res, greedy_res, fair_res],  file=var_f)\n",
    "            var_f.close()\n",
    "            \n",
    "#     return random_sum_power, random_max_min_ratio, greedy_sum_power, greedy_max_min_ratio\n",
    "    return random_res, greedy_res, fair_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rand_res, greedy_res, fair_res = multiple_sus(data_reg[:600], number_samples, None,\n",
    "                                              \"ML/data/pictures_299_299_transfer/splat/pu_circle_su_circle_60/raw_power_min_max_norm/color/log_5/pus_5_sus_3_channels/models/log_vgg16\",\n",
    "                                              4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
