{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datetime, time\n",
    "import os, sys\n",
    "import tqdm\n",
    "import gc\n",
    "import random\n",
    "from multiprocessing import Process\n",
    "Point = namedtuple('Point', ('x', 'y'))\n",
    "Circle = namedtuple('Circle', ('r'))\n",
    "Square = namedtuple('Square', ('side'))\n",
    "Rectangle = namedtuple('Rectangle', ('length', 'width'))\n",
    "PointWithDistance = namedtuple('PointWithDistance', ('p', 'dist'))\n",
    "float_memory_used = 'float16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT\n",
    "# PART 1\n",
    "# number_samples = [5] + list(range(10, 101, 10)) + [120, 150, 200, 250, 300, 400, 500, 700] + list(range(1000, 10001, 1000))\n",
    "number_samples = [120, 220] \n",
    "# number_samples = [210]\n",
    "# number_samples = [4096, 4915, 5734, 6554, 7373, 8192]\n",
    "\n",
    "# cnn_type = \"classification\"  # {\"classification\", \"regression\"}\n",
    "validation_size, noise_floor = 0.1, -90.0#-110.0\n",
    "su_power = 0 # this is not actually su power just a number to show there is an SU in its image\n",
    "max_x, max_y, number_image_channels, su_szie = 100, 100, 6, 10  # su_size:30 for 1000, 10 for 100\n",
    "cell_size = int(max(max_x, max_y)/10)\n",
    "pu_shape, su_shape = 'circle', 'circle' # shape = {'circle', 'square', 'point'}\n",
    "style = \"raw_power_min_max_norm\"  # {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "intensity_degradation, slope = 'log', 5  # 'log', 'linear', slope 3 for 1000, 5 for 100\n",
    "max_pus_num, max_sus_num = 4, 1\n",
    "propagation_model = 'testbed' # 'splat', 'log', 'testbed'\n",
    "noise, std = False, 1 # False for splat\n",
    "K_SIZE = 10 # K-fold\n",
    "if su_shape == 'circle':\n",
    "    su_param = Circle(su_szie)\n",
    "elif su_shape == 'square':\n",
    "    su_param = Square(su_szie)\n",
    "else:\n",
    "    su_param = None\n",
    "    \n",
    "sensors = True\n",
    "if sensors:\n",
    "    sensors_num, sensors_selected_num = 17, 12\n",
    "    sensors_file_path = \"rsc/sensors/10\"  + \"/\" + str(sensors_num) + \"/sensors\"\n",
    "# num_pus = (data_reg.shape[1] - 3)//3\n",
    "\n",
    "# PART 2\n",
    "number_of_proccessors = 5\n",
    "memory_size_allowed = 4 # in Gigabyte\n",
    "float_size = 0\n",
    "if float_memory_used == \"float16\":\n",
    "    float_size = 16\n",
    "elif float_memory_used == \"float\" or \"float32\":\n",
    "    float_size = 32\n",
    "elif float_memory_used == \"float8\":\n",
    "    float_size = 8\n",
    "\n",
    "number_image_channels = 7 if sensors else 5\n",
    "\n",
    "\n",
    "batch_size = int(memory_size_allowed / (max_x * max_y * number_image_channels * float_size/(8 * 1024 ** 3)))\n",
    "\n",
    "\n",
    "dtime = datetime.datetime.now().strftime('_%Y%m_%d%H_%M')\n",
    "color = \"color\" if number_image_channels > 1 else \"gray\"\n",
    "image_dir = 'ML/data/pictures_' + str(max_x) + '_' + str(max_y) + '/' + propagation_model + (\n",
    "    \"/noisy_std_\" + str(std) if noise else \"\") + '/pu_' + pu_shape + '_su_' + su_shape + '_' + (\n",
    "    \"\" if su_shape == 'point' else str(su_szie)) + \"/\" + style + \"/\" + color +'/' + (\n",
    "    \"\" if pu_shape == 'point' and su_shape == 'point' else (intensity_degradation + '_' + str(slope))) + (\n",
    "    \"/\" + str(sensors_num) + \"sensors\" if sensors else \"/pus\") + \"/images\"\n",
    "\n",
    "if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/images'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "num_columns = (sensors_num if sensors else max_pus_num * 3 + 1) + max_sus_num * 3 + 1\n",
    "cols = [i for i in range(num_columns)]\n",
    "dataset_name = \"su_ss_calibrate_shuffled\"\n",
    "# max_dataset_name = \"dynamic_pus_max_power_60000_min10_max20PUs_1SUs_square100grid_splat_2020_06_28_13_45.txt\"\n",
    "with open('/'.join(image_dir.split('/')[:-1]) + '/datasets' + dtime + '.txt', 'w') as set_file:\n",
    "    set_file.write(dataset_name + \"\\n\")\n",
    "#     set_file.write(max_dataset_name)\n",
    "\n",
    "dataframe = pd.read_csv('ML/data/testbed/'\n",
    "                        + dataset_name, delimiter=',', header=None, names=cols)\n",
    "# dataframe_max = pd.read_csv('../../../java_workspace/research/spectrum_allocation/resources/data/'\n",
    "#                             + max_dataset_name, delimiter=',', header=None)\n",
    "\n",
    "dataframe.reset_index(drop=True, inplace=True)\n",
    "# dataframe_max.reset_index(drop=True, inplace=True)\n",
    "# dataframe_max[dataframe_max.shape[1] - 1] = dataframe_max[dataframe_max.shape[1] - 1].astype(float)\n",
    "\n",
    "# dataframe_tot = pd.concat([dataframe, dataframe_max.iloc[:, dataframe_max.columns.values[-1:]]], axis=1,\n",
    "#                         ignore_index=True)\n",
    "\n",
    "# idx = dataframe_tot[dataframe_tot[dataframe_tot.columns[-1]] == -float('inf')].index\n",
    "# dataframe_tot.drop(idx, inplace=True)\n",
    "# dataframe = dataframe.sample(frac=1).reset_index(drop=True)\n",
    "data_reg = dataframe.values\n",
    "data_reg[data_reg < noise_floor] = noise_floor\n",
    "# data_reg = np.concatenate((dataframe_tot.values[:, 0:dataframe_tot.shape[1]-3], \n",
    "#                            dataframe_tot.values[:, dataframe_tot.shape[1]-1:dataframe_tot.shape[1]]), axis=1)\n",
    "# data_class = dataframe_tot.values[:, 0:dataframe_tot.shape[1]-1]\n",
    "# y_class_power = dataframe_tot.values[:, -1]\n",
    "\n",
    "if sensors:\n",
    "    sensors_location = []\n",
    "    with open(sensors_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.split(' ')\n",
    "            sensors_location.append(Point(int(float(line[0]) * cell_size), int(float(line[1])) * cell_size))\n",
    "# if not sensors:\n",
    "#     for sample_idx in range(data_reg.shape[0]):\n",
    "#         data_reg[sample_idx][-1] = data_reg[sample_idx][int(data_reg[sample_idx][0]) * 3 + 3]\n",
    "        \n",
    "del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(519, 21)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pu conversion\n",
    "for i in range(data_reg.shape[0]):\n",
    "    num_pus = int(data_reg[i][0])\n",
    "    data_reg[i][num_pus*3 + 2: num_pus*3 + 5] = data_reg[i][num_pus*3 + 1: num_pus*3 + 4]\n",
    "    data_reg[i][num_pus*3+1] = 1\n",
    "    if num_pus == max_pus_num:\n",
    "        continue\n",
    "    data_reg[i][-1] = data_reg[i][num_pus*3+4]\n",
    "np.random.shuffle(data_reg)\n",
    "np.savetxt('ML/data/testbed/su_pu_calibrate_shuffled',data_reg, delimiter=\",\", fmt=\"%.1f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensor conversion\n",
    "for i in range(data_reg.shape[0]):\n",
    "    for j in range(data_reg.shape[1]-1):\n",
    "        data_reg[i][j] = float(data_reg[i][j])\n",
    "    num_ss = 17\n",
    "    data_reg[i][num_ss+1:num_ss+4] = data_reg[i][num_ss:num_ss+3]\n",
    "    data_reg[i][num_ss] = 1\n",
    "np.random.shuffle(data_reg)\n",
    "np.savetxt('ML/data/testbed/su_ss_calibrate_shuffled',data_reg, delimiter=\",\", fmt=\"%.5f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg = np.concatenate((data_reg[:150][:], data_reg[151:][:]), axis=0)\n",
    "np.savetxt('ML/data/testbed/su_ss_calibrate_shuffled',data_reg, delimiter=\",\", fmt=\"%.5f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_selected_idx = random.sample(range(sensors_num), sensors_selected_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 16, 12, 6, 4, 11, 7, 14, 15, 0, 2, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_selected_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520, 17)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3. ,  3. ,  3. , 62. ,  0. ,  8. , 35. ,  7. ,  9. , 28. ,  6. ,\n",
       "        4. , 36.7,  nan,  nan,  nan,  nan])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg[155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg = np.concatenate((data_reg[:,:2500], np.ones((4000, 1)), data_reg[:, 2500:2504],\n",
    "               data_reg[:, 2505:]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg[0, sensors_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg = data_reg[:][:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg[512:1024, :] = data_reg[:512, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg[4096:8192, sensors_num:] = data_reg[:4096, sensors_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_reg[10, :])\n",
    "print(data_reg[266, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(p1: Point, p2: Point):\n",
    "    return ((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2) ** 0.5\n",
    "\n",
    "def calculate_mu_sigma(data, num_pus):\n",
    "    sum_non_noise = 0\n",
    "    for pu_n in range(num_pus): # calculate mu\n",
    "        sum_non_noise += data[pu_n*3+2]\n",
    "    mu = ((max_x * max_y - num_pus) * noise_floor + sum_non_noise)/(max_x * max_y)\n",
    "    sum_square = 0\n",
    "    for pu_n in range(num_pus): # calculate sigma\n",
    "        sum_square += (data[pu_n*3+2]-mu)**2\n",
    "    sum_square += (max_x * max_y - num_pus) * (noise_floor - mu)**2\n",
    "    sigma = math.sqrt(sum_square/(max_x * max_y))\n",
    "    return mu, sigma\n",
    "\n",
    "def get_pu_param(pu_shape: str, intensity_degradation: str, pu_p: float, noise_floor: float, slope: float):\n",
    "    pu_param = None\n",
    "    if pu_shape == 'circle':\n",
    "        if intensity_degradation == \"linear\":\n",
    "            pu_param = Circle(int((pu_p - noise_floor) / slope)) # linear\n",
    "        elif intensity_degradation == \"log\":\n",
    "            pu_param = Circle(int(10 ** ((pu_p - noise_floor) / (10 *slope)))) # log_based\n",
    "    elif pu_shape == 'square':\n",
    "        if intensity_degradation == \"linear\":\n",
    "            pu_param = Square(int(2 ** 0.5 * (pu_p - noise_floor) / slope)) # linear\n",
    "        elif intensity_degradation == \"log\":\n",
    "            pu_param = Square(int(2 ** 0.5 * 10 ** ((pu_p - noise_floor) / (10 *slope)))) # log_based\n",
    "    elif pu_shape == 'point':\n",
    "        pu_param = None\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported PU shape(create_image)! \", pu_shape)\n",
    "    return pu_param\n",
    "\n",
    "def create_image(data, slope, sensors_num, style=\"raw_power_z_score\", noise_floor=-90, pu_shape= 'circle', pu_param=None, \n",
    "                 su_shape='circle', su_param=None, intensity_degradation=\"log\", max_pu_power: float=0):  \n",
    "    # style = {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "    # intensity_degradation= {\"log\", \"linear\"}\n",
    "    # if param is None, it's automatically calculated. Highest brightness(or power value) (255 or 1.) would\n",
    "    # assigned to the center(PU location) and radius(side) would be calculated based on its power, slope, and noise floor.\n",
    "    # If it is given, intensity(power) of pixel beside center would be calculated in the same fashin with an exception that \n",
    "    # intensity below zero(noise_floor) would be replaced by zero(noise_floor)\n",
    "    if style == \"raw_power_min_max_norm\":\n",
    "        # In this way, PUs' location are replaced with their power(dBm) and the power would fade with \n",
    "        # slope till gets noise_floor(in circle shape)\n",
    "        \n",
    "        # creating pu matrix\n",
    "        image = np.zeros((1,number_image_channels,max_x, max_y), dtype=float_memory_used)\n",
    "        if not sensors:\n",
    "            pus_num = int(data[0])\n",
    "            pus_cluster = [(7, 7), (7, 2), (2, 2), (2, 7)]\n",
    "#             print(pus_num)\n",
    "            for pu_i in range(pus_num):\n",
    "                pu_x = max(0, min(max_x-1, int(data[pu_i * 3 + 1]))) \n",
    "                pu_y = max(0, min(max_x-1, int(data[pu_i * 3 + 2])))\n",
    "                pus_dist_cluster = [math.sqrt((pu_x - x)**2 + (pu_y - y)**2) for x,y in pus_cluster]\n",
    "                pu_p = data[pu_i * 3 + 3]\n",
    "                pu_channel_idx = pus_dist_cluster.index(min(pus_dist_cluster))\n",
    "                pu_x *= cell_size\n",
    "                pu_y *= cell_size\n",
    "#                 print(pu_x, pu_y, pu_p)\n",
    "                if pu_param is None:\n",
    "                    pu_param_p = get_pu_param(pu_shape, intensity_degradation, pu_p, noise_floor, slope)\n",
    "                else:\n",
    "                    pu_param_p = pu_param\n",
    "                points = points_inside_shape(center=Point(pu_x, pu_y), shape=pu_shape, param=pu_param_p)\n",
    "                for point in points:\n",
    "                    if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                        if intensity_degradation == \"linear\":\n",
    "                            image[0][pu_channel_idx][point.p.x][point.p.y] += (pu_p - slope * point.dist - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                        elif intensity_degradation == \"log\":\n",
    "                            if point.dist < 1:\n",
    "                                image[0][pu_channel_idx][point.p.x][point.p.y] += (pu_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                            else:\n",
    "                                image[0][pu_channel_idx][point.p.x][point.p.y] += (pu_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                    max_pu_power - noise_floor)\n",
    "        else:\n",
    "            ss_param, ss_shape = pu_param, pu_shape\n",
    "            for ss_i in range(sensors_num):\n",
    "                if ss_i not in sensor_selected_idx:\n",
    "                    continue\n",
    "                ss_x, ss_y, ss_p = max(0, min(max_x-1, int(sensors_location[ss_i].x))), max(0, min(max_x-1, int(\n",
    "                    sensors_location[ss_i].y))), max(noise_floor, data[ss_i])\n",
    "                ss_channel = 0 \n",
    "                if -70.0 <= ss_p < -65.0:\n",
    "                    ss_channel = 1\n",
    "                elif -75.0 <= ss_p < -70.0:\n",
    "                    ss_channel = 2\n",
    "                elif -80.0 <= ss_p < -75.0:\n",
    "                    ss_channel = 3\n",
    "                elif -85.0 <= ss_p < -80.0:\n",
    "                    ss_channel = 4\n",
    "#                 elif -70.0 <= ss_p < -65.0:\n",
    "#                     ss_channel = 5\n",
    "                elif ss_p < -85.0:\n",
    "                    ss_channel = 5\n",
    "                if ss_param is None:\n",
    "                    ss_param_p = get_pu_param(ss_shape, intensity_degradation, ss_p, noise_floor, slope)\n",
    "                else:\n",
    "                    ss_param_p = ss_param\n",
    "#                 points = points_inside_shape(center=Point(ss_x, ss_y), shape=ss_shape, param=ss_param_p)\n",
    "                coef = max(1, int((ss_p - noise_floor)/(max_pu_power - noise_floor)))\n",
    "                points = points_inside_shape(center=Point(ss_x, ss_y), shape=ss_shape, \n",
    "                                             param=Circle(ss_param_p[0] * coef))\n",
    "                for point in points:\n",
    "                    if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                        if intensity_degradation == \"linear\":\n",
    "                            image[0][ss_channel][point.p.x][point.p.y] += (ss_p - slope * point.dist - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                        elif intensity_degradation == \"log\":\n",
    "                            if point.dist < 1:\n",
    "                                image[0][ss_channel][point.p.x][point.p.y] += (ss_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                            else:\n",
    "                                image[0][ss_channel][point.p.x][point.p.y] += (ss_p - slope * 10*math.log10(point.dist) / coef - noise_floor)/(\n",
    "                                    max_pu_power - noise_floor)\n",
    "        del points\n",
    "        # creating su matrix\n",
    "        su_num_idx = sensors_num if sensors else (pus_num * 3 + 1)\n",
    "        su_num = int(data[su_num_idx])\n",
    "#         print(su_num)\n",
    "#         su_num = (len(data) - pus_num * (3 if not sensors else 1)) // 2\n",
    "#         if not (len(data) - pus_num * (3 if not sensors else 1)) % 2:\n",
    "#             raise ValueError(\"Data provided is not correct; can't get SUs' information(create_image)\")\n",
    "        if su_param is None:\n",
    "            # if su_param is unavailable, a circle(square) with radius(side) 1 is created\n",
    "            if su_shape == 'circle':\n",
    "                su_param = Circle(1)\n",
    "            elif su_shape == 'square':\n",
    "                su_param = Square(1)\n",
    "            elif su_shape == 'point':\n",
    "                su_param = None\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported SU shape(create_image)! \", su_shape)\n",
    "        \n",
    "        for su_i in range(su_num - 1):\n",
    "            su_x = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 1])))\n",
    "            su_y = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 2])))\n",
    "            su_p = data[su_num_idx + su_i * 3 + 3]\n",
    "#             su_p = su_intensity\n",
    "            points = points_inside_shape(center=Point(su_x, su_y), param=su_param, shape=su_shape)\n",
    "            su_channel = 0 if number_image_channels == 1 else -1\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                            su_val = (su_p - slope * point.dist - noise_floor)/(max_pu_power - noise_floor)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "                            su_val = (su_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                        else:\n",
    "                            su_val = (su_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                    image[0][su_channel][point.p.x][point.p.y] += su_val\n",
    "            del points\n",
    "        # the last and  target SU\n",
    "        su_intensity = 1.\n",
    "        su_x = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 1]))) * cell_size\n",
    "        su_y = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 2]))) * cell_size\n",
    "#         print(su_x, su_y)\n",
    "        points = points_inside_shape(center=Point(su_x, su_y), param=su_param, shape=su_shape)\n",
    "        su_channel = 0 if number_image_channels == 1 else -1\n",
    "        for point in points:\n",
    "            if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                image[0][su_channel][point.p.x][point.p.y] += su_intensity\n",
    "        del points\n",
    "        return image\n",
    "        \n",
    "#         pu_image = [[(noise_floor - mu)/sigma] * max_y for _ in range(max_x)]\n",
    "    elif style == \"image_intensity\":\n",
    "        # creating PU image\n",
    "        image = np.zeros((1,number_image_channels,max_x, max_y), dtype=float_memory_used)\n",
    "        for pu_i in range(pus_num):\n",
    "            pu_x, pu_y, pu_p = max(0, min(max_x-1, int(data[pu_i*3]))), max(0, min(max_x-1, int(data[pu_i*3+1]))), data[pu_i*3+2]\n",
    "            if pu_param is None:\n",
    "                pu_param_p = get_pu_param(pu_shape, intensity_degradation, pu_p, noise_floor, slope)\n",
    "            else:\n",
    "                pu_param_p = pu_param\n",
    "            points = points_inside_shape(center=Point(pu_x, pu_y), shape=pu_shape, param=pu_param_p)\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                        image[0][0][point.p.x][point.p.y] += max((pu_p - slope * point.dist + abs(noise_floor))\n",
    "                                                              /(pu_p + abs(noise_floor)), 0)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "                            image[0][0][point.p.x][point.p.y] = 1\n",
    "                        else:\n",
    "                            image[0][0][point.p.x][point.p.y] += max((pu_p - slope * 10*math.log10(point.dist) + abs(noise_floor))\n",
    "                                                                 /(pu_p + abs(noise_floor)), 0)\n",
    "                    image[0][0][point.p.x][point.p.y] = min(image[0][0][point.p.x][point.p.y], 1.0)\n",
    "                        \n",
    "        # creating SU image\n",
    "        su_num = (len(data) - pus_num * 3) // 2\n",
    "        if not (len(data) - pus_num * 3) % 2:\n",
    "            raise ValueError(\"Data provided is not correct; can't get SUs' information(create_image)\")\n",
    "#         su_image = np.zeros((max_x, max_y), dtype=float_memory_used)\n",
    "        if su_param is None:\n",
    "            # if su_param is unavailable, a circle(square) with radius(side) 1 is created\n",
    "            if su_shape == 'circle':\n",
    "                su_param = Circle(1)\n",
    "            elif su_shape == 'square':\n",
    "                su_param = Square(1)\n",
    "            elif su_shape == 'point':\n",
    "                su_param = None\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported SU shape(create_image)! \", su_shape)\n",
    "        su_intensity = 1.\n",
    "        for su_i in range(su_num):\n",
    "            su_x, su_y, su_p = max(0, min(max_x-1, int(data[pus_num * (3 if not sensors else 1) +su_i*2]))\n",
    "                                  ), max(0, min(max_x-1, int(data[pus_num * (3 if not sensors else 1) + su_i*2+1]))), su_intensity\n",
    "            points = points_inside_shape(center=Point(su_x, su_y), param=su_param, shape=su_shape)\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if number_image_channels > 1:\n",
    "                        image[0][1][point.p.x][point.p.y] = su_intensity\n",
    "                    elif number_image_channels == 1:\n",
    "                        image[0][0][point.p.x][point.p.y] = su_intensity\n",
    "#         return np.array([pu_image, su_image, [[0.] * max_y for _ in range(max_x)]], dtype='float32') # return like this to be able to display as an RGB image with pyplot.imshow(imsave)\n",
    "#         return np.append(pu_image, su_image, axis=0)\n",
    "        return image\n",
    "        \n",
    "            \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported style(create_image)! \", style)\n",
    "        \n",
    "def points_inside_shape(center: Point, shape: str, param)-> list:\n",
    "    # This function returns points+distance around center with defined shape\n",
    "    if shape == 'circle':\n",
    "        # First creates points inside a square(around orgigin) with 2*r side and then remove those with distance > r.\n",
    "        # Shift all remaining around center. O(4r^2)\n",
    "        r, origin = param.r, Point(0, 0)\n",
    "        square_points = set((Point(x, y) for x in range(max(-r, -max_x), min(r, max_x) + 1) \n",
    "                             for y in range(max(-r, -max_y), min(r, max_y) + 1)))\n",
    "        points = []\n",
    "        while square_points:\n",
    "            p = square_points.pop()\n",
    "            dist = euclidian_distance(p, origin)\n",
    "            if dist <= r:\n",
    "                points.append(PointWithDistance(Point(p.x + center.x, p.y + center.y), dist))\n",
    "                if p.x != 0:\n",
    "                    points.append(PointWithDistance(Point(-p.x + center.x, p.y + center.y), dist))\n",
    "                    square_points.remove(Point(-p.x, p.y))\n",
    "                if p.y != 0:\n",
    "                    points.append(PointWithDistance(Point(p.x + center.x, -p.y + center.y), dist))\n",
    "                    square_points.remove(Point(p.x, -p.y))\n",
    "                if p.x != 0 and p.y != 0:\n",
    "                    points.append(PointWithDistance(Point(-p.x + center.x, -p.y + center.y), dist))\n",
    "                    square_points.remove(Point(-p.x, -p.y))\n",
    "        del square_points\n",
    "        return points\n",
    "    elif shape == 'square':\n",
    "        half_side = param.side // 2\n",
    "        return [PointWithDistance(Point(x, y), euclidian_distance(Point(x, y), center)) for x in range(-half_side + center.x,\n",
    "                                                                                               half_side + center.x+1) \n",
    "                         for y in range(-half_side + center.y, half_side + center.y + 1)]\n",
    "    elif shape == 'point':\n",
    "        return [PointWithDistance(center, 0)]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported shape(points_inside_shape)! \", shape)\n",
    "        \n",
    "def read_image(image_num):\n",
    "    if style == \"image_intensity\":\n",
    "        image = plt.imread(image_dir + '/image' + str(image_num)+'.png')\n",
    "        image = np.swapaxes(image, 0, 2)\n",
    "        image = np.array(image[:number_image_channels], dtype=float_memory_used).reshape(1, number_image_channels, max_x, max_y)\n",
    "    elif  style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\":\n",
    "        suffix = 'npz'  # npy, npz\n",
    "        image = np.load(image_dir + '/image' + str(image_num) + '.' + suffix)  \n",
    "        if type(image) == np.lib.npyio.NpzFile:\n",
    "            image = image['a']\n",
    "    \n",
    "    return image\n",
    "    \n",
    "# TODO: Consider using min_max normalization becasue difference between values using\n",
    "# z-score is huge since most of the pixels have the same value, noise floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([179.,  85.,  47.,  42.,  43.,  40.,  25.,  19.,   9.,   9.,   8.,\n",
       "          5.,   2.,   1.,   1.,   0.,   2.,   0.,   0.,   0.,   1.,   1.]),\n",
       " array([-89.92236   , -89.44245409, -88.96254818, -88.48264227,\n",
       "        -88.00273636, -87.52283045, -87.04292455, -86.56301864,\n",
       "        -86.08311273, -85.60320682, -85.12330091, -84.643395  ,\n",
       "        -84.16348909, -83.68358318, -83.20367727, -82.72377136,\n",
       "        -82.24386545, -81.76395955, -81.28405364, -80.80414773,\n",
       "        -80.32424182, -79.84433591, -79.36443   ]),\n",
       " <a list of 22 Patch objects>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(data_reg[:,0:1:sensors_num], bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-56.8745"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(data_reg[:, :17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_v = -float('inf')\n",
    "min_v = float('inf')\n",
    "for sample_idx in range(data_reg.shape[0]):\n",
    "    pu_n = int(data_reg[sample_idx][0])\n",
    "    max_v = max(max_v, max(data_reg[sample_idx][3:pu_n*3+1:3]))\n",
    "    min_v = min(min_v, min(data_reg[sample_idx][3:pu_n*3+1:3]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not sensors:\n",
    "    noise_floor = 0\n",
    "if sensors:\n",
    "    max_v = -85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving images once to save time\n",
    "# run this cell just for creating images\n",
    "def creating_image(data_reg, start, end):\n",
    "    # for image_num in range(115, data_reg.shape[0]):\n",
    "    # for image_num in range(1625, 5000):\n",
    "    for image_num in tqdm.tqdm(range(start, end+1)):  #4463, data_reg.shape[0]\n",
    "        image = create_image(data=data_reg[image_num], slope=slope, style=style, \n",
    "                             noise_floor=noise_floor,\n",
    "                             pu_shape=pu_shape, su_shape=su_shape, su_param=su_param, \n",
    "                             sensors_num=(sensors_num if sensors else 0), \n",
    "                             intensity_degradation=intensity_degradation, \n",
    "                             max_pu_power=max_v if sensors else max_v)\n",
    "        if style == \"image_intensity\":\n",
    "            if number_image_channels != 3:\n",
    "                image = np.append(np.array(image[0]), np.zeros((3-number_image_channels,max_x, max_y), \n",
    "                                                               dtype=float_memory_used), axis=0)\n",
    "            image_save = np.swapaxes(image, 0, 2)\n",
    "            plt.imsave(image_dir + '/image' + str(image_num)+'.png', image_save)\n",
    "        elif style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\":\n",
    "    #         np.save(image_dir + '/image' + str(image_num), image)\n",
    "            np.savez_compressed(image_dir + '/image' + str(image_num), a=image)\n",
    "        del image\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-84.61222 -80.16843 -86.55651 -88.96914 -79.58933 -76.66294 -75.69202\n",
      " -74.12955 -71.41905 -89.91388 -62.81991 -90.1087  -89.90674 -81.48449\n",
      " -87.72848 -84.58716 -87.32878   1.        6.        6.       47.     ]\n"
     ]
    }
   ],
   "source": [
    "print(data_reg[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def job_creating_images(data):\n",
    "    jobs = []\n",
    "    proc_sizes = [data.shape[0]//number_of_proccessors] * (number_of_proccessors)\n",
    "    proc_sizes[-1] += data.shape[0]%number_of_proccessors\n",
    "    proc_idx = [(sum(proc_sizes[:i]), sum(proc_sizes[:i+1])-1) for i in range(number_of_proccessors)]\n",
    "\n",
    "    for i in range(number_of_proccessors):\n",
    "        p = Process(target=creating_image, args=(data, proc_idx[i][0], proc_idx[i][1]))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "    for i in range(number_of_proccessors):\n",
    "        jobs[i].join()\n",
    "\n",
    "    for i in range(number_of_proccessors):\n",
    "        jobs[i].terminate()\n",
    "        jobs[i].close()\n",
    "    del jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:00<00:00, 189.69it/s]\n",
      "\n",
      "100%|██████████| 103/103 [00:00<00:00, 188.03it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 184.16it/s]\n",
      "100%|██████████| 107/107 [00:00<00:00, 189.68it/s]\n"
     ]
    }
   ],
   "source": [
    "job_creating_images(data=data_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, point in enumerate(sensors_location):\n",
    "    print(idx+1, point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, point in enumerate(sensors_location):\n",
    "    print(idx+1, point,\"close\") if math.sqrt((point.x-917)**2+(point.y-415)**2)<=1.5 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = [0, 0, 0, 0]\n",
    "idxx = [[],[],[],[]]\n",
    "for i in range(data_reg.shape[0]):\n",
    "    pus_c = int(data_reg[i][0]) * 3 + 1\n",
    "    idx = int(data_reg[i][pus_c]) - 1\n",
    "    count[idx] += 1\n",
    "    idxx[idx].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(count)\n",
    "print(idxx[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm = read_image(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 7, 0, 15, 2, 12, 1, 9, 14, 6, 13, 4, 11, 10, 3, 5, 8]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_selected_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.31"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(imm[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3d5868ec50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 4320x4320 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(60,60))         # Sample figsize in inches\n",
    "sns.heatmap(imm[0][5], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(num_filters, kernel_lam, bias_lam):\n",
    "#     num_filters, lam = 5, 5\n",
    "    data_format = 'channels_first'\n",
    "    convolution_init, dense_init = \"lecun_normal\", \"RandomNormal\"\n",
    "    convolution_filter, dense_filter = 'selu', 'linear' #softsign, sigmoid; relu, linear\n",
    "    filter_shape, pool_size = (3, 3), (2,2)\n",
    "    cnn = models.Sequential()\n",
    "    cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(number_image_channels, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "#                           input_shape=(number_image_channels, max_x, max_y), data_format=data_format,\n",
    "#                           kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                           kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size=pool_size, data_format=data_format))\n",
    "    cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Dropout(0.25))\n",
    "    \n",
    "    cnn.add(layers.Conv2D(2*num_filters, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Conv2D(3*num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "#                           input_shape=(number_image_channels, max_x, max_y), data_format=data_format,\n",
    "#                           kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                           kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size=pool_size, data_format=data_format))\n",
    "    cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Dropout(0.25))\n",
    "    \n",
    "    cnn.add(layers.Conv2D(3*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Conv2D(2*num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "#                           input_shape=(number_image_channels, max_x, max_y), data_format=data_format,\n",
    "#                           kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                           kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "    cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Dropout(0.25))\n",
    "    \n",
    "    cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "#                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                          kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "    cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Dropout(0.25))\n",
    "# from here for 1000\n",
    "    if max(max_x, max_y) == 1000:\n",
    "        cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                             kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                             kernel_initializer=convolution_init))\n",
    "    #     cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "    #                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "    #                          kernel_initializer='lecun_normal'))\n",
    "        cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "        cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Dropout(0.25))\n",
    "\n",
    "        cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                             kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam), \n",
    "                             kernel_initializer=convolution_init))\n",
    "    #     cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Conv2D(2*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "    #                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "    #                          kernel_initializer='lecun_normal'))\n",
    "        cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "        cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Dropout(0.25))\n",
    "\n",
    "        cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                             kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam), \n",
    "                             kernel_initializer=convolution_init))\n",
    "    #     cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Conv2D(3*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "    #                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "    #                          kernel_initializer='lecun_normal'))\n",
    "        cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "        cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Dropout(0.25))\n",
    "    \n",
    "    cnn.add(layers.Flatten())\n",
    "    cnn.add(layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=convolution_init))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization())\n",
    "    cnn.add(layers.Dense(1, activation=dense_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=dense_init))\n",
    "    return cnn\n",
    "\n",
    "\n",
    "class DataBatchGenerator(Sequence):\n",
    "    def __init__(self, dataset:np.ndarray, batch_size:int, start_idx:int,\n",
    "                 number_image_channels:int,\n",
    "                 max_x, max_y, float_memory_used, conserve=0):\n",
    "#         print(dataset.shape[0])\n",
    "        self.dataset, self.batch_size, self.start_idx = dataset, batch_size, start_idx\n",
    "        self.number_image_channels, self.max_x, self.max_y = number_image_channels, max_x, max_y\n",
    "        self.float_memory_used = float_memory_used\n",
    "        self.conserve = conserve\n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.ceil(self.dataset.shape[0] / self.batch_size).astype(np.int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        size = min(self.dataset.shape[0] - idx * self.batch_size, self.batch_size)\n",
    "        batch_x = np.empty((size, self.number_image_channels, self.max_x, self.max_y), dtype=self.float_memory_used)\n",
    "        batch_y = np.empty((size), dtype=self.float_memory_used)\n",
    "        for i in range(size):\n",
    "            batch_x[i] = read_image(self.start_idx + idx * self.batch_size + i)\n",
    "            batch_y[i] = self.dataset[idx * self.batch_size + i][- 1 - self.conserve]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "def custom_loss(fp_penalty_coef, fn_penalty_coef):\n",
    "    # custom loss function that penalize false positive and negative differently\n",
    "    def loss(y_true, y_pred):\n",
    "        res = y_pred - y_true\n",
    "        res = tf.where(res > 0, res * fp_penalty_coef, res * fn_penalty_coef)\n",
    "        return K.mean(K.square(res))\n",
    "    return loss\n",
    "\n",
    "def fp_mae(y_true, y_pred):\n",
    "    # custom metric that replace false negative with zero and return the mean of new vector\n",
    "    res = y_pred - y_true\n",
    "    res = tf.nn.relu(res)\n",
    "#     res = tf.where(res <= 0, 0, res)\n",
    "    return K.mean(res)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_model(10, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 10, 100, 100)      640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 10, 50, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 10, 50, 50)        40        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 20, 50, 50)        1820      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 20, 25, 25)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 20, 25, 25)        80        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 25, 25)        5430      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 30, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 12, 12)        120       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 40, 12, 12)        10840     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 40, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 40, 6, 6)          160       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1440)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                28820     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 48,471\n",
      "Trainable params: 48,231\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(519, 21)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples = [519]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg = data_reg_bak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_samples: 519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 102/103 [00:00<00:00, 194.01it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 193.31it/s]\n",
      "\n",
      "100%|██████████| 103/103 [00:00<00:00, 195.57it/s]\n",
      "100%|██████████| 107/107 [00:00<00:00, 188.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Fold: 0 , Training Size: 420 , Validation size: 47 , Test Size 52\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 49.15274, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 49.15274 to 48.97751, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 48.97751 to 48.78480, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 48.78480 to 48.58834, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 48.58834 to 48.37000, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 48.37000 to 48.10597, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 48.10597 to 47.81573, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 47.81573 to 47.50454, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 47.50454 to 47.19160, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 47.19160 to 46.77909, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 46.77909 to 46.52346, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 46.52346 to 45.98446, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 45.98446 to 45.63681, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 45.63681 to 45.11249, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 45.11249 to 44.65725, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 44.65725 to 44.50631, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 44.50631 to 44.27433, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 44.27433 to 43.65572, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 43.65572 to 41.88204, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 41.88204\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 41.88204\n",
      "\n",
      "Epoch 00022: val_mae improved from 41.88204 to 41.44358, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 41.44358 to 41.43824, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 41.43824 to 41.28263, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 41.28263 to 39.37759, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 39.37759\n",
      "\n",
      "Epoch 00027: val_mae improved from 39.37759 to 39.34925, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 39.34925 to 38.98213, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 38.98213 to 36.56347, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 36.56347\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 36.56347\n",
      "\n",
      "Epoch 00032: val_mae improved from 36.56347 to 34.96880, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 34.96880 to 34.23723, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 34.23723\n",
      "\n",
      "Epoch 00035: val_mae improved from 34.23723 to 32.98222, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00036: val_mae improved from 32.98222 to 30.93200, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 30.93200\n",
      "\n",
      "Epoch 00038: val_mae improved from 30.93200 to 28.32849, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00039: val_mae improved from 28.32849 to 28.11061, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00040: val_mae improved from 28.11061 to 27.08862, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00041: val_mae improved from 27.08862 to 25.70860, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00042: val_mae improved from 25.70860 to 25.07753, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 25.07753\n",
      "\n",
      "Epoch 00044: val_mae improved from 25.07753 to 24.21064, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00045: val_mae improved from 24.21064 to 23.26195, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 23.26195\n",
      "\n",
      "Epoch 00047: val_mae improved from 23.26195 to 20.00248, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00048: val_mae improved from 20.00248 to 18.08249, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 18.08249\n",
      "\n",
      "Epoch 00050: val_mae improved from 18.08249 to 17.53053, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00051: val_mae improved from 17.53053 to 14.87350, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00052: val_mae improved from 14.87350 to 13.78386, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00053: val_mae improved from 13.78386 to 12.10931, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00054: val_mae improved from 12.10931 to 11.03832, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 11.03832\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 11.03832\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 11.03832\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 11.03832\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 11.03832\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 11.03832\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 11.03832\n",
      "\n",
      "Epoch 00062: val_mae improved from 11.03832 to 9.85780, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00063: val_mae improved from 9.85780 to 9.20225, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 9.20225\n",
      "\n",
      "Epoch 00065: val_mae improved from 9.20225 to 8.46769, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00066: val_mae improved from 8.46769 to 8.10954, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00067: val_mae improved from 8.10954 to 7.86382, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 7.86382\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 7.86382\n",
      "\n",
      "Epoch 00070: val_mae improved from 7.86382 to 7.51003, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 7.51003\n",
      "\n",
      "Epoch 00072: val_mae improved from 7.51003 to 7.19985, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 7.19985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00158: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 7.19985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00319: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 7.19985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00480: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 7.19985\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 7.19985\n",
      "\n",
      "Lambda: 0.01 , Time: 0:04:05\n",
      "Train Error(all epochs): 0.7268930077552795 \n",
      " [49.149, 49.06, 48.99, 48.91, 48.813, 48.7, 48.567, 48.41, 48.225, 48.011, 47.759, 47.474, 47.138, 46.758, 46.321, 45.83, 45.276, 44.689, 44.05, 43.332, 42.551, 41.723, 40.854, 39.95, 39.004, 38.03, 37.031, 35.989, 34.955, 33.85, 32.728, 31.606, 30.475, 29.281, 28.067, 26.885, 25.645, 24.449, 23.223, 21.988, 20.788, 19.529, 18.339, 17.167, 16.058, 14.85, 13.857, 12.766, 11.801, 10.903, 10.041, 9.279, 8.567, 7.951, 7.19, 6.661, 6.121, 5.588, 5.257, 5.066, 4.758, 4.677, 4.562, 4.467, 4.502, 4.459, 4.409, 4.144, 4.117, 3.99, 3.815, 3.699, 3.451, 3.4, 3.3, 3.274, 3.097, 2.891, 2.894, 2.709, 2.786, 2.763, 2.76, 2.853, 2.832, 2.835, 2.912, 3.074, 2.995, 2.961, 2.906, 2.837, 2.685, 2.714, 2.591, 2.621, 2.584, 2.64, 2.873, 2.803, 2.791, 2.565, 2.506, 2.253, 2.199, 2.153, 1.993, 2.088, 2.022, 2.018, 2.086, 2.265, 2.446, 2.699, 2.704, 2.666, 2.706, 2.63, 2.447, 2.236, 2.032, 1.906, 1.828, 1.769, 1.668, 1.587, 1.552, 1.6, 1.649, 1.781, 1.943, 2.31, 2.433, 2.573, 2.384, 2.223, 1.966, 1.906, 1.925, 1.809, 1.809, 1.727, 1.687, 1.605, 1.634, 1.648, 1.581, 1.628, 1.682, 1.931, 1.99, 2.159, 1.9, 1.727, 1.629, 1.659, 1.559, 1.509, 1.516, 1.476, 1.517, 1.583, 1.694, 1.828, 1.937, 1.7, 1.696, 1.636, 1.702, 1.71, 1.644, 1.53, 1.49, 1.427, 1.364, 1.328, 1.432, 1.409, 1.47, 1.468, 1.531, 1.703, 1.857, 1.823, 1.765, 1.71, 1.606, 1.449, 1.39, 1.325, 1.362, 1.359, 1.495, 1.316, 1.313, 1.395, 1.537, 1.644, 1.754, 1.861, 1.805, 1.764, 1.81, 1.638, 1.639, 1.516, 1.501, 1.505, 1.631, 1.565, 1.549, 1.526, 1.421, 1.522, 1.356, 1.427, 1.376, 1.383, 1.329, 1.221, 1.258, 1.214, 1.205, 1.184, 1.19, 1.324, 1.437, 1.48, 1.486, 1.486, 1.369, 1.373, 1.407, 1.346, 1.448, 1.497, 1.494, 1.531, 1.421, 1.422, 1.414, 1.278, 1.255, 1.221, 1.174, 1.079, 1.126, 1.105, 1.118, 1.192, 1.173, 1.218, 1.268, 1.334, 1.298, 1.345, 1.365, 1.317, 1.165, 1.204, 1.176, 1.183, 1.287, 1.306, 1.28, 1.324, 1.314, 1.422, 1.322, 1.217, 1.149, 1.098, 0.99, 0.964, 0.985, 1.074, 1.127, 1.213, 1.388, 1.437, 1.32, 1.283, 1.155, 1.076, 1.047, 0.949, 1.009, 1.118, 1.153, 1.129, 1.117, 1.167, 1.212, 1.151, 1.129, 1.28, 1.43, 1.542, 1.516, 1.549, 1.52, 1.477, 1.387, 1.292, 1.209, 1.087, 0.996, 0.953, 0.987, 0.936, 0.854, 0.851, 0.873, 0.879, 0.878, 0.93, 1.027, 1.059, 0.95, 0.977, 0.968, 0.953, 0.969, 1.009, 1.001, 1.097, 1.146, 1.237, 1.34, 1.431, 1.379, 1.323, 1.19, 1.202, 1.199, 1.241, 1.196, 1.092, 1.16, 1.286, 1.261, 1.239, 1.218, 1.254, 1.33, 1.328, 1.257, 1.088, 1.062, 1.075, 1.082, 1.114, 1.095, 1.082, 1.023, 1.041, 0.994, 0.908, 0.794, 0.727, 0.772, 0.894, 0.993, 1.007, 1.01, 1.027, 1.048, 0.959, 0.95, 0.927, 0.943, 0.95, 0.947, 0.969, 1.014, 0.994, 1.016, 1.004, 0.954, 0.996, 1.056, 1.15, 1.01, 1.042, 0.98, 0.994, 0.99, 1.01, 0.909, 0.925, 1.023, 1.063, 1.117, 1.188, 1.288, 1.191, 1.145, 1.165, 1.209, 1.191, 1.224, 1.261, 1.342, 1.284, 1.248, 1.119, 1.091, 1.054, 1.088, 1.0, 1.016, 0.95, 0.98, 1.028, 1.005, 1.006, 0.952, 0.934, 0.976, 1.007, 1.035, 1.009, 1.012, 0.983, 1.003, 1.009, 1.047, 1.019, 0.885, 0.797, 0.841, 0.869, 0.863, 0.899, 0.914, 1.005, 1.057, 1.101, 1.106, 1.078, 1.097, 1.149, 1.026, 0.959, 0.938, 0.931, 0.873, 0.832, 0.785, 0.783, 0.791, 0.816, 0.82, 0.812, 0.778, 0.822, 0.77, 0.788, 0.865, 0.94, 0.902, 0.928, 1.034, 1.107, 1.029, 1.102, 1.097, 1.215, 1.154, 1.126, 1.186, 1.204, 1.238, 1.177, 1.169, 1.128, 1.125, 1.154, 1.156, 1.041, 1.01, 1.053, 1.061, 1.049, 0.871, 0.852, 0.903, 0.932, 0.97, 0.923, 0.872, 0.813, 0.794, 0.809, 0.83, 0.831, 0.832, 0.859, 0.901, 0.919]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004, 0.001, 0.015, 0.019, 0.04, 0.029, 0.064, 0.065, 0.102, 0.129, 0.189, 0.229, 0.319, 0.363, 0.375, 0.446, 0.49, 0.509, 0.592, 0.735, 0.791, 0.916, 1.04, 1.128, 1.257, 1.207, 1.356, 1.335, 1.414, 1.41, 1.408, 1.372, 1.302, 1.331, 1.352, 1.329, 1.276, 1.159, 1.197, 1.071, 1.146, 1.165, 1.143, 1.249, 1.213, 1.266, 1.262, 1.38, 1.335, 1.354, 1.316, 1.31, 1.164, 1.194, 1.107, 1.163, 1.114, 1.172, 1.384, 1.287, 1.333, 1.164, 1.141, 0.943, 0.995, 0.907, 0.85, 0.947, 0.872, 0.91, 0.932, 1.017, 1.152, 1.326, 1.305, 1.263, 1.314, 1.207, 1.17, 1.061, 0.881, 0.816, 0.852, 0.783, 0.74, 0.723, 0.668, 0.736, 0.751, 0.83, 0.913, 1.189, 1.205, 1.317, 1.18, 1.0, 0.865, 0.901, 0.856, 0.829, 0.861, 0.794, 0.804, 0.749, 0.738, 0.791, 0.742, 0.797, 0.791, 0.959, 0.922, 1.114, 0.878, 0.796, 0.743, 0.795, 0.737, 0.697, 0.712, 0.717, 0.734, 0.75, 0.854, 0.82, 1.028, 0.791, 0.751, 0.749, 0.869, 0.74, 0.823, 0.751, 0.703, 0.69, 0.679, 0.614, 0.735, 0.636, 0.749, 0.676, 0.77, 0.818, 0.909, 0.928, 0.818, 0.817, 0.777, 0.641, 0.684, 0.606, 0.688, 0.573, 0.804, 0.584, 0.633, 0.641, 0.793, 0.786, 0.862, 0.978, 0.949, 0.793, 0.943, 0.776, 0.808, 0.66, 0.757, 0.679, 0.822, 0.748, 0.717, 0.817, 0.619, 0.818, 0.612, 0.705, 0.646, 0.684, 0.685, 0.535, 0.677, 0.515, 0.616, 0.521, 0.607, 0.658, 0.703, 0.755, 0.808, 0.767, 0.7, 0.657, 0.659, 0.692, 0.626, 0.778, 0.68, 0.817, 0.627, 0.723, 0.74, 0.578, 0.66, 0.572, 0.584, 0.501, 0.553, 0.528, 0.52, 0.632, 0.528, 0.654, 0.595, 0.678, 0.613, 0.698, 0.648, 0.674, 0.527, 0.592, 0.574, 0.542, 0.655, 0.663, 0.606, 0.686, 0.696, 0.698, 0.658, 0.602, 0.523, 0.558, 0.402, 0.522, 0.454, 0.524, 0.545, 0.648, 0.636, 0.764, 0.593, 0.651, 0.596, 0.465, 0.499, 0.448, 0.506, 0.561, 0.562, 0.546, 0.57, 0.568, 0.62, 0.594, 0.524, 0.673, 0.708, 0.783, 0.7, 0.782, 0.709, 0.758, 0.672, 0.635, 0.567, 0.527, 0.488, 0.451, 0.479, 0.456, 0.395, 0.422, 0.418, 0.42, 0.425, 0.476, 0.488, 0.559, 0.461, 0.479, 0.488, 0.476, 0.469, 0.526, 0.464, 0.564, 0.567, 0.618, 0.668, 0.753, 0.638, 0.665, 0.571, 0.599, 0.588, 0.565, 0.598, 0.518, 0.569, 0.626, 0.608, 0.627, 0.602, 0.601, 0.683, 0.636, 0.665, 0.52, 0.483, 0.541, 0.486, 0.579, 0.521, 0.559, 0.493, 0.57, 0.462, 0.486, 0.388, 0.331, 0.386, 0.456, 0.47, 0.5, 0.512, 0.507, 0.555, 0.452, 0.472, 0.442, 0.468, 0.454, 0.462, 0.509, 0.471, 0.524, 0.476, 0.507, 0.454, 0.537, 0.469, 0.648, 0.448, 0.561, 0.472, 0.483, 0.46, 0.521, 0.428, 0.457, 0.542, 0.471, 0.59, 0.62, 0.573, 0.629, 0.572, 0.539, 0.642, 0.534, 0.662, 0.59, 0.724, 0.574, 0.668, 0.483, 0.527, 0.509, 0.539, 0.47, 0.497, 0.493, 0.462, 0.578, 0.472, 0.507, 0.458, 0.456, 0.502, 0.523, 0.489, 0.527, 0.505, 0.467, 0.524, 0.488, 0.529, 0.526, 0.42, 0.394, 0.423, 0.428, 0.417, 0.459, 0.411, 0.516, 0.538, 0.53, 0.532, 0.562, 0.473, 0.64, 0.445, 0.5, 0.441, 0.48, 0.398, 0.432, 0.353, 0.414, 0.352, 0.42, 0.392, 0.413, 0.353, 0.435, 0.346, 0.409, 0.435, 0.45, 0.456, 0.466, 0.498, 0.56, 0.502, 0.552, 0.57, 0.567, 0.594, 0.529, 0.619, 0.602, 0.55, 0.605, 0.592, 0.548, 0.598, 0.519, 0.621, 0.464, 0.512, 0.519, 0.506, 0.529, 0.406, 0.378, 0.511, 0.399, 0.541, 0.4, 0.439, 0.373, 0.423, 0.372, 0.421, 0.402, 0.407, 0.427, 0.451, 0.463]\n",
      "Val Error(all epochs): 7.1998467445373535 \n",
      " [49.153, 48.978, 48.785, 48.588, 48.37, 48.106, 47.816, 47.505, 47.192, 46.779, 46.523, 45.984, 45.637, 45.112, 44.657, 44.506, 44.274, 43.656, 41.882, 42.607, 42.093, 41.444, 41.438, 41.283, 39.378, 39.495, 39.349, 38.982, 36.563, 36.894, 36.804, 34.969, 34.237, 34.418, 32.982, 30.932, 31.402, 28.328, 28.111, 27.089, 25.709, 25.078, 25.29, 24.211, 23.262, 23.556, 20.002, 18.082, 19.215, 17.531, 14.874, 13.784, 12.109, 11.038, 13.284, 12.35, 13.633, 13.201, 12.97, 12.215, 12.034, 9.858, 9.202, 10.356, 8.468, 8.11, 7.864, 7.883, 8.79, 7.51, 8.055, 7.2, 7.602, 7.504, 8.133, 7.597, 7.559, 7.642, 7.523, 7.467, 7.863, 7.649, 7.835, 7.779, 7.712, 7.784, 7.763, 7.746, 7.876, 8.004, 7.971, 7.961, 7.913, 7.981, 7.914, 8.127, 8.003, 8.416, 8.375, 8.364, 8.709, 8.4, 8.329, 8.254, 8.155, 8.165, 8.2, 8.299, 8.051, 8.344, 7.999, 8.468, 8.242, 8.632, 8.299, 8.945, 8.904, 9.155, 8.527, 9.041, 8.827, 8.655, 8.742, 8.399, 8.651, 8.285, 8.657, 8.296, 8.581, 8.354, 8.466, 8.662, 8.568, 8.551, 8.287, 8.542, 8.562, 8.634, 8.785, 8.653, 8.511, 8.358, 8.566, 8.301, 8.436, 8.373, 8.559, 8.165, 8.729, 8.546, 8.585, 8.737, 8.511, 8.517, 8.423, 8.687, 8.09, 8.605, 8.41, 8.459, 8.426, 8.498, 8.327, 8.494, 8.382, 8.34, 8.258, 8.816, 8.096, 8.842, 8.557, 8.956, 8.613, 8.867, 8.526, 8.758, 8.772, 8.651, 8.512, 9.061, 8.346, 9.114, 8.333, 9.014, 8.109, 8.926, 8.456, 8.746, 8.732, 8.767, 8.546, 8.796, 8.467, 8.662, 8.273, 9.035, 8.209, 9.049, 8.637, 9.396, 8.414, 9.459, 8.894, 8.796, 8.828, 9.139, 8.378, 9.28, 8.504, 8.839, 8.72, 8.839, 8.422, 8.823, 8.303, 8.65, 8.57, 8.535, 8.679, 8.476, 8.554, 8.544, 8.188, 8.62, 8.365, 8.456, 8.255, 8.876, 8.528, 8.267, 8.876, 8.584, 8.514, 9.206, 8.406, 8.863, 8.419, 8.738, 8.812, 8.637, 9.087, 8.222, 9.035, 8.299, 8.379, 8.657, 8.337, 8.365, 8.731, 8.342, 8.561, 8.485, 8.743, 8.087, 9.114, 7.938, 8.56, 8.673, 8.519, 8.613, 8.931, 8.301, 8.974, 8.488, 8.749, 8.494, 9.013, 8.212, 9.141, 8.339, 8.64, 8.448, 8.421, 8.418, 8.386, 8.325, 8.365, 8.447, 8.157, 8.447, 8.324, 8.598, 8.545, 8.695, 8.42, 8.622, 8.547, 8.461, 8.563, 8.72, 8.595, 8.401, 8.859, 8.406, 8.738, 8.623, 8.621, 8.815, 8.682, 9.04, 8.942, 8.609, 8.856, 8.811, 9.246, 8.745, 9.275, 8.636, 9.157, 8.841, 8.727, 8.992, 8.691, 8.603, 9.084, 8.437, 8.854, 8.55, 8.74, 8.426, 9.084, 8.238, 9.071, 8.386, 8.512, 8.629, 8.674, 8.328, 8.967, 8.541, 8.448, 8.46, 8.651, 8.46, 8.888, 8.327, 8.68, 8.664, 8.957, 8.563, 8.702, 8.578, 8.923, 9.017, 8.713, 8.97, 8.613, 8.741, 8.598, 8.468, 8.621, 8.417, 8.351, 8.685, 8.191, 8.826, 8.336, 8.728, 8.496, 8.694, 8.455, 8.522, 8.437, 8.375, 8.773, 8.45, 8.806, 8.582, 8.731, 8.86, 8.637, 8.782, 8.702, 8.661, 8.472, 8.664, 8.525, 8.765, 8.515, 8.9, 8.557, 8.681, 8.634, 8.38, 8.877, 8.374, 8.812, 8.545, 8.703, 8.79, 8.543, 8.739, 8.611, 8.823, 8.79, 8.735, 8.893, 9.083, 8.246, 9.006, 8.443, 8.493, 8.793, 8.194, 8.469, 8.671, 8.394, 8.793, 8.427, 8.842, 8.512, 9.024, 8.382, 8.8, 8.384, 8.774, 8.436, 8.637, 8.505, 8.384, 8.612, 8.189, 8.59, 8.395, 8.849, 8.463, 9.097, 8.39, 8.984, 8.64, 8.792, 8.571, 9.056, 8.352, 9.014, 8.439, 8.661, 8.76, 8.342, 8.836, 8.709, 8.342, 8.607, 8.917, 8.544, 8.968, 8.745, 8.743, 8.822, 8.674, 8.597, 8.801, 8.568, 8.414, 8.944, 8.224, 8.778, 8.445, 8.483, 8.283, 8.681, 8.182, 8.628, 8.26, 8.051, 8.57, 8.403, 8.282, 9.0, 8.23, 8.331, 8.32, 8.236, 8.36, 8.385, 8.323, 8.738, 8.43, 8.721, 8.932, 8.442, 8.557, 8.179, 8.18, 8.56, 8.099, 8.364, 8.217, 8.339, 8.633, 8.609, 8.733, 8.667, 8.781, 8.604, 8.923, 8.279, 8.934, 8.14, 8.745]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.022, 0.0, 0.0, 0.037, 0.004, 0.061, 0.199, 0.106, 0.228, 0.355, 0.707, 0.884, 1.022, 0.69, 0.803, 0.613, 0.65, 0.799, 0.746, 0.907, 1.524, 1.337, 1.337, 1.718, 1.902, 2.562, 2.126, 2.033, 2.291, 2.186, 2.363, 2.5, 2.429, 2.097, 2.624, 2.951, 2.535, 2.806, 2.711, 2.471, 2.768, 2.306, 2.754, 2.785, 2.862, 3.313, 2.953, 4.191, 3.07, 3.657, 3.714, 3.378, 3.954, 4.319, 4.149, 3.765, 5.012, 4.459, 5.37, 5.111, 5.126, 4.756, 4.402, 4.192, 3.966, 4.085, 3.567, 4.076, 3.916, 4.093, 3.539, 4.699, 4.788, 4.648, 5.356, 5.725, 5.703, 5.132, 4.825, 5.29, 4.679, 5.229, 4.199, 4.621, 3.882, 4.565, 3.823, 4.455, 3.609, 4.911, 3.964, 5.017, 3.748, 3.982, 4.048, 4.73, 5.638, 5.379, 5.17, 4.902, 4.689, 4.941, 4.52, 4.48, 4.372, 4.591, 3.547, 5.091, 3.859, 5.103, 4.475, 4.481, 4.717, 5.029, 4.66, 4.555, 4.139, 5.001, 4.311, 4.511, 4.855, 4.426, 4.961, 4.701, 4.954, 4.762, 5.07, 4.602, 5.225, 3.957, 5.115, 3.941, 4.274, 4.333, 4.349, 3.697, 4.521, 3.622, 5.277, 3.617, 5.444, 4.194, 4.301, 4.8, 4.148, 4.734, 4.496, 4.039, 4.874, 3.916, 5.362, 4.072, 4.726, 4.311, 5.064, 4.609, 5.742, 4.917, 6.336, 4.643, 5.065, 4.501, 4.894, 3.968, 4.878, 4.512, 5.828, 4.487, 5.063, 5.093, 4.172, 5.256, 4.119, 4.91, 4.297, 4.47, 5.013, 4.629, 4.904, 4.521, 4.58, 4.118, 4.348, 4.32, 4.275, 4.571, 4.834, 4.731, 4.303, 4.762, 5.129, 5.164, 5.388, 4.986, 4.567, 4.643, 4.424, 4.995, 4.698, 4.888, 4.891, 4.458, 4.684, 4.896, 4.696, 4.588, 4.737, 4.758, 4.429, 5.18, 4.488, 4.908, 4.654, 4.919, 4.309, 5.163, 4.387, 5.129, 4.814, 4.675, 4.542, 4.885, 3.773, 5.21, 4.462, 4.806, 4.564, 5.071, 4.617, 5.218, 4.598, 4.774, 4.62, 4.264, 4.45, 4.665, 4.069, 4.846, 4.468, 4.934, 4.996, 4.613, 5.134, 4.535, 4.867, 4.737, 4.117, 4.909, 4.357, 4.29, 4.764, 4.51, 4.636, 4.943, 4.372, 5.27, 4.719, 5.207, 6.0, 5.268, 4.879, 5.541, 4.855, 5.543, 4.968, 4.958, 4.735, 4.767, 4.47, 4.761, 4.749, 4.526, 4.711, 4.766, 4.138, 4.94, 4.148, 4.876, 4.557, 4.921, 4.495, 5.138, 4.353, 4.869, 4.695, 4.576, 4.552, 5.3, 4.167, 5.143, 4.348, 4.979, 4.881, 4.453, 4.593, 4.765, 4.243, 5.046, 4.119, 4.823, 4.755, 4.677, 5.737, 4.655, 5.593, 5.115, 4.646, 5.204, 4.46, 4.953, 4.503, 4.576, 4.35, 4.422, 4.496, 4.435, 4.666, 4.542, 4.338, 4.673, 4.035, 4.535, 4.579, 4.316, 4.711, 4.641, 4.336, 4.701, 4.742, 4.422, 4.887, 4.613, 4.772, 4.577, 4.557, 4.739, 4.477, 5.198, 4.241, 5.249, 4.34, 5.093, 4.209, 4.942, 4.105, 4.898, 4.333, 4.588, 4.716, 4.259, 4.488, 4.569, 4.627, 4.902, 4.929, 4.785, 5.078, 4.347, 5.461, 4.591, 5.057, 4.943, 4.655, 4.529, 4.804, 4.17, 4.886, 4.599, 4.812, 4.875, 4.894, 4.581, 5.097, 4.524, 5.136, 4.691, 4.836, 4.663, 4.46, 4.563, 4.515, 4.547, 4.588, 4.872, 4.502, 5.304, 4.276, 5.076, 4.535, 4.563, 4.444, 4.749, 4.271, 4.566, 4.168, 4.563, 4.779, 4.631, 5.073, 4.833, 4.018, 4.606, 4.3, 4.648, 4.74, 4.861, 4.641, 5.033, 4.312, 4.817, 4.316, 4.447, 4.063, 4.67, 4.018, 4.739, 4.383, 4.229, 4.351, 4.303, 3.897, 4.684, 4.08, 4.178, 4.661, 4.144, 4.67, 4.677, 4.296, 4.785, 3.767, 4.763, 4.272, 3.799, 4.554, 3.821, 4.824, 4.605, 4.682, 4.741, 4.167, 4.242, 3.863, 4.385, 4.338, 4.072, 4.642, 4.247, 4.904, 4.711, 4.601, 4.614, 4.559, 4.309, 4.725, 4.174, 4.754, 4.126, 4.665]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 49.23119, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 49.23119 to 49.16546, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 49.16546 to 49.06392, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 49.06392 to 48.93443, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 48.93443 to 48.78117, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 48.78117 to 48.62810, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 48.62810 to 48.44793, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 48.44793 to 48.24599, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 48.24599 to 48.01090, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 48.01090 to 47.72017, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 47.72017 to 47.41250, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 47.41250 to 46.94366, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 46.94366 to 46.53916, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 46.53916 to 46.21622, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 46.21622 to 45.96220, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 45.96220 to 45.58497, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 45.58497 to 45.05157, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 45.05157 to 44.92100, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 44.92100 to 44.26656, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 44.26656 to 43.71167, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 43.71167 to 43.39970, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 43.39970 to 42.91330, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 42.91330 to 41.50753, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 41.50753\n",
      "\n",
      "Epoch 00025: val_mae improved from 41.50753 to 40.66922, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 40.66922 to 39.22818, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 39.22818 to 38.73388, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 38.73388 to 38.33653, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 38.33653 to 37.39102, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 37.39102 to 35.97158, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00031: val_mae improved from 35.97158 to 35.46002, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00032: val_mae improved from 35.46002 to 33.81239, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 33.81239 to 32.48621, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00034: val_mae improved from 32.48621 to 32.32042, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00035: val_mae improved from 32.32042 to 29.29571, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 29.29571\n",
      "\n",
      "Epoch 00037: val_mae improved from 29.29571 to 29.25063, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00038: val_mae improved from 29.25063 to 26.98643, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00039: val_mae improved from 26.98643 to 26.20651, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 26.20651\n",
      "\n",
      "Epoch 00041: val_mae improved from 26.20651 to 24.98916, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00042: val_mae improved from 24.98916 to 22.41893, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00043: val_mae improved from 22.41893 to 19.60419, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00044: val_mae improved from 19.60419 to 18.11613, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00045: val_mae improved from 18.11613 to 15.21626, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00046: val_mae improved from 15.21626 to 14.52565, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00047: val_mae improved from 14.52565 to 12.25328, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00048: val_mae improved from 12.25328 to 11.34970, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00049: val_mae improved from 11.34970 to 10.83505, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 10.83505\n",
      "\n",
      "Epoch 00070: val_mae improved from 10.83505 to 10.60670, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 10.60670\n",
      "\n",
      "Epoch 00072: val_mae improved from 10.60670 to 10.24741, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 10.24741\n",
      "\n",
      "Epoch 00074: val_mae improved from 10.24741 to 10.11202, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 10.11202\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 10.11202\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 10.11202\n",
      "\n",
      "Epoch 00078: val_mae improved from 10.11202 to 9.99219, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00079: val_mae improved from 9.99219 to 9.54396, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 9.54396\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 9.54396\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 9.54396\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 9.54396\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 9.54396\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 9.54396\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 9.54396\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 9.54396\n",
      "\n",
      "Epoch 00088: val_mae improved from 9.54396 to 9.43905, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 9.43905\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 9.43905\n",
      "\n",
      "Epoch 00091: val_mae improved from 9.43905 to 8.99336, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 8.99336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00154: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 8.99336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00315: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 8.99336\n",
      "\n",
      "Epoch 00369: val_mae improved from 8.99336 to 8.96053, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 8.96053\n",
      "\n",
      "Epoch 00448: val_mae improved from 8.96053 to 8.88445, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 8.88445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00470: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 8.88445\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 8.88445\n",
      "\n",
      "Lambda: 0.1 , Time: 0:03:58\n",
      "Train Error(all epochs): 0.8750079274177551 \n",
      " [49.143, 49.035, 48.96, 48.876, 48.777, 48.663, 48.532, 48.376, 48.195, 47.985, 47.745, 47.47, 47.139, 46.766, 46.337, 45.86, 45.32, 44.734, 44.062, 43.335, 42.555, 41.703, 40.833, 39.917, 38.961, 37.965, 36.931, 35.86, 34.783, 33.67, 32.557, 31.377, 30.207, 28.996, 27.795, 26.568, 25.342, 24.111, 22.906, 21.718, 20.491, 19.301, 18.233, 17.116, 16.113, 15.054, 14.124, 13.145, 12.141, 11.41, 10.665, 9.894, 9.316, 8.647, 8.061, 7.632, 7.347, 6.984, 6.899, 6.642, 6.428, 6.039, 5.69, 5.383, 5.096, 4.878, 4.592, 4.461, 4.049, 4.147, 3.963, 4.024, 3.795, 3.889, 3.92, 3.998, 3.732, 3.774, 3.45, 3.355, 3.077, 3.011, 2.97, 2.918, 2.913, 2.97, 3.07, 3.332, 3.409, 3.611, 3.442, 3.403, 3.235, 3.029, 2.939, 2.858, 2.919, 2.865, 2.749, 2.783, 2.723, 2.802, 2.829, 2.787, 2.635, 2.671, 2.979, 3.04, 2.746, 2.57, 2.501, 2.461, 2.386, 2.252, 2.195, 2.175, 2.202, 2.278, 2.036, 1.985, 2.035, 2.072, 2.126, 2.223, 2.192, 2.229, 2.202, 2.272, 2.379, 2.407, 2.31, 2.511, 2.489, 2.501, 2.513, 2.384, 2.278, 2.152, 2.162, 2.217, 2.1, 2.043, 1.926, 1.911, 1.836, 1.856, 1.81, 1.928, 2.0, 2.128, 2.124, 2.03, 2.149, 2.294, 2.39, 2.392, 2.179, 2.219, 2.218, 2.217, 2.127, 1.86, 1.941, 1.921, 1.875, 1.889, 1.752, 1.77, 1.832, 1.816, 1.922, 1.945, 2.029, 2.072, 1.814, 1.711, 1.616, 1.498, 1.569, 1.516, 1.625, 1.657, 1.827, 1.744, 1.895, 1.985, 2.03, 2.244, 2.031, 1.967, 1.942, 1.94, 1.889, 1.932, 1.969, 1.854, 1.792, 2.083, 2.257, 2.464, 2.243, 1.991, 1.898, 1.834, 1.761, 1.748, 1.745, 1.69, 1.534, 1.504, 1.449, 1.425, 1.386, 1.429, 1.564, 1.631, 1.804, 1.883, 1.733, 1.731, 1.724, 1.785, 1.623, 1.599, 1.571, 1.561, 1.583, 1.689, 1.843, 1.834, 1.699, 1.755, 1.529, 1.505, 1.583, 1.784, 2.14, 2.051, 2.057, 2.032, 2.138, 2.18, 2.245, 2.084, 2.466, 2.479, 2.521, 2.223, 2.241, 2.213, 2.147, 2.065, 2.086, 1.927, 1.766, 1.702, 1.502, 1.374, 1.273, 1.259, 1.142, 1.149, 1.087, 1.156, 1.154, 1.273, 1.323, 1.457, 1.5, 1.541, 1.553, 1.542, 1.461, 1.522, 1.473, 1.623, 1.647, 1.731, 1.589, 1.661, 1.707, 1.862, 1.887, 1.973, 1.902, 1.924, 1.77, 1.729, 1.851, 1.97, 1.971, 2.026, 1.831, 1.769, 1.62, 1.635, 1.619, 1.635, 1.489, 1.463, 1.32, 1.288, 1.204, 1.227, 1.163, 1.251, 1.247, 1.377, 1.428, 1.554, 1.564, 1.492, 1.56, 1.527, 1.473, 1.376, 1.487, 1.577, 1.613, 1.74, 1.781, 1.736, 1.575, 1.529, 1.41, 1.329, 1.323, 1.518, 1.53, 1.563, 1.572, 1.599, 1.596, 1.588, 1.547, 1.518, 1.471, 1.494, 1.381, 1.282, 1.212, 1.173, 1.174, 1.259, 1.238, 1.357, 1.324, 1.376, 1.351, 1.336, 1.335, 1.293, 1.284, 1.339, 1.432, 1.38, 1.36, 1.33, 1.501, 1.68, 1.663, 1.775, 1.546, 1.503, 1.509, 1.508, 1.542, 1.776, 1.815, 1.857, 1.79, 1.81, 1.804, 1.756, 1.696, 1.66, 1.538, 1.684, 1.66, 1.717, 1.655, 1.683, 1.627, 1.688, 1.608, 1.508, 1.454, 1.354, 1.622, 1.688, 1.694, 1.634, 1.592, 1.432, 1.251, 1.267, 1.276, 1.243, 1.334, 1.244, 1.246, 1.085, 1.12, 1.148, 1.297, 1.363, 1.455, 1.4, 1.477, 1.401, 1.471, 1.493, 1.677, 1.625, 1.762, 1.714, 1.709, 1.761, 1.787, 1.906, 2.029, 2.014, 1.886, 1.577, 1.38, 1.279, 1.223, 1.211, 1.23, 1.134, 1.096, 0.97, 0.97, 0.938, 0.995, 1.099, 1.19, 1.152, 1.166, 1.176, 1.275, 1.379, 1.525, 1.627, 1.748, 1.865, 2.157, 1.855, 1.998, 2.062, 2.123, 1.961, 1.788, 1.69, 1.561, 1.421, 1.412, 1.382, 1.487, 1.544, 1.662, 1.64, 1.66, 1.575, 1.521, 1.336, 1.325, 1.34, 1.306, 1.271, 1.23, 1.136, 1.057, 1.118, 1.192, 1.281, 1.22, 1.195, 1.17, 1.136, 1.101, 1.07, 0.996, 0.949, 0.911, 0.879, 0.875, 0.895, 0.978, 1.071, 1.138, 1.286, 1.29, 1.292, 1.346, 1.501, 1.507, 1.642, 1.599, 1.626]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.014, 0.027, 0.093, 0.065, 0.091, 0.106, 0.134, 0.195, 0.198, 0.244, 0.324, 0.362, 0.362, 0.658, 0.607, 0.943, 0.876, 1.023, 1.035, 1.018, 1.029, 1.013, 1.078, 1.096, 1.106, 0.998, 1.197, 1.236, 1.254, 1.302, 1.256, 1.429, 1.565, 1.282, 1.481, 1.244, 1.345, 1.142, 1.139, 1.167, 1.093, 1.162, 1.22, 1.211, 1.513, 1.382, 1.561, 1.442, 1.478, 1.338, 1.244, 1.265, 1.213, 1.191, 1.234, 1.15, 1.23, 1.148, 1.29, 1.195, 1.273, 1.072, 1.224, 1.266, 1.52, 1.212, 1.215, 1.045, 1.108, 0.968, 1.02, 0.894, 0.978, 0.91, 1.133, 0.862, 0.872, 0.859, 0.935, 0.924, 1.015, 0.938, 1.104, 0.968, 1.048, 1.102, 1.054, 1.051, 1.203, 1.104, 1.202, 1.201, 1.06, 1.066, 0.977, 0.936, 1.07, 0.889, 0.971, 0.875, 0.833, 0.897, 0.808, 0.87, 0.9, 0.949, 0.933, 1.036, 0.893, 1.078, 0.982, 1.244, 1.036, 1.031, 1.045, 1.105, 1.051, 1.064, 0.805, 0.899, 0.877, 0.85, 0.918, 0.796, 0.847, 0.845, 0.816, 0.919, 0.94, 0.908, 1.127, 0.759, 0.853, 0.76, 0.663, 0.752, 0.702, 0.747, 0.789, 0.846, 0.819, 0.907, 0.995, 0.955, 1.168, 0.855, 1.002, 0.859, 0.995, 0.873, 0.974, 0.893, 0.895, 0.8, 1.084, 1.047, 1.273, 0.961, 0.92, 0.877, 0.827, 0.821, 0.829, 0.862, 0.819, 0.672, 0.722, 0.625, 0.679, 0.674, 0.593, 0.809, 0.78, 0.821, 1.002, 0.789, 0.809, 0.852, 0.853, 0.742, 0.757, 0.719, 0.723, 0.744, 0.778, 0.914, 0.897, 0.818, 0.841, 0.715, 0.664, 0.757, 0.829, 1.174, 0.894, 1.045, 0.976, 1.003, 0.989, 1.229, 0.816, 1.173, 1.16, 1.22, 0.942, 1.124, 0.979, 1.083, 0.84, 1.008, 0.884, 0.842, 0.76, 0.757, 0.64, 0.607, 0.596, 0.536, 0.572, 0.478, 0.623, 0.518, 0.693, 0.596, 0.781, 0.673, 0.795, 0.684, 0.817, 0.6, 0.821, 0.604, 0.83, 0.678, 0.914, 0.67, 0.835, 0.773, 0.956, 0.866, 1.034, 0.826, 0.926, 0.803, 0.778, 0.922, 0.976, 0.866, 1.017, 0.804, 0.879, 0.714, 0.817, 0.778, 0.79, 0.641, 0.723, 0.558, 0.654, 0.508, 0.651, 0.48, 0.683, 0.54, 0.75, 0.622, 0.861, 0.641, 0.814, 0.675, 0.763, 0.687, 0.666, 0.701, 0.789, 0.744, 0.886, 0.849, 0.829, 0.705, 0.771, 0.574, 0.664, 0.604, 0.729, 0.754, 0.789, 0.679, 0.791, 0.789, 0.693, 0.795, 0.751, 0.654, 0.788, 0.566, 0.664, 0.513, 0.58, 0.504, 0.695, 0.51, 0.749, 0.571, 0.667, 0.615, 0.644, 0.593, 0.699, 0.534, 0.692, 0.623, 0.689, 0.613, 0.627, 0.733, 0.88, 0.713, 0.909, 0.622, 0.707, 0.691, 0.717, 0.65, 0.95, 0.732, 0.998, 0.805, 0.884, 0.793, 0.892, 0.774, 0.785, 0.674, 0.88, 0.817, 0.838, 0.728, 0.9, 0.668, 0.922, 0.695, 0.693, 0.708, 0.605, 0.779, 0.821, 0.78, 0.772, 0.73, 0.668, 0.608, 0.58, 0.634, 0.565, 0.709, 0.547, 0.645, 0.487, 0.552, 0.524, 0.624, 0.631, 0.729, 0.627, 0.73, 0.638, 0.733, 0.662, 0.861, 0.711, 0.895, 0.814, 0.782, 0.926, 0.843, 0.891, 0.958, 0.994, 0.857, 0.774, 0.673, 0.557, 0.595, 0.576, 0.572, 0.516, 0.553, 0.383, 0.531, 0.386, 0.513, 0.49, 0.607, 0.486, 0.624, 0.488, 0.637, 0.629, 0.718, 0.769, 0.852, 0.757, 1.206, 0.807, 0.888, 0.985, 1.057, 0.8, 0.929, 0.723, 0.769, 0.636, 0.692, 0.637, 0.776, 0.674, 0.848, 0.788, 0.792, 0.721, 0.814, 0.576, 0.645, 0.673, 0.591, 0.576, 0.61, 0.523, 0.508, 0.571, 0.552, 0.651, 0.556, 0.576, 0.57, 0.514, 0.502, 0.532, 0.454, 0.472, 0.408, 0.434, 0.391, 0.447, 0.439, 0.563, 0.503, 0.667, 0.56, 0.642, 0.598, 0.811, 0.618, 0.887, 0.716, 0.787]\n",
      "Val Error(all epochs): 8.884453773498535 \n",
      " [49.231, 49.165, 49.064, 48.934, 48.781, 48.628, 48.448, 48.246, 48.011, 47.72, 47.413, 46.944, 46.539, 46.216, 45.962, 45.585, 45.052, 44.921, 44.267, 43.712, 43.4, 42.913, 41.508, 41.586, 40.669, 39.228, 38.734, 38.337, 37.391, 35.972, 35.46, 33.812, 32.486, 32.32, 29.296, 29.589, 29.251, 26.986, 26.207, 26.443, 24.989, 22.419, 19.604, 18.116, 15.216, 14.526, 12.253, 11.35, 10.835, 12.95, 12.976, 12.54, 14.051, 12.832, 11.407, 12.024, 13.255, 11.014, 11.14, 11.824, 12.388, 12.464, 13.346, 12.57, 12.694, 12.067, 12.345, 11.433, 10.987, 10.607, 10.676, 10.247, 10.859, 10.112, 10.789, 10.787, 10.799, 9.992, 9.544, 9.688, 9.873, 9.629, 10.088, 10.033, 10.213, 10.37, 10.317, 9.439, 9.595, 9.645, 8.993, 9.166, 9.45, 9.085, 9.956, 9.451, 10.412, 10.357, 10.466, 9.901, 10.873, 10.018, 10.969, 10.219, 10.642, 10.301, 9.554, 10.098, 9.645, 9.831, 10.381, 9.735, 10.093, 9.914, 9.758, 9.915, 9.639, 9.253, 9.358, 9.469, 9.193, 9.707, 9.323, 9.962, 9.352, 9.687, 9.51, 9.755, 9.623, 9.638, 9.567, 9.729, 9.504, 9.911, 10.065, 9.612, 10.197, 9.892, 10.015, 9.952, 9.781, 9.544, 9.855, 9.599, 9.649, 9.642, 9.167, 9.668, 9.318, 9.582, 9.326, 9.938, 9.525, 10.063, 9.685, 9.694, 10.041, 9.662, 10.242, 9.792, 9.783, 9.77, 9.391, 9.57, 9.655, 9.35, 9.682, 9.349, 9.72, 9.352, 9.652, 9.692, 9.396, 9.782, 9.718, 9.962, 9.384, 9.607, 9.541, 9.366, 9.518, 9.575, 9.702, 9.524, 10.042, 9.665, 10.429, 9.815, 9.908, 9.67, 9.885, 9.469, 10.029, 9.347, 9.603, 9.75, 9.578, 9.61, 9.659, 9.822, 9.921, 9.894, 9.891, 9.84, 9.332, 9.64, 9.615, 9.36, 9.413, 9.47, 9.381, 9.439, 9.639, 9.339, 9.675, 9.537, 9.401, 9.923, 9.528, 9.69, 9.53, 9.83, 10.038, 9.459, 9.939, 9.693, 9.392, 9.928, 9.212, 9.65, 9.275, 9.505, 9.353, 9.454, 9.485, 9.869, 9.571, 9.484, 9.837, 9.112, 10.047, 9.895, 9.621, 9.579, 10.177, 11.129, 10.569, 11.016, 10.124, 11.752, 10.501, 10.933, 10.419, 10.457, 10.2, 10.294, 9.837, 9.999, 9.776, 9.805, 9.614, 9.637, 9.466, 9.652, 9.331, 9.747, 9.367, 9.805, 9.403, 9.726, 9.374, 9.55, 9.413, 9.198, 9.551, 9.327, 9.467, 9.462, 9.388, 9.608, 9.416, 9.35, 9.634, 9.556, 9.218, 9.583, 9.942, 9.662, 9.727, 9.476, 10.31, 9.737, 9.825, 10.25, 9.188, 10.039, 9.343, 9.518, 9.286, 9.545, 9.249, 9.508, 9.177, 9.312, 9.241, 9.255, 9.374, 9.301, 9.567, 9.439, 9.651, 9.322, 9.794, 9.378, 9.583, 9.634, 9.397, 9.629, 9.365, 9.405, 9.461, 9.341, 9.692, 9.564, 9.365, 9.667, 9.477, 9.416, 9.512, 9.261, 9.256, 9.645, 9.277, 9.583, 9.428, 9.336, 9.603, 9.379, 9.574, 9.365, 9.487, 9.497, 9.182, 9.428, 9.207, 9.294, 9.213, 9.365, 9.276, 9.318, 9.403, 9.407, 9.511, 9.577, 9.378, 9.501, 9.355, 9.618, 9.176, 9.322, 9.406, 9.289, 9.114, 9.535, 9.136, 9.627, 9.459, 9.407, 8.961, 9.821, 8.976, 9.758, 9.178, 9.576, 9.138, 9.683, 9.014, 9.763, 9.369, 9.951, 9.451, 9.376, 9.621, 9.439, 9.409, 9.39, 9.805, 9.378, 9.795, 9.399, 9.558, 9.568, 9.502, 9.763, 9.422, 9.399, 9.651, 8.997, 9.569, 9.233, 9.481, 9.236, 9.34, 9.353, 9.409, 9.229, 9.563, 9.166, 9.376, 9.349, 9.33, 9.528, 9.385, 9.277, 9.357, 9.67, 9.433, 9.787, 9.485, 10.171, 9.748, 9.464, 10.03, 9.785, 9.605, 9.784, 9.254, 9.532, 9.211, 9.476, 9.252, 9.344, 9.328, 9.193, 9.322, 9.122, 9.221, 9.099, 9.274, 8.988, 9.348, 8.961, 9.161, 9.174, 9.656, 9.081, 9.257, 8.884, 9.49, 9.456, 9.076, 9.52, 9.621, 9.443, 9.456, 9.029, 9.351, 9.048, 9.111, 9.323, 9.326, 9.326, 9.609, 9.237, 9.397, 9.297, 9.197, 9.731, 9.346, 9.278, 9.382, 9.134, 9.43, 9.073, 9.382, 9.2, 9.299, 9.251, 9.205, 9.269, 9.313, 9.273, 9.252, 9.179, 9.161, 9.2, 9.146, 9.118, 9.279, 9.185, 9.059, 9.218, 9.023, 9.4, 9.057, 9.745, 9.05, 9.453, 8.944, 9.017]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013, 0.044, 0.027, 0.064, 0.181, 0.307, 0.365, 0.827, 1.152, 1.122, 1.32, 1.417, 0.984, 1.04, 1.042, 0.954, 1.01, 1.498, 1.484, 1.189, 3.027, 3.999, 3.841, 6.077, 7.643, 8.191, 8.144, 7.253, 6.393, 6.259, 4.33, 4.365, 3.908, 4.475, 4.277, 5.213, 3.771, 5.446, 3.424, 5.35, 3.579, 3.736, 3.455, 3.296, 3.816, 3.725, 2.975, 3.015, 2.794, 3.587, 3.545, 4.355, 5.656, 5.023, 4.679, 5.149, 4.235, 5.549, 4.612, 6.478, 6.203, 6.36, 5.412, 6.875, 6.027, 6.84, 5.975, 6.901, 5.037, 6.008, 6.326, 5.66, 5.983, 6.56, 5.265, 5.544, 4.908, 5.016, 4.87, 5.412, 4.347, 4.152, 3.472, 3.337, 3.499, 3.367, 3.369, 3.928, 3.981, 3.892, 5.211, 4.076, 4.987, 5.438, 4.998, 5.125, 5.701, 5.018, 4.811, 5.865, 4.671, 5.875, 5.049, 5.433, 5.285, 5.165, 5.581, 5.174, 4.706, 4.514, 4.748, 3.409, 4.433, 3.457, 4.7, 2.867, 5.353, 3.959, 4.788, 5.24, 5.085, 5.679, 5.596, 5.525, 5.033, 4.33, 4.094, 4.576, 3.653, 4.213, 3.717, 3.981, 3.899, 4.374, 3.81, 5.198, 5.196, 5.899, 5.925, 4.931, 5.049, 4.426, 4.505, 3.918, 4.462, 3.917, 4.075, 5.035, 4.445, 6.381, 4.471, 5.811, 3.987, 5.218, 4.248, 4.934, 3.692, 4.028, 3.844, 4.586, 4.369, 5.604, 5.479, 6.035, 5.614, 6.146, 5.245, 4.937, 5.294, 4.898, 4.173, 4.46, 3.836, 3.929, 3.904, 3.61, 3.551, 4.597, 3.621, 5.074, 5.428, 5.13, 4.628, 4.324, 4.641, 4.891, 4.95, 5.242, 5.476, 4.707, 5.45, 4.181, 4.714, 4.434, 4.301, 4.582, 4.762, 4.107, 5.741, 4.5, 5.37, 3.84, 4.522, 3.566, 5.173, 4.421, 5.223, 6.207, 8.055, 6.58, 8.072, 6.368, 8.867, 6.858, 7.513, 6.41, 6.76, 5.44, 5.894, 5.03, 4.995, 5.026, 5.065, 4.972, 4.595, 4.789, 4.444, 4.542, 4.601, 4.733, 5.059, 5.034, 4.797, 5.182, 4.505, 5.118, 3.695, 5.378, 3.558, 5.256, 3.736, 4.698, 3.66, 4.15, 3.632, 5.452, 4.392, 4.748, 4.937, 5.444, 5.968, 5.905, 4.989, 6.672, 5.525, 6.469, 5.219, 4.581, 4.897, 4.915, 4.728, 4.475, 4.238, 4.886, 3.783, 4.254, 3.672, 4.207, 3.59, 4.245, 3.355, 4.443, 3.488, 4.669, 3.628, 3.869, 3.851, 3.708, 3.926, 3.589, 4.282, 4.098, 3.773, 5.043, 3.493, 5.452, 4.231, 4.712, 4.145, 3.66, 4.369, 4.874, 4.146, 4.182, 4.78, 3.369, 4.381, 4.913, 4.005, 5.103, 3.229, 4.395, 3.503, 4.463, 4.223, 4.667, 4.455, 4.459, 4.916, 4.327, 4.465, 4.297, 3.811, 4.198, 3.234, 4.164, 3.159, 3.817, 4.235, 3.718, 3.724, 3.694, 4.201, 5.472, 4.495, 4.013, 3.219, 3.83, 3.761, 5.142, 3.695, 3.993, 3.537, 3.675, 3.473, 4.026, 4.362, 3.838, 4.079, 3.955, 3.424, 4.021, 3.096, 4.502, 3.419, 4.933, 3.954, 3.184, 3.855, 3.276, 3.182, 3.76, 3.688, 4.211, 4.547, 4.732, 4.964, 4.185, 4.919, 4.267, 4.199, 4.458, 4.125, 5.008, 4.357, 4.537, 5.206, 4.067, 4.757, 3.383, 4.379, 3.338, 4.929, 3.33, 5.322, 4.173, 5.14, 4.669, 5.492, 4.984, 5.215, 4.836, 6.138, 5.8, 5.05, 5.354, 5.547, 5.208, 4.875, 4.126, 4.61, 3.857, 4.926, 3.547, 4.769, 3.63, 4.03, 3.75, 4.024, 3.661, 4.653, 3.996, 4.476, 4.501, 3.82, 3.774, 4.947, 3.067, 4.671, 4.249, 3.949, 3.745, 5.48, 4.455, 5.631, 5.178, 5.733, 4.661, 4.681, 4.327, 4.513, 4.321, 4.929, 3.921, 4.73, 3.248, 4.255, 4.072, 4.089, 3.527, 3.196, 3.389, 3.86, 4.101, 3.452, 4.267, 3.442, 4.025, 3.87, 3.32, 3.814, 3.788, 4.208, 4.076, 4.176, 4.421, 3.926, 4.251, 3.971, 3.898, 3.552, 3.446, 3.198, 3.79, 3.206, 3.978, 2.931, 3.673, 2.776, 3.611, 3.478, 3.659, 3.504]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 49.17677, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 49.17677 to 49.06927, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 49.06927 to 48.98330, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 48.98330 to 48.89344, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 48.89344 to 48.82903, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 48.82903 to 48.75321, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 48.75321 to 48.65572, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 48.65572 to 48.54297, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 48.54297 to 48.39785, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 48.39785 to 48.20236, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 48.20236 to 48.01864, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 48.01864 to 47.68073, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 47.68073 to 47.40057, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 47.40057 to 47.04354, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 47.04354 to 46.59236, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 46.59236 to 46.26769, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 46.26769 to 45.90756, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 45.90756 to 45.15021, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 45.15021 to 44.45287, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 44.45287 to 43.11023, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 43.11023 to 42.10675, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 42.10675 to 40.99076, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 40.99076 to 39.66267, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 39.66267 to 38.88002, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 38.88002 to 37.12117, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 37.12117 to 36.39169, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 36.39169 to 34.77382, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 34.77382 to 33.47594, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 33.47594 to 32.07755, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 32.07755 to 30.12335, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00031: val_mae improved from 30.12335 to 29.93160, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00032: val_mae improved from 29.93160 to 27.58259, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 27.58259 to 26.09012, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00034: val_mae improved from 26.09012 to 25.11464, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00035: val_mae improved from 25.11464 to 23.85067, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00036: val_mae improved from 23.85067 to 21.66127, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00037: val_mae improved from 21.66127 to 20.93328, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 20.93328\n",
      "\n",
      "Epoch 00039: val_mae improved from 20.93328 to 17.03073, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_mae did not improve from 17.03073\n",
      "\n",
      "Epoch 00041: val_mae improved from 17.03073 to 15.87056, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00042: val_mae improved from 15.87056 to 13.63327, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00043: val_mae improved from 13.63327 to 13.60737, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 13.60737\n",
      "\n",
      "Epoch 00045: val_mae improved from 13.60737 to 10.59635, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00046: val_mae improved from 10.59635 to 9.80683, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00047: val_mae improved from 9.80683 to 8.59334, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00048: val_mae improved from 8.59334 to 8.51067, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 8.51067\n",
      "\n",
      "Epoch 00084: val_mae improved from 8.51067 to 8.26000, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 8.26000\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 8.26000\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 8.26000\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 8.26000\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 8.26000\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 8.26000\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 8.26000\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 8.26000\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 8.26000\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 8.26000\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 8.26000\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 8.26000\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 8.26000\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 8.26000\n",
      "\n",
      "Epoch 00099: val_mae improved from 8.26000 to 7.83529, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 7.83529\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 7.83529\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 7.83529\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 7.83529\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 7.83529\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 7.83529\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 7.83529\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 7.83529\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 7.83529\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 7.83529\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 7.83529\n",
      "\n",
      "Epoch 00111: val_mae improved from 7.83529 to 7.81209, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 7.81209\n",
      "\n",
      "Epoch 00153: val_mae improved from 7.81209 to 7.68549, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 7.68549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00166: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 7.68549\n",
      "\n",
      "Epoch 00215: val_mae improved from 7.68549 to 7.65825, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 7.65825\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 7.65825\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 7.65825\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 7.65825\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 7.65825\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 7.65825\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 7.65825\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 7.65825\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 7.65825\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 7.65825\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 7.65825\n",
      "\n",
      "Epoch 00227: val_mae improved from 7.65825 to 7.39113, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 7.39113\n",
      "\n",
      "Epoch 00285: val_mae improved from 7.39113 to 7.29201, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 7.29201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00318: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 7.29201\n",
      "\n",
      "Epoch 00453: val_mae improved from 7.29201 to 7.25207, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_0/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 7.25207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00476: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 7.25207\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 7.25207\n",
      "\n",
      "Lambda: 1 , Time: 0:03:57\n",
      "Train Error(all epochs): 1.775514841079712 \n",
      " [49.144, 49.041, 48.965, 48.885, 48.795, 48.69, 48.57, 48.424, 48.255, 48.056, 47.821, 47.537, 47.202, 46.823, 46.373, 45.887, 45.285, 44.636, 43.923, 43.142, 42.294, 41.415, 40.473, 39.485, 38.449, 37.41, 36.303, 35.211, 34.087, 32.937, 31.76, 30.584, 29.377, 28.194, 26.977, 25.773, 24.582, 23.373, 22.25, 21.044, 19.927, 18.792, 17.703, 16.648, 15.735, 14.75, 13.789, 12.971, 12.151, 11.442, 10.608, 10.102, 9.659, 9.049, 8.555, 8.349, 7.806, 7.547, 7.157, 6.889, 6.648, 6.518, 6.052, 6.111, 6.179, 5.988, 5.928, 5.784, 5.634, 5.403, 4.997, 4.793, 4.738, 4.666, 4.768, 4.986, 4.857, 4.938, 4.715, 5.108, 4.929, 4.624, 4.548, 4.41, 4.517, 4.483, 4.712, 4.47, 4.443, 4.278, 4.06, 3.923, 3.805, 3.957, 3.929, 3.754, 3.816, 3.921, 3.918, 3.835, 4.043, 3.815, 3.75, 4.073, 3.726, 3.846, 3.792, 3.618, 3.577, 3.671, 3.505, 3.508, 3.448, 3.301, 3.581, 3.724, 3.441, 3.616, 3.578, 3.366, 3.567, 3.701, 3.636, 3.507, 3.385, 3.374, 3.445, 3.361, 3.355, 3.192, 3.118, 3.191, 3.295, 3.319, 3.285, 3.397, 3.369, 3.528, 3.504, 3.192, 3.391, 3.246, 3.234, 3.162, 2.953, 2.89, 2.744, 2.768, 2.904, 3.056, 3.081, 3.311, 3.403, 3.586, 3.807, 3.463, 3.561, 3.591, 3.272, 3.119, 3.182, 3.016, 3.258, 3.436, 3.257, 3.05, 3.305, 3.319, 3.412, 3.322, 3.028, 3.171, 2.851, 2.685, 2.672, 2.712, 2.778, 2.981, 2.723, 2.585, 3.092, 3.033, 3.145, 3.17, 3.079, 2.785, 2.753, 2.6, 2.679, 2.863, 3.172, 3.282, 3.21, 3.103, 2.876, 2.939, 3.102, 2.909, 3.049, 2.75, 2.66, 2.607, 2.619, 2.6, 2.702, 2.57, 2.562, 2.593, 2.625, 2.634, 2.737, 2.76, 2.659, 2.836, 2.707, 2.577, 2.49, 2.503, 2.727, 2.754, 2.832, 3.082, 3.043, 3.06, 2.919, 2.78, 2.727, 2.892, 3.02, 2.951, 2.883, 2.807, 2.691, 2.696, 2.674, 2.749, 2.534, 2.783, 2.769, 2.696, 2.867, 2.664, 2.623, 2.754, 2.652, 2.746, 2.809, 2.954, 3.073, 3.038, 3.099, 2.909, 2.628, 2.449, 2.653, 2.596, 2.873, 2.641, 2.684, 2.663, 2.78, 2.751, 2.662, 2.329, 2.512, 2.61, 2.434, 2.256, 2.3, 2.218, 2.402, 2.477, 2.665, 2.667, 2.556, 2.432, 2.53, 2.316, 2.187, 2.181, 2.259, 2.233, 2.49, 2.631, 2.576, 2.576, 2.828, 2.688, 2.918, 2.888, 2.665, 2.867, 3.134, 2.851, 3.047, 2.902, 3.125, 2.898, 2.685, 2.559, 2.508, 2.561, 2.407, 2.324, 2.34, 2.421, 2.759, 2.936, 2.968, 2.71, 2.563, 2.776, 2.659, 2.842, 2.555, 2.56, 2.373, 2.385, 2.5, 2.566, 2.274, 2.461, 2.47, 2.447, 2.533, 2.414, 2.535, 2.691, 2.62, 2.716, 2.513, 2.511, 2.615, 2.394, 2.384, 2.34, 2.321, 2.353, 2.197, 2.144, 2.073, 2.125, 2.137, 2.142, 2.256, 2.294, 2.234, 2.418, 2.389, 2.396, 2.479, 2.403, 2.629, 2.446, 2.43, 2.538, 2.673, 2.732, 2.699, 2.495, 2.505, 2.453, 2.327, 2.394, 2.537, 2.445, 2.579, 2.553, 2.637, 2.619, 2.566, 2.403, 2.43, 2.395, 2.216, 2.226, 2.248, 2.132, 2.339, 2.343, 2.309, 2.183, 2.064, 1.962, 1.967, 1.984, 2.367, 2.359, 2.399, 2.358, 2.494, 2.28, 2.554, 2.671, 2.897, 2.719, 2.788, 2.61, 2.567, 2.457, 2.411, 2.188, 2.217, 2.269, 2.46, 2.425, 2.364, 2.326, 2.331, 2.277, 2.436, 2.475, 2.197, 2.171, 2.326, 2.29, 2.388, 2.405, 2.415, 2.509, 2.771, 2.474, 2.53, 2.541, 2.628, 2.553, 2.39, 2.466, 2.351, 2.504, 2.486, 2.422, 2.555, 2.723, 2.68, 2.489, 2.158, 2.172, 2.073, 2.196, 2.236, 2.184, 2.074, 1.833, 1.776, 1.892, 1.966, 2.08, 2.093, 2.141, 2.03, 1.997, 2.244, 2.354, 2.296, 2.269, 2.446, 2.262, 2.519, 2.449, 2.543, 2.376, 2.23, 2.26, 2.345, 2.283, 2.462, 2.597, 2.767, 2.375, 2.424, 2.314, 2.258, 2.177, 1.912, 1.9, 1.795, 1.957, 2.214, 2.422, 2.095, 2.183, 2.133, 2.162, 2.189, 2.268, 2.297, 2.39, 2.333, 2.55, 2.309, 2.342, 2.352, 2.14, 1.981, 2.207, 2.382, 2.406, 2.425, 2.331]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.017, 0.026, 0.022, 0.042, 0.038, 0.091, 0.108, 0.108, 0.144, 0.204, 0.23, 0.233, 0.328, 0.444, 0.509, 0.555, 0.733, 0.762, 0.907, 0.921, 1.015, 1.084, 1.212, 1.188, 1.244, 1.501, 1.504, 1.622, 1.608, 1.582, 1.596, 1.484, 1.348, 1.341, 1.319, 1.405, 1.498, 1.446, 1.58, 1.472, 1.652, 1.641, 1.522, 1.524, 1.379, 1.485, 1.469, 1.654, 1.599, 1.534, 1.535, 1.454, 1.281, 1.249, 1.466, 1.364, 1.286, 1.412, 1.392, 1.437, 1.35, 1.571, 1.374, 1.392, 1.549, 1.279, 1.478, 1.392, 1.31, 1.388, 1.442, 1.232, 1.278, 1.328, 1.177, 1.505, 1.477, 1.134, 1.598, 1.354, 1.185, 1.407, 1.47, 1.297, 1.428, 1.305, 1.27, 1.328, 1.293, 1.253, 1.339, 1.18, 1.194, 1.347, 1.319, 1.36, 1.363, 1.171, 1.625, 1.284, 1.164, 1.452, 1.207, 1.164, 1.324, 1.041, 1.121, 1.102, 0.965, 1.222, 1.284, 1.199, 1.489, 1.283, 1.492, 1.643, 1.32, 1.371, 1.462, 1.293, 1.197, 1.27, 1.249, 1.322, 1.412, 1.306, 1.165, 1.378, 1.28, 1.318, 1.514, 1.174, 1.236, 1.112, 1.119, 0.999, 1.089, 1.12, 1.272, 1.059, 1.065, 1.343, 1.141, 1.283, 1.325, 1.127, 1.204, 1.074, 0.957, 1.139, 1.143, 1.315, 1.333, 1.425, 1.236, 1.066, 1.258, 1.239, 1.167, 1.367, 1.064, 1.013, 1.154, 0.986, 1.058, 1.08, 1.043, 0.984, 1.184, 0.985, 1.076, 1.131, 1.115, 1.079, 1.184, 1.013, 1.064, 1.058, 0.827, 1.161, 1.202, 1.079, 1.416, 1.163, 1.277, 1.204, 1.059, 1.07, 1.181, 1.269, 1.206, 1.196, 1.273, 0.946, 1.2, 1.139, 1.086, 1.006, 1.206, 1.101, 1.096, 1.269, 1.038, 1.055, 1.128, 1.045, 1.149, 1.135, 1.253, 1.408, 1.163, 1.276, 1.226, 0.992, 1.048, 1.105, 0.988, 1.175, 1.176, 1.122, 1.036, 1.182, 1.103, 1.128, 0.921, 0.964, 1.181, 0.927, 0.933, 0.874, 0.853, 1.084, 0.973, 1.066, 1.203, 1.063, 0.895, 1.149, 0.933, 0.804, 0.898, 0.868, 0.859, 1.068, 1.121, 1.005, 1.197, 1.181, 1.016, 1.237, 1.318, 1.052, 1.249, 1.322, 1.156, 1.255, 1.146, 1.391, 1.212, 1.107, 1.017, 1.008, 1.08, 1.067, 0.953, 0.959, 0.983, 1.15, 1.26, 1.331, 0.964, 1.079, 1.233, 1.09, 1.265, 1.069, 0.991, 1.091, 0.959, 0.964, 1.095, 0.896, 1.109, 1.061, 0.893, 1.223, 0.965, 1.075, 1.157, 1.066, 1.135, 1.074, 0.94, 1.161, 0.9, 1.102, 0.895, 0.953, 0.954, 0.925, 0.864, 0.805, 0.954, 0.799, 0.904, 0.968, 0.918, 0.904, 1.071, 0.859, 1.067, 1.085, 0.969, 1.127, 1.018, 1.011, 1.227, 0.995, 1.171, 1.168, 0.959, 1.142, 1.06, 0.895, 1.084, 1.085, 0.945, 1.147, 1.029, 1.09, 1.175, 1.041, 0.998, 0.939, 1.075, 0.894, 0.885, 0.946, 0.818, 0.918, 1.156, 0.932, 0.945, 0.778, 0.884, 0.797, 0.8, 0.989, 0.956, 1.041, 0.952, 1.152, 0.985, 1.045, 1.18, 1.229, 1.025, 1.314, 1.196, 1.006, 0.998, 1.02, 0.876, 0.973, 0.913, 1.03, 1.076, 0.967, 0.998, 1.015, 1.0, 0.954, 1.131, 0.933, 0.814, 1.074, 0.934, 0.991, 0.991, 1.078, 1.098, 1.213, 1.061, 0.968, 1.112, 1.22, 1.106, 0.968, 1.095, 0.881, 1.211, 1.002, 1.011, 1.105, 1.162, 1.284, 0.964, 0.936, 0.918, 0.822, 1.001, 0.909, 0.952, 0.866, 0.783, 0.682, 0.829, 0.864, 0.793, 0.935, 0.878, 0.835, 0.816, 0.986, 0.975, 1.028, 1.0, 1.051, 0.994, 0.99, 1.089, 1.076, 1.019, 0.968, 0.993, 1.028, 0.953, 1.039, 1.105, 1.363, 0.988, 1.06, 0.93, 0.881, 0.974, 0.906, 0.727, 0.742, 0.834, 1.018, 1.066, 0.835, 0.997, 0.898, 0.952, 0.96, 0.929, 1.051, 0.944, 1.029, 1.217, 0.972, 0.98, 1.002, 0.834, 0.899, 1.003, 1.017, 1.083, 1.056, 0.999]\n",
      "Val Error(all epochs): 7.252073287963867 \n",
      " [49.177, 49.069, 48.983, 48.893, 48.829, 48.753, 48.656, 48.543, 48.398, 48.202, 48.019, 47.681, 47.401, 47.044, 46.592, 46.268, 45.908, 45.15, 44.453, 43.11, 42.107, 40.991, 39.663, 38.88, 37.121, 36.392, 34.774, 33.476, 32.078, 30.123, 29.932, 27.583, 26.09, 25.115, 23.851, 21.661, 20.933, 21.081, 17.031, 17.728, 15.871, 13.633, 13.607, 14.245, 10.596, 9.807, 8.593, 8.511, 9.27, 9.681, 8.857, 8.97, 10.35, 9.221, 9.67, 8.718, 9.417, 9.527, 9.349, 9.752, 10.698, 11.361, 10.09, 10.731, 10.122, 11.034, 11.822, 11.538, 10.509, 11.051, 10.873, 10.819, 11.043, 11.445, 10.672, 10.057, 8.577, 9.014, 8.857, 9.6, 9.969, 9.46, 9.884, 8.26, 8.792, 8.686, 9.389, 10.079, 9.124, 9.117, 8.701, 9.551, 9.42, 9.648, 8.994, 8.493, 8.801, 8.438, 7.835, 7.974, 8.507, 8.202, 8.456, 7.899, 7.846, 8.129, 8.267, 8.938, 7.913, 8.412, 7.812, 8.679, 8.927, 8.07, 8.447, 8.465, 8.201, 8.438, 9.028, 8.71, 8.35, 8.095, 8.167, 7.936, 8.258, 7.816, 8.321, 7.953, 8.367, 8.218, 7.877, 8.422, 8.32, 8.347, 8.178, 8.295, 8.448, 8.517, 8.043, 8.587, 8.343, 8.78, 8.254, 8.477, 8.011, 8.73, 8.706, 8.965, 9.127, 8.737, 7.863, 8.841, 7.685, 8.961, 8.005, 8.169, 8.327, 8.279, 8.423, 8.018, 8.262, 8.296, 8.247, 8.323, 7.882, 8.401, 8.298, 8.776, 8.847, 7.954, 8.559, 8.294, 8.261, 8.237, 8.604, 7.965, 8.604, 8.343, 8.185, 7.714, 7.953, 8.374, 8.123, 8.167, 8.149, 8.557, 8.346, 8.124, 8.467, 8.591, 8.238, 7.923, 8.618, 7.922, 8.25, 8.334, 8.11, 8.2, 8.155, 8.429, 8.146, 8.223, 7.96, 8.155, 8.128, 7.782, 8.157, 8.019, 8.318, 7.802, 8.911, 7.875, 8.121, 8.576, 7.658, 7.981, 8.657, 8.593, 8.066, 8.41, 8.025, 8.081, 8.788, 8.422, 8.929, 8.559, 7.391, 7.866, 8.646, 8.629, 8.268, 8.365, 7.863, 8.228, 8.952, 8.203, 8.251, 9.012, 7.697, 8.437, 8.185, 8.296, 7.795, 8.181, 7.981, 7.976, 7.856, 8.187, 7.921, 8.305, 7.77, 8.054, 7.63, 8.154, 7.764, 7.87, 8.263, 8.287, 8.116, 7.968, 8.374, 7.874, 7.972, 8.201, 7.853, 8.274, 7.647, 7.822, 8.298, 8.468, 8.477, 8.627, 7.86, 8.213, 8.224, 8.081, 8.167, 8.546, 8.248, 8.107, 7.907, 8.048, 8.039, 8.027, 7.292, 8.575, 7.919, 8.06, 8.035, 8.52, 7.982, 9.23, 8.307, 8.425, 8.659, 8.196, 8.174, 8.653, 8.643, 8.116, 8.551, 8.036, 8.088, 8.258, 8.291, 8.415, 8.256, 8.29, 8.072, 8.621, 7.915, 8.297, 7.998, 8.445, 8.473, 7.76, 8.536, 7.925, 7.98, 8.175, 7.815, 8.396, 7.836, 7.662, 8.615, 8.046, 8.528, 7.947, 8.557, 8.61, 8.266, 8.51, 8.38, 8.346, 7.964, 7.979, 8.242, 7.919, 8.242, 7.923, 8.248, 8.146, 8.057, 8.537, 7.937, 8.126, 8.026, 7.653, 8.087, 7.984, 7.983, 7.93, 7.957, 8.129, 8.435, 8.295, 8.495, 8.022, 8.285, 8.106, 8.446, 8.322, 7.782, 8.294, 8.22, 8.217, 8.095, 8.95, 9.125, 8.536, 8.861, 8.133, 8.682, 8.386, 7.958, 8.679, 8.051, 8.586, 7.681, 8.417, 8.277, 8.613, 7.839, 8.536, 8.643, 8.348, 9.062, 8.482, 8.118, 8.236, 8.382, 8.32, 8.677, 8.44, 7.917, 8.726, 8.321, 8.829, 7.745, 8.176, 8.372, 7.743, 8.254, 8.359, 7.83, 8.282, 8.143, 7.974, 8.359, 8.644, 8.171, 8.487, 8.32, 8.104, 7.822, 7.891, 8.498, 7.919, 8.937, 7.759, 8.071, 7.648, 8.225, 8.388, 7.943, 7.882, 7.792, 7.699, 8.343, 8.289, 8.129, 8.621, 8.118, 8.401, 8.413, 8.169, 8.238, 8.064, 8.217, 7.972, 8.331, 8.231, 8.224, 8.074, 8.285, 7.929, 8.507, 7.669, 8.574, 8.011, 8.272, 8.297, 7.252, 8.616, 7.404, 7.955, 8.037, 7.866, 7.348, 8.012, 7.568, 8.217, 8.085, 8.507, 8.068, 8.511, 7.96, 8.237, 8.584, 8.28, 8.299, 7.619, 7.986, 7.99, 7.795, 7.933, 7.738, 8.263, 8.212, 7.941, 8.451, 8.257, 7.93, 8.433, 8.209, 8.251, 7.549, 8.164, 7.566, 7.842, 7.537, 8.563, 8.177, 7.933, 8.192, 7.922, 8.261, 7.359, 8.585, 7.457]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.026, 0.048, 0.08, 0.14, 0.198, 0.19, 0.376, 0.358, 0.434, 0.625, 0.617, 0.608, 1.085, 1.253, 1.633, 1.679, 1.42, 1.377, 1.77, 1.587, 1.612, 1.654, 2.013, 3.269, 2.809, 3.232, 3.843, 4.723, 4.858, 6.705, 3.977, 4.564, 5.316, 4.368, 5.506, 4.424, 4.167, 4.497, 3.508, 3.417, 2.577, 2.721, 2.65, 1.881, 2.654, 2.672, 2.175, 2.848, 2.484, 3.352, 3.431, 2.995, 4.213, 3.596, 4.247, 4.374, 4.416, 4.036, 3.346, 3.896, 4.001, 3.753, 3.228, 3.387, 2.48, 3.22, 3.702, 3.519, 3.73, 3.52, 4.41, 2.746, 3.361, 3.318, 2.829, 3.374, 4.05, 4.618, 4.113, 4.134, 2.41, 3.391, 3.807, 2.948, 3.539, 2.661, 2.273, 2.794, 2.988, 3.298, 4.175, 4.228, 3.486, 3.488, 4.235, 3.947, 3.754, 2.797, 3.24, 3.075, 2.547, 3.556, 3.593, 2.88, 4.355, 4.595, 2.832, 3.926, 3.015, 3.089, 4.026, 3.295, 3.045, 2.951, 2.02, 2.365, 1.924, 2.767, 2.707, 3.12, 2.971, 5.344, 3.954, 4.539, 5.041, 3.775, 3.405, 3.412, 3.636, 3.365, 4.916, 4.598, 3.67, 4.618, 3.646, 4.623, 4.846, 2.251, 3.06, 1.82, 2.989, 2.703, 2.348, 2.984, 2.864, 3.24, 3.319, 3.307, 3.537, 4.381, 3.619, 2.583, 2.887, 2.913, 2.675, 2.915, 2.54, 2.729, 2.748, 3.743, 4.569, 3.268, 4.751, 3.737, 3.689, 3.181, 3.53, 2.92, 3.46, 3.529, 2.875, 3.578, 2.877, 3.364, 3.01, 2.949, 2.895, 2.628, 2.441, 2.457, 3.199, 2.409, 3.601, 2.724, 2.453, 2.457, 2.247, 2.753, 2.967, 2.893, 2.376, 2.385, 2.781, 2.864, 2.819, 4.481, 5.396, 4.893, 4.097, 3.084, 2.78, 3.327, 2.562, 3.127, 2.963, 2.646, 3.178, 3.507, 2.515, 3.296, 3.037, 3.267, 3.218, 3.024, 3.769, 4.544, 3.409, 4.291, 3.775, 3.962, 4.297, 4.696, 2.896, 4.877, 4.624, 4.499, 3.552, 3.702, 4.047, 3.606, 3.601, 3.272, 3.787, 4.528, 3.915, 3.061, 2.535, 2.314, 2.073, 2.258, 2.829, 2.96, 2.315, 2.838, 2.834, 2.671, 3.529, 3.152, 3.371, 3.851, 3.547, 3.618, 2.435, 3.881, 2.748, 3.844, 3.113, 2.826, 2.909, 2.11, 3.05, 4.219, 4.282, 3.677, 3.546, 4.192, 4.493, 3.524, 4.553, 3.305, 3.428, 3.662, 3.24, 3.674, 4.206, 3.427, 3.236, 3.009, 3.11, 2.918, 4.151, 4.889, 5.305, 3.869, 3.541, 3.143, 3.574, 3.11, 3.168, 3.577, 3.756, 3.042, 3.362, 2.497, 3.357, 2.914, 2.921, 2.412, 3.139, 2.874, 2.654, 3.589, 2.874, 2.922, 3.817, 3.036, 2.555, 2.555, 2.517, 2.663, 2.449, 3.117, 2.342, 2.819, 2.769, 2.176, 2.808, 3.234, 2.732, 2.893, 3.167, 2.516, 2.916, 2.716, 2.638, 2.916, 3.453, 3.599, 3.305, 2.757, 2.896, 2.977, 2.383, 2.768, 2.405, 2.061, 2.592, 3.726, 2.772, 3.111, 3.447, 3.375, 2.315, 2.913, 2.792, 3.075, 2.497, 3.253, 2.311, 2.871, 2.207, 2.79, 1.79, 2.735, 1.682, 3.062, 2.074, 2.695, 2.519, 2.316, 2.175, 3.337, 3.212, 3.219, 2.832, 2.514, 3.289, 3.14, 2.725, 2.752, 2.74, 3.418, 3.54, 3.338, 3.11, 3.061, 2.376, 2.658, 2.481, 3.293, 2.719, 2.615, 2.75, 3.387, 3.55, 3.092, 2.138, 3.324, 3.405, 2.771, 2.852, 3.394, 3.693, 3.078, 3.111, 2.946, 4.425, 3.82, 3.225, 3.62, 3.837, 3.02, 3.308, 2.793, 3.735, 2.149, 3.184, 3.505, 3.297, 2.801, 2.843, 2.34, 2.577, 2.713, 2.507, 3.127, 3.673, 2.581, 2.435, 2.536, 2.882, 3.126, 3.359, 3.516, 2.996, 4.001, 3.551, 4.283, 3.435, 2.797, 2.42, 2.879, 3.416, 3.256, 3.01, 3.901, 3.416, 2.655, 3.905, 3.135, 4.088, 4.003, 2.957, 2.67, 2.967, 2.077, 2.843, 2.233, 2.259, 2.551, 2.35, 1.856, 1.989, 2.607, 2.725, 3.039, 3.72, 4.311, 3.3, 4.42, 3.082, 3.433, 2.893, 3.903, 3.689, 3.215, 2.329, 3.165]\n",
      "\n",
      "#Fold: 0 \n",
      "Trainig set size: 420 , Time: 0:12:01 , best_lambda: 0.01 , min_  , error: 7.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test starts:  467 , ends:  518\n",
      "1/1 [==============================] - 0s 770us/step - loss: 151.8357 - mse: 149.3341 - mae: 6.6927 - fp_mae: 1.2793\n",
      "average_error:  6.693 , fp_average_error:  1.279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:00<00:00, 225.19it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 220.76it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 246.55it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 213.51it/s]\n",
      "100%|██████████| 107/107 [00:00<00:00, 231.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Fold: 1 , Training Size: 420 , Validation size: 47 , Test Size 52\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 49.73377, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 49.73377 to 49.63641, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 49.63641 to 49.52221, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 49.52221 to 49.39983, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 49.39983 to 49.25079, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 49.25079 to 49.08331, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 49.08331 to 48.88995, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 48.88995 to 48.64001, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 48.64001 to 48.40118, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 48.40118 to 48.13223, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 48.13223 to 47.78678, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 47.78678 to 47.33378, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 47.33378 to 46.92107, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 46.92107 to 46.57828, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 46.57828 to 46.36525, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 46.36525 to 45.72985, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 45.72985 to 45.40093, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 45.40093 to 44.90116, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 44.90116 to 44.43036, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 44.43036 to 43.55166, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 43.55166 to 42.97621, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 42.97621 to 42.23475, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 42.23475 to 41.75386, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 41.75386 to 40.22639, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 40.22639 to 39.67156, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 39.67156 to 38.75135, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 38.75135 to 37.53260, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 37.53260 to 36.75606, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 36.75606 to 36.19826, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 36.19826 to 35.91864, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00031: val_mae improved from 35.91864 to 34.67133, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00032: val_mae improved from 34.67133 to 34.22976, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 34.22976 to 33.59964, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00034: val_mae improved from 33.59964 to 33.38041, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00035: val_mae improved from 33.38041 to 31.16531, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00036: val_mae improved from 31.16531 to 29.37201, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 29.37201\n",
      "\n",
      "Epoch 00038: val_mae improved from 29.37201 to 27.64081, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00039: val_mae improved from 27.64081 to 26.61804, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_mae improved from 26.61804 to 26.02738, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00041: val_mae improved from 26.02738 to 24.86233, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00042: val_mae improved from 24.86233 to 24.07270, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00043: val_mae improved from 24.07270 to 23.03477, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00044: val_mae improved from 23.03477 to 22.48559, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00045: val_mae improved from 22.48559 to 21.82038, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00046: val_mae improved from 21.82038 to 20.71565, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00047: val_mae improved from 20.71565 to 19.95431, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00048: val_mae improved from 19.95431 to 18.58507, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00049: val_mae improved from 18.58507 to 17.95740, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00050: val_mae improved from 17.95740 to 16.89028, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00051: val_mae improved from 16.89028 to 16.31266, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00052: val_mae improved from 16.31266 to 14.59200, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00053: val_mae improved from 14.59200 to 13.20109, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00054: val_mae improved from 13.20109 to 12.39310, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00055: val_mae improved from 12.39310 to 12.15303, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00056: val_mae improved from 12.15303 to 11.64042, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00057: val_mae improved from 11.64042 to 9.24069, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00058: val_mae improved from 9.24069 to 9.09036, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00059: val_mae improved from 9.09036 to 8.96927, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00060: val_mae improved from 8.96927 to 8.48432, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00061: val_mae improved from 8.48432 to 7.68398, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00062: val_mae improved from 7.68398 to 6.89194, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 6.89194\n",
      "\n",
      "Epoch 00064: val_mae improved from 6.89194 to 6.88233, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 6.88233\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 6.88233\n",
      "\n",
      "Epoch 00067: val_mae improved from 6.88233 to 6.87529, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 6.87529\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 6.87529\n",
      "\n",
      "Epoch 00070: val_mae improved from 6.87529 to 6.44405, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 6.44405\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 6.44405\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 6.44405\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 6.44405\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 6.44405\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 6.44405\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 6.44405\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 6.44405\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 6.44405\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 6.44405\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 6.44405\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 6.44405\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 6.44405\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 6.44405\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 6.44405\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 6.44405\n",
      "\n",
      "Epoch 00087: val_mae improved from 6.44405 to 6.40142, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 6.40142\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 6.40142\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 6.40142\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 6.40142\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 6.40142\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 6.40142\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 6.40142\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 6.40142\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 6.40142\n",
      "\n",
      "Epoch 00097: val_mae improved from 6.40142 to 6.38028, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00098: val_mae improved from 6.38028 to 6.13728, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 6.13728\n",
      "\n",
      "Epoch 00100: val_mae improved from 6.13728 to 5.90618, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 5.90618\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 5.90618\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 5.90618\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 5.90618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00105: val_mae did not improve from 5.90618\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 5.90618\n",
      "\n",
      "Epoch 00107: val_mae improved from 5.90618 to 5.90298, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 5.90298\n",
      "\n",
      "Epoch 00135: val_mae improved from 5.90298 to 5.87989, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 5.87989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00260: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 5.87989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00421: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 5.87989\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 5.87989\n",
      "\n",
      "Lambda: 0.01 , Time: 0:03:59\n",
      "Train Error(all epochs): 0.6393356919288635 \n",
      " [49.099, 48.949, 48.856, 48.755, 48.639, 48.508, 48.358, 48.178, 47.974, 47.739, 47.466, 47.158, 46.808, 46.415, 45.976, 45.486, 44.924, 44.3, 43.608, 42.868, 42.068, 41.24, 40.375, 39.475, 38.534, 37.564, 36.584, 35.527, 34.508, 33.449, 32.358, 31.262, 30.175, 29.041, 27.918, 26.794, 25.631, 24.549, 23.374, 22.156, 20.978, 19.827, 18.681, 17.56, 16.453, 15.381, 14.335, 13.313, 12.347, 11.38, 10.555, 9.826, 8.995, 8.259, 7.704, 7.035, 6.766, 6.429, 6.059, 5.514, 5.267, 5.043, 4.748, 4.397, 4.199, 4.22, 3.675, 3.728, 3.491, 3.634, 3.685, 3.871, 3.926, 3.677, 3.402, 3.163, 2.897, 2.793, 2.746, 2.741, 2.93, 3.126, 3.005, 3.234, 3.144, 3.096, 3.378, 3.308, 3.08, 3.071, 2.916, 2.871, 2.713, 2.572, 2.506, 2.525, 2.442, 2.353, 2.315, 2.432, 2.588, 2.674, 2.599, 2.537, 2.501, 2.311, 2.21, 2.245, 2.185, 2.195, 2.316, 2.36, 2.629, 2.681, 2.699, 2.635, 2.765, 2.851, 2.531, 2.289, 2.219, 2.147, 2.168, 2.254, 2.105, 2.068, 2.047, 1.841, 1.885, 1.838, 1.923, 1.825, 1.903, 1.878, 1.798, 1.905, 1.917, 2.016, 2.179, 2.066, 1.999, 1.769, 1.692, 1.493, 1.477, 1.394, 1.386, 1.319, 1.362, 1.413, 1.702, 1.733, 2.077, 2.543, 2.356, 1.98, 1.808, 1.902, 1.877, 1.932, 1.921, 1.913, 1.689, 1.509, 1.575, 1.578, 1.531, 1.529, 1.506, 1.429, 1.394, 1.278, 1.265, 1.253, 1.48, 1.47, 1.552, 1.757, 1.827, 1.804, 1.598, 1.529, 1.493, 1.594, 1.501, 1.407, 1.411, 1.579, 1.434, 1.558, 1.522, 1.534, 1.395, 1.25, 1.346, 1.265, 1.225, 1.323, 1.257, 1.237, 1.251, 1.368, 1.31, 1.273, 1.346, 1.311, 1.249, 1.246, 1.227, 1.244, 1.267, 1.152, 1.14, 1.127, 1.275, 1.313, 1.226, 1.107, 1.073, 1.087, 1.167, 1.32, 1.335, 1.176, 1.206, 1.202, 1.282, 1.295, 1.259, 1.159, 1.282, 1.327, 1.275, 1.209, 1.214, 1.108, 1.038, 1.108, 1.024, 0.999, 1.176, 1.32, 1.27, 1.222, 1.252, 1.311, 1.24, 1.172, 1.058, 0.951, 0.883, 0.878, 0.881, 0.908, 1.005, 1.023, 0.975, 0.924, 0.929, 1.013, 1.037, 0.999, 1.115, 1.162, 1.267, 1.376, 1.445, 1.337, 1.206, 1.207, 1.204, 1.15, 1.252, 1.312, 1.339, 1.251, 1.252, 1.165, 1.139, 1.09, 1.183, 1.189, 1.195, 1.308, 1.206, 1.197, 1.013, 0.817, 0.819, 0.816, 0.808, 0.824, 0.831, 0.966, 1.106, 1.13, 1.066, 1.101, 1.088, 1.046, 1.143, 1.314, 1.341, 1.197, 1.37, 1.394, 1.348, 1.38, 1.275, 1.21, 1.104, 1.076, 0.984, 0.859, 0.852, 0.787, 0.788, 0.843, 0.845, 0.825, 0.969, 1.102, 1.038, 0.947, 0.938, 0.934, 0.939, 0.911, 0.787, 0.718, 0.714, 0.653, 0.639, 0.757, 0.894, 0.957, 0.846, 0.836, 0.856, 0.924, 0.928, 0.922, 0.982, 1.064, 1.067, 1.002, 1.031, 1.166, 1.13, 0.974, 0.914, 0.851, 0.854, 0.922, 0.86, 0.818, 0.922, 1.025, 1.157, 1.203, 1.14, 1.011, 1.018, 1.064, 1.074, 1.01, 0.961, 1.033, 1.135, 1.087, 1.012, 1.021, 1.046, 0.971, 0.999, 1.114, 1.214, 1.265, 1.203, 1.247, 1.163, 1.169, 1.054, 1.09, 1.019, 1.195, 1.228, 1.201, 1.129, 1.184, 1.249, 1.225, 1.204, 1.283, 1.326, 1.286, 1.346, 1.424, 1.323, 1.303, 1.234, 1.162, 1.01, 0.904, 0.852, 0.85, 0.822, 0.8, 0.823, 0.832, 0.815, 0.803, 0.807, 0.809, 0.763, 0.752, 0.779, 0.829, 0.842, 0.824, 0.79, 0.784, 0.841, 0.866, 0.958, 0.986, 1.072, 1.051, 1.063, 1.044, 1.125, 1.064, 1.063, 0.989, 0.927, 0.896, 0.926, 0.944, 0.952, 0.892, 1.005, 1.0, 1.11, 1.151, 1.219, 1.141, 1.095, 0.936, 0.82, 0.85, 0.848, 0.953, 1.013, 1.036, 0.911, 0.983, 1.002, 0.967, 0.942, 0.933, 0.942, 0.857, 0.863, 0.847, 0.834, 0.806, 0.862, 0.843, 0.748, 0.732, 0.73, 0.737, 0.761, 0.803, 0.804, 0.822, 0.801, 0.811, 0.801, 0.815, 0.949, 1.117, 1.093, 1.093, 1.102, 1.175, 1.195, 1.119, 1.166, 1.073, 1.098, 1.269, 1.247, 1.237, 1.021, 0.953, 0.981, 1.137, 1.112, 1.197]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.005, 0.013, 0.017, 0.019, 0.038, 0.076, 0.136, 0.129, 0.178, 0.258, 0.258, 0.433, 0.533, 0.671, 0.651, 0.762, 0.838, 0.926, 0.892, 0.923, 1.121, 0.933, 1.075, 1.034, 1.19, 1.318, 1.404, 1.532, 1.444, 1.328, 1.278, 1.137, 1.132, 1.142, 1.161, 1.221, 1.485, 1.294, 1.508, 1.44, 1.364, 1.571, 1.464, 1.378, 1.379, 1.323, 1.281, 1.223, 1.155, 1.092, 1.108, 1.071, 1.044, 1.035, 1.105, 1.15, 1.226, 1.189, 1.127, 1.171, 0.991, 1.005, 0.966, 0.986, 0.973, 1.044, 1.096, 1.217, 1.19, 1.281, 1.235, 1.263, 1.322, 1.149, 1.064, 0.998, 0.961, 0.989, 1.054, 0.961, 0.972, 0.986, 0.824, 0.854, 0.8, 0.904, 0.765, 0.985, 0.862, 0.83, 0.887, 0.907, 0.955, 1.042, 1.017, 0.865, 0.878, 0.684, 0.631, 0.691, 0.596, 0.656, 0.589, 0.627, 0.645, 0.88, 0.812, 1.01, 1.339, 1.131, 0.953, 0.771, 0.944, 0.835, 0.925, 0.842, 0.943, 0.789, 0.713, 0.676, 0.774, 0.735, 0.704, 0.772, 0.623, 0.697, 0.611, 0.556, 0.601, 0.726, 0.68, 0.713, 0.925, 0.824, 0.913, 0.732, 0.733, 0.683, 0.784, 0.699, 0.704, 0.641, 0.768, 0.676, 0.747, 0.726, 0.787, 0.657, 0.58, 0.638, 0.61, 0.578, 0.646, 0.608, 0.616, 0.609, 0.612, 0.653, 0.603, 0.638, 0.654, 0.579, 0.593, 0.583, 0.549, 0.686, 0.503, 0.563, 0.521, 0.641, 0.574, 0.665, 0.485, 0.513, 0.51, 0.592, 0.62, 0.676, 0.585, 0.535, 0.633, 0.585, 0.621, 0.638, 0.524, 0.622, 0.64, 0.629, 0.568, 0.639, 0.483, 0.54, 0.528, 0.489, 0.498, 0.56, 0.675, 0.619, 0.594, 0.562, 0.697, 0.549, 0.618, 0.463, 0.482, 0.403, 0.429, 0.418, 0.409, 0.498, 0.482, 0.475, 0.434, 0.473, 0.444, 0.571, 0.436, 0.58, 0.521, 0.663, 0.642, 0.749, 0.614, 0.615, 0.555, 0.595, 0.55, 0.631, 0.622, 0.71, 0.578, 0.626, 0.511, 0.571, 0.529, 0.555, 0.609, 0.544, 0.674, 0.578, 0.533, 0.523, 0.358, 0.391, 0.402, 0.381, 0.386, 0.416, 0.506, 0.486, 0.609, 0.534, 0.477, 0.561, 0.495, 0.58, 0.619, 0.653, 0.622, 0.639, 0.7, 0.65, 0.649, 0.623, 0.557, 0.554, 0.489, 0.493, 0.396, 0.389, 0.404, 0.358, 0.434, 0.417, 0.406, 0.47, 0.563, 0.516, 0.431, 0.479, 0.421, 0.476, 0.446, 0.379, 0.362, 0.336, 0.328, 0.298, 0.383, 0.419, 0.497, 0.409, 0.372, 0.467, 0.428, 0.485, 0.434, 0.486, 0.508, 0.544, 0.501, 0.465, 0.579, 0.561, 0.462, 0.477, 0.411, 0.387, 0.485, 0.397, 0.427, 0.447, 0.482, 0.565, 0.617, 0.533, 0.529, 0.479, 0.516, 0.521, 0.499, 0.435, 0.538, 0.547, 0.551, 0.48, 0.499, 0.51, 0.474, 0.496, 0.541, 0.538, 0.699, 0.52, 0.625, 0.548, 0.59, 0.476, 0.603, 0.443, 0.634, 0.574, 0.572, 0.564, 0.591, 0.642, 0.587, 0.553, 0.637, 0.626, 0.648, 0.645, 0.713, 0.639, 0.671, 0.54, 0.617, 0.399, 0.472, 0.412, 0.421, 0.417, 0.366, 0.41, 0.378, 0.423, 0.382, 0.4, 0.409, 0.356, 0.397, 0.348, 0.429, 0.397, 0.418, 0.391, 0.384, 0.423, 0.414, 0.51, 0.446, 0.574, 0.461, 0.564, 0.46, 0.583, 0.494, 0.531, 0.489, 0.446, 0.454, 0.463, 0.451, 0.514, 0.379, 0.544, 0.47, 0.564, 0.522, 0.627, 0.53, 0.579, 0.415, 0.409, 0.404, 0.417, 0.478, 0.485, 0.532, 0.43, 0.493, 0.475, 0.474, 0.461, 0.434, 0.493, 0.397, 0.458, 0.396, 0.401, 0.419, 0.376, 0.447, 0.328, 0.376, 0.346, 0.365, 0.373, 0.415, 0.353, 0.435, 0.352, 0.412, 0.394, 0.398, 0.473, 0.57, 0.531, 0.542, 0.494, 0.62, 0.571, 0.529, 0.586, 0.521, 0.504, 0.667, 0.579, 0.627, 0.465, 0.458, 0.494, 0.594, 0.509, 0.654]\n",
      "Val Error(all epochs): 5.8798933029174805 \n",
      " [49.734, 49.636, 49.522, 49.4, 49.251, 49.083, 48.89, 48.64, 48.401, 48.132, 47.787, 47.334, 46.921, 46.578, 46.365, 45.73, 45.401, 44.901, 44.43, 43.552, 42.976, 42.235, 41.754, 40.226, 39.672, 38.751, 37.533, 36.756, 36.198, 35.919, 34.671, 34.23, 33.6, 33.38, 31.165, 29.372, 29.675, 27.641, 26.618, 26.027, 24.862, 24.073, 23.035, 22.486, 21.82, 20.716, 19.954, 18.585, 17.957, 16.89, 16.313, 14.592, 13.201, 12.393, 12.153, 11.64, 9.241, 9.09, 8.969, 8.484, 7.684, 6.892, 7.236, 6.882, 7.23, 7.443, 6.875, 7.239, 7.296, 6.444, 6.977, 7.547, 8.359, 7.954, 8.202, 8.476, 7.807, 7.175, 6.945, 6.516, 6.841, 6.763, 6.845, 7.12, 7.009, 8.126, 6.401, 7.284, 7.55, 7.866, 7.581, 7.588, 6.51, 8.119, 6.996, 6.503, 6.38, 6.137, 6.176, 5.906, 6.039, 6.845, 6.315, 6.366, 6.233, 6.169, 5.903, 6.473, 6.397, 6.036, 6.255, 7.126, 6.489, 6.902, 7.017, 7.495, 7.363, 6.725, 7.063, 6.618, 7.233, 6.609, 6.861, 7.162, 6.305, 6.932, 6.675, 6.995, 6.561, 6.492, 6.55, 6.321, 6.587, 6.044, 5.88, 6.576, 6.259, 6.585, 6.461, 6.937, 6.932, 6.524, 6.147, 6.843, 6.408, 6.312, 6.762, 6.279, 6.003, 7.046, 6.448, 6.604, 7.335, 8.411, 9.317, 8.063, 7.515, 7.573, 7.064, 6.849, 6.491, 6.814, 6.844, 6.487, 6.928, 7.072, 6.606, 6.68, 6.929, 6.579, 6.848, 6.93, 6.635, 7.137, 6.564, 6.837, 7.19, 7.079, 8.227, 7.548, 8.492, 6.773, 7.703, 6.848, 6.622, 7.044, 6.75, 6.684, 6.538, 7.209, 6.546, 6.888, 7.117, 6.447, 7.098, 6.553, 7.182, 6.507, 6.574, 7.284, 6.505, 6.906, 6.618, 6.599, 6.737, 6.813, 6.518, 6.949, 6.448, 6.867, 6.759, 6.841, 6.68, 6.992, 6.665, 6.834, 7.104, 6.777, 6.497, 7.083, 6.559, 6.996, 6.957, 6.557, 6.913, 7.058, 6.58, 7.236, 6.872, 6.78, 7.24, 6.598, 7.343, 6.685, 7.339, 6.824, 7.027, 7.147, 6.464, 7.365, 6.852, 6.676, 7.392, 6.797, 6.842, 7.191, 7.139, 6.702, 7.155, 6.524, 6.851, 6.675, 6.557, 6.756, 6.604, 6.438, 6.905, 6.547, 6.573, 6.85, 6.464, 7.05, 6.488, 6.859, 7.18, 7.252, 7.494, 8.046, 9.711, 8.163, 8.85, 8.741, 7.619, 7.655, 8.025, 8.022, 8.183, 7.466, 7.753, 7.641, 7.131, 7.085, 7.664, 6.858, 6.965, 7.258, 6.782, 6.926, 7.033, 6.735, 6.774, 6.845, 6.706, 6.441, 7.13, 6.402, 6.688, 6.729, 7.145, 7.101, 6.602, 7.535, 6.694, 6.639, 7.018, 7.049, 7.123, 6.638, 6.961, 6.931, 6.484, 6.859, 6.83, 6.711, 6.95, 6.704, 6.785, 6.512, 6.99, 6.53, 6.905, 6.503, 6.466, 6.766, 6.622, 6.702, 6.669, 6.502, 6.553, 6.706, 6.514, 6.557, 6.666, 6.419, 6.75, 6.409, 6.601, 6.585, 6.475, 6.807, 6.501, 6.793, 6.629, 6.549, 6.679, 6.832, 6.541, 6.974, 6.869, 6.555, 6.78, 6.422, 7.124, 7.618, 6.606, 6.831, 7.212, 6.282, 7.325, 6.481, 6.761, 6.895, 6.656, 6.762, 7.039, 6.613, 6.721, 6.95, 6.296, 6.862, 6.352, 6.503, 6.586, 6.426, 6.651, 6.403, 6.703, 6.892, 6.26, 6.815, 6.576, 6.245, 6.964, 6.461, 6.837, 6.751, 6.557, 6.886, 6.648, 6.916, 7.05, 6.884, 6.976, 6.632, 6.788, 6.797, 6.762, 6.53, 6.94, 6.714, 6.604, 6.803, 6.644, 6.683, 6.615, 6.58, 6.748, 6.362, 6.76, 6.503, 6.493, 6.644, 6.441, 6.546, 6.475, 6.516, 6.595, 6.494, 6.535, 6.561, 6.412, 6.518, 6.423, 6.639, 6.475, 6.706, 6.481, 6.763, 6.791, 6.889, 7.075, 6.749, 6.89, 6.733, 6.655, 6.569, 6.535, 6.712, 6.518, 6.839, 6.66, 6.709, 6.311, 7.097, 6.476, 6.459, 6.868, 6.396, 6.387, 6.618, 6.307, 6.713, 6.39, 6.42, 6.503, 6.835, 6.423, 6.725, 6.542, 6.816, 6.868, 6.906, 6.569, 6.626, 6.802, 6.35, 6.893, 6.386, 6.938, 6.575, 6.449, 6.489, 6.763, 6.498, 7.151, 6.863, 6.722, 7.176, 6.734, 6.776, 6.838, 6.52, 6.663, 6.512, 6.882, 6.908, 6.717, 6.901, 6.771, 6.518, 6.699, 6.652, 6.53, 6.367, 6.597, 6.455, 6.387, 6.573, 6.635, 6.515]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.049, 0.044, 0.071, 0.083, 0.11, 0.183, 0.196, 0.269, 0.343, 0.464, 0.566, 0.688, 0.714, 0.804, 1.056, 1.346, 1.646, 1.557, 2.434, 3.16, 3.864, 3.351, 4.023, 4.225, 3.776, 3.387, 3.3, 3.17, 4.19, 5.979, 7.102, 6.694, 7.163, 7.252, 6.161, 5.407, 5.011, 4.188, 5.137, 5.174, 5.299, 5.885, 5.972, 7.071, 5.051, 6.034, 6.91, 6.911, 6.669, 6.863, 5.122, 7.415, 5.872, 4.883, 5.081, 4.183, 4.085, 4.26, 4.163, 5.379, 4.84, 4.734, 4.421, 3.583, 3.533, 4.98, 4.276, 3.468, 4.453, 5.36, 2.763, 2.918, 3.639, 1.826, 5.107, 3.683, 4.135, 3.112, 2.903, 2.806, 2.927, 2.698, 2.636, 3.503, 2.801, 2.607, 3.024, 3.57, 2.678, 3.418, 4.802, 3.599, 3.747, 3.989, 4.435, 4.438, 4.737, 5.558, 5.487, 3.803, 4.043, 4.921, 4.25, 4.611, 4.47, 3.905, 3.529, 5.148, 4.717, 4.773, 5.894, 7.385, 8.643, 7.423, 6.048, 6.762, 5.709, 3.926, 4.589, 3.919, 4.5, 4.418, 4.385, 5.442, 4.396, 4.701, 4.548, 4.566, 4.81, 4.568, 5.009, 4.742, 4.488, 5.282, 5.075, 5.012, 7.328, 6.429, 7.468, 5.226, 6.329, 4.969, 4.555, 5.397, 4.086, 4.247, 4.196, 5.543, 4.44, 4.788, 5.851, 3.975, 5.487, 4.861, 4.992, 4.796, 4.539, 5.754, 3.797, 4.398, 4.601, 4.247, 4.745, 4.496, 3.866, 4.679, 3.637, 4.453, 4.416, 5.009, 4.448, 5.042, 4.601, 4.831, 4.965, 4.744, 3.862, 4.642, 4.523, 4.14, 4.634, 4.34, 4.845, 4.464, 4.602, 5.465, 4.224, 4.853, 5.62, 4.462, 5.979, 5.046, 5.571, 5.355, 5.356, 5.569, 4.533, 5.735, 4.59, 4.633, 6.12, 4.702, 5.165, 5.55, 5.727, 4.514, 5.686, 4.73, 4.661, 4.977, 4.55, 4.504, 3.969, 4.178, 4.302, 3.965, 3.89, 4.482, 4.334, 5.112, 4.363, 4.796, 5.789, 6.007, 2.653, 2.204, 1.393, 2.071, 1.529, 1.896, 1.906, 2.663, 2.259, 2.006, 2.15, 2.699, 1.875, 3.077, 2.807, 2.842, 2.464, 3.595, 2.847, 2.782, 3.2, 2.929, 2.872, 3.189, 2.889, 2.948, 3.55, 3.088, 3.186, 3.553, 4.028, 2.83, 2.66, 3.244, 2.622, 2.622, 2.696, 3.607, 2.804, 2.333, 3.873, 3.059, 3.28, 3.697, 4.003, 3.483, 3.8, 3.55, 3.437, 3.316, 3.52, 3.328, 4.138, 3.375, 4.248, 4.201, 3.824, 3.891, 4.328, 4.425, 3.791, 4.24, 4.183, 4.018, 4.09, 4.214, 4.279, 3.904, 4.876, 4.24, 4.158, 4.166, 4.063, 4.981, 4.193, 4.866, 4.287, 4.652, 4.656, 4.916, 4.783, 5.162, 5.294, 4.505, 4.721, 4.087, 5.687, 6.525, 4.948, 5.146, 5.808, 4.22, 5.615, 3.959, 3.369, 3.949, 3.509, 2.872, 3.567, 2.847, 3.197, 3.424, 3.661, 4.191, 3.936, 4.074, 4.571, 3.827, 4.122, 3.535, 3.769, 4.007, 4.121, 3.076, 4.493, 4.029, 3.588, 3.999, 5.261, 3.847, 4.112, 3.364, 2.909, 3.909, 2.651, 3.098, 2.961, 4.125, 3.76, 3.261, 3.749, 3.698, 3.69, 3.438, 4.19, 3.473, 3.974, 4.172, 3.914, 4.134, 4.164, 4.057, 4.412, 3.769, 4.184, 3.742, 4.069, 3.831, 4.122, 3.893, 4.014, 4.048, 3.547, 4.494, 3.526, 4.243, 3.922, 3.87, 3.986, 4.1, 4.103, 3.959, 3.157, 3.351, 3.16, 3.392, 3.483, 3.662, 3.571, 4.124, 3.59, 3.992, 3.454, 3.854, 3.139, 4.789, 3.386, 3.405, 4.541, 3.572, 4.133, 4.175, 4.156, 4.245, 3.837, 4.444, 4.641, 4.554, 4.279, 5.414, 4.35, 5.022, 4.515, 5.203, 5.56, 5.237, 4.515, 5.049, 4.578, 4.306, 5.207, 4.073, 5.828, 3.977, 4.253, 4.158, 4.052, 3.205, 2.966, 2.808, 3.172, 2.744, 3.114, 3.199, 3.374, 3.425, 4.417, 3.833, 3.001, 3.867, 3.282, 3.488, 3.595, 3.803, 3.949, 3.986, 4.602, 3.968, 4.685, 4.078, 4.17, 4.258, 3.976, 3.514]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 49.70045, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 49.70045 to 49.61170, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 49.61170 to 49.52858, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 49.52858 to 49.44988, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 49.44988 to 49.36835, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 49.36835 to 49.25924, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 49.25924 to 49.16307, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 49.16307 to 49.04792, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 49.04792 to 48.92884, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 48.92884 to 48.79159, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 48.79159 to 48.64875, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 48.64875 to 48.25548, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 48.25548 to 48.12452, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 48.12452 to 47.88340, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 47.88340 to 47.65311, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 47.65311 to 47.30779, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 47.30779 to 46.93011, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 46.93011 to 46.92683, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 46.92683 to 46.57275, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 46.57275 to 46.26499, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 46.26499 to 45.72559, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 45.72559 to 44.88004, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 44.88004\n",
      "\n",
      "Epoch 00024: val_mae improved from 44.88004 to 44.30159, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 44.30159 to 43.10064, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 43.10064 to 42.80203, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 42.80203 to 42.78604, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 42.78604 to 41.79839, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 41.79839 to 40.65821, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 40.65821\n",
      "\n",
      "Epoch 00031: val_mae improved from 40.65821 to 40.12756, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00032: val_mae improved from 40.12756 to 38.88380, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 38.88380 to 38.71865, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00034: val_mae improved from 38.71865 to 37.96428, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00035: val_mae improved from 37.96428 to 36.20478, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 36.20478\n",
      "\n",
      "Epoch 00037: val_mae improved from 36.20478 to 35.08154, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00038: val_mae improved from 35.08154 to 34.57225, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00039: val_mae improved from 34.57225 to 34.54953, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 34.54953\n",
      "\n",
      "Epoch 00041: val_mae improved from 34.54953 to 34.42739, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00042: val_mae improved from 34.42739 to 33.68752, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00043: val_mae improved from 33.68752 to 33.22937, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00044: val_mae improved from 33.22937 to 32.92038, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00045: val_mae improved from 32.92038 to 32.35871, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00046: val_mae improved from 32.35871 to 31.46166, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00047: val_mae improved from 31.46166 to 30.83013, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00048: val_mae improved from 30.83013 to 27.40563, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 27.40563\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 27.40563\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 27.40563\n",
      "\n",
      "Epoch 00052: val_mae improved from 27.40563 to 25.80111, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 25.80111\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 25.80111\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 25.80111\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 25.80111\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 25.80111\n",
      "\n",
      "Epoch 00058: val_mae improved from 25.80111 to 25.30825, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 25.30825\n",
      "\n",
      "Epoch 00060: val_mae improved from 25.30825 to 24.88812, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00061: val_mae improved from 24.88812 to 23.77643, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 23.77643\n",
      "\n",
      "Epoch 00063: val_mae improved from 23.77643 to 23.19897, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00064: val_mae improved from 23.19897 to 20.95283, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 20.95283\n",
      "\n",
      "Epoch 00066: val_mae improved from 20.95283 to 20.41157, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 20.41157\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 20.41157\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 20.41157\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 20.41157\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 20.41157\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 20.41157\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 20.41157\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 20.41157\n",
      "\n",
      "Epoch 00075: val_mae improved from 20.41157 to 19.80995, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00076: val_mae improved from 19.80995 to 16.04291, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 16.04291\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 16.04291\n",
      "\n",
      "Epoch 00079: val_mae improved from 16.04291 to 14.39522, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 14.39522\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 14.39522\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 14.39522\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 14.39522\n",
      "\n",
      "Epoch 00084: val_mae improved from 14.39522 to 13.22974, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00085: val_mae improved from 13.22974 to 11.54446, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00086: val_mae improved from 11.54446 to 10.79605, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 10.79605\n",
      "\n",
      "Epoch 00088: val_mae improved from 10.79605 to 10.79531, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 10.79531\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 10.79531\n",
      "\n",
      "Epoch 00091: val_mae improved from 10.79531 to 9.54311, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 9.54311\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 9.54311\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 9.54311\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 9.54311\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 9.54311\n",
      "\n",
      "Epoch 00097: val_mae improved from 9.54311 to 9.53627, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 9.53627\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 9.53627\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 9.53627\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 9.53627\n",
      "\n",
      "Epoch 00102: val_mae improved from 9.53627 to 8.20927, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 8.20927\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 8.20927\n",
      "\n",
      "Epoch 00105: val_mae improved from 8.20927 to 8.12116, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 8.12116\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 8.12116\n",
      "\n",
      "Epoch 00108: val_mae improved from 8.12116 to 6.97745, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 6.97745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00119: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 6.97745\n",
      "\n",
      "Epoch 00180: val_mae improved from 6.97745 to 6.85767, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 6.85767\n",
      "\n",
      "Epoch 00200: val_mae improved from 6.85767 to 6.40477, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 6.40477\n",
      "\n",
      "Epoch 00223: val_mae improved from 6.40477 to 6.37984, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 6.37984\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 6.37984\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 6.37984\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 6.37984\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 6.37984\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 6.37984\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 6.37984\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 6.37984\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 6.37984\n",
      "\n",
      "Epoch 00233: val_mae improved from 6.37984 to 6.27395, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 6.27395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00267: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 6.27395\n",
      "\n",
      "Epoch 00271: val_mae improved from 6.27395 to 6.25620, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00272: val_mae improved from 6.25620 to 6.25230, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 6.25230\n",
      "\n",
      "Epoch 00274: val_mae improved from 6.25230 to 6.01453, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 6.01453\n",
      "\n",
      "Epoch 00394: val_mae improved from 6.01453 to 5.72220, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 5.72220\n",
      "\n",
      "Epoch 00396: val_mae improved from 5.72220 to 5.69504, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 5.69504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00412: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 5.69504\n",
      "\n",
      "Epoch 00497: val_mae improved from 5.69504 to 5.61072, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 5.61072\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 5.61072\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 5.61072\n",
      "\n",
      "Lambda: 0.1 , Time: 0:03:58\n",
      "Train Error(all epochs): 0.8805498480796814 \n",
      " [49.096, 48.928, 48.829, 48.729, 48.615, 48.484, 48.331, 48.154, 47.947, 47.701, 47.415, 47.08, 46.703, 46.266, 45.769, 45.22, 44.606, 43.934, 43.215, 42.448, 41.633, 40.784, 39.911, 39.002, 38.048, 37.067, 36.042, 35.016, 33.948, 32.864, 31.754, 30.628, 29.476, 28.323, 27.149, 25.964, 24.818, 23.582, 22.363, 21.185, 19.969, 18.792, 17.634, 16.494, 15.37, 14.297, 13.331, 12.303, 11.413, 10.547, 9.694, 9.001, 8.225, 7.791, 7.156, 6.787, 6.359, 6.078, 5.914, 5.554, 5.323, 5.387, 5.233, 5.212, 5.146, 4.942, 4.876, 4.644, 4.368, 4.2, 4.069, 3.964, 4.035, 4.075, 3.99, 3.877, 4.06, 3.954, 4.198, 3.817, 3.634, 3.492, 3.568, 3.305, 3.305, 3.336, 3.256, 3.189, 3.157, 3.06, 3.244, 3.32, 3.298, 3.253, 3.283, 3.267, 3.253, 3.268, 3.034, 3.11, 2.815, 2.653, 2.535, 2.355, 2.411, 2.615, 2.808, 2.85, 3.089, 2.693, 2.606, 2.653, 2.552, 2.438, 2.569, 2.76, 2.735, 2.518, 2.57, 2.554, 2.599, 2.604, 2.842, 3.075, 2.903, 2.393, 2.253, 2.016, 1.916, 1.935, 2.197, 2.199, 2.261, 2.321, 2.246, 2.303, 2.305, 2.298, 2.095, 1.915, 1.746, 1.744, 1.728, 1.704, 1.864, 2.028, 2.068, 2.056, 2.075, 1.978, 2.14, 2.273, 1.98, 2.028, 1.979, 1.853, 1.915, 1.87, 1.984, 2.01, 1.994, 2.122, 2.105, 1.904, 2.053, 1.982, 1.913, 1.849, 1.809, 1.66, 1.599, 1.456, 1.412, 1.398, 1.525, 1.743, 1.835, 1.79, 1.748, 1.964, 1.877, 1.616, 1.632, 1.674, 1.692, 1.684, 1.711, 1.422, 1.406, 1.361, 1.366, 1.395, 1.519, 1.591, 1.638, 1.793, 1.823, 1.822, 1.864, 1.934, 1.774, 1.712, 1.796, 1.467, 1.269, 1.294, 1.545, 1.526, 1.611, 1.432, 1.437, 1.349, 1.361, 1.443, 1.486, 1.499, 1.41, 1.387, 1.286, 1.437, 1.614, 1.466, 1.451, 1.51, 1.463, 1.584, 1.325, 1.27, 1.205, 1.222, 1.313, 1.45, 1.633, 1.751, 1.767, 1.744, 1.811, 1.644, 1.616, 1.435, 1.369, 1.34, 1.298, 1.348, 1.396, 1.452, 1.444, 1.603, 1.558, 1.518, 1.472, 1.514, 1.479, 1.33, 1.215, 1.227, 1.246, 1.326, 1.27, 1.401, 1.471, 1.6, 1.643, 1.722, 1.657, 1.684, 1.714, 1.449, 1.548, 1.44, 1.361, 1.314, 1.395, 1.371, 1.326, 1.234, 1.259, 1.371, 1.4, 1.288, 1.239, 1.203, 1.275, 1.475, 1.552, 1.784, 1.787, 1.541, 1.367, 1.311, 1.378, 1.519, 1.579, 1.532, 1.615, 1.539, 1.604, 1.452, 1.539, 1.639, 1.713, 1.602, 1.57, 1.493, 1.431, 1.333, 1.348, 1.508, 1.359, 1.413, 1.258, 1.373, 1.586, 1.423, 1.398, 1.473, 1.487, 1.444, 1.435, 1.376, 1.241, 1.11, 1.095, 1.179, 1.155, 1.242, 1.287, 1.266, 1.173, 1.242, 1.185, 1.26, 1.362, 1.563, 1.59, 1.602, 1.593, 1.458, 1.436, 1.522, 1.627, 1.565, 1.725, 1.809, 1.689, 1.556, 1.395, 1.266, 1.302, 1.281, 1.29, 1.33, 1.279, 1.443, 1.48, 1.403, 1.306, 1.231, 1.157, 1.142, 1.157, 1.241, 1.262, 1.256, 1.141, 1.205, 1.256, 1.195, 1.145, 1.153, 1.065, 0.986, 1.04, 1.044, 1.113, 1.072, 1.096, 1.181, 1.188, 1.27, 1.293, 1.43, 1.444, 1.528, 1.572, 1.751, 2.135, 2.032, 2.004, 2.013, 1.814, 1.854, 1.966, 1.983, 1.83, 1.839, 1.646, 1.701, 1.676, 1.526, 1.333, 1.249, 1.128, 1.091, 0.993, 0.954, 0.968, 1.01, 0.99, 1.079, 1.144, 1.146, 1.118, 1.221, 1.292, 1.269, 1.286, 1.287, 1.184, 1.129, 1.12, 1.109, 1.035, 1.084, 1.053, 1.045, 1.005, 1.062, 1.022, 1.04, 1.213, 1.292, 1.329, 1.326, 1.635, 1.746, 1.984, 1.87, 1.973, 2.06, 2.108, 2.103, 2.093, 2.023, 1.803, 1.549, 1.468, 1.243, 1.121, 1.049, 1.001, 1.006, 1.109, 1.238, 1.192, 1.226, 1.224, 1.264, 1.245, 1.154, 1.088, 0.995, 0.881, 0.887, 1.026, 1.043, 1.12, 1.212, 1.229, 1.348, 1.369, 1.399, 1.3, 1.296, 1.282, 1.261, 1.213, 1.302, 1.227, 1.181, 1.073, 1.065, 1.117, 1.007, 0.985, 0.984, 1.038, 1.16, 1.294, 1.374, 1.433, 1.348, 1.301, 1.279, 1.345, 1.383, 1.392, 1.281, 1.319, 1.237]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.012, 0.007, 0.021, 0.062, 0.076, 0.108, 0.155, 0.199, 0.258, 0.303, 0.406, 0.475, 0.606, 0.628, 0.79, 0.975, 0.928, 1.058, 1.243, 1.284, 1.435, 1.404, 1.507, 1.512, 1.445, 1.355, 1.287, 1.248, 1.343, 1.377, 1.49, 1.372, 1.537, 1.44, 1.419, 1.745, 1.433, 1.247, 1.381, 1.395, 1.27, 1.245, 1.3, 1.309, 1.24, 1.272, 1.241, 1.347, 1.386, 1.403, 1.375, 1.447, 1.341, 1.473, 1.42, 1.253, 1.371, 1.203, 1.103, 1.047, 1.059, 0.883, 1.233, 1.255, 1.174, 1.44, 1.184, 1.219, 1.158, 1.095, 1.048, 1.117, 1.251, 1.272, 1.024, 1.126, 1.205, 1.084, 1.268, 1.333, 1.371, 1.359, 1.153, 0.893, 0.868, 0.796, 0.814, 1.048, 0.954, 1.029, 1.148, 0.94, 1.096, 1.025, 1.032, 0.96, 0.823, 0.747, 0.709, 0.902, 0.702, 0.867, 0.921, 0.949, 0.917, 1.028, 0.915, 0.898, 1.236, 0.791, 1.019, 0.866, 0.861, 0.874, 0.802, 0.939, 0.979, 0.83, 1.013, 1.028, 0.802, 1.001, 0.882, 0.872, 0.85, 0.832, 0.752, 0.699, 0.677, 0.583, 0.666, 0.679, 0.829, 0.873, 0.825, 0.793, 1.01, 0.881, 0.672, 0.754, 0.75, 0.814, 0.76, 0.883, 0.535, 0.704, 0.596, 0.585, 0.689, 0.644, 0.838, 0.729, 0.93, 0.813, 0.86, 0.866, 0.859, 0.844, 0.755, 0.85, 0.688, 0.53, 0.592, 0.737, 0.728, 0.758, 0.657, 0.715, 0.617, 0.608, 0.712, 0.628, 0.763, 0.611, 0.641, 0.584, 0.667, 0.785, 0.689, 0.666, 0.698, 0.628, 0.834, 0.532, 0.608, 0.529, 0.584, 0.585, 0.716, 0.79, 0.798, 0.849, 0.821, 0.917, 0.652, 0.902, 0.583, 0.636, 0.642, 0.577, 0.636, 0.655, 0.664, 0.73, 0.704, 0.766, 0.655, 0.765, 0.663, 0.739, 0.665, 0.468, 0.61, 0.537, 0.686, 0.54, 0.683, 0.738, 0.678, 0.805, 0.827, 0.744, 0.815, 0.843, 0.566, 0.784, 0.637, 0.623, 0.591, 0.699, 0.644, 0.616, 0.586, 0.571, 0.646, 0.664, 0.599, 0.562, 0.565, 0.569, 0.755, 0.706, 0.872, 0.881, 0.665, 0.611, 0.642, 0.627, 0.78, 0.719, 0.686, 0.832, 0.667, 0.751, 0.757, 0.691, 0.768, 0.781, 0.736, 0.75, 0.627, 0.731, 0.578, 0.598, 0.815, 0.544, 0.789, 0.518, 0.64, 0.841, 0.62, 0.618, 0.742, 0.654, 0.74, 0.655, 0.622, 0.588, 0.517, 0.476, 0.592, 0.517, 0.61, 0.604, 0.6, 0.531, 0.625, 0.464, 0.678, 0.625, 0.771, 0.736, 0.805, 0.702, 0.77, 0.62, 0.708, 0.859, 0.624, 0.876, 0.857, 0.807, 0.728, 0.664, 0.482, 0.692, 0.532, 0.624, 0.638, 0.562, 0.709, 0.702, 0.727, 0.529, 0.623, 0.495, 0.581, 0.508, 0.674, 0.501, 0.679, 0.52, 0.562, 0.6, 0.546, 0.534, 0.559, 0.527, 0.415, 0.531, 0.473, 0.53, 0.516, 0.505, 0.578, 0.52, 0.617, 0.674, 0.568, 0.794, 0.688, 0.692, 0.92, 0.96, 0.987, 0.906, 0.989, 0.729, 0.935, 0.897, 0.964, 0.786, 0.947, 0.728, 0.836, 0.77, 0.713, 0.594, 0.628, 0.468, 0.543, 0.457, 0.449, 0.441, 0.508, 0.444, 0.523, 0.515, 0.581, 0.477, 0.643, 0.596, 0.592, 0.641, 0.567, 0.571, 0.517, 0.504, 0.56, 0.427, 0.589, 0.411, 0.567, 0.41, 0.542, 0.435, 0.52, 0.568, 0.604, 0.692, 0.585, 0.812, 0.76, 1.001, 0.812, 1.028, 0.959, 0.958, 1.028, 1.002, 0.98, 0.81, 0.729, 0.653, 0.596, 0.474, 0.56, 0.41, 0.532, 0.49, 0.628, 0.603, 0.563, 0.629, 0.564, 0.575, 0.568, 0.44, 0.562, 0.374, 0.422, 0.5, 0.454, 0.576, 0.569, 0.518, 0.675, 0.646, 0.651, 0.637, 0.578, 0.562, 0.65, 0.505, 0.678, 0.581, 0.541, 0.513, 0.474, 0.551, 0.445, 0.496, 0.46, 0.467, 0.57, 0.623, 0.611, 0.696, 0.607, 0.561, 0.66, 0.61, 0.706, 0.593, 0.622, 0.688, 0.514]\n",
      "Val Error(all epochs): 5.61072301864624 \n",
      " [49.7, 49.612, 49.529, 49.45, 49.368, 49.259, 49.163, 49.048, 48.929, 48.792, 48.649, 48.255, 48.125, 47.883, 47.653, 47.308, 46.93, 46.927, 46.573, 46.265, 45.726, 44.88, 45.513, 44.302, 43.101, 42.802, 42.786, 41.798, 40.658, 41.521, 40.128, 38.884, 38.719, 37.964, 36.205, 37.468, 35.082, 34.572, 34.55, 34.607, 34.427, 33.688, 33.229, 32.92, 32.359, 31.462, 30.83, 27.406, 28.776, 28.702, 29.244, 25.801, 27.48, 26.804, 27.388, 27.11, 26.89, 25.308, 27.597, 24.888, 23.776, 24.023, 23.199, 20.953, 22.345, 20.412, 22.03, 22.581, 22.449, 23.139, 22.548, 21.275, 21.595, 23.945, 19.81, 16.043, 20.436, 17.814, 14.395, 19.326, 19.238, 16.212, 15.549, 13.23, 11.544, 10.796, 12.082, 10.795, 11.811, 11.967, 9.543, 11.42, 10.213, 9.55, 13.47, 11.541, 9.536, 10.877, 12.431, 11.381, 10.138, 8.209, 10.788, 9.588, 8.121, 9.727, 8.979, 6.977, 8.638, 8.735, 15.378, 13.505, 10.496, 9.707, 7.932, 8.202, 7.568, 7.513, 9.258, 8.827, 9.016, 7.402, 7.769, 7.621, 8.813, 11.779, 9.502, 11.187, 8.767, 8.596, 8.815, 7.687, 8.201, 8.581, 9.791, 8.855, 8.396, 11.665, 8.827, 11.509, 9.069, 10.076, 9.064, 9.753, 9.595, 8.855, 10.967, 8.495, 7.912, 7.619, 7.101, 7.987, 10.487, 9.166, 9.862, 10.295, 9.052, 10.34, 7.78, 9.308, 9.532, 7.808, 10.309, 12.945, 8.535, 8.588, 8.402, 7.434, 7.398, 7.819, 7.472, 7.752, 7.887, 7.517, 7.712, 7.184, 7.595, 7.447, 7.128, 6.858, 7.174, 7.175, 7.959, 7.821, 7.091, 7.903, 7.36, 7.823, 8.047, 7.746, 7.759, 7.94, 7.244, 8.339, 7.923, 7.781, 7.114, 7.35, 6.935, 6.405, 6.949, 6.754, 7.019, 6.762, 6.682, 6.485, 7.164, 6.802, 7.654, 6.987, 6.917, 6.667, 6.682, 6.615, 6.613, 6.918, 6.757, 6.558, 6.513, 6.766, 6.581, 6.709, 6.38, 6.823, 6.728, 6.663, 6.912, 6.756, 6.896, 6.685, 6.808, 6.902, 6.274, 6.294, 6.76, 7.12, 6.29, 6.87, 6.655, 6.484, 6.686, 7.099, 6.632, 6.5, 6.926, 6.795, 6.648, 6.503, 6.451, 6.343, 6.66, 7.478, 7.863, 8.014, 7.256, 8.086, 6.53, 6.905, 6.9, 6.717, 6.459, 7.207, 6.542, 6.719, 6.754, 7.574, 7.045, 7.247, 7.075, 7.11, 6.256, 6.252, 6.425, 6.015, 6.579, 6.507, 6.558, 6.425, 6.997, 6.364, 6.538, 6.766, 6.892, 6.898, 7.327, 6.468, 6.88, 7.246, 6.231, 6.567, 7.04, 7.18, 9.117, 6.582, 6.453, 7.091, 6.831, 7.199, 6.854, 7.203, 6.739, 7.179, 6.584, 7.106, 6.671, 7.431, 7.112, 6.964, 7.649, 7.746, 8.137, 7.702, 7.209, 7.429, 6.362, 7.085, 6.778, 6.405, 6.18, 6.211, 6.202, 6.207, 6.268, 6.346, 6.331, 6.314, 6.395, 6.54, 6.461, 6.86, 6.538, 6.478, 6.428, 7.212, 6.204, 6.544, 6.273, 6.708, 6.451, 7.086, 6.64, 7.403, 6.477, 7.279, 7.181, 6.157, 6.391, 6.535, 6.543, 6.308, 6.394, 6.873, 7.108, 6.753, 6.783, 6.17, 6.457, 6.706, 6.38, 6.421, 6.513, 6.571, 6.561, 6.311, 6.126, 6.268, 6.278, 6.171, 6.532, 6.376, 6.55, 6.251, 6.342, 6.325, 6.338, 6.2, 6.639, 6.617, 7.412, 6.497, 6.652, 6.144, 6.395, 6.358, 6.618, 6.22, 6.662, 6.129, 7.217, 6.291, 8.747, 6.13, 6.935, 5.722, 6.036, 5.695, 6.639, 5.913, 5.921, 5.725, 6.124, 5.701, 5.952, 5.908, 5.922, 5.912, 5.929, 5.924, 6.254, 6.294, 6.268, 6.212, 6.279, 6.015, 6.521, 5.833, 6.216, 5.939, 6.015, 6.22, 6.065, 6.154, 6.151, 6.177, 6.405, 6.248, 6.286, 6.895, 7.397, 6.797, 7.299, 6.491, 6.837, 6.54, 6.549, 7.24, 6.613, 6.147, 8.189, 6.95, 10.223, 8.584, 10.111, 11.493, 15.357, 13.442, 10.57, 9.997, 8.974, 9.48, 9.643, 10.176, 10.574, 8.53, 9.321, 8.48, 8.406, 9.016, 8.64, 8.362, 7.919, 7.058, 7.575, 6.802, 6.893, 7.138, 6.62, 7.143, 6.682, 6.281, 6.823, 6.264, 6.753, 7.687, 7.16, 7.524, 7.24, 6.849, 7.326, 6.82, 7.032, 6.651, 6.598, 6.848, 6.579, 6.558, 6.347, 6.22, 6.26, 6.265, 5.882, 5.911, 6.226, 6.459, 6.084, 5.88, 5.611, 6.653, 6.528, 6.291]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.032, 0.046, 0.081, 0.09, 0.094, 0.062, 0.134, 0.166, 0.084, 0.114, 0.071, 0.133, 0.146, 0.171, 0.167, 0.17, 0.141, 0.157, 0.207, 0.157, 0.229, 0.233, 0.254, 0.334, 0.243, 0.221, 0.243, 0.394, 0.344, 0.278, 0.36, 0.277, 0.366, 0.591, 0.612, 0.571, 0.474, 0.51, 0.636, 0.636, 0.659, 0.753, 0.911, 0.48, 0.603, 0.763, 0.597, 0.765, 0.646, 0.627, 1.211, 0.632, 0.73, 1.406, 0.877, 0.882, 2.352, 1.331, 0.963, 0.46, 0.568, 0.662, 0.925, 2.601, 1.838, 1.816, 2.053, 1.207, 1.029, 1.137, 3.056, 2.46, 1.879, 1.85, 0.626, 0.99, 0.759, 1.022, 1.273, 1.445, 3.164, 2.007, 1.58, 0.845, 1.128, 1.486, 0.728, 1.262, 0.657, 1.117, 0.851, 1.058, 0.988, 1.003, 1.025, 0.82, 1.144, 1.486, 1.656, 3.052, 2.325, 0.892, 1.457, 0.964, 0.848, 1.188, 0.962, 2.533, 0.948, 0.829, 1.89, 0.747, 0.612, 1.159, 1.145, 1.11, 1.827, 2.062, 1.492, 3.037, 1.713, 2.37, 3.232, 2.251, 2.491, 2.03, 2.513, 2.219, 3.144, 3.046, 2.721, 4.86, 4.424, 3.073, 4.247, 3.284, 2.256, 3.031, 2.583, 2.295, 2.166, 2.586, 1.94, 2.03, 2.252, 2.05, 1.698, 2.413, 2.855, 1.978, 3.805, 1.89, 2.139, 2.492, 2.409, 1.944, 1.708, 1.882, 1.651, 2.162, 2.251, 2.688, 3.568, 3.151, 4.425, 3.186, 3.448, 3.164, 3.663, 3.429, 2.126, 2.716, 2.43, 2.031, 3.177, 1.944, 3.123, 2.555, 3.157, 4.174, 1.951, 3.497, 2.655, 2.311, 4.208, 2.952, 4.368, 3.33, 3.101, 3.487, 2.261, 2.79, 2.824, 2.693, 3.052, 2.214, 3.321, 2.574, 3.23, 2.193, 1.474, 1.36, 1.087, 1.58, 1.156, 2.142, 2.121, 1.854, 2.761, 3.103, 1.855, 3.613, 3.612, 4.236, 5.233, 4.854, 5.221, 5.186, 5.035, 3.643, 3.916, 3.473, 3.147, 3.095, 2.726, 3.029, 2.452, 2.026, 3.266, 3.206, 3.095, 4.368, 3.146, 2.036, 2.935, 2.35, 1.653, 3.074, 2.313, 1.923, 2.011, 0.818, 2.542, 3.04, 1.797, 2.354, 2.0, 3.013, 2.38, 2.441, 4.187, 2.545, 2.182, 2.476, 1.846, 2.143, 2.314, 1.432, 1.666, 1.427, 1.787, 2.438, 1.538, 2.769, 2.039, 2.135, 3.185, 3.22, 3.154, 3.668, 2.945, 2.613, 2.682, 2.465, 3.065, 2.792, 2.887, 3.825, 3.05, 2.382, 2.641, 2.873, 1.746, 3.171, 2.228, 2.952, 2.351, 2.751, 4.646, 2.595, 1.788, 2.199, 1.712, 1.627, 2.991, 2.14, 2.731, 2.2, 2.895, 3.285, 4.17, 3.224, 3.55, 3.741, 3.29, 3.247, 3.238, 2.522, 3.701, 2.755, 3.777, 3.003, 3.425, 2.879, 2.535, 2.62, 3.201, 3.466, 4.086, 3.587, 4.051, 3.407, 3.381, 2.815, 2.658, 2.118, 2.03, 1.399, 2.358, 2.348, 2.947, 3.767, 2.078, 2.322, 2.729, 2.281, 2.286, 2.05, 2.42, 1.326, 3.005, 2.155, 3.128, 3.291, 3.691, 2.456, 3.464, 3.254, 2.647, 2.875, 3.544, 3.023, 3.28, 3.014, 3.014, 3.116, 2.962, 2.393, 3.627, 2.398, 3.146, 2.458, 2.726, 2.349, 2.712, 2.686, 3.359, 2.686, 3.962, 2.782, 3.255, 2.603, 2.95, 2.676, 2.761, 2.68, 2.29, 1.862, 2.59, 1.903, 3.147, 2.48, 3.062, 3.444, 1.807, 3.17, 3.388, 1.663, 1.938, 0.933, 1.371, 0.879, 0.577, 0.4, 0.554, 0.802, 0.782, 1.205, 0.966, 1.026, 0.899, 0.665, 1.268, 0.947, 1.251, 1.312, 1.103, 1.224, 1.293, 1.582, 2.218, 1.862, 2.701, 2.273, 1.883, 2.702, 2.007, 2.829, 2.959, 2.75, 2.745, 2.916, 1.886, 2.217, 1.982, 1.972, 2.354, 1.946, 2.186, 2.616, 2.204, 2.946, 2.062, 3.315, 2.867, 2.592, 2.954, 2.408, 2.714, 3.321, 2.964, 2.548, 2.516, 3.03, 3.681, 3.433, 4.961, 4.687, 4.198]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 49.71437, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 49.71437 to 49.68753, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 49.68753 to 49.67092, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 49.67092 to 49.63468, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 49.63468 to 49.55863, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 49.55863 to 49.44071, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 49.44071 to 49.25742, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 49.25742 to 49.10350, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 49.10350 to 48.89450, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 48.89450 to 48.66837, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 48.66837 to 48.54547, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 48.54547 to 48.23706, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 48.23706 to 47.79404, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 47.79404 to 47.55593, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 47.55593 to 46.81362, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 46.81362 to 46.17817, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 46.17817 to 45.59415, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 45.59415 to 44.75668, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 44.75668 to 44.49526, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 44.49526 to 43.80345, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 43.80345 to 42.60258, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 42.60258 to 42.30773, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 42.30773 to 41.05796, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 41.05796 to 40.36542, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 40.36542 to 39.70649, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 39.70649 to 38.52762, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 38.52762 to 37.24855, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 37.24855 to 36.02192, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 36.02192 to 32.54431, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 32.54431 to 30.43402, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 30.43402\n",
      "\n",
      "Epoch 00032: val_mae improved from 30.43402 to 28.92853, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 28.92853 to 27.53678, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00034: val_mae improved from 27.53678 to 25.32060, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00035: val_mae improved from 25.32060 to 24.97123, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00036: val_mae improved from 24.97123 to 22.17573, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00037: val_mae improved from 22.17573 to 18.27435, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00038: val_mae improved from 18.27435 to 15.92400, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00039: val_mae improved from 15.92400 to 13.23171, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_mae improved from 13.23171 to 10.40445, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00041: val_mae improved from 10.40445 to 10.38040, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 10.38040\n",
      "\n",
      "Epoch 00043: val_mae improved from 10.38040 to 7.16699, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 7.16699\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 7.16699\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 7.16699\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 7.16699\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 7.16699\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 7.16699\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 7.16699\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 7.16699\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 7.16699\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 7.16699\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 7.16699\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 7.16699\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 7.16699\n",
      "\n",
      "Epoch 00057: val_mae improved from 7.16699 to 6.83312, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 6.83312\n",
      "\n",
      "Epoch 00108: val_mae improved from 6.83312 to 6.36066, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00109: val_mae improved from 6.36066 to 6.17084, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 6.17084\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 6.17084\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 6.17084\n",
      "\n",
      "Epoch 00113: val_mae improved from 6.17084 to 5.99400, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 5.99400\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 5.99400\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 5.99400\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 5.99400\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 5.99400\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 5.99400\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 5.99400\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 5.99400\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 5.99400\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 5.99400\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 5.99400\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 5.99400\n",
      "\n",
      "Epoch 00126: val_mae improved from 5.99400 to 5.88386, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 5.88386\n",
      "\n",
      "Epoch 00166: val_mae improved from 5.88386 to 5.35716, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_1/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 5.35716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00172: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 5.35716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00333: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 5.35716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00494: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 5.35716\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 5.35716\n",
      "\n",
      "Lambda: 1 , Time: 0:03:57\n",
      "Train Error(all epochs): 1.8049590587615967 \n",
      " [49.113, 48.967, 48.88, 48.787, 48.677, 48.55, 48.403, 48.225, 48.015, 47.778, 47.506, 47.189, 46.826, 46.42, 45.944, 45.417, 44.838, 44.2, 43.493, 42.724, 41.898, 40.994, 40.055, 39.03, 37.966, 36.845, 35.648, 34.435, 33.15, 31.837, 30.5, 29.141, 27.727, 26.33, 24.948, 23.521, 22.18, 20.804, 19.534, 18.199, 17.132, 15.961, 14.918, 14.019, 13.008, 12.156, 11.458, 10.629, 10.071, 9.398, 8.969, 8.401, 7.984, 7.717, 7.332, 7.063, 6.869, 6.616, 6.528, 6.386, 6.228, 6.212, 5.99, 5.737, 5.568, 5.484, 5.502, 5.245, 5.193, 5.134, 5.142, 5.21, 5.288, 5.4, 5.226, 5.267, 5.235, 5.12, 5.075, 4.891, 4.927, 4.944, 5.046, 4.908, 5.102, 4.95, 4.83, 4.701, 4.644, 4.47, 4.636, 4.517, 4.598, 4.649, 4.75, 4.661, 4.557, 4.391, 4.322, 4.272, 4.197, 4.414, 4.139, 4.275, 4.591, 4.562, 4.465, 4.63, 4.754, 4.438, 4.412, 4.553, 4.331, 4.395, 4.052, 4.027, 3.962, 3.898, 4.128, 4.257, 4.273, 4.315, 4.479, 4.639, 4.404, 4.305, 4.181, 4.036, 4.004, 4.012, 3.831, 3.864, 3.836, 3.653, 3.633, 3.577, 3.62, 3.74, 3.849, 3.816, 3.861, 3.532, 3.609, 3.832, 3.663, 3.62, 3.628, 3.453, 3.536, 3.408, 3.358, 3.579, 3.746, 3.507, 3.296, 3.285, 3.313, 3.055, 3.215, 3.079, 3.138, 3.416, 3.806, 3.754, 4.231, 3.824, 3.736, 3.73, 3.428, 3.528, 3.348, 3.427, 3.337, 3.287, 3.261, 3.494, 3.287, 3.035, 3.199, 3.224, 3.161, 3.279, 3.296, 3.282, 3.093, 3.025, 3.128, 3.364, 3.285, 3.016, 2.941, 2.839, 2.901, 2.968, 3.18, 3.367, 3.224, 3.155, 3.026, 2.771, 2.739, 2.739, 2.953, 3.016, 3.365, 3.364, 3.278, 3.058, 2.949, 2.91, 2.981, 3.082, 3.105, 3.171, 2.717, 2.663, 2.588, 2.83, 2.915, 3.179, 3.012, 3.094, 2.824, 2.953, 2.781, 2.794, 2.871, 2.736, 2.792, 2.841, 2.978, 3.334, 3.37, 3.377, 3.296, 3.633, 3.468, 3.512, 3.346, 3.226, 3.173, 3.17, 3.294, 3.263, 3.455, 3.077, 2.997, 2.81, 2.574, 2.596, 2.319, 2.284, 2.251, 2.361, 2.404, 2.742, 2.846, 2.779, 2.792, 2.538, 2.524, 2.833, 2.939, 3.016, 2.842, 2.719, 2.731, 2.704, 2.581, 2.631, 2.747, 2.78, 2.958, 3.136, 3.224, 3.184, 3.016, 2.998, 3.053, 2.741, 2.736, 2.785, 2.567, 2.448, 2.511, 2.48, 2.294, 2.255, 2.143, 2.19, 2.512, 2.844, 2.934, 2.815, 2.791, 2.646, 2.579, 2.467, 2.7, 2.857, 2.654, 2.73, 2.689, 2.803, 2.935, 2.666, 2.683, 2.679, 2.739, 2.625, 2.64, 2.745, 2.384, 2.39, 2.641, 2.887, 2.945, 3.04, 2.74, 2.728, 2.762, 2.488, 2.411, 2.288, 2.291, 2.214, 2.115, 2.286, 2.548, 2.909, 2.773, 2.646, 2.73, 2.737, 2.607, 2.702, 2.759, 2.637, 2.631, 2.318, 2.302, 2.23, 2.142, 2.301, 2.199, 2.756, 2.988, 3.09, 3.122, 2.738, 2.685, 2.813, 2.628, 2.641, 2.41, 2.421, 2.256, 2.135, 2.065, 2.199, 2.268, 2.317, 2.16, 2.233, 2.196, 2.471, 2.378, 2.259, 2.379, 2.758, 2.569, 2.929, 2.811, 3.072, 3.257, 3.049, 2.926, 3.09, 2.585, 2.42, 2.252, 2.463, 2.491, 2.654, 2.317, 2.221, 2.132, 2.16, 2.442, 2.345, 2.276, 2.318, 2.541, 2.785, 2.615, 2.599, 2.501, 2.594, 2.62, 2.669, 2.388, 2.243, 2.165, 2.025, 2.025, 2.049, 2.226, 2.22, 2.036, 2.261, 2.348, 2.261, 2.408, 2.428, 2.717, 2.756, 2.738, 2.81, 2.48, 2.695, 2.915, 2.879, 2.764, 2.735, 2.679, 2.478, 2.509, 2.305, 2.262, 2.369, 2.441, 2.505, 2.48, 2.271, 2.146, 2.096, 2.075, 2.024, 1.99, 1.996, 2.052, 2.275, 2.225, 2.201, 2.251, 2.407, 2.654, 2.183, 2.276, 2.202, 2.398, 2.317, 2.358, 2.343, 2.665, 2.822, 2.429, 2.363, 2.386, 2.332, 2.495, 2.527, 2.483, 2.249, 2.17, 2.14, 2.052, 2.108, 2.09, 2.125, 2.053, 2.018, 1.988, 2.166, 2.276, 2.544, 2.736, 2.867, 2.457, 2.375, 2.234, 2.195, 1.805, 1.861, 1.99, 1.959, 1.974, 2.169, 2.34, 2.553, 2.737, 2.676, 2.624, 2.582, 2.301, 2.34, 2.418, 2.271, 1.999, 1.977]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004, 0.007, 0.02, 0.025, 0.008, 0.083, 0.077, 0.128, 0.207, 0.204, 0.266, 0.366, 0.38, 0.485, 0.549, 0.672, 0.706, 0.783, 0.967, 1.017, 1.132, 1.17, 1.287, 1.479, 1.535, 1.613, 1.767, 1.758, 1.736, 1.716, 1.749, 1.851, 1.728, 1.76, 1.755, 1.845, 1.838, 1.92, 2.125, 1.932, 1.892, 1.971, 1.912, 1.916, 1.795, 1.774, 1.82, 2.016, 1.745, 1.891, 1.831, 1.868, 1.771, 1.644, 1.663, 1.741, 1.631, 1.686, 1.716, 1.938, 1.67, 1.705, 1.679, 1.542, 1.594, 1.478, 1.744, 1.494, 1.578, 1.729, 1.897, 1.669, 1.938, 1.721, 1.797, 1.802, 1.834, 1.663, 1.773, 1.497, 1.427, 1.562, 1.37, 1.608, 1.715, 1.701, 1.727, 1.63, 1.919, 1.932, 1.672, 1.558, 1.588, 1.546, 1.588, 1.369, 1.63, 1.411, 1.39, 1.402, 1.352, 1.346, 1.508, 1.509, 1.59, 1.498, 1.342, 1.424, 1.564, 1.413, 1.49, 1.36, 1.344, 1.377, 1.401, 1.247, 1.547, 1.46, 1.36, 1.253, 1.318, 1.281, 1.202, 1.231, 1.245, 1.159, 1.492, 1.57, 1.477, 1.854, 1.649, 1.476, 1.663, 1.277, 1.485, 1.384, 1.31, 1.339, 1.3, 1.322, 1.409, 1.441, 1.1, 1.308, 1.329, 1.22, 1.301, 1.365, 1.258, 1.235, 1.27, 1.224, 1.406, 1.402, 1.105, 1.183, 1.093, 1.144, 1.21, 1.322, 1.436, 1.364, 1.292, 1.239, 1.035, 1.088, 1.015, 1.187, 1.286, 1.408, 1.385, 1.352, 1.339, 1.097, 1.274, 1.139, 1.303, 1.24, 1.302, 1.07, 1.08, 0.911, 1.191, 1.205, 1.29, 1.25, 1.267, 1.228, 1.151, 1.085, 1.129, 1.179, 1.078, 1.262, 1.072, 1.215, 1.5, 1.384, 1.338, 1.477, 1.408, 1.555, 1.456, 1.484, 1.254, 1.285, 1.343, 1.376, 1.271, 1.491, 1.281, 1.153, 1.155, 1.0, 1.093, 0.83, 0.873, 0.831, 0.982, 0.96, 1.142, 1.173, 1.196, 1.126, 0.992, 1.075, 1.134, 1.226, 1.33, 1.087, 1.142, 1.166, 1.026, 1.1, 1.022, 1.141, 1.172, 1.22, 1.353, 1.378, 1.287, 1.2, 1.339, 1.144, 1.128, 1.118, 1.11, 1.084, 0.926, 1.021, 0.996, 0.905, 0.845, 0.914, 0.768, 1.097, 1.157, 1.364, 1.108, 1.199, 1.078, 1.076, 0.953, 1.133, 1.168, 1.118, 1.102, 1.179, 1.082, 1.498, 0.923, 1.055, 1.083, 1.092, 1.2, 1.005, 1.135, 0.949, 0.897, 1.181, 1.214, 1.172, 1.327, 1.121, 1.108, 1.197, 0.961, 0.934, 0.967, 0.913, 0.893, 0.803, 0.934, 1.142, 1.183, 1.208, 1.137, 1.118, 1.169, 1.1, 1.143, 1.157, 1.022, 1.161, 0.922, 0.921, 0.905, 0.859, 0.98, 0.872, 1.179, 1.337, 1.167, 1.51, 1.112, 1.108, 1.13, 1.082, 1.196, 0.954, 1.098, 0.883, 0.854, 0.809, 0.921, 0.898, 1.056, 0.83, 0.961, 0.897, 1.039, 1.065, 0.821, 1.048, 1.161, 1.044, 1.325, 1.175, 1.351, 1.552, 1.234, 1.079, 1.49, 0.926, 0.931, 0.962, 1.005, 1.09, 1.193, 0.853, 0.981, 0.815, 0.905, 1.107, 0.94, 0.998, 1.006, 1.002, 1.333, 0.96, 1.094, 1.147, 1.074, 1.026, 1.236, 0.986, 0.833, 0.898, 0.776, 0.915, 0.815, 0.886, 1.015, 0.759, 1.015, 1.036, 0.917, 1.036, 1.111, 1.1, 1.165, 1.224, 1.254, 0.949, 1.105, 1.339, 1.203, 1.171, 1.22, 1.229, 0.994, 1.121, 0.897, 0.929, 1.091, 0.989, 1.041, 1.111, 0.884, 0.917, 0.781, 0.928, 0.851, 0.811, 0.906, 0.876, 0.922, 0.971, 0.882, 0.967, 1.074, 1.165, 0.903, 0.875, 0.89, 1.124, 0.947, 0.971, 0.967, 1.292, 1.275, 0.938, 1.063, 0.863, 1.057, 1.071, 1.032, 1.142, 0.953, 0.803, 1.06, 0.768, 0.933, 0.884, 0.872, 0.929, 0.877, 0.727, 1.064, 0.953, 1.014, 1.367, 1.179, 1.095, 0.941, 0.841, 1.002, 0.675, 0.783, 0.883, 0.772, 0.859, 0.925, 1.049, 1.129, 1.211, 1.211, 1.194, 0.987, 1.003, 1.02, 1.003, 1.024, 0.785, 0.873]\n",
      "Val Error(all epochs): 5.357161521911621 \n",
      " [49.714, 49.688, 49.671, 49.635, 49.559, 49.441, 49.257, 49.104, 48.895, 48.668, 48.545, 48.237, 47.794, 47.556, 46.814, 46.178, 45.594, 44.757, 44.495, 43.803, 42.603, 42.308, 41.058, 40.365, 39.706, 38.528, 37.249, 36.022, 32.544, 30.434, 31.366, 28.929, 27.537, 25.321, 24.971, 22.176, 18.274, 15.924, 13.232, 10.404, 10.38, 11.901, 7.167, 8.12, 7.261, 7.316, 8.079, 7.818, 7.436, 9.137, 9.124, 8.9, 7.889, 7.869, 8.192, 8.729, 6.833, 8.467, 9.424, 10.585, 11.844, 10.608, 11.062, 9.778, 10.363, 10.414, 10.207, 10.161, 9.995, 10.375, 11.605, 10.258, 10.175, 10.535, 9.623, 9.926, 9.476, 9.634, 9.096, 9.978, 9.71, 9.699, 9.714, 8.921, 8.387, 8.659, 9.19, 9.329, 9.001, 9.733, 9.741, 9.612, 9.93, 7.635, 7.939, 7.628, 8.157, 8.65, 8.937, 11.339, 9.099, 9.799, 10.157, 8.93, 8.149, 7.277, 7.921, 6.361, 6.171, 8.403, 8.02, 7.011, 5.994, 6.806, 7.638, 7.149, 7.302, 7.656, 7.489, 7.169, 7.316, 7.326, 6.403, 6.196, 6.516, 5.884, 6.55, 6.848, 7.263, 7.57, 8.432, 6.919, 7.84, 7.974, 8.159, 8.688, 8.855, 7.612, 8.73, 6.198, 7.433, 8.328, 10.119, 8.396, 8.036, 6.863, 7.293, 6.868, 7.082, 7.108, 8.952, 7.413, 7.38, 6.332, 7.656, 7.708, 7.158, 8.461, 8.689, 8.903, 8.39, 7.305, 7.477, 6.377, 6.826, 5.357, 6.438, 6.241, 5.518, 6.426, 5.946, 6.988, 6.56, 7.442, 7.489, 6.428, 6.724, 7.26, 7.129, 8.043, 6.666, 6.647, 6.799, 7.055, 6.319, 8.044, 7.128, 6.776, 6.54, 6.626, 6.82, 7.136, 6.688, 7.204, 6.458, 6.798, 6.713, 7.155, 6.254, 7.104, 7.362, 7.159, 7.842, 6.678, 6.89, 6.621, 7.447, 7.219, 8.417, 7.794, 7.611, 6.691, 7.829, 6.895, 7.505, 8.039, 8.22, 8.834, 7.034, 8.532, 7.315, 6.675, 6.878, 6.887, 7.188, 7.618, 7.732, 7.635, 8.083, 7.995, 7.454, 7.282, 6.549, 7.094, 6.21, 7.197, 6.043, 6.507, 6.775, 5.831, 5.687, 6.104, 7.123, 8.03, 6.818, 6.754, 6.594, 7.753, 7.532, 7.535, 8.391, 7.358, 8.85, 7.95, 8.59, 10.168, 8.637, 7.931, 8.355, 8.463, 8.208, 8.041, 7.901, 7.599, 7.268, 6.934, 6.863, 7.238, 6.912, 7.234, 6.571, 6.858, 8.143, 8.768, 6.576, 6.066, 6.139, 7.472, 7.713, 7.205, 7.051, 7.349, 6.889, 7.343, 7.811, 7.589, 7.086, 8.075, 8.54, 8.431, 8.241, 7.145, 6.869, 7.726, 6.413, 7.435, 6.772, 7.632, 7.583, 7.456, 6.821, 7.913, 7.471, 7.854, 7.678, 7.341, 7.395, 7.882, 8.377, 7.259, 6.446, 7.309, 7.734, 8.241, 6.839, 7.265, 6.638, 7.013, 6.567, 7.563, 6.882, 6.72, 7.237, 6.321, 6.783, 7.475, 6.807, 7.59, 7.033, 6.706, 6.544, 6.912, 6.806, 7.087, 6.658, 6.862, 6.32, 7.118, 7.12, 7.289, 6.785, 7.221, 6.804, 7.44, 6.908, 7.515, 7.369, 6.49, 5.728, 7.078, 7.308, 6.914, 6.292, 6.642, 6.809, 6.71, 6.514, 7.39, 6.555, 7.79, 6.859, 7.239, 7.259, 7.087, 7.204, 7.463, 6.876, 7.362, 6.845, 7.291, 6.473, 7.231, 5.993, 6.453, 7.066, 8.86, 8.138, 7.372, 6.367, 7.733, 6.37, 6.573, 6.174, 6.171, 6.154, 6.797, 7.272, 7.568, 6.465, 7.217, 7.119, 7.348, 7.166, 7.843, 6.614, 7.379, 8.039, 7.54, 6.952, 6.728, 7.463, 7.263, 7.045, 7.609, 6.969, 7.281, 7.609, 6.925, 7.96, 7.358, 7.797, 7.137, 7.185, 7.163, 6.468, 6.991, 5.519, 6.597, 6.169, 6.344, 6.5, 6.56, 7.229, 7.489, 7.175, 7.571, 7.331, 6.554, 6.485, 6.027, 6.715, 5.779, 6.235, 6.459, 6.577, 6.312, 6.46, 7.164, 6.816, 6.726, 6.906, 6.837, 6.628, 7.046, 6.118, 6.532, 5.99, 6.427, 6.92, 5.962, 6.124, 6.711, 6.163, 5.992, 7.305, 8.031, 7.776, 7.285, 8.823, 6.505, 7.838, 6.464, 6.402, 6.494, 7.076, 7.079, 6.835, 7.143, 6.143, 7.029, 6.191, 6.759, 6.728, 7.159, 7.16, 6.102, 6.465, 7.688, 6.936, 8.338, 6.345, 6.202, 6.914, 6.273, 6.586, 6.423, 6.264, 6.404, 6.213, 6.321, 5.929, 5.945, 6.709, 6.325, 6.484, 5.605, 6.166, 5.547, 6.056, 5.929]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.045, 0.107, 0.193, 0.281, 0.403, 0.725, 0.76, 0.542, 1.621, 1.385, 1.898, 1.859, 1.38, 1.391, 1.56, 1.089, 1.066, 0.982, 1.512, 2.149, 1.705, 1.207, 2.336, 4.465, 4.274, 4.111, 5.828, 2.889, 4.916, 3.502, 1.797, 3.427, 1.353, 1.399, 1.396, 1.175, 0.848, 1.328, 1.399, 1.133, 2.284, 2.566, 1.752, 1.333, 1.717, 1.423, 1.607, 1.483, 1.748, 1.783, 2.105, 1.894, 1.25, 0.909, 1.561, 0.898, 0.736, 0.672, 0.852, 1.622, 1.72, 1.421, 1.107, 0.848, 0.694, 0.501, 0.626, 0.595, 0.653, 0.698, 0.975, 1.384, 1.907, 1.708, 1.662, 1.248, 1.322, 0.98, 2.918, 2.236, 1.389, 1.777, 1.624, 1.784, 1.421, 2.205, 3.398, 3.633, 2.093, 4.855, 3.114, 2.472, 2.004, 1.588, 1.735, 1.684, 0.802, 1.584, 0.87, 1.257, 1.379, 1.094, 0.69, 1.108, 0.611, 1.449, 1.031, 0.937, 0.457, 1.275, 0.922, 1.46, 1.483, 1.663, 1.66, 1.259, 1.035, 1.414, 1.777, 1.874, 1.455, 0.671, 1.487, 1.056, 1.005, 0.949, 1.344, 1.783, 0.97, 1.89, 1.69, 2.311, 2.091, 2.476, 3.136, 2.573, 2.322, 1.913, 3.47, 1.588, 1.436, 1.61, 2.465, 1.574, 2.083, 1.796, 2.31, 1.851, 5.074, 2.006, 2.547, 1.772, 2.789, 2.403, 3.234, 2.856, 3.199, 2.31, 2.302, 1.834, 3.722, 2.031, 3.8, 2.251, 2.356, 1.783, 1.394, 1.727, 1.384, 3.183, 1.606, 2.069, 1.062, 1.379, 1.064, 1.468, 1.342, 2.068, 1.795, 1.64, 1.236, 1.191, 1.069, 0.982, 2.168, 1.542, 2.285, 2.084, 2.265, 1.604, 1.402, 2.65, 1.433, 1.716, 1.696, 1.003, 1.85, 1.403, 2.318, 1.345, 4.87, 1.776, 2.004, 2.758, 5.425, 4.496, 3.973, 3.353, 5.032, 6.219, 3.962, 3.307, 2.328, 2.642, 1.698, 2.057, 1.626, 1.657, 1.22, 1.39, 0.934, 0.492, 0.941, 1.032, 0.868, 1.298, 1.853, 1.153, 1.825, 1.272, 2.019, 1.961, 2.274, 1.778, 2.281, 2.074, 1.598, 2.504, 0.698, 1.037, 3.544, 3.561, 2.289, 1.319, 1.337, 2.276, 2.27, 2.385, 2.881, 1.676, 1.632, 1.451, 1.773, 1.225, 1.09, 0.901, 1.25, 1.787, 1.715, 1.266, 2.131, 1.649, 2.068, 1.249, 1.59, 1.118, 1.76, 2.184, 0.973, 1.776, 2.67, 2.035, 1.832, 1.037, 0.818, 1.072, 2.143, 0.996, 1.12, 0.799, 2.252, 1.499, 2.216, 2.331, 1.567, 1.489, 2.509, 2.406, 2.931, 2.771, 2.406, 1.881, 1.681, 1.458, 1.384, 2.354, 2.844, 3.81, 2.977, 3.767, 2.89, 2.882, 3.672, 4.005, 2.683, 2.29, 2.2, 2.244, 2.244, 1.548, 1.702, 1.468, 3.071, 2.91, 2.805, 1.32, 1.275, 1.835, 2.246, 1.706, 1.114, 1.91, 1.623, 1.62, 2.434, 1.472, 2.265, 2.042, 2.498, 2.012, 2.037, 1.896, 2.12, 1.729, 2.763, 2.477, 2.82, 1.896, 2.568, 2.375, 4.811, 6.605, 5.954, 3.09, 2.582, 1.711, 2.273, 3.361, 2.24, 2.845, 2.669, 2.036, 2.047, 2.09, 2.309, 2.009, 2.346, 2.972, 1.541, 2.302, 2.55, 3.934, 3.535, 2.207, 3.721, 3.559, 3.241, 2.436, 2.4, 1.742, 2.497, 1.917, 2.359, 2.394, 1.634, 1.716, 2.163, 2.038, 1.473, 2.035, 1.595, 2.018, 3.096, 3.492, 3.409, 2.014, 4.372, 2.962, 4.535, 4.877, 5.379, 5.57, 4.827, 4.254, 3.95, 3.84, 2.762, 2.759, 3.298, 2.791, 2.555, 2.569, 2.473, 1.61, 2.207, 2.387, 2.245, 2.576, 3.061, 2.153, 2.803, 2.802, 2.99, 2.541, 2.629, 3.299, 2.615, 2.137, 3.004, 1.977, 2.243, 1.251, 1.143, 1.108, 0.975, 2.552, 1.3, 2.149, 1.798, 1.703, 1.344, 1.354, 1.702, 2.242, 2.025, 2.125, 2.811, 1.847, 2.031, 1.883, 1.413, 2.077, 2.461, 1.621, 2.38, 1.296, 3.517, 2.938, 2.615, 3.588, 3.781, 2.299, 2.937, 2.669, 2.626, 2.848, 3.587, 2.446, 3.075, 3.065, 4.116, 3.326, 3.774, 2.954, 2.672, 2.668]\n",
      "\n",
      "#Fold: 1 \n",
      "Trainig set size: 420 , Time: 0:11:56 , best_lambda: 1 , min_  , error: 5.357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test starts:  467 , ends:  518\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 156.0125 - mse: 85.3174 - mae: 7.0917 - fp_mae: 2.9470\n",
      "average_error:  7.092 , fp_average_error:  2.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:00<00:00, 242.20it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 261.61it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 234.58it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 232.09it/s]\n",
      "100%|██████████| 107/107 [00:00<00:00, 245.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Fold: 2 , Training Size: 420 , Validation size: 47 , Test Size 52\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 48.59294, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 48.59294 to 48.51162, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 48.51162 to 48.41771, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 48.41771 to 48.34321, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 48.34321 to 48.22878, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 48.22878 to 48.12853, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 48.12853 to 47.99920, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 47.99920 to 47.81695, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 47.81695 to 47.55347, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 47.55347 to 47.18704, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 47.18704 to 46.93751, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 46.93751 to 46.56923, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 46.56923 to 46.20687, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 46.20687 to 46.01591, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 46.01591 to 45.70876, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 45.70876 to 45.22247, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 45.22247 to 44.62968, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 44.62968\n",
      "\n",
      "Epoch 00019: val_mae improved from 44.62968 to 43.97631, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 43.97631 to 43.43235, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 43.43235 to 43.25317, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 43.25317 to 42.00274, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 42.00274\n",
      "\n",
      "Epoch 00024: val_mae improved from 42.00274 to 41.64124, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 41.64124 to 41.48943, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 41.48943 to 39.99290, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 39.99290\n",
      "\n",
      "Epoch 00028: val_mae improved from 39.99290 to 39.08734, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 39.08734 to 38.10422, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 38.10422\n",
      "\n",
      "Epoch 00031: val_mae improved from 38.10422 to 36.37764, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 36.37764\n",
      "\n",
      "Epoch 00033: val_mae improved from 36.37764 to 35.46498, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 35.46498\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 35.46498\n",
      "\n",
      "Epoch 00036: val_mae improved from 35.46498 to 33.99033, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00037: val_mae improved from 33.99033 to 33.56475, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 33.56475\n",
      "\n",
      "Epoch 00039: val_mae improved from 33.56475 to 30.02151, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00040: val_mae improved from 30.02151 to 29.73103, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00041: val_mae improved from 29.73103 to 27.68993, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 27.68993\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 27.68993\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 27.68993\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 27.68993\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 27.68993\n",
      "\n",
      "Epoch 00047: val_mae improved from 27.68993 to 27.64710, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 27.64710\n",
      "\n",
      "Epoch 00049: val_mae improved from 27.64710 to 25.25458, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00050: val_mae did not improve from 25.25458\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 25.25458\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 25.25458\n",
      "\n",
      "Epoch 00053: val_mae improved from 25.25458 to 23.96426, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 23.96426\n",
      "\n",
      "Epoch 00055: val_mae improved from 23.96426 to 19.69616, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 19.69616\n",
      "\n",
      "Epoch 00057: val_mae improved from 19.69616 to 19.65331, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 19.65331\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 19.65331\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 19.65331\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 19.65331\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 19.65331\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 19.65331\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 19.65331\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 19.65331\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 19.65331\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 19.65331\n",
      "\n",
      "Epoch 00068: val_mae improved from 19.65331 to 19.21556, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 19.21556\n",
      "\n",
      "Epoch 00070: val_mae improved from 19.21556 to 18.30564, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00071: val_mae improved from 18.30564 to 17.48210, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 17.48210\n",
      "\n",
      "Epoch 00073: val_mae improved from 17.48210 to 17.45188, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 17.45188\n",
      "\n",
      "Epoch 00075: val_mae improved from 17.45188 to 16.87651, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00076: val_mae improved from 16.87651 to 14.76167, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00077: val_mae improved from 14.76167 to 14.37842, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 14.37842\n",
      "\n",
      "Epoch 00079: val_mae improved from 14.37842 to 13.24384, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 13.24384\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 13.24384\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 13.24384\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 13.24384\n",
      "\n",
      "Epoch 00084: val_mae improved from 13.24384 to 12.78316, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 12.78316\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 12.78316\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 12.78316\n",
      "\n",
      "Epoch 00088: val_mae improved from 12.78316 to 11.63902, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 11.63902\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 11.63902\n",
      "\n",
      "Epoch 00091: val_mae improved from 11.63902 to 11.43029, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00092: val_mae improved from 11.43029 to 11.34952, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 11.34952\n",
      "\n",
      "Epoch 00094: val_mae improved from 11.34952 to 10.43395, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 10.43395\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 10.43395\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 10.43395\n",
      "\n",
      "Epoch 00098: val_mae improved from 10.43395 to 9.98164, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 9.98164\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 9.98164\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 9.98164\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 9.98164\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 9.98164\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 9.98164\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 9.98164\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 9.98164\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 9.98164\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 9.98164\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 9.98164\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 9.98164\n",
      "\n",
      "Epoch 00111: val_mae improved from 9.98164 to 9.90487, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 9.90487\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 9.90487\n",
      "\n",
      "Epoch 00114: val_mae improved from 9.90487 to 9.69585, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00115: val_mae improved from 9.69585 to 9.05736, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00116: val_mae improved from 9.05736 to 8.34974, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00117: val_mae improved from 8.34974 to 7.69132, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 7.69132\n",
      "\n",
      "Epoch 00119: val_mae improved from 7.69132 to 6.82107, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 6.82107\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 6.82107\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 6.82107\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 6.82107\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 6.82107\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 6.82107\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 6.82107\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 6.82107\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 6.82107\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 6.82107\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 6.82107\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 6.82107\n",
      "\n",
      "Epoch 00132: val_mae improved from 6.82107 to 6.18546, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00133: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 6.18546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00294: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 6.18546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00455: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 6.18546\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 6.18546\n",
      "\n",
      "Lambda: 0.01 , Time: 0:03:57\n",
      "Train Error(all epochs): 0.6357986330986023 \n",
      " [49.169, 49.04, 48.954, 48.86, 48.75, 48.622, 48.468, 48.282, 48.054, 47.782, 47.473, 47.118, 46.718, 46.273, 45.763, 45.201, 44.58, 43.924, 43.166, 42.374, 41.529, 40.612, 39.645, 38.645, 37.608, 36.535, 35.447, 34.274, 33.059, 31.858, 30.579, 29.345, 28.023, 26.753, 25.469, 24.121, 22.743, 21.381, 19.978, 18.672, 17.283, 16.007, 14.73, 13.579, 12.415, 11.42, 10.396, 9.491, 8.69, 7.968, 7.286, 6.863, 6.343, 6.356, 6.071, 5.668, 5.284, 5.052, 4.775, 4.594, 4.607, 4.302, 4.34, 3.993, 3.877, 3.94, 3.825, 3.868, 3.98, 3.593, 3.78, 3.771, 3.396, 3.441, 3.321, 3.212, 3.222, 3.478, 3.549, 3.346, 3.23, 3.079, 3.096, 2.951, 2.938, 2.902, 2.813, 2.788, 2.806, 2.797, 2.718, 2.857, 2.775, 2.732, 2.749, 2.724, 2.813, 2.855, 2.951, 2.575, 2.579, 2.368, 2.358, 2.546, 2.391, 2.513, 2.463, 2.351, 2.512, 2.344, 2.276, 2.237, 2.23, 2.283, 2.551, 2.39, 2.285, 2.283, 2.08, 2.022, 1.877, 1.955, 1.839, 1.904, 1.85, 2.042, 2.185, 2.334, 2.475, 2.257, 1.999, 1.81, 1.859, 1.662, 1.724, 1.998, 2.064, 2.387, 1.922, 2.043, 2.103, 2.13, 2.389, 2.498, 2.193, 1.922, 1.982, 1.871, 1.828, 1.59, 1.618, 1.497, 1.515, 1.504, 1.691, 1.736, 1.737, 1.67, 1.589, 1.516, 1.482, 1.582, 1.545, 1.609, 1.464, 1.436, 1.368, 1.339, 1.414, 1.42, 1.439, 1.551, 1.576, 1.539, 2.053, 1.969, 2.02, 2.023, 2.032, 1.923, 1.749, 1.654, 1.535, 1.403, 1.359, 1.345, 1.203, 1.222, 1.074, 1.121, 1.124, 1.184, 1.294, 1.223, 1.331, 1.361, 1.414, 1.364, 1.356, 1.367, 1.201, 1.261, 1.306, 1.286, 1.415, 1.334, 1.271, 1.166, 1.34, 1.199, 1.227, 1.221, 1.264, 1.195, 1.297, 1.44, 1.557, 1.57, 1.414, 1.48, 1.48, 1.41, 1.319, 1.408, 1.342, 1.594, 1.545, 1.314, 1.321, 1.325, 1.075, 1.012, 1.078, 1.048, 0.927, 1.118, 1.225, 1.262, 1.114, 1.144, 1.175, 1.182, 1.153, 0.984, 0.984, 1.042, 1.289, 1.364, 1.435, 1.303, 1.198, 1.189, 1.116, 1.147, 1.182, 1.183, 1.032, 1.114, 1.143, 1.077, 0.992, 0.983, 0.939, 0.927, 1.177, 1.162, 1.163, 1.225, 1.166, 1.198, 1.442, 1.202, 1.053, 1.02, 1.12, 1.181, 1.156, 1.248, 1.218, 1.269, 1.104, 1.208, 1.144, 1.149, 1.195, 1.166, 1.003, 1.001, 0.947, 0.937, 0.864, 0.936, 0.79, 0.755, 0.789, 0.861, 0.862, 0.968, 0.903, 0.856, 0.883, 0.871, 0.883, 1.016, 1.06, 0.98, 1.13, 1.23, 1.115, 0.921, 1.02, 1.052, 1.121, 1.234, 1.303, 1.188, 1.095, 1.109, 1.11, 0.997, 0.975, 0.988, 0.952, 0.827, 0.89, 0.871, 0.891, 0.971, 1.005, 0.971, 0.927, 0.937, 0.912, 0.946, 0.931, 1.018, 1.07, 1.167, 1.096, 1.115, 1.172, 1.224, 1.164, 1.112, 1.156, 1.053, 1.116, 1.133, 1.178, 1.132, 1.165, 1.112, 1.188, 1.179, 1.073, 1.003, 0.964, 0.952, 0.914, 1.011, 1.036, 0.966, 0.913, 0.892, 0.842, 0.837, 0.842, 0.732, 0.674, 0.769, 0.808, 0.806, 0.949, 1.067, 1.012, 1.024, 1.1, 1.046, 0.943, 0.962, 0.803, 0.686, 0.737, 0.742, 0.736, 0.784, 0.82, 0.661, 0.75, 0.887, 0.991, 0.954, 1.09, 1.116, 1.027, 0.861, 0.84, 0.836, 0.888, 0.961, 0.932, 0.904, 0.955, 1.016, 1.008, 0.984, 0.94, 0.821, 0.753, 0.729, 0.749, 0.738, 0.799, 0.844, 0.861, 0.862, 0.886, 0.792, 0.745, 0.762, 0.762, 0.897, 1.064, 1.071, 0.88, 0.885, 0.839, 0.875, 0.936, 0.984, 0.926, 1.021, 1.049, 0.97, 0.925, 1.072, 1.009, 1.015, 1.128, 1.313, 1.117, 1.086, 1.12, 1.142, 1.054, 1.055, 0.983, 1.006, 1.01, 1.16, 1.385, 1.392, 1.406, 1.259, 1.291, 1.188, 1.247, 1.271, 1.228, 1.098, 1.079, 0.986, 0.945, 0.873, 0.785, 0.729, 0.801, 0.781, 0.699, 0.691, 0.787, 0.912, 0.845, 0.898, 0.877, 0.87, 0.823, 0.864, 0.848, 0.892, 0.932, 0.894, 0.768, 0.717, 0.719, 0.697, 0.74, 0.722, 0.663, 0.636, 0.685, 0.678, 0.642, 0.681, 0.803, 0.759, 0.721, 0.759, 0.8, 0.775]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.003, 0.011, 0.014, 0.025, 0.044, 0.078, 0.093, 0.137, 0.186, 0.267, 0.276, 0.463, 0.517, 0.772, 0.917, 0.946, 0.979, 1.033, 1.057, 1.042, 1.205, 1.105, 1.209, 1.141, 1.138, 1.215, 1.248, 1.327, 1.42, 1.276, 1.35, 1.405, 1.207, 1.315, 1.264, 1.246, 1.258, 1.41, 1.409, 1.41, 1.354, 1.253, 1.217, 1.178, 1.156, 1.204, 1.105, 1.129, 1.155, 1.164, 1.121, 1.245, 1.113, 1.128, 1.169, 1.147, 1.25, 1.167, 1.34, 1.015, 1.098, 0.95, 0.966, 1.12, 0.939, 1.128, 1.059, 0.924, 1.172, 1.009, 0.88, 1.014, 0.924, 0.945, 1.141, 1.017, 0.858, 1.116, 0.757, 0.92, 0.722, 0.85, 0.784, 0.811, 0.754, 0.919, 0.961, 1.028, 1.158, 0.94, 0.889, 0.694, 0.848, 0.681, 0.713, 1.022, 0.805, 1.194, 0.841, 0.861, 0.908, 1.016, 1.005, 1.206, 1.038, 0.755, 0.972, 0.77, 0.844, 0.649, 0.735, 0.617, 0.663, 0.631, 0.777, 0.707, 0.83, 0.729, 0.708, 0.669, 0.635, 0.731, 0.645, 0.804, 0.605, 0.652, 0.588, 0.59, 0.623, 0.642, 0.634, 0.76, 0.671, 0.74, 1.01, 0.804, 1.037, 0.846, 0.945, 0.935, 0.734, 0.768, 0.666, 0.643, 0.54, 0.664, 0.458, 0.605, 0.399, 0.553, 0.479, 0.519, 0.642, 0.472, 0.68, 0.569, 0.694, 0.597, 0.602, 0.656, 0.496, 0.6, 0.617, 0.544, 0.74, 0.509, 0.732, 0.389, 0.722, 0.473, 0.608, 0.539, 0.6, 0.518, 0.623, 0.689, 0.709, 0.813, 0.601, 0.73, 0.672, 0.693, 0.533, 0.731, 0.58, 0.808, 0.646, 0.704, 0.608, 0.593, 0.489, 0.515, 0.444, 0.564, 0.347, 0.612, 0.503, 0.638, 0.493, 0.513, 0.586, 0.541, 0.532, 0.455, 0.479, 0.438, 0.702, 0.535, 0.792, 0.555, 0.579, 0.61, 0.429, 0.629, 0.47, 0.62, 0.434, 0.55, 0.549, 0.476, 0.509, 0.404, 0.501, 0.379, 0.66, 0.441, 0.69, 0.485, 0.614, 0.572, 0.696, 0.544, 0.525, 0.442, 0.552, 0.565, 0.538, 0.663, 0.497, 0.719, 0.415, 0.671, 0.477, 0.598, 0.537, 0.563, 0.467, 0.495, 0.388, 0.531, 0.316, 0.517, 0.314, 0.386, 0.35, 0.424, 0.382, 0.493, 0.397, 0.431, 0.416, 0.422, 0.432, 0.466, 0.571, 0.381, 0.632, 0.529, 0.575, 0.4, 0.511, 0.456, 0.607, 0.523, 0.669, 0.561, 0.463, 0.612, 0.459, 0.538, 0.465, 0.439, 0.519, 0.305, 0.496, 0.356, 0.429, 0.51, 0.432, 0.539, 0.385, 0.474, 0.419, 0.478, 0.386, 0.57, 0.399, 0.696, 0.405, 0.613, 0.56, 0.557, 0.606, 0.492, 0.581, 0.469, 0.55, 0.539, 0.586, 0.481, 0.627, 0.45, 0.648, 0.496, 0.557, 0.496, 0.389, 0.517, 0.37, 0.537, 0.457, 0.477, 0.417, 0.43, 0.4, 0.396, 0.388, 0.356, 0.304, 0.364, 0.411, 0.339, 0.523, 0.472, 0.503, 0.497, 0.569, 0.487, 0.461, 0.458, 0.376, 0.301, 0.38, 0.319, 0.387, 0.344, 0.414, 0.311, 0.363, 0.41, 0.495, 0.435, 0.555, 0.524, 0.514, 0.376, 0.445, 0.329, 0.497, 0.399, 0.479, 0.45, 0.423, 0.513, 0.463, 0.489, 0.478, 0.348, 0.405, 0.282, 0.417, 0.289, 0.429, 0.369, 0.43, 0.389, 0.449, 0.332, 0.395, 0.333, 0.385, 0.43, 0.52, 0.512, 0.434, 0.414, 0.391, 0.43, 0.44, 0.489, 0.42, 0.512, 0.482, 0.507, 0.393, 0.573, 0.434, 0.536, 0.512, 0.683, 0.483, 0.571, 0.501, 0.601, 0.457, 0.578, 0.398, 0.541, 0.495, 0.511, 0.804, 0.519, 0.857, 0.437, 0.767, 0.46, 0.598, 0.669, 0.523, 0.547, 0.522, 0.453, 0.429, 0.42, 0.365, 0.348, 0.376, 0.396, 0.317, 0.337, 0.377, 0.425, 0.428, 0.436, 0.416, 0.45, 0.347, 0.482, 0.333, 0.477, 0.414, 0.447, 0.363, 0.348, 0.344, 0.328, 0.367, 0.336, 0.336, 0.288, 0.349, 0.319, 0.335, 0.286, 0.452, 0.293, 0.402, 0.331, 0.381, 0.397]\n",
      "Val Error(all epochs): 6.185457706451416 \n",
      " [48.593, 48.512, 48.418, 48.343, 48.229, 48.129, 47.999, 47.817, 47.553, 47.187, 46.938, 46.569, 46.207, 46.016, 45.709, 45.222, 44.63, 44.763, 43.976, 43.432, 43.253, 42.003, 42.353, 41.641, 41.489, 39.993, 40.279, 39.087, 38.104, 38.854, 36.378, 38.029, 35.465, 35.827, 35.507, 33.99, 33.565, 33.685, 30.022, 29.731, 27.69, 28.836, 28.519, 28.942, 27.955, 28.452, 27.647, 28.199, 25.255, 26.489, 25.409, 26.029, 23.964, 26.139, 19.696, 20.484, 19.653, 20.077, 20.474, 21.497, 22.203, 20.64, 23.016, 21.876, 22.452, 21.928, 22.087, 19.216, 19.64, 18.306, 17.482, 19.069, 17.452, 18.578, 16.877, 14.762, 14.378, 16.399, 13.244, 14.371, 13.682, 14.465, 15.264, 12.783, 15.438, 13.145, 15.891, 11.639, 12.662, 12.194, 11.43, 11.35, 11.571, 10.434, 14.234, 12.051, 14.45, 9.982, 14.011, 10.502, 12.8, 12.352, 12.056, 12.411, 10.595, 10.619, 12.731, 11.647, 11.899, 11.248, 9.905, 11.051, 12.48, 9.696, 9.057, 8.35, 7.691, 8.368, 6.821, 8.252, 7.543, 8.1, 7.754, 7.769, 8.135, 7.736, 8.327, 8.132, 7.002, 7.068, 7.074, 6.185, 7.449, 7.652, 7.891, 9.52, 7.339, 8.39, 6.757, 7.436, 6.968, 6.926, 6.817, 7.081, 6.535, 7.283, 6.522, 6.98, 6.426, 6.515, 6.678, 6.795, 6.563, 6.95, 6.705, 6.623, 6.761, 6.362, 6.939, 6.574, 6.764, 6.74, 6.757, 6.767, 6.793, 6.646, 6.548, 6.455, 6.715, 6.749, 6.213, 6.968, 6.85, 6.725, 7.183, 7.124, 6.61, 8.125, 7.046, 9.036, 6.969, 6.949, 6.607, 6.509, 6.717, 7.084, 6.916, 8.023, 6.748, 7.793, 6.605, 7.345, 6.417, 7.01, 6.583, 6.866, 6.654, 6.989, 6.559, 7.079, 6.827, 6.918, 6.838, 6.924, 6.802, 6.922, 6.994, 6.842, 7.182, 6.794, 6.907, 6.683, 7.222, 6.486, 6.769, 7.237, 6.815, 7.148, 7.317, 7.012, 7.051, 7.529, 6.497, 6.766, 7.12, 6.712, 6.798, 7.092, 6.675, 6.89, 6.982, 6.746, 7.039, 7.142, 6.525, 7.216, 6.661, 7.08, 6.738, 7.087, 6.544, 7.059, 6.682, 6.855, 6.991, 6.906, 6.73, 6.875, 6.9, 6.803, 7.677, 6.76, 7.094, 7.044, 6.797, 6.805, 7.2, 6.522, 7.024, 6.973, 6.642, 6.883, 6.993, 6.468, 7.254, 6.864, 6.699, 6.974, 7.048, 6.416, 7.243, 6.653, 6.886, 6.957, 6.736, 6.856, 7.12, 6.661, 6.827, 7.118, 6.609, 7.133, 7.005, 6.987, 7.26, 6.958, 6.644, 7.358, 6.704, 7.378, 6.905, 7.004, 6.56, 7.215, 6.721, 6.957, 6.935, 6.873, 6.688, 7.166, 6.648, 6.757, 7.146, 6.719, 6.834, 7.192, 6.447, 7.273, 6.956, 6.757, 7.12, 6.822, 6.889, 7.364, 6.656, 7.038, 7.004, 6.906, 7.041, 7.175, 6.904, 6.95, 7.068, 7.079, 6.993, 6.877, 6.786, 7.106, 6.786, 6.83, 7.393, 6.94, 7.249, 7.131, 6.66, 6.925, 6.944, 6.84, 6.882, 7.032, 6.552, 6.998, 6.806, 6.736, 6.652, 7.102, 6.655, 6.785, 6.92, 6.695, 6.679, 6.84, 6.742, 6.969, 6.878, 6.688, 6.851, 6.846, 6.41, 6.989, 6.611, 6.797, 6.837, 6.936, 6.503, 7.148, 6.63, 6.887, 6.963, 6.816, 6.826, 7.071, 6.663, 6.994, 7.004, 6.788, 6.921, 6.967, 6.843, 6.956, 7.076, 6.604, 7.201, 6.757, 6.695, 7.059, 6.952, 6.709, 7.31, 6.554, 7.032, 7.099, 6.767, 6.702, 7.166, 6.674, 7.035, 6.917, 6.757, 7.016, 7.164, 6.843, 7.315, 6.715, 6.915, 6.879, 6.833, 6.764, 7.111, 6.591, 6.936, 6.84, 6.956, 6.671, 7.144, 6.479, 7.078, 6.868, 6.819, 6.773, 6.998, 6.728, 7.057, 7.129, 6.687, 7.103, 6.866, 6.506, 7.177, 6.767, 6.525, 7.709, 7.295, 7.168, 7.918, 7.139, 6.44, 7.144, 6.707, 6.735, 6.927, 6.686, 6.788, 7.019, 7.165, 7.103, 6.728, 6.891, 6.835, 6.582, 6.899, 7.053, 7.069, 7.333, 6.912, 7.456, 6.939, 7.24, 7.245, 6.729, 6.826, 6.962, 6.59, 6.912, 6.967, 6.647, 7.083, 6.8, 6.767, 6.944, 6.726, 6.862, 7.067, 6.707, 6.943, 7.017, 6.699, 6.919, 7.015, 6.79, 7.056, 6.886, 6.925, 7.03, 6.823, 6.846, 6.92, 6.786, 6.79, 6.981, 6.774, 6.945, 7.052, 6.641, 6.978, 7.095, 6.696, 7.164, 6.903, 6.721]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.026, 0.077, 0.089, 0.083, 0.104, 0.075, 0.062, 0.111, 0.161, 0.141, 0.115, 0.137, 0.154, 0.13, 0.155, 0.121, 0.201, 0.249, 0.122, 0.248, 0.25, 0.337, 0.254, 0.288, 0.34, 0.244, 0.37, 0.237, 0.485, 0.346, 0.435, 0.555, 0.532, 0.611, 0.97, 0.406, 1.362, 0.546, 2.296, 0.697, 1.613, 0.731, 0.837, 1.032, 0.736, 1.543, 1.049, 0.807, 1.127, 0.879, 0.956, 1.343, 1.007, 0.911, 1.619, 2.065, 1.702, 3.039, 1.638, 3.154, 1.858, 1.991, 1.807, 1.66, 2.052, 1.451, 2.757, 1.426, 2.982, 2.148, 2.788, 2.51, 3.264, 2.326, 2.062, 2.038, 1.291, 2.86, 1.769, 3.029, 2.704, 4.249, 2.546, 4.225, 5.325, 4.59, 5.353, 3.441, 3.053, 3.073, 3.664, 3.649, 4.444, 3.659, 4.583, 4.574, 4.269, 4.638, 4.09, 4.407, 4.199, 4.263, 3.586, 4.518, 3.842, 4.759, 3.833, 3.8, 4.399, 3.222, 4.037, 3.253, 4.321, 3.648, 4.607, 4.19, 5.1, 4.422, 6.573, 5.144, 8.029, 4.278, 5.623, 4.016, 3.362, 3.641, 2.643, 3.861, 1.981, 3.202, 2.292, 3.044, 2.9, 3.65, 3.265, 3.766, 3.963, 3.639, 3.651, 3.626, 3.373, 4.533, 3.512, 3.96, 4.404, 3.721, 4.596, 3.294, 5.074, 2.963, 4.537, 3.228, 4.257, 3.214, 4.349, 3.824, 4.211, 4.783, 4.649, 3.868, 4.667, 4.654, 4.727, 3.593, 3.634, 4.183, 3.815, 4.459, 4.088, 3.274, 3.221, 3.137, 2.863, 3.079, 2.813, 3.317, 2.793, 3.848, 2.75, 3.477, 3.056, 3.331, 3.422, 3.436, 3.294, 3.246, 3.311, 3.825, 3.959, 5.161, 3.98, 6.2, 3.569, 5.558, 3.687, 5.04, 3.923, 4.889, 4.523, 4.526, 4.615, 4.097, 3.667, 3.55, 3.336, 2.963, 3.384, 3.081, 3.614, 3.636, 3.18, 3.155, 3.38, 3.102, 3.312, 3.767, 4.27, 4.292, 4.073, 4.315, 4.205, 3.53, 3.041, 3.296, 2.695, 3.0, 2.942, 3.249, 2.654, 3.352, 2.308, 3.584, 2.722, 3.513, 2.738, 3.215, 2.671, 3.742, 3.205, 4.026, 3.763, 3.73, 3.851, 3.387, 3.589, 3.602, 4.06, 3.981, 4.524, 4.734, 4.799, 3.48, 4.782, 3.001, 5.137, 4.457, 4.643, 4.182, 5.135, 4.419, 5.544, 3.714, 5.327, 3.486, 5.555, 3.928, 4.371, 4.378, 3.827, 4.457, 4.072, 5.093, 4.262, 5.144, 3.96, 4.274, 3.494, 4.635, 4.174, 4.344, 3.744, 3.049, 3.178, 3.024, 3.227, 2.974, 3.216, 3.086, 3.45, 3.851, 3.034, 3.982, 3.57, 4.806, 3.46, 4.87, 3.215, 4.43, 3.21, 4.01, 2.974, 3.69, 3.099, 3.7, 3.226, 3.408, 3.502, 3.235, 3.314, 3.523, 3.303, 3.276, 3.446, 3.459, 3.622, 4.257, 3.116, 4.16, 3.723, 3.705, 4.39, 3.965, 3.992, 3.745, 3.997, 3.677, 4.062, 3.791, 4.522, 3.673, 3.82, 2.788, 4.276, 3.37, 3.97, 3.849, 4.599, 3.475, 4.232, 3.058, 3.222, 2.795, 2.876, 2.756, 3.627, 2.916, 4.3, 3.117, 4.72, 3.176, 4.336, 3.216, 3.725, 2.893, 3.633, 3.111, 3.44, 3.006, 3.233, 3.23, 3.44, 3.843, 3.731, 4.45, 4.421, 3.877, 4.172, 3.9, 3.764, 3.839, 3.008, 3.902, 2.151, 2.48, 2.201, 2.345, 2.261, 3.312, 2.668, 2.864, 3.078, 3.344, 2.999, 3.497, 4.436, 5.274, 4.047, 4.333, 3.945, 3.524, 3.701, 3.01, 5.464, 3.166, 5.627, 4.255, 4.985, 4.42, 5.231, 4.318, 4.807, 4.262, 4.336, 4.355, 3.988, 4.586, 3.955, 4.46, 3.812, 4.324, 3.665, 4.44, 3.897, 4.611, 3.739, 4.453, 3.997, 4.094, 4.013, 3.984, 3.525, 4.068, 3.523, 4.268, 3.997, 3.789, 3.975, 3.454, 3.846, 3.437, 4.069, 3.513, 4.352, 3.62, 4.177, 3.749, 4.303, 3.895, 4.151, 3.593, 3.971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 48.60146, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 48.60146 to 48.52135, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 48.52135 to 48.43906, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 48.43906 to 48.36176, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 48.36176 to 48.24412, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 48.24412 to 48.09099, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 48.09099 to 47.92128, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 47.92128 to 47.76671, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 47.76671 to 47.57546, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 47.57546 to 47.37801, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 47.37801 to 47.19530, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 47.19530 to 46.87923, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 46.87923 to 46.54893, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 46.54893 to 46.30515, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 46.30515 to 46.13986, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 46.13986 to 45.64441, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 45.64441 to 45.43664, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 45.43664 to 44.85781, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 44.85781 to 44.09848, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 44.09848 to 43.45057, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 43.45057 to 42.45094, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 42.45094 to 41.51710, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 41.51710 to 40.85868, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 40.85868 to 39.71980, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 39.71980 to 38.75926, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 38.75926 to 38.74281, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 38.74281 to 38.39177, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 38.39177 to 37.53104, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 37.53104 to 37.23552, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 37.23552 to 35.58738, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00031: val_mae improved from 35.58738 to 35.38898, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 35.38898\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 35.38898\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 35.38898\n",
      "\n",
      "Epoch 00035: val_mae improved from 35.38898 to 35.07740, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 35.07740\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 35.07740\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 35.07740\n",
      "\n",
      "Epoch 00039: val_mae improved from 35.07740 to 34.06943, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00040: val_mae improved from 34.06943 to 33.06474, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00041: val_mae improved from 33.06474 to 33.05075, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00042: val_mae improved from 33.05075 to 29.20532, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 29.20532\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 29.20532\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 29.20532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00046: val_mae improved from 29.20532 to 29.15486, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 29.15486\n",
      "\n",
      "Epoch 00048: val_mae improved from 29.15486 to 27.15325, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00049: val_mae improved from 27.15325 to 25.29774, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00050: val_mae improved from 25.29774 to 24.19904, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00051: val_mae improved from 24.19904 to 22.56885, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00052: val_mae improved from 22.56885 to 21.60728, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00053: val_mae improved from 21.60728 to 20.48729, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00054: val_mae improved from 20.48729 to 19.42524, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00055: val_mae improved from 19.42524 to 18.70381, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 18.70381\n",
      "\n",
      "Epoch 00057: val_mae improved from 18.70381 to 14.48997, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 14.48997\n",
      "\n",
      "Epoch 00059: val_mae improved from 14.48997 to 12.88479, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00060: val_mae improved from 12.88479 to 12.47589, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 12.47589\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 12.47589\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 12.47589\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 12.47589\n",
      "\n",
      "Epoch 00065: val_mae improved from 12.47589 to 11.46340, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 11.46340\n",
      "\n",
      "Epoch 00067: val_mae improved from 11.46340 to 9.59373, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 9.59373\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 9.59373\n",
      "\n",
      "Epoch 00070: val_mae improved from 9.59373 to 9.33074, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 9.33074\n",
      "\n",
      "Epoch 00072: val_mae improved from 9.33074 to 8.98163, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00073: val_mae improved from 8.98163 to 8.83062, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00074: val_mae improved from 8.83062 to 7.76248, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00075: val_mae improved from 7.76248 to 7.44968, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00076: val_mae improved from 7.44968 to 7.41653, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 7.41653\n",
      "\n",
      "Epoch 00100: val_mae improved from 7.41653 to 7.17535, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 7.17535\n",
      "\n",
      "Epoch 00102: val_mae improved from 7.17535 to 6.98172, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 6.98172\n",
      "\n",
      "Epoch 00104: val_mae improved from 6.98172 to 6.67960, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 6.67960\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 6.67960\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 6.67960\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 6.67960\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 6.67960\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 6.67960\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 6.67960\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 6.67960\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 6.67960\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 6.67960\n",
      "\n",
      "Epoch 00115: val_mae improved from 6.67960 to 6.47251, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 6.47251\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 6.47251\n",
      "\n",
      "Epoch 00118: val_mae improved from 6.47251 to 6.25773, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 6.25773\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 6.25773\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 6.25773\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 6.25773\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 6.25773\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 6.25773\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 6.25773\n",
      "\n",
      "Epoch 00126: val_mae improved from 6.25773 to 6.02961, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00127: val_mae did not improve from 6.02961\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 6.02961\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 6.02961\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 6.02961\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 6.02961\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 6.02961\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 6.02961\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 6.02961\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 6.02961\n",
      "\n",
      "Epoch 00136: val_mae improved from 6.02961 to 5.96222, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 5.96222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00285: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 5.96222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00446: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 5.96222\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 5.96222\n",
      "\n",
      "Lambda: 0.1 , Time: 0:03:57\n",
      "Train Error(all epochs): 1.0181846618652344 \n",
      " [49.202, 49.096, 49.019, 48.929, 48.827, 48.707, 48.561, 48.393, 48.189, 47.952, 47.671, 47.354, 46.988, 46.574, 46.106, 45.568, 44.979, 44.343, 43.639, 42.87, 42.075, 41.207, 40.259, 39.212, 38.201, 37.142, 36.031, 34.89, 33.743, 32.558, 31.352, 30.144, 28.875, 27.65, 26.383, 25.131, 23.865, 22.598, 21.37, 20.153, 18.942, 17.828, 16.628, 15.623, 14.495, 13.536, 12.544, 11.681, 10.663, 9.634, 8.915, 8.103, 7.689, 6.96, 6.787, 6.468, 6.479, 6.104, 6.063, 5.527, 5.291, 5.142, 4.983, 4.915, 4.806, 4.774, 4.894, 4.588, 4.549, 4.42, 4.326, 4.35, 4.434, 4.556, 4.482, 4.21, 4.027, 4.112, 3.849, 4.008, 3.771, 3.764, 3.644, 3.571, 3.447, 3.445, 3.442, 3.415, 3.552, 3.401, 3.276, 3.388, 3.206, 3.017, 3.008, 2.91, 2.888, 2.956, 2.869, 3.024, 3.282, 3.21, 3.18, 3.147, 3.065, 2.9, 2.947, 2.855, 2.877, 2.836, 3.011, 2.876, 3.247, 3.143, 3.012, 2.956, 2.967, 2.902, 2.816, 2.916, 2.761, 2.759, 3.063, 2.869, 2.811, 2.681, 2.627, 2.395, 2.432, 2.289, 2.297, 2.25, 2.34, 2.25, 2.45, 2.401, 2.45, 2.532, 2.477, 2.31, 2.424, 2.186, 2.124, 2.213, 2.384, 2.363, 2.401, 2.261, 2.269, 2.229, 2.229, 2.038, 1.925, 1.805, 1.934, 1.919, 2.194, 2.337, 2.307, 2.344, 2.269, 2.488, 2.682, 2.67, 2.27, 2.096, 1.924, 1.818, 1.945, 1.977, 2.029, 2.008, 1.909, 1.919, 1.987, 2.068, 1.926, 1.882, 1.605, 1.72, 1.834, 1.874, 1.938, 1.818, 1.838, 2.165, 2.497, 2.716, 2.648, 2.402, 2.469, 2.26, 1.946, 1.975, 1.96, 1.948, 1.733, 1.772, 1.843, 1.984, 1.983, 1.829, 1.796, 1.705, 1.74, 1.821, 1.988, 2.033, 2.023, 2.01, 2.094, 2.139, 1.94, 1.762, 1.613, 1.684, 1.718, 1.682, 1.611, 1.546, 1.427, 1.687, 1.851, 1.839, 1.81, 1.786, 1.78, 1.63, 1.557, 1.628, 1.563, 1.582, 1.545, 1.63, 1.624, 1.8, 1.819, 1.989, 1.9, 1.741, 1.684, 1.649, 1.685, 1.856, 1.855, 2.093, 2.118, 1.999, 1.833, 1.792, 1.857, 1.692, 1.722, 1.654, 1.5, 1.403, 1.517, 1.53, 1.627, 1.623, 1.926, 2.037, 1.967, 2.024, 1.992, 2.078, 2.168, 2.292, 2.173, 1.973, 1.802, 1.648, 1.472, 1.509, 1.609, 1.474, 1.53, 1.44, 1.499, 1.54, 1.554, 1.62, 1.617, 1.718, 1.859, 1.712, 1.557, 1.618, 1.829, 1.717, 1.592, 1.457, 1.412, 1.382, 1.512, 1.443, 1.309, 1.292, 1.341, 1.279, 1.299, 1.311, 1.455, 1.506, 1.701, 1.835, 2.048, 2.197, 2.17, 1.791, 1.673, 1.591, 1.584, 1.539, 1.585, 1.503, 1.332, 1.342, 1.509, 1.502, 1.473, 1.276, 1.284, 1.324, 1.465, 1.54, 1.522, 1.42, 1.398, 1.452, 1.437, 1.448, 1.614, 1.594, 1.597, 1.703, 1.925, 1.875, 1.771, 1.646, 1.743, 1.65, 1.6, 1.653, 1.632, 1.552, 1.451, 1.338, 1.313, 1.261, 1.221, 1.175, 1.25, 1.451, 1.638, 1.685, 1.67, 1.703, 1.729, 1.766, 1.878, 1.823, 1.887, 1.815, 1.628, 1.701, 1.742, 1.729, 1.769, 1.836, 1.787, 1.76, 1.749, 1.729, 1.582, 1.698, 1.924, 1.918, 1.648, 1.468, 1.375, 1.327, 1.248, 1.229, 1.274, 1.279, 1.231, 1.119, 1.018, 1.091, 1.139, 1.177, 1.255, 1.377, 1.379, 1.424, 1.627, 1.725, 1.61, 1.528, 1.561, 1.614, 1.727, 1.722, 1.588, 1.527, 1.503, 1.623, 1.631, 1.538, 1.506, 1.516, 1.414, 1.387, 1.343, 1.457, 1.581, 1.513, 1.423, 1.44, 1.214, 1.35, 1.61, 1.777, 1.868, 1.848, 1.803, 1.818, 1.797, 1.75, 1.601, 1.449, 1.355, 1.472, 1.714, 1.785, 1.5, 1.253, 1.152, 1.106, 1.204, 1.243, 1.292, 1.335, 1.373, 1.455, 1.577, 1.449, 1.428, 1.44, 1.658, 1.895, 1.804, 1.695, 1.655, 1.605, 1.737, 1.951, 1.903, 1.755, 1.983, 1.772, 1.667, 1.711, 1.772, 1.813, 1.853, 1.662, 1.541, 1.646, 1.66, 1.582, 1.496, 1.396, 1.366, 1.355, 1.399, 1.347, 1.238, 1.195, 1.155, 1.172, 1.275, 1.547, 1.714, 1.702, 1.537, 1.394, 1.278, 1.328, 1.22, 1.14, 1.16, 1.288, 1.416, 1.374, 1.237, 1.154, 1.135, 1.089]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.007, 0.012, 0.042, 0.021, 0.083, 0.066, 0.103, 0.099, 0.173, 0.215, 0.227, 0.352, 0.364, 0.49, 0.501, 0.691, 0.79, 1.008, 1.077, 1.284, 1.117, 1.153, 1.243, 1.336, 1.381, 1.461, 1.522, 1.683, 1.569, 1.522, 1.56, 1.469, 1.585, 1.561, 1.707, 1.636, 1.543, 1.48, 1.548, 1.384, 1.534, 1.423, 1.394, 1.343, 1.339, 1.29, 1.276, 1.344, 1.251, 1.416, 1.337, 1.294, 1.347, 1.276, 1.097, 1.169, 1.099, 1.114, 1.182, 1.149, 1.211, 1.407, 1.395, 1.369, 1.214, 1.329, 1.199, 1.175, 1.218, 1.306, 1.113, 1.377, 1.157, 1.407, 1.456, 1.22, 1.29, 1.327, 1.179, 1.182, 1.3, 1.212, 1.116, 1.482, 1.175, 1.19, 1.183, 1.123, 0.994, 1.082, 0.967, 1.008, 0.944, 1.094, 0.938, 1.074, 1.149, 0.979, 1.24, 1.112, 0.983, 1.117, 1.024, 0.88, 1.028, 1.11, 0.996, 1.155, 1.009, 0.94, 1.084, 0.962, 0.96, 0.814, 0.734, 0.981, 0.774, 1.091, 1.05, 1.034, 1.099, 1.012, 1.188, 1.182, 1.296, 0.98, 0.986, 0.817, 0.761, 0.947, 0.822, 0.929, 0.963, 0.864, 0.868, 0.865, 1.08, 0.756, 0.922, 0.72, 0.715, 0.872, 0.904, 0.827, 0.784, 0.922, 0.986, 1.154, 1.282, 1.288, 1.045, 1.143, 1.027, 0.825, 1.016, 0.835, 0.88, 0.836, 0.77, 0.822, 0.921, 0.923, 0.792, 0.91, 0.701, 0.825, 0.831, 0.919, 0.932, 0.964, 0.962, 0.871, 1.096, 0.823, 0.89, 0.657, 0.706, 0.883, 0.686, 0.78, 0.709, 0.617, 0.826, 0.893, 0.783, 0.947, 0.777, 0.789, 0.818, 0.656, 0.782, 0.671, 0.758, 0.713, 0.744, 0.744, 0.838, 0.828, 0.929, 0.896, 0.748, 0.854, 0.656, 0.818, 0.915, 0.841, 0.999, 0.988, 0.917, 0.835, 0.833, 0.827, 0.744, 0.817, 0.786, 0.674, 0.615, 0.749, 0.677, 0.785, 0.763, 0.91, 0.933, 0.942, 1.028, 0.87, 0.99, 1.013, 0.961, 1.067, 0.932, 0.786, 0.773, 0.666, 0.652, 0.818, 0.579, 0.805, 0.64, 0.641, 0.832, 0.644, 0.781, 0.819, 0.712, 0.9, 0.847, 0.674, 0.719, 0.961, 0.752, 0.735, 0.656, 0.625, 0.648, 0.677, 0.688, 0.602, 0.57, 0.674, 0.543, 0.632, 0.592, 0.701, 0.698, 0.797, 0.893, 0.94, 1.077, 1.04, 0.758, 0.792, 0.759, 0.66, 0.777, 0.684, 0.698, 0.629, 0.628, 0.689, 0.695, 0.69, 0.618, 0.579, 0.662, 0.627, 0.753, 0.71, 0.64, 0.672, 0.652, 0.669, 0.708, 0.721, 0.767, 0.835, 0.709, 0.9, 0.959, 0.741, 0.818, 0.806, 0.758, 0.685, 0.839, 0.701, 0.73, 0.64, 0.624, 0.595, 0.591, 0.526, 0.551, 0.585, 0.696, 0.783, 0.783, 0.783, 0.824, 0.822, 0.817, 0.943, 0.871, 0.837, 0.876, 0.675, 0.796, 0.932, 0.733, 0.8, 0.925, 0.788, 0.857, 0.807, 0.758, 0.783, 0.78, 0.874, 1.006, 0.657, 0.693, 0.643, 0.545, 0.643, 0.558, 0.593, 0.598, 0.535, 0.534, 0.432, 0.546, 0.519, 0.528, 0.611, 0.611, 0.632, 0.703, 0.755, 0.808, 0.754, 0.719, 0.741, 0.762, 0.835, 0.745, 0.766, 0.667, 0.722, 0.708, 0.767, 0.701, 0.738, 0.701, 0.621, 0.673, 0.594, 0.686, 0.741, 0.692, 0.722, 0.652, 0.546, 0.727, 0.719, 0.853, 0.932, 0.806, 0.87, 0.864, 0.762, 0.981, 0.648, 0.686, 0.603, 0.663, 0.842, 0.821, 0.7, 0.604, 0.494, 0.536, 0.54, 0.618, 0.588, 0.621, 0.649, 0.709, 0.722, 0.725, 0.664, 0.628, 0.745, 1.049, 0.702, 0.81, 0.811, 0.687, 0.783, 0.977, 0.847, 0.804, 0.94, 0.913, 0.687, 0.895, 0.826, 0.786, 0.97, 0.691, 0.713, 0.788, 0.773, 0.707, 0.754, 0.608, 0.685, 0.604, 0.671, 0.674, 0.496, 0.595, 0.53, 0.559, 0.595, 0.713, 0.876, 0.762, 0.766, 0.615, 0.579, 0.631, 0.539, 0.552, 0.526, 0.595, 0.697, 0.632, 0.58, 0.518, 0.567, 0.483]\n",
      "Val Error(all epochs): 5.96222448348999 \n",
      " [48.601, 48.521, 48.439, 48.362, 48.244, 48.091, 47.921, 47.767, 47.575, 47.378, 47.195, 46.879, 46.549, 46.305, 46.14, 45.644, 45.437, 44.858, 44.098, 43.451, 42.451, 41.517, 40.859, 39.72, 38.759, 38.743, 38.392, 37.531, 37.236, 35.587, 35.389, 35.426, 35.679, 36.045, 35.077, 35.646, 36.242, 35.522, 34.069, 33.065, 33.051, 29.205, 31.096, 29.666, 29.531, 29.155, 30.373, 27.153, 25.298, 24.199, 22.569, 21.607, 20.487, 19.425, 18.704, 18.882, 14.49, 14.865, 12.885, 12.476, 13.946, 13.391, 13.333, 13.476, 11.463, 12.5, 9.594, 9.737, 9.8, 9.331, 10.16, 8.982, 8.831, 7.762, 7.45, 7.417, 7.817, 7.566, 7.933, 7.814, 8.325, 7.971, 8.914, 8.335, 8.424, 8.49, 8.442, 7.882, 8.987, 7.59, 9.261, 7.611, 8.986, 8.511, 9.768, 9.071, 8.611, 8.791, 8.028, 7.175, 7.458, 6.982, 7.134, 6.68, 7.039, 7.071, 7.482, 7.673, 7.624, 7.745, 6.899, 6.96, 7.289, 6.817, 6.473, 7.806, 6.879, 6.258, 7.276, 7.921, 6.627, 7.417, 6.385, 6.564, 6.926, 6.03, 6.472, 6.209, 6.332, 6.686, 6.843, 6.782, 6.578, 6.552, 6.546, 5.962, 6.747, 6.134, 6.735, 7.08, 6.398, 6.196, 6.699, 7.527, 7.123, 8.553, 8.124, 7.426, 8.411, 7.368, 6.818, 6.676, 6.383, 6.668, 6.554, 6.883, 6.419, 6.778, 6.899, 6.086, 6.607, 6.068, 7.234, 7.792, 8.275, 7.804, 7.407, 6.581, 6.643, 6.453, 6.467, 6.318, 6.411, 6.29, 6.676, 6.145, 6.54, 6.56, 6.568, 6.489, 6.716, 6.841, 6.446, 6.553, 6.888, 6.129, 6.984, 6.358, 6.535, 6.714, 6.314, 6.583, 6.383, 6.297, 6.289, 6.396, 6.122, 6.321, 6.526, 6.288, 6.473, 6.801, 6.243, 6.451, 6.429, 6.48, 6.702, 6.906, 6.521, 6.157, 6.742, 6.131, 6.839, 6.896, 6.555, 7.026, 6.414, 6.716, 6.841, 6.355, 6.785, 6.64, 6.372, 7.357, 7.122, 7.069, 7.524, 7.042, 7.189, 6.73, 6.611, 6.664, 6.489, 6.613, 6.633, 6.767, 6.872, 7.677, 7.507, 7.799, 7.219, 7.119, 7.162, 6.936, 7.547, 8.393, 8.725, 7.996, 8.3, 8.837, 7.739, 7.883, 8.092, 7.444, 6.822, 7.616, 6.673, 7.094, 6.9, 7.898, 6.957, 7.3, 7.694, 7.414, 6.989, 7.899, 6.647, 7.628, 7.469, 7.094, 6.78, 7.35, 7.018, 7.299, 6.343, 6.851, 6.36, 6.186, 6.914, 6.391, 6.371, 7.115, 6.504, 6.755, 6.428, 6.385, 6.348, 6.51, 6.5, 6.237, 6.593, 6.447, 6.578, 6.458, 6.614, 6.586, 6.602, 6.609, 6.521, 6.637, 6.612, 6.517, 6.837, 6.394, 6.754, 6.177, 6.514, 6.596, 6.303, 6.436, 6.649, 6.817, 6.584, 6.709, 6.597, 6.806, 6.537, 6.439, 6.76, 6.544, 6.451, 6.466, 6.498, 6.568, 6.616, 6.527, 6.494, 7.172, 7.358, 7.144, 7.117, 6.749, 7.531, 6.851, 7.577, 7.088, 6.941, 6.523, 6.502, 6.269, 6.428, 6.205, 6.566, 6.83, 6.607, 6.199, 6.494, 6.376, 6.346, 6.642, 6.262, 6.517, 6.683, 6.352, 6.531, 6.017, 6.477, 6.583, 6.433, 6.82, 6.744, 6.83, 6.858, 6.763, 6.507, 6.739, 6.75, 6.949, 6.781, 6.624, 7.889, 7.544, 7.106, 7.071, 6.974, 6.5, 7.241, 7.044, 6.73, 7.091, 6.599, 6.558, 6.287, 6.497, 6.539, 6.405, 6.498, 6.422, 6.384, 6.411, 6.404, 6.396, 6.468, 6.138, 6.676, 6.383, 6.365, 6.922, 6.502, 6.696, 6.262, 6.661, 6.184, 6.569, 6.496, 6.357, 7.133, 6.54, 6.242, 6.472, 6.18, 6.263, 6.373, 6.119, 6.475, 6.058, 6.084, 6.314, 6.084, 6.247, 6.351, 6.215, 6.347, 6.64, 6.517, 7.577, 7.108, 6.813, 7.812, 6.907, 7.909, 8.743, 7.379, 8.503, 7.335, 7.638, 6.761, 6.806, 6.818, 6.52, 6.68, 6.161, 6.537, 6.317, 6.452, 6.292, 6.171, 6.446, 6.355, 6.278, 7.014, 6.251, 6.516, 6.545, 6.345, 6.498, 6.741, 6.328, 6.627, 6.959, 7.415, 7.069, 7.034, 7.168, 6.348, 6.519, 6.507, 6.619, 6.348, 6.508, 6.399, 6.205, 6.457, 6.426, 6.567, 6.258, 6.204, 6.245, 6.495, 6.328, 6.467, 6.386, 6.184, 6.676, 6.105, 7.112, 6.501, 7.151, 7.096, 6.629, 6.818, 6.411, 6.409, 6.383, 6.361, 6.501, 6.329, 6.49, 6.437, 6.664]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.053, 0.088, 0.13, 0.141, 0.182, 0.216, 0.255, 0.275, 0.208, 0.42, 0.468, 0.409, 0.451, 0.33, 0.399, 0.373, 0.333, 0.562, 0.451, 1.186, 1.355, 1.421, 1.463, 0.945, 1.212, 1.539, 1.605, 2.007, 1.514, 2.104, 1.682, 3.102, 1.594, 1.49, 1.195, 0.9, 1.186, 1.237, 1.082, 1.043, 1.244, 1.023, 1.242, 0.99, 1.14, 0.812, 0.927, 0.694, 0.752, 0.968, 0.907, 1.672, 1.397, 2.408, 1.599, 1.43, 2.009, 2.715, 3.568, 3.628, 4.356, 3.682, 4.508, 3.309, 3.455, 4.184, 4.241, 3.916, 5.8, 3.554, 2.525, 2.024, 1.656, 1.914, 1.828, 3.602, 3.707, 3.439, 3.183, 3.083, 2.713, 2.329, 1.613, 1.5, 1.984, 1.672, 2.382, 2.182, 2.279, 1.872, 2.398, 1.81, 1.897, 2.578, 2.519, 3.232, 3.982, 3.982, 5.354, 5.287, 4.563, 5.659, 4.875, 4.35, 4.217, 3.117, 3.605, 3.108, 3.191, 3.469, 4.012, 4.499, 3.454, 3.49, 3.423, 4.803, 6.317, 6.53, 5.702, 4.955, 3.594, 3.155, 2.642, 2.692, 3.332, 2.669, 2.799, 3.496, 3.064, 3.429, 2.744, 2.417, 2.62, 3.008, 2.521, 3.18, 3.191, 2.759, 2.825, 3.276, 4.501, 3.236, 4.454, 3.371, 4.252, 3.434, 3.879, 2.711, 3.045, 3.197, 2.232, 3.28, 2.693, 3.507, 3.096, 3.2, 2.778, 2.825, 2.861, 2.152, 2.163, 2.492, 3.27, 4.006, 3.362, 4.198, 4.284, 3.32, 3.87, 3.543, 3.066, 3.452, 3.184, 2.781, 3.606, 2.666, 4.379, 4.193, 4.08, 4.712, 3.865, 4.373, 4.003, 3.783, 3.713, 3.992, 3.182, 4.401, 3.932, 4.966, 5.841, 6.013, 5.907, 5.711, 4.409, 5.045, 4.749, 5.328, 7.596, 7.104, 6.652, 7.139, 7.446, 6.472, 6.076, 6.533, 5.718, 4.427, 5.462, 3.942, 4.83, 4.275, 6.135, 4.683, 5.019, 5.662, 4.827, 5.335, 5.827, 4.538, 5.387, 6.202, 5.167, 4.791, 5.844, 5.1, 5.69, 4.159, 4.284, 3.728, 3.157, 4.288, 3.64, 3.24, 4.69, 3.783, 4.056, 3.367, 3.484, 2.693, 3.384, 3.487, 3.341, 3.609, 3.011, 3.208, 3.262, 2.781, 3.587, 3.032, 3.335, 3.329, 3.24, 3.494, 3.294, 3.879, 3.567, 4.059, 3.146, 2.775, 4.655, 3.615, 3.03, 2.831, 2.134, 2.63, 2.367, 2.605, 2.214, 2.598, 2.592, 1.958, 2.447, 2.746, 2.021, 3.029, 2.022, 2.43, 2.383, 2.268, 1.918, 1.87, 2.003, 1.908, 2.489, 1.9, 1.898, 2.066, 1.968, 2.247, 2.627, 3.046, 3.2, 2.91, 3.473, 3.474, 3.955, 3.753, 3.254, 3.589, 2.467, 3.319, 1.892, 2.583, 2.116, 2.12, 2.637, 2.695, 3.287, 3.279, 4.299, 3.772, 4.299, 4.902, 4.483, 4.946, 4.657, 3.542, 3.885, 4.663, 4.85, 4.132, 4.775, 6.246, 5.904, 5.428, 4.828, 4.897, 3.825, 5.117, 4.721, 4.537, 4.938, 4.217, 3.611, 3.641, 3.165, 3.745, 3.436, 3.629, 3.636, 2.937, 3.016, 2.733, 2.791, 3.279, 2.428, 2.868, 2.806, 3.328, 2.641, 2.708, 2.568, 2.417, 3.632, 3.47, 3.209, 2.228, 2.554, 1.883, 3.011, 3.377, 3.57, 3.705, 3.14, 4.213, 3.038, 3.749, 3.703, 2.877, 3.524, 3.71, 3.608, 3.497, 3.61, 4.041, 3.627, 5.225, 5.843, 5.616, 4.96, 6.371, 4.745, 6.149, 7.569, 5.226, 7.028, 6.025, 6.04, 4.834, 4.883, 4.939, 4.27, 4.576, 3.474, 3.828, 4.244, 3.552, 3.511, 3.296, 2.731, 2.889, 2.671, 3.048, 2.969, 3.206, 4.219, 3.227, 3.601, 4.275, 3.855, 4.062, 5.006, 5.796, 5.549, 5.059, 5.756, 3.203, 3.121, 3.12, 2.943, 3.9, 3.006, 3.345, 3.239, 3.729, 3.883, 4.357, 3.229, 3.178, 3.232, 2.869, 2.505, 3.025, 2.783, 2.921, 3.424, 3.557, 4.951, 4.181, 5.44, 5.314, 4.626, 4.631, 4.033, 3.299, 2.894, 2.708, 3.322, 2.452, 2.447, 2.397, 1.974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 48.58016, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 48.58016 to 48.54683, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 48.54683 to 48.49854, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 48.49854 to 48.42234, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 48.42234 to 48.30329, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 48.30329 to 48.14864, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 48.14864 to 47.97071, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 47.97071 to 47.74467, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 47.74467 to 47.44903, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 47.44903 to 47.09253, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 47.09253 to 46.65224, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 46.65224 to 46.20498, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 46.20498 to 45.60467, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 45.60467 to 44.91213, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 44.91213 to 44.04322, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 44.04322 to 43.09203, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 43.09203 to 42.40584, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 42.40584 to 42.29074, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 42.29074 to 41.93237, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 41.93237 to 41.83140, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 41.83140 to 40.01376, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 40.01376 to 38.05326, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 38.05326 to 36.01844, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 36.01844 to 32.92587, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 32.92587 to 31.48453, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 31.48453 to 28.93866, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 28.93866 to 26.65818, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 26.65818 to 25.22695, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 25.22695 to 22.83021, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 22.83021 to 19.65184, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00031: val_mae improved from 19.65184 to 18.16479, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00032: val_mae improved from 18.16479 to 14.61483, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 14.61483 to 13.51222, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00034: val_mae improved from 13.51222 to 10.32829, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00035: val_mae improved from 10.32829 to 9.83478, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00036: val_mae improved from 9.83478 to 7.84092, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00037: val_mae improved from 7.84092 to 7.13839, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 7.13839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00043: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 7.13839\n",
      "\n",
      "Epoch 00129: val_mae improved from 7.13839 to 6.53357, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 6.53357\n",
      "\n",
      "Epoch 00157: val_mae improved from 6.53357 to 6.51059, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 6.51059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00198: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 6.51059\n",
      "\n",
      "Epoch 00207: val_mae improved from 6.51059 to 6.42360, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 6.42360\n",
      "\n",
      "Epoch 00265: val_mae improved from 6.42360 to 6.41441, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 6.41441\n",
      "\n",
      "Epoch 00329: val_mae improved from 6.41441 to 6.23028, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 6.23028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00350: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 6.23028\n",
      "\n",
      "Epoch 00359: val_mae improved from 6.23028 to 5.82129, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_2/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 5.82129\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 5.82129\n",
      "\n",
      "Lambda: 1 , Time: 0:03:57\n",
      "Train Error(all epochs): 1.7854667901992798 \n",
      " [49.207, 49.116, 49.046, 48.97, 48.878, 48.767, 48.636, 48.483, 48.302, 48.093, 47.84, 47.551, 47.221, 46.844, 46.423, 45.934, 45.395, 44.775, 43.986, 43.19, 42.331, 41.412, 40.427, 39.341, 38.181, 37.0, 35.755, 34.482, 33.178, 31.874, 30.509, 29.183, 27.824, 26.437, 25.065, 23.74, 22.399, 21.145, 19.857, 18.729, 17.562, 16.567, 15.5, 14.679, 13.919, 13.321, 12.443, 11.829, 11.071, 10.605, 9.983, 9.815, 9.23, 8.839, 8.488, 8.086, 7.664, 7.436, 7.083, 6.841, 6.59, 6.585, 6.19, 6.103, 5.764, 5.644, 5.56, 5.479, 5.633, 5.235, 5.212, 5.032, 4.879, 5.007, 5.177, 5.246, 4.992, 4.877, 4.746, 4.787, 4.633, 4.577, 4.476, 4.549, 4.639, 4.808, 4.763, 4.71, 4.544, 4.47, 4.564, 4.431, 4.551, 4.466, 4.374, 4.484, 4.444, 4.516, 4.43, 4.718, 4.829, 4.751, 4.524, 4.116, 4.133, 4.071, 4.06, 3.991, 4.059, 3.891, 4.144, 4.093, 4.232, 3.997, 4.162, 3.996, 4.18, 4.161, 4.306, 4.009, 4.236, 4.171, 3.966, 3.838, 3.732, 3.765, 3.791, 3.962, 4.24, 4.129, 4.002, 3.87, 3.843, 3.84, 3.999, 3.803, 3.887, 3.674, 3.814, 3.72, 4.103, 3.991, 3.815, 3.585, 3.307, 3.191, 3.264, 3.444, 3.517, 3.939, 4.182, 3.808, 3.628, 3.362, 3.429, 3.644, 3.711, 3.594, 3.542, 3.409, 3.369, 3.312, 3.328, 3.105, 3.192, 3.151, 3.229, 3.211, 3.419, 3.405, 3.404, 3.364, 3.161, 3.346, 3.48, 3.54, 3.499, 3.349, 3.6, 3.504, 3.656, 3.16, 3.14, 3.052, 3.107, 3.349, 3.388, 3.124, 3.243, 3.168, 3.101, 3.185, 3.34, 3.328, 3.406, 3.399, 3.262, 3.144, 3.083, 3.042, 3.184, 3.277, 3.598, 3.549, 3.596, 3.248, 3.093, 2.873, 2.766, 2.846, 2.728, 2.759, 3.049, 3.223, 3.635, 3.198, 3.518, 3.76, 3.509, 3.451, 3.379, 3.599, 3.459, 3.52, 3.255, 2.995, 2.933, 2.832, 2.905, 2.741, 2.784, 2.801, 2.932, 2.95, 3.021, 3.156, 3.147, 3.087, 3.145, 3.154, 2.995, 2.946, 3.014, 2.807, 2.884, 2.887, 3.004, 2.892, 2.863, 2.719, 2.86, 2.91, 2.986, 2.972, 3.199, 3.18, 3.479, 3.343, 3.436, 2.974, 3.085, 2.848, 2.886, 2.835, 2.919, 2.844, 2.771, 2.704, 2.772, 2.717, 2.533, 2.54, 2.537, 2.644, 2.586, 2.737, 2.689, 3.071, 3.39, 3.316, 3.183, 3.144, 3.149, 3.35, 3.244, 3.555, 3.189, 3.202, 2.947, 2.871, 2.561, 2.708, 2.631, 2.743, 2.509, 2.597, 2.466, 2.78, 2.819, 2.815, 2.753, 2.643, 2.733, 2.858, 2.8, 2.769, 2.697, 2.51, 2.685, 2.502, 2.497, 2.533, 2.622, 2.731, 2.901, 2.919, 2.798, 2.707, 2.745, 2.79, 2.925, 2.882, 2.996, 2.772, 2.884, 3.259, 3.188, 3.278, 3.058, 3.28, 2.944, 2.907, 2.838, 2.927, 3.021, 3.232, 3.007, 3.029, 2.909, 2.786, 2.61, 2.341, 2.354, 2.263, 2.471, 2.345, 2.477, 2.387, 2.321, 2.131, 2.163, 2.334, 2.567, 2.697, 2.892, 2.695, 3.035, 3.102, 3.112, 2.831, 2.851, 3.068, 2.923, 3.056, 3.274, 2.888, 2.635, 2.489, 2.475, 2.614, 2.675, 2.441, 2.402, 2.215, 2.239, 2.508, 2.359, 2.559, 2.743, 2.976, 3.065, 3.128, 2.693, 2.611, 2.467, 2.507, 2.502, 2.623, 2.683, 2.614, 2.794, 2.718, 2.807, 2.484, 2.362, 2.332, 2.372, 2.446, 2.595, 2.774, 2.612, 2.594, 2.661, 2.581, 2.554, 2.52, 2.315, 2.233, 2.264, 2.415, 2.637, 2.915, 2.796, 2.89, 2.672, 2.785, 2.505, 2.601, 2.582, 2.397, 2.419, 2.232, 2.227, 2.126, 2.179, 2.337, 2.296, 2.642, 2.747, 2.485, 2.52, 2.592, 2.602, 2.795, 2.86, 2.875, 3.048, 3.165, 3.015, 3.265, 2.741, 2.601, 2.665, 2.535, 2.606, 2.554, 2.776, 2.778, 2.7, 2.658, 2.447, 2.184, 2.051, 1.956, 1.999, 1.929, 1.898, 1.785, 1.799, 1.995, 2.176, 2.371, 2.363, 2.465, 2.449, 2.564, 2.548, 2.696, 2.628, 2.832, 2.735, 2.548, 2.611, 2.614, 2.477, 2.662, 2.497, 2.646, 2.762, 2.582, 2.514, 2.439, 2.639, 2.544, 2.548, 2.307, 2.256, 2.172, 2.23, 2.137, 2.121, 2.006, 2.024, 2.19, 2.381, 2.529, 2.376, 2.328, 2.234, 2.391]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.007, 0.003, 0.024, 0.029, 0.075, 0.071, 0.136, 0.206, 0.345, 0.324, 0.423, 0.412, 0.554, 0.588, 0.814, 0.818, 0.854, 0.984, 1.013, 1.023, 1.093, 1.122, 1.147, 1.21, 1.302, 1.308, 1.344, 1.35, 1.323, 1.416, 1.442, 1.689, 1.435, 1.545, 1.523, 1.507, 1.624, 1.766, 1.849, 1.697, 1.731, 1.607, 1.731, 1.608, 1.61, 1.586, 1.617, 1.72, 1.735, 1.834, 1.675, 1.676, 1.623, 1.698, 1.693, 1.736, 1.659, 1.59, 1.719, 1.603, 1.757, 1.728, 1.696, 1.894, 1.728, 1.803, 1.419, 1.479, 1.493, 1.496, 1.418, 1.503, 1.345, 1.611, 1.519, 1.523, 1.536, 1.607, 1.468, 1.542, 1.652, 1.797, 1.36, 1.709, 1.569, 1.521, 1.341, 1.403, 1.319, 1.475, 1.406, 1.776, 1.487, 1.537, 1.362, 1.564, 1.434, 1.513, 1.421, 1.51, 1.305, 1.473, 1.334, 1.627, 1.68, 1.321, 1.432, 1.139, 1.233, 1.193, 1.329, 1.332, 1.657, 1.735, 1.26, 1.482, 1.243, 1.167, 1.509, 1.406, 1.397, 1.328, 1.223, 1.425, 1.216, 1.274, 1.194, 1.184, 1.214, 1.262, 1.172, 1.354, 1.413, 1.231, 1.404, 1.07, 1.393, 1.355, 1.412, 1.384, 1.225, 1.516, 1.297, 1.6, 1.117, 1.215, 1.173, 1.181, 1.328, 1.363, 1.129, 1.373, 1.283, 1.217, 1.201, 1.404, 1.35, 1.28, 1.442, 1.278, 1.201, 1.242, 1.206, 1.25, 1.343, 1.546, 1.431, 1.355, 1.382, 1.129, 1.16, 0.947, 1.146, 1.038, 1.049, 1.313, 1.348, 1.516, 1.157, 1.457, 1.64, 1.34, 1.529, 1.222, 1.677, 1.231, 1.565, 1.269, 1.104, 1.23, 1.085, 1.139, 1.076, 1.049, 1.074, 1.263, 1.146, 1.179, 1.33, 1.301, 1.134, 1.349, 1.204, 1.242, 1.164, 1.214, 1.003, 1.277, 1.093, 1.245, 1.125, 1.148, 1.105, 1.101, 1.23, 1.167, 1.328, 1.211, 1.238, 1.625, 1.199, 1.552, 1.128, 1.342, 1.058, 1.238, 1.053, 1.317, 1.091, 1.074, 1.056, 1.103, 1.13, 1.037, 0.952, 1.014, 1.016, 1.06, 1.068, 1.033, 1.355, 1.568, 1.101, 1.583, 1.166, 1.315, 1.433, 1.107, 1.729, 1.195, 1.293, 1.163, 1.233, 0.934, 1.18, 0.992, 1.21, 0.927, 1.049, 0.964, 1.095, 1.232, 1.082, 1.069, 1.187, 0.975, 1.286, 1.164, 1.139, 1.039, 1.056, 1.039, 0.989, 1.048, 0.968, 1.142, 1.201, 1.1, 1.384, 1.106, 1.026, 1.329, 0.96, 1.403, 1.01, 1.376, 1.093, 1.149, 1.479, 1.191, 1.434, 1.257, 1.317, 1.13, 1.374, 1.106, 1.312, 1.155, 1.43, 1.191, 1.277, 1.231, 1.081, 1.08, 0.893, 0.898, 0.893, 1.016, 0.868, 1.077, 0.919, 0.955, 0.828, 0.949, 0.906, 1.113, 1.062, 1.37, 0.945, 1.349, 1.368, 1.328, 1.016, 1.147, 1.323, 1.204, 1.266, 1.601, 0.916, 1.259, 0.915, 1.035, 1.088, 1.196, 0.877, 1.057, 0.885, 0.851, 1.142, 0.838, 1.127, 1.122, 1.339, 1.249, 1.287, 1.174, 0.964, 1.137, 0.929, 1.092, 1.067, 1.116, 1.15, 1.106, 1.1, 1.289, 0.903, 0.942, 0.956, 0.986, 0.992, 1.174, 1.149, 1.146, 1.05, 1.098, 0.994, 1.176, 0.963, 0.973, 0.896, 0.86, 1.127, 0.932, 1.459, 1.091, 1.273, 1.058, 1.187, 1.032, 1.142, 1.0, 0.959, 1.055, 0.914, 0.865, 0.94, 0.892, 0.997, 0.899, 1.206, 1.144, 0.951, 1.207, 1.038, 0.976, 1.29, 1.261, 1.089, 1.434, 1.315, 1.121, 1.581, 0.962, 1.278, 1.019, 1.094, 1.19, 0.912, 1.343, 1.138, 1.093, 1.274, 0.886, 0.923, 0.877, 0.712, 0.867, 0.697, 0.822, 0.722, 0.701, 0.851, 0.943, 1.051, 0.909, 1.096, 0.939, 1.122, 0.961, 1.256, 1.113, 1.131, 1.345, 1.002, 1.105, 1.018, 1.119, 1.178, 0.923, 1.182, 1.288, 0.922, 1.119, 0.959, 1.178, 1.065, 1.145, 0.915, 1.009, 0.859, 0.848, 1.048, 0.78, 0.803, 0.928, 0.991, 0.874, 1.098, 1.073, 0.908, 0.878, 1.179]\n",
      "Val Error(all epochs): 5.8212890625 \n",
      " [48.58, 48.547, 48.499, 48.422, 48.303, 48.149, 47.971, 47.745, 47.449, 47.093, 46.652, 46.205, 45.605, 44.912, 44.043, 43.092, 42.406, 42.291, 41.932, 41.831, 40.014, 38.053, 36.018, 32.926, 31.485, 28.939, 26.658, 25.227, 22.83, 19.652, 18.165, 14.615, 13.512, 10.328, 9.835, 7.841, 7.138, 7.66, 7.524, 8.106, 8.157, 8.28, 8.084, 8.591, 10.581, 11.676, 11.086, 13.352, 17.821, 16.986, 15.973, 20.246, 16.449, 17.164, 15.081, 14.733, 15.852, 14.86, 16.445, 14.507, 14.637, 15.233, 14.735, 15.792, 14.122, 13.242, 13.828, 15.977, 14.112, 16.241, 14.916, 15.008, 14.342, 14.621, 14.048, 16.294, 15.473, 14.527, 13.063, 12.353, 11.212, 13.039, 12.267, 11.377, 11.498, 9.21, 11.127, 11.317, 10.942, 10.096, 11.153, 10.056, 10.105, 9.506, 9.461, 10.157, 10.509, 8.876, 9.525, 9.315, 8.73, 9.011, 8.875, 8.036, 8.636, 9.246, 8.594, 9.131, 8.883, 9.684, 8.18, 9.18, 7.757, 8.968, 10.531, 8.612, 8.59, 8.879, 8.981, 7.853, 7.338, 7.478, 7.593, 9.099, 7.955, 7.711, 8.202, 7.529, 6.534, 7.104, 7.721, 7.228, 6.913, 7.041, 8.015, 7.544, 7.383, 7.477, 8.545, 8.44, 7.802, 7.922, 7.75, 7.781, 7.62, 8.359, 8.314, 8.145, 8.265, 6.863, 7.943, 7.921, 7.312, 8.244, 6.836, 7.194, 6.511, 6.776, 7.474, 7.673, 7.555, 7.056, 7.778, 7.969, 8.082, 7.376, 7.784, 7.722, 6.955, 8.941, 7.31, 8.042, 8.735, 7.45, 7.005, 6.804, 7.126, 7.956, 7.306, 7.205, 8.371, 9.992, 8.166, 8.279, 7.673, 6.693, 7.316, 8.14, 10.193, 8.994, 8.656, 7.908, 6.733, 6.941, 6.808, 6.901, 6.951, 6.956, 7.881, 7.646, 6.812, 7.062, 6.86, 6.614, 6.584, 8.523, 6.424, 7.145, 7.064, 7.39, 7.602, 7.219, 8.531, 6.461, 6.705, 6.641, 7.759, 8.917, 7.723, 7.901, 6.647, 6.786, 7.101, 7.452, 8.053, 6.793, 6.965, 7.57, 7.029, 6.885, 7.55, 6.672, 7.02, 7.329, 7.515, 6.783, 6.786, 6.5, 6.721, 6.816, 6.58, 6.652, 7.283, 6.722, 7.641, 7.674, 6.946, 6.606, 7.752, 7.415, 7.143, 6.739, 7.436, 6.503, 7.498, 7.249, 11.24, 8.186, 6.714, 8.005, 8.165, 9.223, 8.139, 7.723, 6.414, 6.925, 7.052, 6.714, 7.599, 7.696, 7.292, 8.416, 7.354, 7.696, 6.991, 7.279, 8.114, 7.51, 7.846, 7.546, 6.455, 6.949, 7.217, 7.684, 8.186, 7.247, 8.88, 8.632, 7.177, 7.926, 7.035, 7.745, 6.773, 6.969, 7.003, 6.719, 7.451, 7.051, 7.353, 7.147, 6.843, 7.054, 6.931, 8.226, 9.012, 8.122, 8.205, 9.533, 8.934, 8.942, 7.465, 8.948, 7.664, 8.507, 8.188, 7.194, 8.574, 7.742, 7.213, 7.41, 7.068, 7.166, 6.611, 7.652, 7.091, 6.776, 7.179, 6.877, 6.23, 7.157, 7.215, 8.719, 7.97, 6.313, 7.585, 6.372, 7.454, 6.352, 7.206, 6.973, 6.661, 6.95, 7.558, 6.915, 7.136, 6.895, 7.51, 7.831, 7.51, 7.382, 8.422, 7.088, 7.378, 8.031, 7.215, 8.095, 7.525, 7.3, 5.821, 7.223, 7.529, 7.963, 9.589, 9.484, 10.409, 8.149, 7.339, 7.429, 7.007, 7.489, 6.914, 7.018, 7.041, 7.741, 7.036, 7.546, 7.103, 8.807, 6.476, 6.877, 7.276, 6.927, 7.503, 6.834, 7.458, 6.84, 7.116, 7.026, 6.636, 7.263, 7.348, 7.848, 7.327, 6.487, 7.193, 6.544, 7.214, 6.999, 7.508, 7.027, 7.617, 7.057, 7.591, 7.001, 7.384, 6.912, 7.134, 7.614, 6.869, 6.872, 7.048, 7.169, 7.146, 6.712, 6.984, 7.17, 7.03, 6.773, 6.623, 6.644, 6.835, 7.145, 6.79, 7.625, 6.424, 6.881, 7.144, 6.678, 6.623, 7.406, 6.729, 6.656, 6.797, 7.564, 6.623, 6.815, 6.982, 8.082, 10.781, 10.13, 9.004, 7.326, 8.896, 7.751, 8.207, 7.946, 8.095, 8.062, 6.98, 7.727, 7.402, 7.927, 7.022, 7.245, 6.9, 6.896, 6.921, 6.516, 7.004, 7.595, 6.742, 7.079, 7.434, 6.346, 6.729, 6.809, 7.401, 6.578, 7.041, 6.6, 7.199, 6.798, 7.02, 7.118, 6.45, 6.955, 7.282, 7.113, 7.47, 7.083, 7.842, 7.515, 7.238, 6.657, 7.466, 6.721, 6.716, 7.419, 6.762, 6.983, 7.03, 7.022, 7.037, 6.513, 7.031, 7.289, 7.375, 8.655, 6.961, 6.83]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.021, 0.084, 0.118, 0.254, 0.351, 0.927, 1.048, 1.974, 4.743, 6.048, 5.689, 7.067, 6.924, 6.393, 5.533, 6.946, 9.955, 10.77, 9.429, 12.833, 17.69, 16.755, 15.275, 20.118, 15.948, 15.644, 12.013, 11.26, 12.601, 10.677, 11.945, 9.56, 9.634, 9.167, 6.407, 8.468, 5.856, 6.658, 4.872, 6.507, 3.923, 7.885, 5.295, 6.029, 6.063, 4.297, 4.181, 7.326, 7.152, 5.686, 3.506, 2.945, 2.188, 3.404, 3.414, 1.879, 1.786, 2.102, 2.173, 3.632, 2.718, 2.052, 1.809, 2.309, 1.834, 2.277, 2.4, 2.354, 2.024, 2.16, 2.915, 3.192, 3.291, 5.329, 5.484, 3.305, 1.838, 3.548, 2.272, 2.644, 2.426, 2.827, 2.253, 2.595, 3.41, 3.711, 4.441, 3.511, 3.214, 4.639, 3.384, 2.687, 3.03, 3.354, 2.737, 4.63, 2.897, 2.343, 1.784, 2.593, 3.913, 3.467, 3.89, 2.47, 2.47, 2.986, 1.916, 2.795, 2.763, 2.815, 3.039, 3.746, 3.762, 3.876, 3.737, 3.405, 2.788, 2.505, 2.815, 2.049, 1.736, 3.505, 3.701, 3.739, 3.506, 3.441, 2.624, 2.691, 2.592, 2.903, 1.644, 1.809, 1.751, 2.132, 2.321, 2.731, 2.435, 2.804, 2.231, 2.148, 2.647, 3.098, 2.737, 3.35, 3.898, 2.783, 2.552, 2.556, 2.266, 3.006, 3.609, 3.337, 3.388, 5.428, 3.725, 3.709, 2.243, 2.526, 3.208, 3.656, 5.036, 5.197, 4.105, 2.712, 2.323, 2.367, 2.181, 3.089, 3.046, 2.67, 2.086, 2.052, 1.779, 2.045, 2.743, 2.669, 3.643, 4.706, 2.489, 1.994, 2.235, 1.988, 1.668, 1.867, 1.413, 3.272, 2.544, 2.558, 4.325, 4.956, 5.017, 2.564, 4.054, 4.667, 4.928, 5.2, 5.345, 3.725, 3.446, 3.367, 2.732, 2.176, 1.392, 1.869, 1.471, 1.529, 1.668, 1.567, 2.257, 2.631, 2.086, 2.357, 2.119, 2.385, 2.023, 2.051, 1.698, 3.579, 1.993, 2.041, 1.677, 2.002, 2.143, 2.542, 1.677, 2.519, 1.693, 3.678, 6.497, 5.564, 3.477, 4.992, 4.853, 5.872, 3.468, 3.767, 2.338, 2.801, 2.364, 2.556, 1.892, 2.59, 2.174, 2.069, 2.075, 1.777, 1.828, 1.355, 1.742, 1.98, 1.884, 2.27, 3.038, 3.14, 3.103, 4.232, 4.83, 3.948, 5.916, 3.799, 3.491, 3.235, 3.26, 2.981, 2.437, 1.793, 2.19, 1.812, 2.19, 1.443, 1.874, 1.792, 2.295, 1.979, 2.634, 3.122, 4.613, 4.346, 4.139, 4.53, 4.566, 4.39, 2.116, 3.513, 2.698, 3.06, 3.487, 2.56, 2.798, 4.171, 2.91, 1.961, 1.88, 1.758, 2.74, 2.221, 2.463, 2.698, 3.3, 2.685, 3.142, 3.03, 4.166, 4.524, 4.558, 3.326, 4.075, 2.899, 4.728, 3.85, 4.088, 4.957, 3.529, 2.197, 1.802, 1.845, 2.046, 2.053, 1.445, 1.408, 1.5, 1.37, 1.296, 1.694, 1.808, 1.283, 1.3, 1.258, 1.767, 3.071, 3.183, 3.951, 1.473, 1.795, 1.462, 3.316, 1.517, 2.138, 1.728, 2.212, 1.922, 2.12, 2.753, 2.665, 2.065, 1.675, 2.338, 1.403, 1.633, 1.156, 2.125, 2.251, 3.22, 2.872, 2.728, 3.32, 2.858, 2.746, 2.381, 2.294, 2.692, 2.409, 1.876, 2.528, 1.395, 2.466, 1.743, 2.836, 2.231, 2.806, 3.652, 3.827, 4.478, 2.992, 4.088, 3.251, 2.314, 1.918, 2.314, 1.748, 2.029, 2.163, 2.971, 3.432, 3.083, 3.473, 4.42, 3.629, 2.729, 3.12, 3.165, 2.833, 3.035, 2.345, 2.494, 1.537, 1.859, 1.731, 1.896, 2.509, 2.398, 3.282, 2.664, 2.829, 3.306, 2.93, 3.005, 3.247, 3.838, 5.942, 8.058, 7.426, 5.699, 5.106, 5.182, 5.015, 4.631, 5.06, 5.752, 5.064, 3.891, 5.097, 4.788, 4.842, 3.488, 4.089, 3.114, 3.625, 2.7, 2.227, 1.882, 1.589, 1.719, 1.659, 1.952, 1.808, 1.857, 1.675, 1.814, 2.077, 2.056, 2.836, 3.526, 2.519, 3.963, 4.466, 3.435, 3.278, 4.315, 2.761, 3.437, 3.624, 3.724, 4.516, 3.845, 4.029, 4.015, 3.657, 3.363, 4.089, 2.98, 3.825, 2.937, 2.721, 2.01, 2.694, 3.021, 3.345, 3.861, 4.659, 2.735, 2.486]\n",
      "\n",
      "#Fold: 2 \n",
      "Trainig set size: 420 , Time: 0:11:52 , best_lambda: 1 , min_  , error: 5.821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test starts:  467 , ends:  518\n",
      "1/1 [==============================] - 0s 583us/step - loss: 115.4274 - mse: 72.3410 - mae: 6.6404 - fp_mae: 3.2744\n",
      "average_error:  6.64 , fp_average_error:  3.274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:00<00:00, 257.97it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 255.46it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 226.97it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 234.54it/s]\n",
      "100%|██████████| 107/107 [00:00<00:00, 227.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Fold: 3 , Training Size: 420 , Validation size: 47 , Test Size 52\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 48.81469, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 48.81469 to 48.73613, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 48.73613 to 48.63399, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 48.63399 to 48.52990, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 48.52990 to 48.40090, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 48.40090 to 48.27003, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 48.27003 to 48.07899, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 48.07899 to 47.85554, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 47.85554 to 47.60667, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 47.60667 to 47.30030, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 47.30030 to 47.00429, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 47.00429 to 46.57612, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 46.57612 to 46.11495, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 46.11495 to 45.76077, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 45.76077 to 45.04066, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 45.04066 to 44.50254, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 44.50254 to 43.76667, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 43.76667 to 43.18673, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 43.18673 to 42.63409, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 42.63409 to 41.71896, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 41.71896 to 41.50457, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 41.50457 to 40.68888, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 40.68888 to 39.58133, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 39.58133\n",
      "\n",
      "Epoch 00025: val_mae improved from 39.58133 to 38.07269, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 38.07269 to 36.73844, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 36.73844 to 35.97053, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 35.97053 to 34.77132, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 34.77132 to 34.04399, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 34.04399 to 33.18740, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00031: val_mae improved from 33.18740 to 32.48445, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00032: val_mae improved from 32.48445 to 31.03775, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 31.03775 to 30.10879, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00034: val_mae improved from 30.10879 to 28.68101, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00035: val_mae improved from 28.68101 to 27.19468, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00036: val_mae improved from 27.19468 to 25.60340, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00037: val_mae improved from 25.60340 to 24.57283, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00038: val_mae improved from 24.57283 to 24.43528, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00039: val_mae improved from 24.43528 to 23.79863, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_mae improved from 23.79863 to 22.42096, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00041: val_mae improved from 22.42096 to 21.66929, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00042: val_mae improved from 21.66929 to 20.62536, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00043: val_mae improved from 20.62536 to 19.73415, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00044: val_mae improved from 19.73415 to 19.15157, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00045: val_mae improved from 19.15157 to 18.68920, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00046: val_mae improved from 18.68920 to 17.46013, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00047: val_mae improved from 17.46013 to 16.58587, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00048: val_mae improved from 16.58587 to 15.42917, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00049: val_mae improved from 15.42917 to 15.26253, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 15.26253\n",
      "\n",
      "Epoch 00051: val_mae improved from 15.26253 to 14.29359, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00052: val_mae improved from 14.29359 to 14.07164, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 14.07164\n",
      "\n",
      "Epoch 00054: val_mae improved from 14.07164 to 14.02621, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 14.02621\n",
      "\n",
      "Epoch 00056: val_mae improved from 14.02621 to 13.34541, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00057: val_mae improved from 13.34541 to 13.08166, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00058: val_mae improved from 13.08166 to 13.01103, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00059: val_mae improved from 13.01103 to 12.75808, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 12.75808\n",
      "\n",
      "Epoch 00061: val_mae improved from 12.75808 to 12.23831, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00062: val_mae improved from 12.23831 to 12.06311, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00063: val_mae improved from 12.06311 to 12.02963, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00064: val_mae improved from 12.02963 to 12.01810, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00065: val_mae improved from 12.01810 to 11.68695, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00066: val_mae improved from 11.68695 to 11.50500, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00067: val_mae improved from 11.50500 to 10.53843, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 10.53843\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 10.53843\n",
      "\n",
      "Epoch 00070: val_mae improved from 10.53843 to 8.69158, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 8.69158\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 8.69158\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 8.69158\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 8.69158\n",
      "\n",
      "Epoch 00075: val_mae improved from 8.69158 to 8.34261, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 8.34261\n",
      "\n",
      "Epoch 00077: val_mae improved from 8.34261 to 7.20345, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 7.20345\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 7.20345\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 7.20345\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 7.20345\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 7.20345\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 7.20345\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 7.20345\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 7.20345\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 7.20345\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 7.20345\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 7.20345\n",
      "\n",
      "Epoch 00089: val_mae improved from 7.20345 to 7.17001, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 7.17001\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 7.17001\n",
      "\n",
      "Epoch 00092: val_mae improved from 7.17001 to 6.94450, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 6.94450\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 6.94450\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 6.94450\n",
      "\n",
      "Epoch 00096: val_mae improved from 6.94450 to 6.73580, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 6.73580\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 6.73580\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 6.73580\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 6.73580\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 6.73580\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 6.73580\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 6.73580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00104: val_mae did not improve from 6.73580\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 6.73580\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 6.73580\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 6.73580\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 6.73580\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 6.73580\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 6.73580\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 6.73580\n",
      "\n",
      "Epoch 00112: val_mae improved from 6.73580 to 6.59912, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 6.59912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00262: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 6.59912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00423: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 6.59912\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 6.59912\n",
      "\n",
      "Lambda: 0.01 , Time: 0:03:59\n",
      "Train Error(all epochs): 0.7665370106697083 \n",
      " [49.094, 48.964, 48.861, 48.755, 48.639, 48.507, 48.355, 48.178, 47.973, 47.736, 47.459, 47.143, 46.788, 46.382, 45.927, 45.405, 44.805, 44.126, 43.372, 42.563, 41.707, 40.812, 39.863, 38.901, 37.9, 36.82, 35.681, 34.537, 33.36, 32.194, 30.99, 29.778, 28.539, 27.217, 25.901, 24.551, 23.228, 21.893, 20.624, 19.349, 18.065, 16.782, 15.56, 14.366, 13.254, 12.113, 11.178, 10.289, 9.268, 8.568, 7.908, 7.304, 6.772, 6.279, 6.057, 5.662, 5.474, 5.188, 5.117, 5.05, 5.104, 4.632, 4.545, 4.462, 4.403, 4.625, 4.32, 4.359, 4.444, 4.439, 4.527, 4.24, 4.264, 4.279, 4.33, 4.215, 4.239, 4.297, 3.952, 3.843, 3.764, 3.701, 3.568, 3.458, 3.481, 3.592, 3.633, 3.585, 3.888, 4.115, 3.709, 3.827, 3.673, 3.6, 3.299, 3.228, 3.141, 2.984, 2.946, 2.884, 2.819, 2.835, 2.98, 2.864, 3.093, 2.909, 3.009, 2.802, 2.777, 2.768, 2.608, 2.853, 2.895, 2.687, 2.581, 2.509, 2.704, 2.823, 2.661, 2.642, 2.497, 2.491, 2.315, 2.221, 2.293, 2.427, 2.473, 2.514, 2.417, 2.421, 2.543, 2.39, 2.236, 2.183, 2.256, 2.461, 2.23, 2.16, 2.423, 2.374, 2.304, 2.4, 2.642, 2.758, 2.542, 2.59, 2.44, 2.324, 2.238, 2.155, 2.178, 2.232, 2.283, 2.31, 2.168, 2.122, 1.986, 2.11, 1.914, 1.894, 1.835, 1.953, 1.985, 1.89, 1.71, 1.683, 1.736, 1.669, 1.625, 1.76, 1.845, 1.809, 1.726, 1.796, 1.769, 1.815, 1.807, 1.863, 1.947, 1.991, 1.958, 2.176, 2.004, 1.971, 2.018, 2.12, 1.851, 2.009, 2.014, 2.146, 1.924, 1.9, 1.704, 1.694, 1.607, 1.658, 1.611, 1.674, 1.709, 1.722, 1.717, 1.738, 1.629, 1.573, 1.484, 1.443, 1.367, 1.464, 1.544, 1.543, 1.657, 1.744, 1.663, 1.758, 1.826, 1.932, 1.756, 1.641, 1.572, 1.453, 1.422, 1.297, 1.355, 1.402, 1.467, 1.447, 1.558, 1.615, 1.532, 1.521, 1.653, 1.765, 1.571, 1.427, 1.41, 1.415, 1.361, 1.286, 1.256, 1.218, 1.213, 1.232, 1.209, 1.213, 1.206, 1.255, 1.314, 1.326, 1.306, 1.209, 1.183, 1.178, 1.202, 1.085, 1.02, 0.964, 0.988, 0.994, 0.938, 0.962, 1.053, 1.056, 1.058, 1.252, 1.331, 1.347, 1.521, 1.703, 1.601, 1.623, 1.698, 1.792, 1.567, 1.4, 1.361, 1.358, 1.455, 1.5, 1.568, 1.681, 1.715, 1.679, 1.621, 1.753, 1.503, 1.484, 1.346, 1.359, 1.506, 1.615, 1.571, 1.602, 1.564, 1.396, 1.245, 1.186, 1.089, 1.055, 0.988, 0.972, 0.973, 0.978, 0.997, 1.056, 1.018, 0.975, 0.863, 0.864, 0.871, 0.998, 1.179, 1.208, 1.126, 1.242, 1.274, 1.263, 1.264, 1.321, 1.262, 1.294, 1.263, 1.294, 1.243, 1.193, 1.133, 1.218, 1.313, 1.327, 1.239, 1.174, 1.233, 1.249, 1.259, 1.234, 1.145, 1.082, 1.061, 0.969, 0.918, 0.94, 0.924, 0.947, 0.978, 0.905, 0.899, 0.867, 0.844, 0.854, 0.939, 1.021, 0.965, 0.94, 0.931, 0.969, 0.933, 0.873, 0.831, 0.869, 0.835, 0.879, 0.988, 1.032, 1.03, 1.132, 1.185, 1.119, 1.041, 1.186, 1.319, 1.309, 1.17, 1.098, 1.194, 1.214, 1.186, 1.186, 1.26, 1.276, 1.304, 1.359, 1.35, 1.382, 1.338, 1.326, 1.243, 1.168, 1.108, 1.221, 1.187, 1.219, 1.237, 1.292, 1.248, 1.148, 1.04, 1.022, 1.006, 0.996, 1.04, 1.057, 1.185, 1.228, 1.25, 1.286, 1.285, 1.305, 1.27, 1.23, 1.111, 1.059, 0.992, 0.974, 1.03, 1.11, 1.239, 1.326, 1.336, 1.213, 1.172, 1.144, 1.097, 1.016, 1.009, 0.879, 0.876, 0.873, 0.88, 0.886, 0.938, 1.013, 0.986, 0.954, 0.934, 0.88, 0.888, 0.897, 0.888, 0.873, 0.901, 0.925, 0.866, 0.839, 0.918, 0.97, 0.959, 0.926, 0.908, 0.977, 1.057, 1.067, 0.946, 0.978, 1.055, 1.048, 1.06, 1.241, 1.332, 1.32, 1.37, 1.355, 1.465, 1.467, 1.578, 1.455, 1.494, 1.367, 1.285, 1.212, 1.225, 1.143, 1.048, 0.905, 0.882, 0.852, 0.848, 0.861, 0.821, 0.793, 0.79, 0.81, 0.85, 0.79, 0.767, 0.79, 0.867, 0.904, 0.89, 0.866, 0.95, 1.046, 1.119, 1.114, 1.146, 1.126, 1.127, 1.138, 1.119, 1.078, 1.076, 1.102]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008, 0.015, 0.024, 0.029, 0.034, 0.045, 0.054, 0.077, 0.086, 0.131, 0.206, 0.214, 0.291, 0.382, 0.501, 0.563, 0.638, 0.799, 0.86, 1.011, 1.067, 1.214, 1.326, 1.509, 1.377, 1.452, 1.416, 1.487, 1.687, 1.594, 1.643, 1.646, 1.69, 1.762, 1.628, 1.646, 1.683, 1.726, 1.571, 1.665, 1.785, 1.556, 1.495, 1.487, 1.393, 1.295, 1.32, 1.3, 1.35, 1.441, 1.437, 1.52, 1.758, 1.432, 1.683, 1.404, 1.43, 1.364, 1.236, 1.262, 1.175, 1.139, 1.135, 1.097, 1.132, 1.245, 1.172, 1.364, 1.138, 1.321, 1.095, 1.15, 1.185, 1.003, 1.277, 1.216, 1.088, 1.112, 0.978, 1.116, 1.349, 1.064, 1.184, 0.998, 1.049, 0.948, 0.896, 0.949, 1.069, 1.048, 1.186, 0.982, 1.025, 1.133, 1.017, 0.893, 0.934, 0.959, 1.141, 0.941, 0.967, 1.054, 1.055, 0.99, 1.103, 1.222, 1.211, 1.167, 1.222, 1.026, 1.106, 1.019, 0.861, 1.058, 0.97, 1.047, 1.046, 0.946, 1.001, 0.811, 0.944, 0.867, 0.777, 0.845, 0.816, 0.954, 0.803, 0.762, 0.737, 0.76, 0.735, 0.733, 0.768, 0.842, 0.859, 0.71, 0.87, 0.749, 0.85, 0.808, 0.852, 0.864, 0.982, 0.874, 1.009, 0.946, 0.899, 0.926, 0.972, 0.893, 0.911, 0.906, 1.005, 0.885, 0.843, 0.791, 0.713, 0.787, 0.695, 0.783, 0.724, 0.788, 0.824, 0.748, 0.807, 0.749, 0.681, 0.687, 0.626, 0.63, 0.652, 0.73, 0.691, 0.769, 0.808, 0.753, 0.85, 0.818, 0.917, 0.912, 0.61, 0.857, 0.596, 0.686, 0.566, 0.639, 0.612, 0.686, 0.677, 0.686, 0.836, 0.605, 0.793, 0.739, 0.864, 0.767, 0.571, 0.748, 0.569, 0.674, 0.573, 0.562, 0.582, 0.508, 0.585, 0.529, 0.582, 0.525, 0.614, 0.582, 0.658, 0.618, 0.539, 0.59, 0.511, 0.552, 0.529, 0.418, 0.502, 0.424, 0.476, 0.448, 0.432, 0.486, 0.528, 0.448, 0.646, 0.633, 0.629, 0.707, 0.904, 0.672, 0.874, 0.753, 0.89, 0.749, 0.62, 0.708, 0.616, 0.67, 0.734, 0.696, 0.895, 0.68, 0.951, 0.58, 1.041, 0.621, 0.735, 0.622, 0.625, 0.73, 0.73, 0.808, 0.704, 0.762, 0.682, 0.551, 0.542, 0.556, 0.423, 0.513, 0.396, 0.469, 0.46, 0.452, 0.53, 0.444, 0.475, 0.393, 0.413, 0.385, 0.5, 0.553, 0.588, 0.528, 0.616, 0.582, 0.676, 0.535, 0.698, 0.57, 0.646, 0.609, 0.652, 0.534, 0.655, 0.424, 0.659, 0.634, 0.584, 0.698, 0.455, 0.651, 0.591, 0.543, 0.675, 0.431, 0.609, 0.423, 0.523, 0.378, 0.48, 0.437, 0.427, 0.521, 0.355, 0.503, 0.351, 0.431, 0.414, 0.411, 0.571, 0.385, 0.526, 0.402, 0.49, 0.441, 0.406, 0.393, 0.41, 0.408, 0.413, 0.512, 0.471, 0.536, 0.558, 0.525, 0.624, 0.386, 0.652, 0.631, 0.596, 0.632, 0.455, 0.647, 0.539, 0.588, 0.556, 0.605, 0.677, 0.535, 0.772, 0.514, 0.783, 0.521, 0.736, 0.53, 0.612, 0.529, 0.586, 0.562, 0.59, 0.6, 0.615, 0.623, 0.513, 0.546, 0.443, 0.512, 0.495, 0.451, 0.587, 0.489, 0.665, 0.555, 0.672, 0.578, 0.672, 0.553, 0.645, 0.453, 0.562, 0.426, 0.489, 0.479, 0.551, 0.596, 0.613, 0.681, 0.574, 0.561, 0.542, 0.551, 0.438, 0.573, 0.302, 0.529, 0.339, 0.457, 0.462, 0.383, 0.583, 0.415, 0.489, 0.485, 0.348, 0.522, 0.347, 0.479, 0.414, 0.425, 0.465, 0.432, 0.357, 0.507, 0.385, 0.529, 0.405, 0.445, 0.516, 0.447, 0.609, 0.389, 0.499, 0.548, 0.439, 0.587, 0.568, 0.638, 0.707, 0.612, 0.656, 0.735, 0.645, 0.867, 0.613, 0.786, 0.594, 0.663, 0.56, 0.575, 0.577, 0.468, 0.437, 0.428, 0.394, 0.466, 0.367, 0.45, 0.316, 0.432, 0.344, 0.448, 0.362, 0.375, 0.381, 0.413, 0.436, 0.438, 0.395, 0.479, 0.491, 0.564, 0.52, 0.574, 0.508, 0.566, 0.51, 0.576, 0.456, 0.578, 0.466]\n",
      "Val Error(all epochs): 6.599117279052734 \n",
      " [48.815, 48.736, 48.634, 48.53, 48.401, 48.27, 48.079, 47.856, 47.607, 47.3, 47.004, 46.576, 46.115, 45.761, 45.041, 44.503, 43.767, 43.187, 42.634, 41.719, 41.505, 40.689, 39.581, 39.635, 38.073, 36.738, 35.971, 34.771, 34.044, 33.187, 32.484, 31.038, 30.109, 28.681, 27.195, 25.603, 24.573, 24.435, 23.799, 22.421, 21.669, 20.625, 19.734, 19.152, 18.689, 17.46, 16.586, 15.429, 15.263, 15.372, 14.294, 14.072, 14.276, 14.026, 14.214, 13.345, 13.082, 13.011, 12.758, 13.014, 12.238, 12.063, 12.03, 12.018, 11.687, 11.505, 10.538, 11.196, 10.878, 8.692, 9.305, 8.726, 9.389, 9.18, 8.343, 8.984, 7.203, 8.284, 8.232, 8.466, 8.296, 8.103, 8.167, 8.079, 8.716, 8.381, 7.632, 8.505, 7.17, 7.506, 7.229, 6.945, 7.352, 6.953, 7.201, 6.736, 7.247, 7.073, 7.49, 7.221, 7.741, 7.329, 7.806, 6.951, 7.555, 7.309, 6.909, 7.495, 7.012, 7.021, 7.531, 6.599, 7.589, 7.317, 6.772, 7.602, 7.149, 7.149, 7.773, 6.726, 7.592, 7.362, 7.34, 7.285, 7.375, 7.07, 7.399, 7.177, 7.664, 6.99, 7.644, 7.721, 7.205, 7.373, 7.862, 7.952, 7.501, 7.937, 8.747, 8.235, 8.283, 8.563, 7.79, 8.475, 7.784, 7.496, 7.885, 7.452, 7.84, 7.478, 7.513, 7.864, 7.535, 7.197, 7.886, 7.247, 7.713, 7.448, 7.469, 7.759, 7.441, 8.015, 7.743, 7.618, 8.154, 7.403, 7.662, 7.685, 7.192, 7.747, 7.699, 7.174, 7.971, 7.312, 7.802, 7.828, 7.338, 7.65, 7.606, 7.396, 8.423, 7.689, 7.772, 7.923, 8.282, 7.89, 7.75, 7.972, 7.714, 7.993, 7.818, 8.02, 7.685, 7.909, 7.602, 7.826, 7.332, 8.148, 7.508, 7.777, 7.828, 7.629, 7.725, 7.894, 7.463, 7.605, 7.715, 7.527, 7.616, 7.831, 7.707, 7.758, 7.839, 8.016, 7.998, 8.748, 7.729, 8.442, 7.719, 7.957, 7.688, 7.555, 7.83, 7.568, 7.628, 7.866, 7.772, 7.757, 7.96, 7.374, 8.238, 7.528, 7.745, 7.85, 7.724, 7.942, 7.867, 7.558, 7.841, 7.58, 7.735, 7.745, 7.553, 7.735, 7.731, 7.464, 8.188, 7.478, 8.008, 7.572, 7.659, 7.845, 7.539, 7.695, 7.725, 7.405, 7.801, 7.539, 7.593, 7.736, 7.676, 7.434, 8.112, 7.374, 7.748, 8.043, 7.585, 7.863, 7.982, 7.569, 8.45, 8.055, 7.933, 7.784, 7.634, 7.909, 7.775, 7.926, 8.273, 8.136, 8.177, 8.561, 8.155, 8.537, 8.01, 8.814, 8.084, 8.247, 7.936, 8.373, 7.789, 8.353, 7.937, 8.3, 7.959, 8.289, 7.759, 7.865, 7.585, 7.82, 7.638, 7.691, 7.835, 7.645, 7.709, 7.819, 7.518, 7.753, 7.774, 7.303, 7.937, 7.569, 7.502, 7.931, 7.536, 7.54, 8.052, 7.544, 7.647, 7.985, 7.287, 7.938, 7.577, 7.449, 7.947, 7.384, 7.979, 7.625, 7.595, 7.877, 7.44, 7.924, 7.473, 7.793, 7.698, 7.598, 7.865, 7.579, 7.811, 7.643, 7.783, 7.716, 7.786, 7.649, 7.825, 7.607, 7.721, 7.819, 7.589, 7.851, 7.653, 7.535, 7.863, 7.49, 7.763, 7.73, 7.573, 7.683, 7.768, 7.437, 7.86, 7.722, 7.474, 8.06, 7.466, 7.618, 8.019, 7.389, 8.141, 7.777, 7.714, 7.857, 7.674, 7.702, 7.859, 7.902, 7.8, 8.071, 7.694, 8.116, 7.628, 7.964, 7.797, 7.784, 7.8, 7.691, 7.457, 7.973, 7.519, 7.556, 7.979, 7.338, 7.997, 7.631, 7.621, 7.904, 7.433, 7.846, 7.529, 7.701, 7.739, 7.808, 7.545, 8.0, 7.317, 7.947, 7.383, 7.685, 7.731, 7.757, 7.761, 7.879, 7.561, 7.887, 7.652, 7.807, 7.827, 7.717, 8.031, 7.619, 7.942, 7.699, 7.711, 7.825, 7.843, 7.811, 7.726, 7.667, 7.724, 7.663, 7.728, 7.61, 7.656, 7.704, 7.588, 7.731, 7.643, 7.584, 7.654, 7.599, 7.586, 7.686, 7.7, 7.662, 7.823, 7.624, 7.656, 7.842, 7.555, 7.724, 7.915, 7.463, 7.871, 7.672, 7.595, 7.922, 7.861, 7.441, 8.309, 7.775, 7.985, 8.12, 7.774, 8.307, 7.548, 7.926, 7.78, 7.758, 8.025, 7.59, 8.023, 7.643, 7.876, 7.678, 7.736, 7.719, 7.712, 7.791, 7.719, 7.789, 7.74, 7.667, 7.737, 7.537, 7.775, 7.592, 7.671, 7.74, 7.571, 7.716, 7.692, 7.552, 7.712, 7.578, 7.507, 7.825, 7.634, 7.618, 7.962, 7.624]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004, 0.084, 0.163, 0.18, 0.272, 0.428, 0.339, 0.552, 1.016, 0.981, 1.204, 1.175, 1.623, 1.862, 2.035, 1.389, 2.046, 1.589, 2.052, 2.077, 2.012, 2.549, 2.171, 2.674, 1.613, 1.685, 1.697, 2.034, 2.024, 1.717, 2.049, 2.427, 2.041, 1.85, 3.133, 3.439, 2.102, 2.585, 2.269, 1.9, 1.853, 1.505, 2.185, 1.988, 2.04, 2.057, 2.027, 1.862, 2.097, 2.57, 2.711, 3.31, 3.83, 4.008, 3.82, 3.766, 3.418, 3.323, 3.075, 2.975, 3.107, 2.569, 2.61, 2.718, 2.698, 2.656, 3.527, 3.233, 3.669, 3.732, 3.526, 3.675, 4.019, 3.583, 4.26, 3.924, 3.834, 3.889, 4.049, 4.054, 4.357, 3.974, 4.343, 4.118, 3.999, 3.526, 4.107, 3.838, 3.94, 4.408, 4.259, 4.342, 4.669, 4.635, 4.26, 4.254, 4.315, 4.995, 3.852, 4.257, 5.307, 4.504, 4.6, 5.143, 4.474, 5.378, 5.156, 4.559, 4.895, 5.098, 4.674, 4.757, 5.044, 4.98, 5.03, 4.458, 5.214, 4.588, 4.531, 4.456, 4.495, 4.231, 4.371, 4.277, 4.48, 4.342, 4.591, 4.05, 4.474, 4.171, 4.057, 4.33, 4.309, 4.186, 4.377, 3.974, 4.106, 4.585, 4.278, 4.541, 4.571, 4.639, 4.994, 5.233, 4.976, 4.877, 5.669, 5.317, 5.298, 5.011, 5.056, 5.665, 5.201, 5.582, 5.068, 5.371, 4.911, 4.982, 4.688, 5.061, 4.892, 4.955, 4.796, 4.952, 4.749, 4.637, 4.671, 4.343, 4.552, 4.396, 4.249, 4.327, 4.911, 4.431, 5.02, 5.131, 4.834, 6.04, 4.845, 5.419, 5.214, 4.904, 4.881, 4.977, 4.755, 4.706, 4.978, 4.436, 5.322, 4.746, 5.035, 4.896, 5.188, 4.828, 4.861, 4.881, 5.0, 4.909, 5.108, 4.811, 4.991, 4.751, 4.881, 4.703, 4.809, 4.521, 4.849, 4.518, 4.9, 4.822, 4.867, 4.661, 4.782, 4.375, 4.817, 4.46, 4.649, 4.382, 4.452, 4.342, 4.521, 4.234, 4.727, 4.437, 4.769, 4.455, 4.782, 4.475, 5.066, 4.487, 5.036, 4.811, 4.71, 5.149, 4.546, 4.972, 4.771, 4.472, 4.841, 4.613, 4.9, 4.833, 5.301, 5.571, 5.776, 6.029, 5.621, 6.329, 5.83, 5.622, 5.419, 5.707, 5.14, 5.729, 5.246, 5.896, 5.223, 5.683, 4.949, 4.958, 4.681, 4.619, 4.421, 4.697, 4.512, 4.649, 4.515, 4.656, 4.35, 4.515, 4.253, 4.247, 4.372, 4.408, 4.455, 4.647, 4.44, 4.591, 4.682, 4.578, 4.571, 4.69, 4.477, 4.482, 4.626, 4.27, 4.545, 4.506, 4.466, 4.487, 4.42, 4.44, 4.612, 4.547, 4.595, 4.569, 4.64, 4.548, 4.736, 4.465, 4.715, 4.39, 4.576, 4.5, 4.346, 4.374, 4.407, 4.297, 4.603, 4.328, 4.692, 4.45, 4.635, 4.384, 4.422, 4.17, 4.434, 4.166, 4.347, 4.121, 4.287, 4.219, 4.479, 4.207, 4.589, 4.264, 4.457, 4.218, 4.558, 4.425, 4.186, 4.654, 4.39, 4.542, 4.564, 4.527, 4.742, 4.705, 4.692, 4.691, 4.755, 4.639, 5.048, 4.953, 4.899, 5.008, 4.72, 4.301, 4.257, 4.329, 4.11, 4.421, 4.339, 4.263, 4.55, 4.48, 4.462, 4.635, 4.308, 4.417, 4.2, 4.365, 4.373, 4.458, 4.704, 4.479, 4.48, 4.429, 4.261, 4.158, 4.312, 4.303, 4.376, 4.369, 4.324, 4.359, 4.388, 4.86, 4.559, 5.112, 4.812, 5.022, 4.673, 4.572, 4.375, 4.513, 4.787, 4.499, 4.659, 4.349, 4.528, 4.352, 4.462, 4.367, 4.442, 4.604, 4.347, 4.466, 4.215, 4.258, 4.298, 4.17, 4.247, 4.306, 4.151, 4.465, 4.213, 4.424, 4.26, 4.308, 4.345, 4.453, 4.44, 4.501, 4.485, 4.409, 4.393, 4.544, 4.515, 4.84, 4.934, 5.023, 4.746, 5.239, 5.213, 5.112, 4.658, 4.388, 4.028, 4.093, 3.923, 4.186, 4.057, 4.363, 4.144, 4.334, 4.14, 4.226, 3.988, 4.23, 3.878, 4.253, 3.883, 4.232, 4.031, 4.178, 4.106, 4.128, 4.116, 4.424, 4.144, 4.704, 4.241, 4.715, 4.135, 4.317, 4.344, 4.457, 4.485, 4.627, 4.511, 4.567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 48.87556, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 48.87556 to 48.73210, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 48.73210 to 48.58700, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 48.58700 to 48.46492, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 48.46492 to 48.33086, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 48.33086 to 48.14877, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 48.14877 to 47.90432, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 47.90432 to 47.52607, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 47.52607 to 47.18219, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 47.18219 to 46.50684, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 46.50684 to 45.74746, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 45.74746 to 45.10419, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 45.10419 to 43.86226, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 43.86226 to 42.28346, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 42.28346 to 41.04644, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 41.04644 to 39.06761, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 39.06761 to 35.42492, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 35.42492\n",
      "\n",
      "Epoch 00019: val_mae improved from 35.42492 to 31.17829, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 31.17829 to 30.33125, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 30.33125 to 27.01815, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 27.01815\n",
      "\n",
      "Epoch 00023: val_mae improved from 27.01815 to 25.44220, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 25.44220 to 23.19806, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 23.19806\n",
      "\n",
      "Epoch 00026: val_mae improved from 23.19806 to 22.75345, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 22.75345 to 17.90772, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 17.90772\n",
      "\n",
      "Epoch 00029: val_mae improved from 17.90772 to 17.50132, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 17.50132 to 12.15199, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 12.15199\n",
      "\n",
      "Epoch 00068: val_mae improved from 12.15199 to 11.63114, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 11.63114\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 11.63114\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 11.63114\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 11.63114\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 11.63114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00074: val_mae did not improve from 11.63114\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 11.63114\n",
      "\n",
      "Epoch 00076: val_mae improved from 11.63114 to 11.07023, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 11.07023\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 11.07023\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 11.07023\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 11.07023\n",
      "\n",
      "Epoch 00081: val_mae improved from 11.07023 to 9.63821, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 9.63821\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 9.63821\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 9.63821\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 9.63821\n",
      "\n",
      "Epoch 00086: val_mae improved from 9.63821 to 9.60582, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 9.60582\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 9.60582\n",
      "\n",
      "Epoch 00089: val_mae improved from 9.60582 to 9.21282, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 9.21282\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 9.21282\n",
      "\n",
      "Epoch 00092: val_mae improved from 9.21282 to 8.93497, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 8.93497\n",
      "\n",
      "Epoch 00094: val_mae improved from 8.93497 to 8.62915, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 8.62915\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 8.62915\n",
      "\n",
      "Epoch 00097: val_mae improved from 8.62915 to 8.05758, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 8.05758\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 8.05758\n",
      "\n",
      "Epoch 00100: val_mae improved from 8.05758 to 7.84803, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 7.84803\n",
      "\n",
      "Epoch 00102: val_mae improved from 7.84803 to 7.29333, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 7.29333\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 7.29333\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 7.29333\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 7.29333\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 7.29333\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 7.29333\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 7.29333\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 7.29333\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 7.29333\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 7.29333\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 7.29333\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 7.29333\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 7.29333\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 7.29333\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 7.29333\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 7.29333\n",
      "\n",
      "Epoch 00119: val_mae improved from 7.29333 to 6.95465, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 6.95465\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 6.95465\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 6.95465\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 6.95465\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 6.95465\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 6.95465\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 6.95465\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 6.95465\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 6.95465\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 6.95465\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 6.95465\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 6.95465\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 6.95465\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 6.95465\n",
      "\n",
      "Epoch 00134: val_mae improved from 6.95465 to 6.92793, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 6.92793\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 6.92793\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 6.92793\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 6.92793\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 6.92793\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 6.92793\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 6.92793\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 6.92793\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 6.92793\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 6.92793\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 6.92793\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 6.92793\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 6.92793\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 6.92793\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 6.92793\n",
      "\n",
      "Epoch 00150: val_mae improved from 6.92793 to 6.90173, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 6.90173\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 6.90173\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 6.90173\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 6.90173\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 6.90173\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 6.90173\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 6.90173\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 6.90173\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 6.90173\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 6.90173\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 6.90173\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 6.90173\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 6.90173\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 6.90173\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 6.90173\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 6.90173\n",
      "\n",
      "Epoch 00167: val_mae improved from 6.90173 to 6.80054, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 6.80054\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 6.80054\n",
      "\n",
      "Epoch 00170: val_mae improved from 6.80054 to 6.69251, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 6.69251\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 6.69251\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 6.69251\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 6.69251\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 6.69251\n",
      "\n",
      "Epoch 00176: val_mae improved from 6.69251 to 6.53210, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 6.53210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00187: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 6.53210\n",
      "\n",
      "Epoch 00211: val_mae improved from 6.53210 to 6.53071, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 6.53071\n",
      "\n",
      "Epoch 00245: val_mae improved from 6.53071 to 6.42551, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 6.42551\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 6.42551\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 6.42551\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 6.42551\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 6.42551\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 6.42551\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 6.42551\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 6.42551\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 6.42551\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 6.42551\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 6.42551\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 6.42551\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 6.42551\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 6.42551\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 6.42551\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 6.42551\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 6.42551\n",
      "\n",
      "Epoch 00263: val_mae improved from 6.42551 to 6.34168, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 6.34168\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 6.34168\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 6.34168\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 6.34168\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 6.34168\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 6.34168\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 6.34168\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 6.34168\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 6.34168\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 6.34168\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 6.34168\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 6.34168\n",
      "\n",
      "Epoch 00276: val_mae improved from 6.34168 to 6.26913, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 6.26913\n",
      "\n",
      "Epoch 00300: val_mae improved from 6.26913 to 6.23041, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 6.23041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00332: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 6.23041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00493: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 6.23041\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 6.23041\n",
      "\n",
      "Lambda: 0.1 , Time: 0:03:57\n",
      "Train Error(all epochs): 1.137649655342102 \n",
      " [49.081, 48.946, 48.85, 48.744, 48.616, 48.461, 48.273, 48.048, 47.768, 47.414, 47.004, 46.542, 46.01, 45.382, 44.694, 43.925, 43.08, 42.184, 41.199, 40.136, 38.995, 37.803, 36.535, 35.198, 33.804, 32.382, 30.887, 29.402, 27.874, 26.352, 24.86, 23.344, 21.909, 20.561, 19.318, 18.154, 17.176, 16.252, 15.875, 15.172, 14.678, 14.278, 13.785, 13.479, 13.05, 12.723, 12.504, 12.185, 11.85, 11.653, 11.405, 11.247, 10.955, 10.873, 10.735, 10.356, 10.331, 10.048, 9.78, 9.623, 9.227, 9.034, 8.938, 8.702, 8.68, 8.545, 8.159, 8.137, 7.881, 7.678, 7.587, 7.318, 7.142, 7.086, 6.83, 6.766, 6.626, 6.359, 6.319, 6.19, 6.115, 6.104, 5.971, 5.852, 5.753, 5.582, 5.377, 5.148, 5.084, 5.128, 4.968, 4.991, 4.891, 4.77, 4.749, 4.721, 4.827, 4.865, 4.84, 4.815, 4.942, 4.822, 4.536, 4.271, 4.146, 4.041, 4.037, 3.953, 3.906, 4.129, 4.179, 4.156, 4.156, 4.413, 4.338, 4.125, 4.13, 4.217, 4.046, 4.199, 4.063, 4.065, 4.027, 3.932, 4.015, 3.911, 3.875, 3.737, 3.643, 3.722, 3.699, 3.661, 3.653, 3.77, 3.791, 3.723, 3.622, 3.687, 3.604, 3.415, 3.497, 3.613, 3.6, 3.592, 3.823, 3.939, 3.795, 3.84, 4.02, 4.365, 4.219, 4.237, 3.758, 3.621, 3.608, 3.397, 3.265, 3.236, 3.109, 3.116, 3.065, 3.047, 3.268, 3.415, 3.451, 3.376, 3.476, 3.69, 3.731, 3.668, 3.84, 3.573, 3.382, 3.392, 3.223, 3.299, 3.292, 3.345, 3.144, 3.126, 3.025, 3.067, 3.087, 3.107, 3.325, 3.418, 3.389, 3.412, 3.076, 3.036, 2.902, 3.152, 3.067, 2.997, 3.05, 3.07, 3.177, 3.495, 3.452, 3.441, 3.313, 3.248, 3.177, 3.326, 3.146, 2.992, 2.975, 2.963, 2.987, 2.874, 2.644, 2.913, 2.942, 2.828, 2.719, 2.551, 2.53, 2.561, 2.389, 2.443, 2.566, 2.703, 2.795, 3.024, 2.973, 2.978, 2.709, 2.791, 2.843, 2.812, 2.686, 2.578, 2.766, 2.865, 2.837, 2.823, 2.859, 2.897, 2.674, 2.653, 2.381, 2.38, 2.416, 2.302, 2.41, 2.621, 2.528, 2.465, 2.839, 2.713, 2.675, 3.03, 3.371, 3.265, 2.857, 2.651, 2.622, 2.632, 2.741, 2.611, 2.608, 2.788, 2.742, 2.742, 2.68, 2.432, 2.12, 2.036, 1.939, 1.896, 1.834, 1.839, 1.961, 2.085, 2.148, 2.301, 2.513, 2.831, 2.789, 2.65, 2.261, 2.211, 2.093, 2.052, 2.079, 1.986, 1.953, 2.023, 2.137, 2.237, 2.371, 2.528, 2.397, 2.381, 2.266, 2.06, 1.912, 2.077, 2.174, 2.294, 2.22, 2.029, 2.055, 2.109, 2.062, 1.976, 1.899, 1.982, 2.101, 2.302, 2.216, 2.34, 2.2, 2.153, 2.054, 1.85, 2.001, 2.054, 1.901, 1.889, 1.928, 1.902, 1.901, 1.975, 2.06, 2.041, 1.93, 1.748, 1.712, 1.799, 1.954, 1.955, 2.36, 2.77, 2.854, 2.65, 2.297, 1.907, 1.84, 1.628, 1.618, 1.626, 1.709, 1.923, 2.114, 2.208, 1.897, 1.815, 1.625, 1.686, 1.634, 1.731, 1.688, 1.596, 1.501, 1.535, 1.551, 1.725, 1.792, 1.765, 1.799, 1.887, 1.637, 1.562, 1.396, 1.233, 1.205, 1.246, 1.433, 1.72, 2.065, 2.515, 2.662, 2.645, 2.501, 2.247, 2.046, 1.714, 1.484, 1.402, 1.444, 1.411, 1.482, 1.576, 1.711, 1.679, 1.825, 2.036, 2.016, 1.929, 1.679, 1.732, 1.603, 1.527, 1.46, 1.389, 1.333, 1.442, 1.495, 1.712, 1.643, 1.59, 1.399, 1.37, 1.363, 1.381, 1.439, 1.473, 1.478, 1.49, 1.521, 1.541, 1.622, 1.639, 1.687, 1.661, 1.501, 1.398, 1.75, 1.805, 1.667, 1.433, 1.308, 1.363, 1.35, 1.429, 1.308, 1.224, 1.215, 1.27, 1.356, 1.443, 1.448, 1.553, 1.523, 1.626, 1.935, 2.008, 1.849, 1.621, 1.482, 1.299, 1.264, 1.253, 1.228, 1.189, 1.269, 1.172, 1.264, 1.314, 1.376, 1.449, 1.499, 1.847, 1.949, 2.232, 2.092, 2.066, 2.135, 2.223, 1.885, 1.616, 1.506, 1.455, 1.447, 1.5, 1.457, 1.471, 1.475, 1.344, 1.334, 1.41, 1.362, 1.489, 1.583, 1.592, 1.483, 1.486, 1.679, 1.776, 1.764, 1.624, 1.716, 2.097, 2.273, 2.218, 2.041, 2.011, 1.875, 1.69, 1.7, 1.747, 1.618, 1.488, 1.363, 1.321, 1.229, 1.174, 1.185, 1.138]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013, 0.01, 0.026, 0.077, 0.101, 0.174, 0.24, 0.474, 0.541, 0.642, 0.801, 0.824, 0.945, 0.958, 1.0, 1.073, 1.09, 1.067, 1.113, 1.141, 1.207, 1.158, 1.252, 1.314, 1.239, 1.343, 1.322, 1.294, 1.35, 1.228, 1.296, 1.338, 1.325, 1.403, 1.463, 1.324, 1.467, 1.406, 1.428, 1.464, 1.404, 1.403, 1.515, 1.442, 1.516, 1.552, 1.428, 1.537, 1.506, 1.57, 1.608, 1.603, 1.63, 1.649, 1.583, 1.618, 1.517, 1.522, 1.601, 1.553, 1.642, 1.604, 1.544, 1.656, 1.622, 1.708, 1.779, 1.735, 1.829, 1.867, 1.747, 1.759, 1.593, 1.524, 1.516, 1.532, 1.544, 1.448, 1.646, 1.691, 1.634, 1.626, 1.833, 1.784, 1.616, 1.568, 1.786, 1.657, 1.711, 1.623, 1.662, 1.654, 1.599, 1.619, 1.527, 1.702, 1.506, 1.486, 1.51, 1.449, 1.55, 1.454, 1.536, 1.566, 1.533, 1.472, 1.489, 1.534, 1.382, 1.408, 1.498, 1.548, 1.475, 1.596, 1.784, 1.511, 1.643, 1.691, 1.978, 1.795, 1.829, 1.551, 1.493, 1.472, 1.425, 1.293, 1.319, 1.259, 1.245, 1.275, 1.244, 1.33, 1.448, 1.532, 1.388, 1.439, 1.553, 1.637, 1.588, 1.707, 1.46, 1.368, 1.474, 1.374, 1.353, 1.379, 1.437, 1.281, 1.291, 1.238, 1.254, 1.282, 1.33, 1.423, 1.427, 1.442, 1.405, 1.272, 1.344, 1.165, 1.308, 1.297, 1.293, 1.266, 1.389, 1.27, 1.559, 1.538, 1.426, 1.457, 1.439, 1.271, 1.461, 1.299, 1.196, 1.338, 1.221, 1.216, 1.223, 1.075, 1.199, 1.294, 1.18, 1.17, 1.031, 0.971, 1.108, 0.952, 0.996, 1.109, 1.108, 1.173, 1.332, 1.231, 1.312, 1.147, 1.206, 1.235, 1.124, 1.163, 1.097, 1.137, 1.27, 1.239, 1.216, 1.248, 1.221, 1.193, 1.071, 1.043, 0.905, 1.168, 0.899, 1.009, 1.115, 1.149, 0.957, 1.411, 1.094, 1.106, 1.428, 1.433, 1.497, 1.184, 1.198, 1.166, 1.117, 1.139, 1.115, 1.088, 1.361, 1.124, 1.329, 1.1, 1.066, 0.865, 0.826, 0.802, 0.776, 0.767, 0.766, 0.821, 0.899, 0.909, 1.036, 1.169, 1.243, 1.299, 1.144, 0.987, 0.945, 0.9, 0.844, 0.957, 0.8, 0.91, 0.848, 0.994, 0.972, 1.127, 1.218, 0.91, 1.244, 0.903, 0.962, 0.821, 0.899, 0.978, 1.01, 1.044, 0.808, 0.971, 0.959, 0.902, 0.803, 0.91, 0.81, 1.01, 1.012, 1.061, 1.008, 0.966, 1.002, 0.888, 0.858, 0.882, 0.913, 0.811, 0.866, 0.833, 0.874, 0.786, 0.968, 0.866, 0.98, 0.872, 0.675, 0.948, 0.687, 0.956, 0.892, 1.061, 1.307, 1.229, 1.297, 1.117, 0.732, 0.965, 0.591, 0.711, 0.721, 0.829, 0.823, 1.049, 0.974, 0.873, 0.836, 0.701, 0.788, 0.699, 0.809, 0.747, 0.71, 0.688, 0.672, 0.744, 0.752, 0.918, 0.778, 0.799, 0.924, 0.638, 0.8, 0.595, 0.543, 0.571, 0.533, 0.671, 0.752, 1.03, 1.26, 1.122, 1.453, 1.044, 1.064, 1.002, 0.683, 0.73, 0.588, 0.636, 0.662, 0.622, 0.768, 0.759, 0.776, 0.886, 0.899, 1.028, 0.84, 0.722, 0.918, 0.615, 0.808, 0.602, 0.618, 0.667, 0.596, 0.708, 0.829, 0.705, 0.835, 0.571, 0.682, 0.596, 0.652, 0.682, 0.656, 0.712, 0.665, 0.735, 0.764, 0.699, 0.863, 0.716, 0.791, 0.712, 0.598, 0.906, 0.791, 0.816, 0.663, 0.572, 0.621, 0.641, 0.663, 0.594, 0.62, 0.543, 0.548, 0.683, 0.667, 0.676, 0.793, 0.644, 0.872, 0.847, 1.021, 0.805, 0.779, 0.721, 0.502, 0.688, 0.505, 0.596, 0.542, 0.599, 0.549, 0.546, 0.674, 0.603, 0.693, 0.721, 0.896, 0.944, 1.161, 0.921, 1.055, 0.834, 1.166, 0.826, 0.709, 0.785, 0.581, 0.716, 0.71, 0.645, 0.714, 0.659, 0.626, 0.583, 0.715, 0.564, 0.75, 0.751, 0.727, 0.717, 0.643, 0.84, 0.784, 0.877, 0.756, 0.737, 1.037, 1.088, 0.992, 0.985, 0.914, 0.834, 0.826, 0.708, 0.932, 0.654, 0.732, 0.627, 0.581, 0.62, 0.515, 0.581, 0.498]\n",
      "Val Error(all epochs): 6.23040771484375 \n",
      " [48.876, 48.732, 48.587, 48.465, 48.331, 48.149, 47.904, 47.526, 47.182, 46.507, 45.747, 45.104, 43.862, 42.283, 41.046, 39.068, 35.425, 36.421, 31.178, 30.331, 27.018, 27.797, 25.442, 23.198, 24.286, 22.753, 17.908, 20.296, 17.501, 12.152, 13.081, 12.253, 12.36, 12.749, 13.749, 13.955, 13.327, 14.173, 17.155, 17.587, 19.034, 21.568, 19.565, 21.369, 21.573, 17.964, 19.384, 17.943, 16.717, 17.645, 15.695, 15.547, 16.483, 14.658, 15.775, 15.747, 14.102, 15.087, 14.465, 14.049, 14.415, 13.146, 13.731, 13.695, 12.804, 13.39, 13.415, 11.631, 12.706, 12.403, 11.71, 12.814, 12.368, 11.989, 12.385, 11.07, 11.46, 11.619, 11.077, 11.344, 9.638, 10.968, 11.108, 9.958, 12.05, 9.606, 11.059, 10.325, 9.213, 10.149, 9.642, 8.935, 10.172, 8.629, 8.659, 9.856, 8.058, 9.072, 8.324, 7.848, 9.471, 7.293, 8.218, 8.79, 8.955, 8.362, 8.666, 8.219, 8.379, 7.983, 8.954, 7.545, 8.033, 7.323, 7.847, 8.537, 8.139, 7.763, 6.955, 9.419, 7.045, 7.176, 8.976, 7.548, 8.986, 7.015, 7.994, 8.484, 7.764, 8.992, 8.093, 7.66, 9.518, 6.928, 9.021, 9.137, 7.235, 9.1, 8.357, 7.846, 9.637, 7.766, 8.503, 6.937, 7.663, 8.567, 7.42, 7.09, 10.142, 6.902, 10.552, 8.205, 8.486, 7.381, 8.936, 7.613, 8.351, 9.317, 8.001, 8.725, 7.76, 8.146, 8.317, 7.22, 8.11, 9.248, 6.801, 9.373, 7.865, 6.693, 7.58, 6.953, 7.232, 7.025, 8.019, 6.532, 8.793, 7.311, 7.364, 7.849, 7.186, 7.229, 7.313, 7.121, 7.926, 6.883, 8.96, 6.784, 7.4, 9.136, 8.279, 7.922, 7.667, 7.208, 7.964, 8.33, 7.839, 7.377, 7.944, 6.575, 9.251, 7.222, 7.651, 7.247, 6.991, 6.79, 7.778, 7.674, 7.236, 7.514, 6.531, 8.254, 6.951, 7.296, 8.064, 6.973, 7.313, 7.285, 7.115, 6.988, 7.519, 7.166, 7.532, 7.307, 8.265, 7.57, 7.407, 6.961, 8.554, 8.157, 7.631, 7.78, 6.796, 7.286, 7.677, 6.587, 8.04, 6.644, 9.668, 7.367, 9.013, 7.082, 6.935, 7.271, 6.426, 7.21, 7.591, 6.63, 8.039, 6.67, 6.557, 9.045, 6.597, 7.943, 7.199, 8.367, 8.187, 7.114, 7.152, 7.34, 6.539, 9.102, 6.342, 8.312, 6.631, 7.928, 7.275, 6.924, 7.002, 7.431, 7.258, 8.302, 6.973, 8.34, 7.766, 6.269, 8.333, 6.472, 8.948, 7.453, 8.385, 9.028, 7.2, 7.119, 6.909, 7.668, 6.516, 7.009, 7.547, 7.543, 6.777, 7.835, 6.983, 7.527, 6.378, 7.022, 6.936, 6.563, 7.137, 6.23, 7.118, 6.433, 6.948, 6.615, 7.0, 6.618, 7.088, 6.855, 7.702, 7.155, 8.356, 7.901, 6.82, 7.105, 7.072, 6.533, 7.938, 6.79, 7.067, 7.113, 6.948, 6.679, 7.745, 7.08, 8.741, 6.724, 8.179, 6.388, 7.537, 6.324, 6.912, 6.927, 6.81, 7.815, 6.392, 7.261, 6.461, 6.609, 6.644, 6.673, 7.016, 6.696, 6.679, 6.531, 7.685, 6.331, 7.25, 7.156, 7.486, 7.511, 7.501, 7.503, 8.099, 6.946, 8.114, 6.516, 8.233, 6.687, 7.281, 7.499, 6.847, 6.977, 6.823, 6.664, 6.81, 7.019, 6.604, 7.041, 7.243, 6.845, 6.965, 8.224, 6.367, 9.163, 9.304, 7.659, 8.182, 6.911, 7.92, 6.783, 7.004, 7.091, 6.495, 7.649, 6.645, 6.518, 7.469, 6.319, 6.977, 6.789, 6.635, 7.251, 6.532, 6.837, 6.592, 6.469, 6.942, 6.383, 6.991, 6.576, 6.766, 6.646, 6.714, 6.631, 6.753, 7.209, 6.556, 7.304, 6.656, 6.727, 6.646, 6.922, 6.729, 7.607, 6.561, 7.284, 7.116, 6.923, 7.481, 6.704, 6.792, 6.684, 6.961, 6.903, 6.993, 7.121, 6.733, 6.753, 6.87, 6.577, 7.123, 6.513, 7.136, 7.226, 6.734, 7.517, 6.565, 7.323, 6.895, 6.847, 7.016, 7.388, 6.732, 7.153, 6.94, 6.673, 6.996, 6.89, 6.946, 7.116, 7.005, 7.257, 6.972, 6.761, 7.659, 7.513, 7.554, 7.138, 8.411, 7.497, 6.751, 7.249, 7.423, 8.357, 8.365, 6.849, 7.402, 6.895, 6.85, 6.765, 6.874, 6.455, 6.51, 7.078, 7.214, 6.552, 6.972, 6.622, 7.074, 7.317, 6.74, 6.652, 7.369, 7.471, 7.87, 8.018, 6.294, 7.13, 6.709, 6.87, 7.093, 6.694, 7.161, 6.775, 6.71, 7.15, 6.697, 6.72, 6.469, 6.672]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013, 0.277, 0.102, 0.385, 2.373, 1.777, 4.159, 4.524, 4.727, 6.863, 6.514, 5.01, 6.111, 10.421, 10.698, 12.511, 16.806, 12.357, 14.51, 14.997, 9.981, 11.912, 9.933, 8.309, 9.48, 6.997, 6.778, 7.861, 5.788, 7.144, 7.006, 5.24, 6.375, 5.876, 5.495, 5.926, 4.406, 5.229, 5.062, 4.535, 5.396, 5.294, 3.47, 5.011, 4.477, 4.209, 5.54, 4.879, 4.945, 5.228, 4.367, 4.452, 5.159, 4.465, 4.637, 3.368, 5.064, 5.163, 4.573, 6.231, 4.521, 5.606, 4.431, 3.574, 4.343, 3.22, 3.452, 3.791, 2.834, 3.671, 4.016, 2.396, 4.139, 3.533, 3.566, 4.171, 3.285, 4.2, 4.487, 4.407, 3.304, 3.54, 3.202, 3.267, 3.14, 3.904, 2.813, 4.079, 3.016, 3.919, 4.97, 4.055, 4.103, 3.62, 5.122, 2.346, 3.366, 5.07, 4.08, 4.934, 3.316, 4.638, 4.663, 4.35, 4.748, 4.163, 4.234, 5.105, 3.167, 5.122, 5.26, 3.941, 5.18, 4.412, 4.209, 5.826, 3.994, 4.532, 3.322, 4.421, 4.967, 4.088, 4.291, 6.972, 4.471, 8.642, 6.17, 6.264, 4.73, 6.343, 5.209, 5.577, 5.926, 4.746, 5.135, 3.968, 4.403, 4.056, 3.384, 4.647, 5.068, 2.643, 5.904, 4.62, 2.939, 3.37, 3.481, 3.046, 3.216, 4.107, 2.839, 5.06, 4.096, 4.086, 3.943, 3.659, 3.405, 2.963, 3.545, 4.145, 2.863, 4.812, 3.34, 4.008, 5.592, 5.49, 4.023, 4.026, 3.769, 4.557, 4.763, 5.035, 3.985, 4.392, 3.463, 6.34, 4.086, 5.065, 3.39, 3.302, 3.456, 3.735, 2.269, 2.918, 4.065, 2.651, 4.018, 3.235, 3.5, 4.261, 3.07, 3.198, 2.965, 2.745, 2.642, 2.496, 2.524, 3.039, 2.297, 4.354, 3.105, 3.0, 2.799, 4.132, 2.39, 2.199, 3.491, 2.615, 3.125, 4.015, 2.786, 4.331, 3.69, 6.693, 4.314, 5.493, 3.355, 3.435, 3.361, 3.233, 2.943, 3.938, 2.791, 4.152, 3.182, 3.589, 4.717, 3.105, 4.688, 5.101, 6.182, 5.306, 4.807, 3.64, 4.25, 3.215, 5.747, 4.027, 5.191, 4.232, 5.146, 4.172, 3.882, 3.348, 3.963, 3.679, 4.432, 3.684, 4.344, 4.338, 2.867, 4.661, 3.249, 5.633, 4.435, 5.577, 5.399, 4.438, 3.729, 3.494, 4.1, 3.189, 3.662, 4.122, 4.502, 2.674, 4.099, 2.427, 3.903, 2.702, 2.81, 2.762, 2.455, 3.384, 2.848, 3.531, 2.875, 3.307, 2.521, 3.14, 2.793, 3.308, 3.408, 3.91, 3.715, 5.348, 3.91, 3.255, 3.018, 3.669, 3.23, 4.32, 3.608, 3.701, 3.774, 3.652, 3.305, 4.557, 4.094, 5.423, 3.8, 4.607, 3.306, 3.812, 2.715, 3.417, 3.316, 2.593, 3.873, 3.008, 4.16, 3.285, 3.21, 3.083, 2.893, 2.436, 2.787, 3.41, 3.174, 4.589, 3.15, 4.05, 4.037, 4.499, 4.167, 4.421, 4.07, 4.885, 3.166, 4.731, 2.902, 4.559, 3.669, 3.948, 3.92, 3.466, 2.086, 3.006, 2.366, 3.225, 3.397, 3.054, 3.695, 3.408, 3.627, 3.516, 4.236, 3.371, 5.631, 5.993, 5.17, 4.952, 4.316, 4.614, 3.602, 3.745, 3.372, 3.247, 3.826, 3.153, 3.057, 3.899, 2.59, 3.057, 3.245, 3.056, 3.457, 2.703, 3.085, 2.52, 3.029, 2.995, 2.664, 2.715, 2.238, 2.758, 2.626, 2.712, 2.957, 3.055, 3.751, 3.092, 4.172, 2.685, 3.688, 2.774, 3.135, 2.924, 4.198, 2.732, 4.033, 3.04, 2.353, 3.461, 2.581, 2.957, 2.879, 2.557, 2.351, 2.052, 2.544, 2.432, 2.514, 3.178, 3.225, 3.245, 3.239, 2.906, 3.431, 3.206, 3.298, 2.269, 3.716, 2.884, 3.611, 2.928, 4.109, 2.962, 3.522, 2.411, 2.512, 2.236, 2.977, 2.33, 3.438, 2.834, 3.853, 2.857, 2.682, 4.27, 2.135, 2.161, 3.572, 2.094, 4.321, 3.198, 2.394, 2.158, 2.03, 1.981, 2.365, 2.363, 2.282, 2.711, 2.158, 2.739, 2.243, 3.155, 2.418, 2.338, 2.674, 2.819, 2.532, 3.111, 2.292, 2.632, 2.721, 2.109, 2.961, 2.382, 4.856, 3.655, 3.838, 2.748, 2.636, 2.716, 2.372, 2.383, 2.362, 2.79, 2.133, 2.355, 2.664, 2.412, 3.058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 48.87207, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 48.87207 to 48.79113, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 48.79113 to 48.71663, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 48.71663 to 48.60970, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 48.60970 to 48.51655, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 48.51655 to 48.36766, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 48.36766 to 48.21035, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 48.21035 to 48.07326, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 48.07326 to 47.86060, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 47.86060 to 47.64237, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 47.64237 to 47.31602, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 47.31602 to 47.04338, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 47.04338 to 46.91518, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 46.91518 to 46.44244, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 46.44244 to 45.78668, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 45.78668 to 45.60342, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 45.60342 to 45.01154, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 45.01154 to 44.59915, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 44.59915 to 43.57946, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 43.57946 to 43.47470, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 43.47470 to 42.91937, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 42.91937 to 41.62708, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 41.62708 to 40.64423, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 40.64423 to 40.20304, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 40.20304 to 39.19135, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 39.19135 to 36.92937, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 36.92937 to 36.48043, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 36.48043\n",
      "\n",
      "Epoch 00029: val_mae improved from 36.48043 to 35.36660, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 35.36660 to 35.00541, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00031: val_mae improved from 35.00541 to 32.81157, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00032: val_mae improved from 32.81157 to 31.30051, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 31.30051 to 28.35320, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 28.35320\n",
      "\n",
      "Epoch 00035: val_mae improved from 28.35320 to 25.89669, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 25.89669\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 25.89669\n",
      "\n",
      "Epoch 00038: val_mae improved from 25.89669 to 23.11143, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 23.11143\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 23.11143\n",
      "\n",
      "Epoch 00041: val_mae improved from 23.11143 to 21.32881, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00042: val_mae improved from 21.32881 to 21.17106, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00043: val_mae improved from 21.17106 to 16.30439, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00044: val_mae improved from 16.30439 to 14.07640, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 14.07640\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 14.07640\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 14.07640\n",
      "\n",
      "Epoch 00048: val_mae improved from 14.07640 to 11.95883, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00049: val_mae improved from 11.95883 to 10.82050, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00050: val_mae improved from 10.82050 to 10.26792, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00051: val_mae improved from 10.26792 to 9.39932, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00052: val_mae improved from 9.39932 to 9.07785, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00053: val_mae improved from 9.07785 to 8.96696, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 8.96696\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 8.96696\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 8.96696\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 8.96696\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 8.96696\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 8.96696\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 8.96696\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 8.96696\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 8.96696\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 8.96696\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 8.96696\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 8.96696\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 8.96696\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 8.96696\n",
      "\n",
      "Epoch 00068: val_mae improved from 8.96696 to 8.95115, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00069: val_mae improved from 8.95115 to 8.79441, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 8.79441\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 8.79441\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 8.79441\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 8.79441\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 8.79441\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 8.79441\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 8.79441\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 8.79441\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 8.79441\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 8.79441\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 8.79441\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 8.79441\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 8.79441\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 8.79441\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 8.79441\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 8.79441\n",
      "\n",
      "Epoch 00086: val_mae improved from 8.79441 to 7.42963, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 7.42963\n",
      "\n",
      "Epoch 00106: val_mae improved from 7.42963 to 7.37942, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 7.37942\n",
      "\n",
      "Epoch 00108: val_mae improved from 7.37942 to 7.22531, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 7.22531\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 7.22531\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 7.22531\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 7.22531\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 7.22531\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 7.22531\n",
      "\n",
      "Epoch 00115: val_mae improved from 7.22531 to 6.99445, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 6.99445\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 6.99445\n",
      "\n",
      "Epoch 00118: val_mae improved from 6.99445 to 6.67304, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 6.67304\n",
      "\n",
      "Epoch 00138: val_mae improved from 6.67304 to 6.54291, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 6.54291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00157: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 6.54291\n",
      "\n",
      "Epoch 00202: val_mae improved from 6.54291 to 6.35139, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 6.35139\n",
      "\n",
      "Epoch 00300: val_mae improved from 6.35139 to 6.26495, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 6.26495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00312: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 6.26495\n",
      "\n",
      "Epoch 00338: val_mae improved from 6.26495 to 6.21208, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_3/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 6.21208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00470: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 6.21208\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 6.21208\n",
      "\n",
      "Lambda: 1 , Time: 0:03:57\n",
      "Train Error(all epochs): 1.4566679000854492 \n",
      " [49.099, 48.996, 48.92, 48.834, 48.737, 48.623, 48.492, 48.341, 48.162, 47.949, 47.705, 47.42, 47.087, 46.714, 46.285, 45.785, 45.226, 44.601, 43.937, 43.21, 42.426, 41.583, 40.634, 39.613, 38.561, 37.467, 36.297, 35.119, 33.901, 32.654, 31.352, 30.09, 28.782, 27.455, 26.228, 24.899, 23.654, 22.367, 21.07, 19.695, 18.534, 17.322, 16.197, 15.17, 14.135, 13.131, 12.338, 11.517, 10.746, 10.092, 9.52, 9.294, 8.453, 8.157, 7.754, 7.385, 7.087, 6.536, 6.45, 6.577, 5.799, 5.529, 5.361, 5.536, 5.463, 5.353, 5.13, 4.87, 4.623, 4.327, 4.34, 4.25, 4.082, 4.125, 4.084, 4.016, 3.766, 3.538, 3.518, 3.593, 3.642, 3.663, 4.066, 4.476, 3.973, 4.069, 4.297, 3.885, 3.847, 3.894, 3.61, 3.763, 3.504, 3.323, 3.218, 3.213, 3.27, 3.124, 3.037, 3.395, 3.343, 3.387, 3.584, 3.555, 3.298, 3.218, 3.106, 3.425, 3.635, 3.33, 3.366, 3.144, 3.397, 3.035, 3.129, 3.125, 3.093, 3.273, 3.232, 3.15, 3.814, 4.036, 3.546, 3.496, 3.277, 2.957, 2.905, 2.893, 2.947, 3.161, 2.951, 3.018, 2.885, 2.98, 3.081, 3.21, 3.171, 3.47, 3.343, 3.192, 3.032, 3.222, 3.568, 3.343, 3.329, 3.275, 3.097, 3.178, 3.204, 3.128, 2.969, 2.881, 2.67, 2.868, 2.701, 2.845, 2.988, 3.094, 2.937, 2.756, 2.743, 2.817, 2.892, 2.808, 2.572, 2.379, 2.513, 2.659, 3.064, 2.961, 3.267, 3.131, 3.386, 3.363, 3.194, 2.901, 2.863, 2.71, 2.974, 2.983, 2.6, 2.7, 2.528, 2.689, 2.571, 2.574, 2.594, 2.663, 2.714, 2.699, 2.716, 2.713, 2.695, 2.853, 2.775, 2.991, 2.638, 2.747, 2.751, 3.002, 2.936, 2.875, 2.844, 2.672, 2.535, 2.344, 2.367, 2.425, 2.379, 2.871, 3.212, 3.309, 3.022, 2.858, 2.879, 2.594, 2.619, 2.404, 2.539, 2.325, 2.475, 2.434, 2.356, 2.645, 2.725, 2.911, 3.243, 3.113, 2.887, 2.939, 2.766, 2.667, 2.782, 2.847, 2.597, 2.595, 2.349, 2.22, 2.179, 2.382, 2.512, 2.588, 2.739, 2.877, 2.894, 2.431, 2.482, 2.684, 2.575, 2.959, 3.011, 2.789, 3.009, 2.706, 2.699, 2.674, 2.66, 2.648, 2.772, 2.874, 2.528, 2.447, 2.167, 2.138, 2.204, 2.357, 2.505, 2.565, 2.741, 2.846, 2.654, 2.536, 2.501, 2.339, 2.261, 2.245, 2.361, 2.385, 2.25, 2.159, 2.09, 2.333, 2.734, 2.784, 2.769, 2.432, 2.622, 2.522, 2.606, 2.637, 2.894, 2.778, 2.787, 2.551, 2.48, 2.211, 2.226, 2.655, 2.559, 2.739, 2.64, 2.545, 2.451, 2.478, 2.407, 2.275, 2.334, 2.665, 2.654, 2.398, 2.466, 2.269, 2.312, 2.197, 2.176, 2.016, 1.957, 2.017, 1.844, 1.982, 2.121, 2.303, 2.465, 2.486, 2.572, 2.886, 2.818, 2.783, 2.591, 2.497, 2.963, 3.349, 3.329, 3.231, 2.847, 2.954, 2.648, 2.883, 3.143, 2.957, 2.668, 2.287, 2.007, 1.993, 1.91, 2.13, 2.136, 2.291, 2.208, 2.204, 2.225, 2.052, 2.131, 2.062, 2.258, 2.52, 2.446, 2.735, 2.659, 2.52, 2.334, 2.357, 2.245, 1.875, 1.875, 1.876, 2.084, 2.546, 2.656, 2.54, 2.812, 2.648, 2.299, 2.163, 2.113, 2.262, 2.408, 2.198, 2.4, 2.294, 2.103, 2.018, 2.002, 2.01, 1.994, 2.093, 2.118, 2.196, 2.444, 2.459, 2.412, 2.265, 2.394, 2.369, 2.464, 2.46, 2.355, 2.226, 2.038, 2.185, 2.172, 2.407, 2.557, 2.634, 2.549, 2.537, 2.63, 2.721, 2.653, 2.63, 2.365, 1.945, 1.936, 2.038, 2.099, 2.091, 2.131, 2.097, 2.046, 1.978, 1.888, 1.738, 2.014, 2.253, 2.094, 2.132, 2.131, 2.05, 2.283, 2.212, 2.227, 2.353, 2.554, 2.498, 2.549, 2.798, 2.726, 2.567, 2.41, 2.344, 2.067, 2.167, 2.198, 2.075, 2.342, 2.682, 2.657, 2.486, 2.184, 1.962, 1.864, 1.911, 2.262, 2.378, 2.667, 2.357, 2.494, 2.199, 2.16, 2.354, 2.24, 2.429, 2.133, 1.989, 1.814, 1.629, 1.552, 1.457, 1.528, 1.564, 1.785, 1.908, 1.986, 1.963, 2.382, 2.605, 2.508, 2.33, 2.297, 2.291, 2.205, 2.351, 2.427, 2.236, 2.131, 2.247, 2.068, 2.135, 1.926, 1.816, 1.783, 1.795, 1.745, 1.9, 1.92, 2.179, 2.032, 1.947, 2.121, 2.133]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008, 0.002, 0.038, 0.03, 0.066, 0.077, 0.148, 0.169, 0.211, 0.226, 0.374, 0.57, 0.441, 0.602, 0.596, 0.799, 0.824, 0.694, 0.95, 1.101, 0.936, 1.064, 1.078, 1.172, 1.366, 1.387, 1.49, 1.248, 1.281, 1.188, 1.275, 1.315, 1.262, 1.28, 1.397, 1.27, 1.208, 1.12, 1.075, 1.237, 1.149, 1.274, 1.467, 1.871, 1.322, 1.496, 1.495, 1.32, 1.358, 1.387, 1.37, 1.393, 1.276, 1.11, 1.147, 1.141, 1.235, 1.078, 1.017, 1.334, 1.24, 1.187, 1.332, 1.373, 1.172, 1.126, 1.146, 1.154, 1.433, 1.187, 1.342, 1.097, 1.415, 1.059, 1.259, 1.113, 1.183, 1.199, 1.265, 1.117, 1.343, 1.647, 1.358, 1.231, 1.211, 1.068, 1.116, 1.014, 1.06, 1.332, 1.029, 1.077, 1.113, 1.154, 1.064, 1.335, 1.201, 1.265, 1.294, 1.095, 1.115, 1.194, 1.425, 1.26, 1.217, 1.326, 1.136, 1.197, 1.196, 1.137, 1.093, 1.118, 0.983, 1.021, 1.044, 1.076, 1.044, 1.279, 1.083, 1.106, 0.977, 1.057, 1.125, 1.083, 0.98, 0.816, 0.936, 0.981, 1.238, 1.092, 1.223, 1.266, 1.422, 1.204, 1.238, 1.054, 1.055, 1.036, 1.144, 1.171, 0.913, 1.09, 0.928, 1.004, 0.934, 0.996, 0.956, 1.01, 1.027, 1.014, 1.073, 1.003, 0.928, 1.17, 1.028, 1.222, 1.004, 1.089, 1.079, 1.155, 1.165, 1.063, 1.035, 1.033, 0.94, 0.862, 0.836, 0.95, 0.864, 1.055, 1.28, 1.312, 1.157, 1.054, 1.151, 1.007, 0.934, 0.921, 0.97, 0.853, 0.891, 0.947, 0.898, 0.989, 1.216, 1.11, 1.344, 1.236, 0.966, 1.141, 1.106, 1.056, 1.043, 1.207, 0.953, 0.965, 0.917, 0.804, 0.841, 0.927, 0.958, 1.004, 1.101, 1.172, 1.117, 0.927, 0.94, 1.045, 1.003, 1.155, 1.287, 1.073, 1.204, 1.047, 1.039, 0.997, 1.079, 1.023, 1.061, 1.221, 0.967, 0.976, 0.757, 0.841, 0.8, 0.904, 1.01, 1.031, 1.092, 1.187, 1.004, 1.028, 0.95, 0.895, 0.85, 0.877, 0.928, 0.988, 0.829, 0.832, 0.735, 0.941, 1.166, 1.087, 1.17, 0.865, 1.098, 0.982, 1.105, 1.019, 1.213, 1.084, 1.101, 0.979, 1.011, 0.788, 0.872, 1.183, 0.969, 1.114, 1.122, 0.968, 0.99, 1.055, 0.918, 0.94, 0.971, 1.023, 1.098, 0.934, 0.943, 0.909, 0.952, 0.861, 0.836, 0.7, 0.751, 0.791, 0.736, 0.767, 0.802, 0.937, 0.94, 1.025, 1.115, 1.18, 1.161, 1.178, 1.004, 0.95, 1.194, 1.433, 1.341, 1.485, 1.19, 1.111, 1.084, 1.195, 1.282, 1.28, 1.136, 0.842, 0.729, 0.779, 0.76, 0.916, 0.827, 0.978, 0.828, 0.914, 0.886, 0.823, 0.844, 0.758, 0.878, 1.073, 0.907, 1.165, 1.059, 0.999, 0.992, 0.98, 0.879, 0.662, 0.776, 0.742, 0.881, 1.02, 1.187, 1.036, 1.176, 0.967, 1.033, 0.826, 0.852, 0.907, 1.104, 0.877, 0.919, 0.981, 0.867, 0.783, 0.756, 0.841, 0.787, 0.896, 0.808, 0.847, 1.07, 0.995, 1.03, 0.903, 0.937, 0.967, 1.058, 1.058, 0.949, 0.881, 0.885, 0.857, 0.966, 0.975, 1.047, 1.107, 1.121, 0.958, 1.102, 1.176, 1.047, 1.08, 0.984, 0.79, 0.784, 0.826, 0.906, 0.848, 0.837, 0.931, 0.851, 0.746, 0.723, 0.733, 0.895, 0.951, 0.774, 0.945, 0.818, 0.905, 0.943, 0.989, 0.924, 0.95, 1.087, 1.054, 1.074, 1.268, 1.153, 1.042, 1.007, 1.045, 0.834, 0.885, 0.934, 0.84, 1.05, 1.108, 1.112, 0.952, 0.95, 0.795, 0.793, 0.838, 0.889, 0.994, 1.197, 0.904, 1.125, 0.93, 0.907, 0.994, 0.967, 0.958, 0.899, 0.788, 0.808, 0.625, 0.626, 0.585, 0.639, 0.648, 0.755, 0.781, 0.869, 0.811, 1.042, 1.176, 0.981, 0.976, 1.003, 0.919, 0.956, 1.024, 1.093, 0.906, 0.902, 0.961, 0.814, 0.89, 0.9, 0.726, 0.733, 0.729, 0.79, 0.807, 0.754, 1.026, 0.81, 0.814, 0.911, 0.852]\n",
      "Val Error(all epochs): 6.212077617645264 \n",
      " [48.872, 48.791, 48.717, 48.61, 48.517, 48.368, 48.21, 48.073, 47.861, 47.642, 47.316, 47.043, 46.915, 46.442, 45.787, 45.603, 45.012, 44.599, 43.579, 43.475, 42.919, 41.627, 40.644, 40.203, 39.191, 36.929, 36.48, 36.63, 35.367, 35.005, 32.812, 31.301, 28.353, 29.829, 25.897, 27.165, 26.452, 23.111, 23.467, 24.943, 21.329, 21.171, 16.304, 14.076, 15.122, 14.286, 14.808, 11.959, 10.821, 10.268, 9.399, 9.078, 8.967, 9.082, 10.366, 9.916, 9.976, 9.713, 10.726, 11.668, 9.309, 9.192, 9.085, 9.969, 9.249, 9.952, 9.358, 8.951, 8.794, 9.17, 9.362, 9.453, 10.264, 10.362, 9.97, 9.432, 10.014, 10.746, 10.068, 11.159, 11.399, 10.777, 9.201, 9.79, 9.452, 7.43, 8.604, 8.591, 7.593, 8.566, 8.158, 7.718, 8.801, 7.994, 8.23, 8.739, 8.16, 9.279, 9.302, 7.738, 9.032, 8.733, 7.5, 8.41, 7.681, 7.379, 9.054, 7.225, 8.531, 8.07, 8.126, 8.039, 7.731, 7.875, 6.994, 8.42, 7.85, 6.673, 7.532, 9.09, 6.789, 6.76, 6.832, 6.986, 6.712, 6.781, 7.422, 8.13, 6.943, 7.491, 7.782, 7.184, 7.662, 7.91, 7.525, 6.821, 8.42, 6.543, 7.612, 7.702, 8.393, 6.988, 7.666, 8.063, 7.142, 6.889, 6.778, 6.726, 6.737, 8.261, 6.825, 7.723, 7.97, 7.661, 7.725, 8.541, 8.847, 6.957, 8.494, 7.829, 8.584, 8.627, 8.286, 8.201, 8.57, 8.487, 8.557, 8.491, 7.44, 8.252, 7.017, 7.524, 7.345, 7.304, 8.152, 8.671, 8.869, 8.629, 8.581, 8.643, 8.854, 8.516, 8.543, 8.189, 7.588, 7.51, 8.288, 8.061, 7.713, 7.86, 7.801, 8.135, 7.463, 8.029, 7.501, 6.999, 7.777, 8.041, 7.712, 7.307, 7.5, 6.351, 7.776, 6.895, 7.48, 7.835, 7.738, 7.507, 7.836, 8.262, 6.954, 7.467, 6.72, 6.926, 8.127, 7.08, 7.638, 7.156, 7.19, 7.512, 6.8, 7.962, 7.708, 7.628, 7.017, 9.048, 6.557, 7.872, 7.454, 6.893, 7.535, 7.046, 7.114, 7.273, 7.235, 6.912, 7.276, 7.478, 7.196, 6.941, 7.814, 7.767, 8.397, 8.163, 7.544, 7.447, 6.729, 7.627, 7.121, 6.773, 7.547, 7.651, 7.653, 8.181, 7.718, 7.425, 7.185, 7.615, 7.535, 8.182, 8.048, 8.031, 7.316, 7.183, 7.726, 7.383, 7.521, 7.531, 7.23, 7.457, 7.544, 7.58, 7.663, 7.136, 7.369, 7.139, 7.559, 7.002, 7.448, 7.219, 7.949, 6.522, 7.636, 7.287, 7.141, 6.849, 6.81, 6.84, 7.321, 7.294, 7.172, 7.969, 8.282, 7.949, 7.29, 8.087, 6.988, 7.96, 7.432, 6.265, 7.43, 7.452, 6.831, 8.919, 8.047, 8.389, 8.802, 6.553, 7.498, 7.973, 7.57, 7.2, 6.847, 6.951, 6.958, 6.982, 7.416, 7.174, 7.238, 7.148, 7.301, 6.958, 7.209, 6.928, 6.917, 7.737, 7.226, 7.27, 7.222, 7.847, 8.274, 7.127, 7.37, 7.262, 7.029, 6.623, 7.038, 6.212, 8.815, 7.704, 8.048, 9.786, 9.243, 9.652, 7.281, 8.336, 7.586, 7.021, 7.647, 7.305, 7.323, 6.917, 7.572, 7.235, 6.793, 8.133, 7.456, 6.526, 7.681, 6.77, 7.767, 7.173, 7.477, 7.336, 7.227, 7.273, 8.353, 7.101, 8.195, 6.785, 6.795, 9.06, 8.354, 8.042, 7.94, 7.115, 7.268, 7.553, 7.687, 7.185, 7.488, 6.787, 7.492, 7.101, 7.147, 7.337, 7.308, 7.376, 7.74, 6.855, 7.126, 7.596, 7.944, 7.217, 7.589, 8.442, 7.768, 7.493, 8.043, 7.39, 7.589, 7.171, 7.302, 7.238, 7.591, 7.817, 6.823, 7.732, 6.603, 7.204, 6.683, 7.12, 6.587, 7.032, 6.675, 7.519, 6.717, 6.696, 7.063, 7.215, 7.075, 7.431, 7.353, 8.045, 6.932, 8.011, 7.006, 6.882, 7.557, 6.788, 7.086, 7.276, 7.502, 7.279, 8.497, 7.533, 7.116, 7.704, 6.866, 6.853, 7.077, 7.106, 7.016, 6.279, 7.759, 6.68, 7.125, 7.305, 7.287, 6.565, 6.277, 6.793, 6.944, 6.71, 6.776, 7.073, 8.589, 9.687, 9.727, 9.495, 10.351, 8.911, 7.68, 8.618, 9.108, 8.765, 8.67, 8.048, 7.878, 7.695, 7.955, 8.209, 7.208, 8.093, 6.423, 8.12, 7.195, 6.939, 6.883, 6.48, 7.317, 6.892, 7.371, 7.101, 7.172, 6.97, 7.098, 6.92, 6.968, 7.034, 6.985, 7.122, 7.503, 7.458, 7.425, 7.598, 8.145, 8.14, 7.752, 7.584]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.043, 0.013, 0.21, 0.42, 0.289, 0.344, 0.293, 0.734, 1.065, 1.397, 1.783, 2.456, 2.203, 2.086, 5.494, 5.766, 5.629, 5.413, 6.344, 7.82, 3.998, 3.667, 3.148, 2.704, 3.297, 2.633, 4.11, 3.503, 2.631, 2.884, 2.786, 2.0, 1.485, 1.133, 2.018, 1.794, 1.392, 1.12, 1.22, 0.85, 0.778, 0.836, 1.836, 1.46, 2.366, 1.948, 2.423, 1.984, 2.498, 2.778, 1.515, 2.35, 1.534, 1.694, 2.084, 1.665, 1.828, 1.716, 1.418, 2.847, 1.533, 1.877, 2.613, 2.844, 2.164, 2.115, 1.271, 2.191, 1.456, 1.852, 2.194, 2.973, 2.569, 2.248, 1.937, 1.865, 1.722, 2.141, 2.628, 1.458, 3.867, 2.881, 2.777, 2.491, 2.366, 2.77, 1.717, 1.984, 2.206, 2.669, 1.614, 1.908, 1.747, 2.095, 1.588, 2.136, 1.242, 2.754, 1.699, 1.581, 1.646, 2.158, 2.483, 2.825, 2.826, 3.165, 3.658, 1.93, 2.508, 1.704, 2.203, 2.068, 1.409, 1.911, 1.274, 1.704, 0.71, 2.171, 1.393, 1.377, 1.018, 1.37, 1.218, 1.551, 1.238, 1.018, 1.339, 0.944, 2.483, 1.361, 2.34, 1.936, 1.695, 2.327, 1.39, 1.297, 1.103, 1.215, 1.468, 1.271, 1.541, 1.565, 1.199, 1.309, 1.823, 1.434, 1.536, 1.641, 1.849, 1.829, 2.791, 1.735, 2.469, 2.149, 2.673, 1.961, 2.181, 1.937, 1.463, 2.574, 2.197, 2.214, 1.755, 2.568, 1.887, 1.865, 1.682, 2.052, 2.098, 1.941, 2.213, 4.742, 2.257, 1.777, 1.724, 1.678, 2.05, 1.952, 2.452, 1.947, 2.656, 1.634, 2.044, 1.459, 2.099, 1.328, 2.295, 1.914, 1.78, 2.932, 2.939, 1.859, 3.371, 1.888, 2.601, 2.467, 2.92, 2.537, 3.179, 3.27, 2.697, 3.55, 4.698, 3.36, 3.109, 2.495, 2.371, 2.725, 1.97, 2.612, 2.785, 3.241, 3.858, 4.906, 4.176, 4.061, 3.048, 4.041, 3.679, 4.622, 3.694, 4.157, 2.728, 2.578, 2.02, 2.661, 1.821, 2.935, 1.916, 2.851, 3.359, 3.698, 2.329, 2.471, 2.096, 2.439, 2.052, 2.752, 2.623, 1.727, 2.445, 2.324, 1.984, 2.796, 2.829, 2.868, 3.284, 2.862, 2.64, 2.789, 1.506, 2.052, 1.21, 2.307, 2.077, 1.962, 2.651, 2.057, 2.415, 3.289, 3.169, 2.746, 2.435, 1.756, 1.55, 1.806, 1.081, 2.212, 2.53, 1.969, 1.688, 3.198, 2.796, 3.134, 2.059, 2.713, 2.361, 2.61, 2.457, 2.691, 2.926, 2.151, 3.21, 2.689, 2.135, 2.763, 3.163, 3.868, 2.786, 1.681, 2.554, 2.986, 3.373, 2.09, 2.44, 3.458, 3.453, 2.248, 2.651, 1.819, 2.223, 1.805, 1.563, 1.373, 1.997, 1.964, 1.563, 2.228, 1.809, 2.074, 1.998, 2.599, 1.41, 2.384, 3.035, 3.028, 3.877, 2.73, 4.438, 2.674, 4.222, 2.4, 2.897, 2.712, 2.585, 2.642, 3.152, 2.868, 4.86, 2.807, 1.934, 1.383, 1.284, 1.278, 2.019, 2.432, 2.352, 2.331, 1.544, 3.632, 1.554, 2.452, 2.102, 2.578, 3.068, 2.195, 2.912, 3.802, 2.546, 2.825, 3.448, 2.22, 1.914, 2.98, 3.654, 5.175, 3.533, 4.642, 4.447, 3.618, 3.789, 2.805, 2.463, 2.585, 1.788, 2.815, 1.772, 4.402, 2.904, 2.092, 2.655, 3.323, 2.677, 3.011, 2.402, 2.044, 3.871, 3.462, 2.903, 1.812, 2.118, 2.753, 1.639, 2.255, 2.438, 1.8, 3.32, 2.709, 2.98, 2.464, 3.093, 2.05, 2.091, 2.198, 2.654, 1.616, 2.823, 4.527, 2.338, 3.171, 3.575, 2.181, 2.941, 2.111, 3.394, 1.639, 2.594, 1.997, 1.495, 2.605, 2.444, 2.764, 3.274, 3.207, 3.187, 2.265, 1.772, 1.635, 1.374, 1.903, 1.1, 1.764, 2.159, 1.565, 1.379, 1.404, 1.467, 1.596, 1.853, 1.891, 1.785, 1.695, 1.773, 2.542, 1.742, 2.286, 1.582, 2.454, 2.22, 2.359, 2.146, 2.61, 2.285, 2.797, 2.297, 2.336, 2.429, 2.838, 3.018, 2.527, 3.304, 2.595, 2.598, 1.902, 2.663, 2.282, 1.843, 2.091, 1.742, 2.01]\n",
      "\n",
      "#Fold: 3 \n",
      "Trainig set size: 420 , Time: 0:11:54 , best_lambda: 1 , min_  , error: 6.212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test starts:  467 , ends:  518\n",
      "1/1 [==============================] - 0s 771us/step - loss: 133.7227 - mse: 88.4634 - mae: 7.4766 - fp_mae: 3.8018\n",
      "average_error:  7.477 , fp_average_error:  3.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:00<00:00, 235.89it/s]\n",
      " 94%|█████████▍| 97/103 [00:00<00:00, 231.48it/s]]\n",
      "100%|██████████| 103/103 [00:00<00:00, 219.29it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 235.38it/s]\n",
      "100%|██████████| 107/107 [00:00<00:00, 220.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Fold: 4 , Training Size: 420 , Validation size: 47 , Test Size 52\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 49.39677, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 49.39677 to 49.25984, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 49.25984 to 49.08397, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 49.08397 to 48.86666, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 48.86666 to 48.63976, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 48.63976 to 48.39996, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 48.39996 to 48.09639, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 48.09639 to 47.61882, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 47.61882 to 47.07682, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 47.07682 to 46.40542, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 46.40542 to 45.74617, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 45.74617 to 44.48745, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 44.48745 to 42.78102, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 42.78102 to 42.34535, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 42.34535 to 40.50965, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 40.50965 to 38.54762, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 38.54762\n",
      "\n",
      "Epoch 00018: val_mae improved from 38.54762 to 37.44897, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 37.44897 to 35.05226, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 35.05226\n",
      "\n",
      "Epoch 00021: val_mae improved from 35.05226 to 34.68485, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 34.68485 to 32.96764, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 32.96764\n",
      "\n",
      "Epoch 00024: val_mae improved from 32.96764 to 28.29728, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 28.29728 to 26.96163, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 26.96163 to 24.17728, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 24.17728 to 19.43993, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 19.43993\n",
      "\n",
      "Epoch 00029: val_mae improved from 19.43993 to 16.78842, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 16.78842\n",
      "\n",
      "Epoch 00031: val_mae improved from 16.78842 to 14.81183, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 14.81183\n",
      "\n",
      "Epoch 00033: val_mae improved from 14.81183 to 13.32239, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00034: val_mae improved from 13.32239 to 12.45234, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00035: val_mae improved from 12.45234 to 12.16876, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 12.16876\n",
      "\n",
      "Epoch 00037: val_mae improved from 12.16876 to 11.72380, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 11.72380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00063: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 11.72380\n",
      "\n",
      "Epoch 00123: val_mae improved from 11.72380 to 11.43747, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 11.43747\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 11.43747\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 11.43747\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 11.43747\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 11.43747\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 11.43747\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 11.43747\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 11.43747\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 11.43747\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 11.43747\n",
      "\n",
      "Epoch 00134: val_mae improved from 11.43747 to 11.19875, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 11.19875\n",
      "\n",
      "Epoch 00136: val_mae improved from 11.19875 to 10.75765, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 10.75765\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 10.75765\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 10.75765\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 10.75765\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 10.75765\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 10.75765\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 10.75765\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 10.75765\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 10.75765\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 10.75765\n",
      "\n",
      "Epoch 00147: val_mae improved from 10.75765 to 10.23456, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 10.23456\n",
      "\n",
      "Epoch 00149: val_mae improved from 10.23456 to 10.21420, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 10.21420\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 10.21420\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 10.21420\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 10.21420\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 10.21420\n",
      "\n",
      "Epoch 00155: val_mae improved from 10.21420 to 10.18237, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 10.18237\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 10.18237\n",
      "\n",
      "Epoch 00158: val_mae improved from 10.18237 to 10.11969, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 10.11969\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 10.11969\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 10.11969\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 10.11969\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 10.11969\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 10.11969\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 10.11969\n",
      "\n",
      "Epoch 00166: val_mae improved from 10.11969 to 10.07512, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 10.07512\n",
      "\n",
      "Epoch 00168: val_mae improved from 10.07512 to 9.90922, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 9.90922\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 9.90922\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 9.90922\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 9.90922\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 9.90922\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 9.90922\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 9.90922\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 9.90922\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 9.90922\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 9.90922\n",
      "\n",
      "Epoch 00179: val_mae improved from 9.90922 to 9.85649, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 9.85649\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 9.85649\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 9.85649\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 9.85649\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 9.85649\n",
      "\n",
      "Epoch 00185: val_mae improved from 9.85649 to 9.71111, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 9.71111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00187: val_mae improved from 9.71111 to 9.70381, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 9.70381\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 9.70381\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 9.70381\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 9.70381\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 9.70381\n",
      "\n",
      "Epoch 00193: val_mae improved from 9.70381 to 9.64793, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 9.64793\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 9.64793\n",
      "\n",
      "Epoch 00196: val_mae improved from 9.64793 to 9.55520, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00197: val_mae improved from 9.55520 to 9.40583, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 9.40583\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 9.40583\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 9.40583\n",
      "\n",
      "Epoch 00201: val_mae improved from 9.40583 to 9.31413, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 9.31413\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 9.31413\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 9.31413\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 9.31413\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 9.31413\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 9.31413\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 9.31413\n",
      "\n",
      "Epoch 00209: val_mae improved from 9.31413 to 9.15404, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 9.15404\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 9.15404\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 9.15404\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 9.15404\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 9.15404\n",
      "\n",
      "Epoch 00215: val_mae improved from 9.15404 to 9.03299, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 9.03299\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 9.03299\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 9.03299\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 9.03299\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 9.03299\n",
      "\n",
      "Epoch 00221: val_mae improved from 9.03299 to 8.89617, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 8.89617\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 8.89617\n",
      "\n",
      "Epoch 00224: val_mae improved from 8.89617 to 8.83265, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 8.83265\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 8.83265\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 8.83265\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 8.83265\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 8.83265\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 8.83265\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 8.83265\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 8.83265\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 8.83265\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 8.83265\n",
      "\n",
      "Epoch 00235: val_mae improved from 8.83265 to 8.82348, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 8.82348\n",
      "\n",
      "Epoch 00237: val_mae improved from 8.82348 to 8.62123, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 8.62123\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 8.62123\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 8.62123\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 8.62123\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 8.62123\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 8.62123\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 8.62123\n",
      "\n",
      "Epoch 00245: val_mae improved from 8.62123 to 8.61841, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 8.61841\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 8.61841\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 8.61841\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 8.61841\n",
      "\n",
      "Epoch 00250: val_mae improved from 8.61841 to 8.46859, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 8.46859\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 8.46859\n",
      "\n",
      "Epoch 00253: val_mae improved from 8.46859 to 8.39594, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 8.39594\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 8.39594\n",
      "\n",
      "Epoch 00256: val_mae improved from 8.39594 to 8.33624, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 8.33624\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 8.33624\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 8.33624\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 8.33624\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 8.33624\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 8.33624\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 8.33624\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 8.33624\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 8.33624\n",
      "\n",
      "Epoch 00266: val_mae improved from 8.33624 to 8.20011, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 8.20011\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 8.20011\n",
      "\n",
      "Epoch 00269: val_mae improved from 8.20011 to 8.17856, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 8.17856\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 8.17856\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 8.17856\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 8.17856\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 8.17856\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 8.17856\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 8.17856\n",
      "\n",
      "Epoch 00277: val_mae improved from 8.17856 to 8.16243, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 8.16243\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 8.16243\n",
      "\n",
      "Epoch 00280: val_mae improved from 8.16243 to 8.12323, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 8.12323\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 8.12323\n",
      "\n",
      "Epoch 00283: val_mae improved from 8.12323 to 8.11666, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00284: val_mae did not improve from 8.11666\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 8.11666\n",
      "\n",
      "Epoch 00286: val_mae improved from 8.11666 to 8.04155, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 8.04155\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 8.04155\n",
      "\n",
      "Epoch 00289: val_mae improved from 8.04155 to 8.03775, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00290: val_mae improved from 8.03775 to 7.88238, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 7.88238\n",
      "\n",
      "Epoch 00292: val_mae improved from 7.88238 to 7.79035, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 7.79035\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 7.79035\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 7.79035\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 7.79035\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 7.79035\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 7.79035\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 7.79035\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 7.79035\n",
      "\n",
      "Epoch 00301: val_mae improved from 7.79035 to 7.74618, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 7.74618\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 7.74618\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 7.74618\n",
      "\n",
      "Epoch 00305: val_mae improved from 7.74618 to 7.65135, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 7.65135\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 7.65135\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 7.65135\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 7.65135\n",
      "\n",
      "Epoch 00310: val_mae improved from 7.65135 to 7.63429, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 7.63429\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 7.63429\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 7.63429\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 7.63429\n",
      "\n",
      "Epoch 00315: val_mae improved from 7.63429 to 7.60166, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 7.60166\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 7.60166\n",
      "\n",
      "Epoch 00318: val_mae improved from 7.60166 to 7.57222, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 7.57222\n",
      "\n",
      "Epoch 00320: val_mae improved from 7.57222 to 7.46288, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 7.46288\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 7.46288\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 7.46288\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 7.46288\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 7.46288\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 7.46288\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 7.46288\n",
      "\n",
      "Epoch 00328: val_mae improved from 7.46288 to 7.38564, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 7.38564\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 7.38564\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 7.38564\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 7.38564\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 7.38564\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 7.38564\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 7.38564\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 7.38564\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 7.38564\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 7.38564\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 7.38564\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 7.38564\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 7.38564\n",
      "\n",
      "Epoch 00342: val_mae improved from 7.38564 to 7.36192, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 7.36192\n",
      "\n",
      "Epoch 00344: val_mae improved from 7.36192 to 7.32489, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 7.32489\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 7.32489\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 7.32489\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 7.32489\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 7.32489\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 7.32489\n",
      "\n",
      "Epoch 00351: val_mae improved from 7.32489 to 7.18215, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 7.18215\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 7.18215\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 7.18215\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 7.18215\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 7.18215\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 7.18215\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 7.18215\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 7.18215\n",
      "\n",
      "Epoch 00360: val_mae improved from 7.18215 to 7.15204, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 7.15204\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 7.15204\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 7.15204\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 7.15204\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 7.15204\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 7.15204\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 7.15204\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 7.15204\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 7.15204\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 7.15204\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 7.15204\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 7.15204\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 7.15204\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 7.15204\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 7.15204\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 7.15204\n",
      "\n",
      "Epoch 00377: val_mae improved from 7.15204 to 7.12261, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 7.12261\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 7.12261\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 7.12261\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 7.12261\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 7.12261\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 7.12261\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 7.12261\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 7.12261\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 7.12261\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 7.12261\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 7.12261\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 7.12261\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 7.12261\n",
      "\n",
      "Epoch 00391: val_mae improved from 7.12261 to 7.09311, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00392: val_mae did not improve from 7.09311\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 7.09311\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 7.09311\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 7.09311\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 7.09311\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 7.09311\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 7.09311\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 7.09311\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 7.09311\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 7.09311\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 7.09311\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 7.09311\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 7.09311\n",
      "\n",
      "Epoch 00405: val_mae improved from 7.09311 to 7.00674, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 7.00674\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 7.00674\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 7.00674\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 7.00674\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 7.00674\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 7.00674\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 7.00674\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 7.00674\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 7.00674\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 7.00674\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 7.00674\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 7.00674\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 7.00674\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 7.00674\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 7.00674\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 7.00674\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 7.00674\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 7.00674\n",
      "\n",
      "Epoch 00424: val_mae improved from 7.00674 to 6.90871, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 6.90871\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 6.90871\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 6.90871\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 6.90871\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 6.90871\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 6.90871\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 6.90871\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 6.90871\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 6.90871\n",
      "\n",
      "Epoch 00434: val_mae improved from 6.90871 to 6.89381, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 6.89381\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 6.89381\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 6.89381\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 6.89381\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 6.89381\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 6.89381\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 6.89381\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 6.89381\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 6.89381\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 6.89381\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 6.89381\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 6.89381\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 6.89381\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 6.89381\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 6.89381\n",
      "\n",
      "Epoch 00450: val_mae improved from 6.89381 to 6.82746, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 6.82746\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 6.82746\n",
      "\n",
      "Epoch 00453: val_mae improved from 6.82746 to 6.80912, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 6.80912\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 6.80912\n",
      "\n",
      "Epoch 00456: val_mae improved from 6.80912 to 6.77961, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 6.77961\n",
      "\n",
      "Epoch 00477: val_mae improved from 6.77961 to 6.76820, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 6.76820\n",
      "\n",
      "Epoch 00499: val_mae improved from 6.76820 to 6.65163, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 6.65163\n",
      "\n",
      "Lambda: 0.01 , Time: 0:03:57\n",
      "Train Error(all epochs): 1.6326017379760742 \n",
      " [49.223, 49.099, 49.005, 48.902, 48.789, 48.663, 48.518, 48.347, 48.15, 47.917, 47.657, 47.348, 46.985, 46.586, 46.134, 45.616, 45.042, 44.383, 43.686, 42.879, 41.986, 41.053, 40.028, 38.94, 37.771, 36.537, 35.297, 34.009, 32.712, 31.346, 29.995, 28.602, 27.245, 25.865, 24.542, 23.209, 21.992, 20.763, 19.753, 18.559, 17.589, 16.89, 16.137, 15.549, 15.21, 14.597, 14.32, 14.463, 14.061, 14.049, 13.504, 13.333, 13.042, 12.858, 12.729, 12.552, 12.527, 12.363, 12.36, 12.34, 12.33, 12.297, 12.274, 12.259, 12.22, 12.281, 12.141, 12.114, 11.981, 12.008, 11.818, 11.738, 11.657, 11.477, 11.45, 11.366, 11.363, 11.451, 11.327, 11.249, 11.231, 11.165, 11.172, 11.011, 10.892, 10.774, 10.7, 10.577, 10.55, 10.523, 10.572, 10.632, 10.662, 10.659, 10.634, 10.641, 10.556, 10.527, 10.403, 10.468, 10.531, 10.235, 10.273, 10.381, 10.457, 10.325, 10.352, 10.176, 10.094, 9.804, 9.43, 9.484, 9.343, 9.199, 9.135, 8.93, 8.944, 8.955, 8.887, 8.98, 8.865, 8.851, 8.985, 8.917, 9.005, 9.089, 8.987, 9.131, 8.989, 8.83, 8.731, 8.509, 8.449, 8.242, 8.219, 8.192, 8.102, 8.098, 8.063, 8.002, 8.073, 8.098, 8.297, 8.492, 8.299, 8.204, 7.971, 7.736, 7.706, 7.647, 7.698, 7.822, 7.72, 7.77, 7.761, 7.724, 7.734, 7.627, 7.522, 7.469, 7.372, 7.402, 7.529, 7.439, 7.393, 7.239, 7.164, 7.231, 7.321, 7.312, 7.454, 7.262, 7.293, 7.3, 7.245, 7.187, 6.913, 6.796, 6.752, 6.77, 6.748, 6.823, 6.639, 6.579, 6.521, 6.413, 6.458, 6.54, 6.665, 6.819, 6.839, 6.8, 6.882, 6.786, 6.659, 6.604, 6.473, 6.339, 6.17, 6.121, 6.139, 6.036, 6.076, 6.113, 6.131, 6.22, 6.051, 5.975, 6.005, 6.034, 6.082, 5.99, 5.974, 6.066, 5.966, 5.929, 5.93, 5.9, 5.942, 5.982, 5.975, 5.78, 5.698, 5.555, 5.477, 5.481, 5.473, 5.426, 5.465, 5.39, 5.409, 5.387, 5.392, 5.424, 5.364, 5.303, 5.251, 5.206, 5.175, 5.062, 5.049, 5.066, 5.145, 5.257, 5.205, 5.081, 4.946, 4.856, 4.825, 4.839, 4.85, 4.828, 4.881, 5.07, 5.18, 5.07, 5.065, 5.045, 4.88, 4.901, 4.843, 4.797, 4.861, 4.696, 4.567, 4.514, 4.453, 4.463, 4.519, 4.648, 4.783, 4.649, 4.535, 4.382, 4.32, 4.332, 4.36, 4.317, 4.35, 4.294, 4.264, 4.369, 4.393, 4.426, 4.372, 4.307, 4.119, 4.112, 4.143, 4.206, 4.113, 4.053, 3.999, 4.061, 4.05, 4.131, 4.153, 4.08, 4.117, 4.263, 4.161, 4.137, 4.006, 3.974, 4.107, 4.13, 4.101, 4.05, 3.982, 3.853, 3.818, 3.759, 3.693, 3.619, 3.56, 3.643, 3.813, 3.831, 3.769, 3.632, 3.578, 3.48, 3.332, 3.32, 3.294, 3.396, 3.387, 3.378, 3.364, 3.306, 3.24, 3.353, 3.514, 3.556, 3.494, 3.575, 3.443, 3.327, 3.323, 3.271, 3.228, 3.114, 3.125, 3.157, 3.18, 3.184, 3.19, 3.248, 3.318, 3.325, 3.274, 3.173, 3.141, 3.131, 3.174, 3.21, 3.116, 3.077, 3.112, 3.143, 3.109, 3.147, 3.034, 2.933, 2.893, 2.827, 2.823, 2.897, 2.876, 2.897, 2.826, 2.848, 3.14, 3.222, 3.101, 3.156, 2.955, 2.891, 2.928, 2.884, 2.854, 2.843, 2.771, 2.725, 2.727, 2.79, 2.75, 2.649, 2.591, 2.564, 2.415, 2.423, 2.468, 2.468, 2.575, 2.62, 2.543, 2.484, 2.497, 2.651, 2.701, 2.696, 2.724, 2.641, 2.552, 2.494, 2.362, 2.417, 2.417, 2.516, 2.448, 2.426, 2.469, 2.39, 2.367, 2.254, 2.334, 2.278, 2.294, 2.237, 2.228, 2.245, 2.189, 2.204, 2.149, 2.08, 2.003, 2.025, 1.966, 1.988, 2.009, 2.012, 1.94, 1.915, 1.857, 1.791, 1.728, 1.728, 1.776, 1.811, 1.925, 1.98, 2.094, 2.039, 2.083, 2.035, 2.045, 2.143, 2.065, 2.131, 2.1, 2.086, 2.165, 2.256, 2.258, 2.124, 2.075, 2.059, 1.975, 1.946, 1.987, 2.036, 2.018, 1.926, 1.964, 1.908, 1.783, 1.721, 1.689, 1.687, 1.681, 1.686, 1.752, 1.704, 1.729, 1.677, 1.728, 1.715, 1.806, 1.839, 1.885, 1.868, 1.957, 1.911, 1.818, 1.797, 1.731, 1.726, 1.647, 1.68, 1.662, 1.656, 1.633, 1.675, 1.701, 1.775, 1.877, 1.808, 1.772, 1.739]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.012, 0.011, 0.08, 0.04, 0.081, 0.185, 0.241, 0.349, 0.505, 0.562, 0.625, 0.937, 0.889, 1.048, 0.977, 1.017, 0.954, 0.969, 0.961, 0.963, 0.992, 0.978, 0.993, 1.065, 1.048, 1.129, 1.103, 1.183, 1.157, 1.232, 1.155, 1.186, 1.087, 1.182, 1.021, 1.115, 1.013, 1.003, 0.952, 0.975, 0.959, 1.065, 1.006, 0.979, 0.945, 1.03, 0.942, 1.004, 0.869, 0.878, 0.8, 0.793, 0.806, 0.789, 0.877, 0.871, 1.021, 0.937, 0.995, 0.959, 1.004, 0.971, 0.865, 1.005, 1.025, 0.873, 0.894, 1.046, 1.04, 1.101, 1.12, 1.23, 1.267, 1.151, 0.883, 0.949, 0.848, 0.875, 0.799, 0.732, 0.753, 0.772, 0.775, 0.865, 0.761, 0.822, 0.959, 0.803, 1.007, 1.053, 0.958, 1.045, 0.949, 1.02, 0.815, 0.795, 0.713, 0.704, 0.625, 0.697, 0.607, 0.683, 0.62, 0.657, 0.689, 0.738, 0.836, 1.095, 0.802, 0.893, 0.712, 0.626, 0.586, 0.596, 0.616, 0.778, 0.658, 0.788, 0.692, 0.755, 0.754, 0.676, 0.699, 0.671, 0.61, 0.595, 0.794, 0.619, 0.727, 0.607, 0.593, 0.618, 0.725, 0.716, 0.838, 0.697, 0.746, 0.804, 0.667, 0.795, 0.543, 0.541, 0.529, 0.56, 0.587, 0.611, 0.548, 0.486, 0.499, 0.436, 0.512, 0.561, 0.592, 0.794, 0.741, 0.799, 0.812, 0.808, 0.603, 0.73, 0.586, 0.565, 0.473, 0.476, 0.536, 0.418, 0.522, 0.528, 0.558, 0.658, 0.466, 0.546, 0.552, 0.528, 0.652, 0.575, 0.601, 0.712, 0.517, 0.612, 0.618, 0.543, 0.656, 0.683, 0.716, 0.484, 0.583, 0.457, 0.453, 0.448, 0.485, 0.467, 0.537, 0.457, 0.519, 0.495, 0.506, 0.567, 0.501, 0.548, 0.473, 0.46, 0.506, 0.383, 0.471, 0.421, 0.531, 0.62, 0.533, 0.499, 0.453, 0.353, 0.427, 0.416, 0.415, 0.494, 0.46, 0.554, 0.733, 0.577, 0.638, 0.649, 0.56, 0.542, 0.589, 0.461, 0.608, 0.484, 0.446, 0.441, 0.341, 0.467, 0.453, 0.529, 0.706, 0.481, 0.599, 0.406, 0.416, 0.401, 0.511, 0.387, 0.55, 0.385, 0.496, 0.53, 0.536, 0.634, 0.497, 0.558, 0.397, 0.453, 0.487, 0.498, 0.51, 0.42, 0.433, 0.537, 0.467, 0.543, 0.6, 0.509, 0.591, 0.709, 0.532, 0.677, 0.49, 0.557, 0.598, 0.613, 0.691, 0.526, 0.6, 0.48, 0.518, 0.53, 0.418, 0.495, 0.363, 0.472, 0.599, 0.524, 0.628, 0.448, 0.487, 0.45, 0.35, 0.419, 0.326, 0.463, 0.448, 0.396, 0.461, 0.386, 0.351, 0.491, 0.554, 0.663, 0.484, 0.73, 0.492, 0.538, 0.493, 0.47, 0.467, 0.368, 0.437, 0.452, 0.444, 0.55, 0.464, 0.538, 0.622, 0.532, 0.635, 0.476, 0.596, 0.523, 0.556, 0.624, 0.474, 0.488, 0.583, 0.422, 0.535, 0.663, 0.51, 0.495, 0.426, 0.509, 0.463, 0.515, 0.572, 0.529, 0.485, 0.546, 0.583, 0.711, 0.561, 0.767, 0.599, 0.618, 0.654, 0.568, 0.66, 0.568, 0.578, 0.571, 0.526, 0.567, 0.609, 0.553, 0.423, 0.513, 0.346, 0.384, 0.481, 0.417, 0.492, 0.505, 0.508, 0.438, 0.475, 0.544, 0.622, 0.59, 0.529, 0.58, 0.503, 0.5, 0.425, 0.471, 0.438, 0.593, 0.479, 0.468, 0.55, 0.423, 0.515, 0.376, 0.531, 0.392, 0.477, 0.42, 0.42, 0.485, 0.368, 0.485, 0.384, 0.409, 0.332, 0.392, 0.339, 0.363, 0.401, 0.37, 0.31, 0.365, 0.298, 0.288, 0.259, 0.281, 0.299, 0.329, 0.4, 0.429, 0.508, 0.427, 0.506, 0.453, 0.444, 0.524, 0.489, 0.562, 0.496, 0.551, 0.494, 0.672, 0.681, 0.453, 0.581, 0.495, 0.474, 0.486, 0.48, 0.524, 0.548, 0.497, 0.525, 0.435, 0.414, 0.407, 0.331, 0.4, 0.363, 0.402, 0.446, 0.378, 0.447, 0.377, 0.445, 0.41, 0.515, 0.458, 0.586, 0.479, 0.614, 0.543, 0.457, 0.522, 0.41, 0.485, 0.388, 0.476, 0.38, 0.496, 0.397, 0.453, 0.502, 0.485, 0.594, 0.581, 0.471, 0.509]\n",
      "Val Error(all epochs): 6.651627063751221 \n",
      " [49.397, 49.26, 49.084, 48.867, 48.64, 48.4, 48.096, 47.619, 47.077, 46.405, 45.746, 44.487, 42.781, 42.345, 40.51, 38.548, 38.722, 37.449, 35.052, 36.603, 34.685, 32.968, 33.492, 28.297, 26.962, 24.177, 19.44, 21.282, 16.788, 17.814, 14.812, 15.116, 13.322, 12.452, 12.169, 12.734, 11.724, 12.808, 13.146, 14.769, 14.066, 18.136, 16.295, 19.936, 18.454, 18.326, 20.459, 21.813, 25.668, 25.659, 26.94, 24.962, 25.014, 21.4, 21.595, 18.938, 19.788, 17.814, 19.106, 17.474, 18.819, 17.576, 18.921, 17.299, 18.369, 17.462, 18.993, 16.811, 18.698, 17.378, 18.588, 17.783, 18.286, 17.754, 18.119, 16.831, 18.378, 16.073, 17.211, 16.03, 16.616, 16.722, 16.951, 16.843, 16.459, 16.04, 15.284, 14.935, 14.272, 14.704, 14.568, 14.632, 14.197, 14.284, 14.233, 14.153, 15.694, 15.402, 14.189, 14.986, 15.264, 13.633, 13.175, 14.171, 15.015, 14.545, 12.491, 12.982, 12.618, 13.485, 12.726, 12.465, 12.713, 12.51, 12.436, 12.071, 12.441, 12.94, 12.252, 12.146, 12.313, 12.58, 11.437, 13.178, 13.365, 14.381, 13.704, 12.878, 14.824, 14.825, 12.808, 11.584, 11.64, 11.199, 11.384, 10.758, 11.234, 11.499, 10.942, 11.105, 10.793, 11.082, 10.896, 11.501, 10.8, 10.84, 10.235, 10.627, 10.214, 10.484, 10.252, 10.549, 10.61, 10.42, 10.182, 10.608, 10.203, 10.12, 10.413, 10.286, 10.251, 10.543, 10.714, 10.44, 10.632, 10.075, 10.338, 9.909, 10.499, 10.262, 10.722, 10.38, 11.139, 10.502, 10.455, 10.12, 10.064, 10.042, 9.856, 10.21, 9.894, 10.315, 9.886, 10.235, 9.711, 10.034, 9.704, 9.817, 9.747, 10.091, 9.836, 9.84, 9.648, 9.758, 9.695, 9.555, 9.406, 9.677, 9.494, 9.753, 9.314, 9.595, 9.357, 9.465, 9.646, 9.346, 9.379, 9.709, 9.154, 9.609, 9.496, 9.212, 9.906, 9.169, 9.033, 9.648, 9.059, 9.107, 9.281, 9.102, 8.896, 9.447, 9.217, 8.833, 9.255, 8.913, 9.052, 9.096, 9.024, 8.961, 9.054, 8.992, 8.965, 8.902, 8.823, 8.934, 8.621, 8.863, 8.69, 9.097, 8.691, 8.693, 8.726, 8.652, 8.618, 8.883, 8.685, 8.641, 8.741, 8.469, 8.675, 8.589, 8.396, 8.617, 8.688, 8.336, 8.702, 8.352, 8.814, 8.751, 8.444, 8.399, 8.338, 8.337, 8.51, 8.2, 8.373, 8.348, 8.179, 8.486, 8.247, 8.246, 8.57, 8.35, 8.253, 8.231, 8.162, 8.214, 8.294, 8.123, 8.445, 8.161, 8.117, 8.313, 8.124, 8.042, 8.194, 8.162, 8.038, 7.882, 8.271, 7.79, 8.216, 8.104, 7.993, 8.268, 8.008, 8.158, 8.531, 7.938, 7.746, 7.919, 7.969, 8.114, 7.651, 7.794, 7.868, 7.679, 7.705, 7.634, 7.705, 7.66, 7.779, 7.696, 7.602, 7.898, 7.606, 7.572, 7.818, 7.463, 7.728, 7.616, 7.802, 7.578, 7.642, 7.611, 7.74, 7.386, 7.909, 7.459, 7.795, 7.39, 7.43, 7.436, 7.689, 7.481, 7.588, 7.761, 7.526, 7.452, 7.503, 7.362, 7.652, 7.325, 7.665, 7.514, 7.522, 7.657, 7.35, 7.52, 7.182, 7.509, 7.432, 7.311, 7.497, 7.667, 7.378, 7.707, 7.717, 7.152, 7.543, 7.528, 7.447, 7.617, 7.407, 7.453, 7.261, 7.216, 7.53, 7.197, 7.346, 7.469, 7.651, 7.477, 7.363, 7.424, 7.123, 7.637, 7.133, 7.181, 7.456, 7.408, 7.138, 7.507, 7.19, 7.315, 7.296, 7.227, 7.215, 7.216, 7.093, 7.288, 7.311, 7.214, 7.249, 7.13, 7.247, 7.191, 7.303, 7.309, 7.498, 7.37, 7.125, 7.422, 7.007, 7.114, 7.31, 7.166, 7.062, 7.309, 7.027, 7.4, 7.094, 7.089, 7.078, 7.247, 7.075, 7.227, 7.007, 7.027, 7.182, 7.081, 7.277, 6.909, 7.106, 7.04, 7.135, 6.934, 7.066, 7.058, 7.122, 6.951, 7.031, 6.894, 7.122, 7.108, 7.079, 7.065, 7.068, 7.04, 6.963, 7.05, 7.056, 6.993, 7.106, 7.32, 6.968, 6.923, 7.237, 6.827, 7.022, 7.138, 6.809, 7.154, 7.095, 6.78, 7.268, 7.056, 6.93, 7.259, 6.855, 7.014, 7.111, 7.279, 6.926, 7.032, 7.029, 6.816, 6.989, 6.904, 6.893, 7.121, 6.882, 7.009, 6.801, 7.032, 6.768, 7.127, 6.83, 6.973, 7.211, 6.795, 6.922, 6.886, 6.969, 6.976, 7.017, 6.982, 7.028, 7.089, 7.024, 7.183, 7.073, 7.013, 7.047, 6.84, 6.838, 7.126, 6.652, 6.998]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.042, 0.112, 0.209, 0.187, 0.374, 0.359, 0.646, 0.633, 1.019, 1.145, 1.646, 1.274, 3.256, 2.483, 4.99, 7.316, 5.141, 12.099, 8.822, 13.582, 11.553, 12.034, 13.62, 16.702, 21.593, 21.007, 20.805, 17.901, 17.949, 13.569, 13.912, 10.45, 11.27, 8.92, 10.249, 8.243, 9.93, 8.155, 10.045, 7.798, 9.161, 8.322, 9.854, 7.389, 9.662, 8.299, 9.691, 8.838, 9.422, 9.003, 9.285, 8.046, 9.747, 7.018, 8.547, 6.881, 7.836, 7.965, 8.26, 8.205, 7.646, 7.223, 6.179, 5.689, 4.681, 5.399, 5.25, 5.425, 4.722, 4.847, 4.999, 4.93, 7.017, 7.046, 5.133, 6.282, 6.879, 4.296, 3.087, 5.194, 7.417, 6.958, 5.847, 6.851, 6.63, 7.706, 7.058, 6.471, 7.009, 6.593, 6.566, 6.129, 6.561, 7.415, 6.284, 6.319, 6.608, 6.756, 5.583, 7.611, 7.887, 9.089, 8.432, 7.482, 9.87, 9.501, 7.51, 5.857, 5.927, 5.404, 5.409, 4.677, 5.103, 5.755, 4.91, 5.079, 5.058, 5.047, 5.098, 5.894, 5.084, 4.369, 3.122, 3.567, 3.447, 3.627, 3.672, 4.481, 4.703, 4.175, 3.809, 4.658, 4.025, 3.738, 4.348, 4.625, 4.327, 4.634, 5.557, 4.506, 5.312, 3.827, 4.382, 3.628, 4.839, 4.339, 5.227, 4.945, 6.168, 5.439, 5.054, 4.686, 4.113, 3.728, 3.653, 4.638, 4.08, 5.06, 4.638, 5.003, 4.218, 4.527, 4.17, 3.854, 4.14, 4.658, 4.84, 3.845, 4.278, 4.527, 4.223, 3.96, 3.562, 4.128, 4.076, 4.53, 4.055, 4.21, 3.893, 4.132, 4.479, 3.795, 4.196, 4.283, 4.126, 4.476, 4.62, 3.373, 5.414, 3.704, 4.002, 4.547, 3.964, 3.155, 4.589, 3.462, 3.829, 5.078, 4.339, 3.64, 3.844, 3.462, 3.856, 4.339, 4.042, 4.481, 3.873, 4.663, 3.944, 4.52, 3.303, 4.207, 3.344, 4.045, 3.763, 4.857, 3.965, 4.227, 4.132, 3.792, 4.13, 4.327, 3.648, 3.945, 3.861, 3.641, 3.728, 3.442, 3.185, 3.932, 3.624, 2.952, 3.143, 3.405, 2.693, 3.448, 3.326, 3.889, 3.809, 4.266, 3.907, 3.673, 3.73, 3.516, 3.689, 4.072, 4.142, 3.961, 4.232, 3.795, 4.369, 3.835, 3.962, 3.594, 3.958, 3.533, 4.489, 3.692, 3.858, 3.894, 3.627, 3.175, 3.512, 3.357, 3.653, 3.389, 3.744, 3.067, 3.963, 3.531, 2.72, 3.725, 3.474, 2.599, 3.937, 3.168, 2.81, 3.189, 3.398, 3.632, 3.078, 3.593, 2.73, 3.176, 3.438, 3.57, 3.396, 3.551, 3.657, 3.522, 3.084, 3.959, 2.9, 3.585, 3.458, 3.377, 2.777, 3.339, 3.363, 3.466, 2.921, 3.45, 2.871, 3.289, 2.701, 3.444, 3.274, 3.923, 3.12, 3.149, 3.91, 3.416, 3.166, 3.689, 2.804, 3.39, 3.338, 3.642, 3.066, 3.758, 2.819, 3.325, 2.911, 3.239, 3.514, 2.924, 2.891, 3.775, 3.511, 3.275, 3.188, 4.382, 3.464, 4.317, 3.276, 3.648, 3.291, 2.902, 2.91, 2.912, 3.127, 3.464, 3.463, 3.423, 3.779, 3.475, 3.054, 3.5, 3.572, 4.127, 3.968, 3.898, 3.476, 4.196, 3.275, 3.691, 4.02, 3.857, 3.503, 3.954, 3.891, 3.347, 3.789, 3.113, 3.527, 3.574, 3.848, 3.545, 3.936, 3.086, 3.989, 2.932, 3.052, 3.343, 3.868, 3.157, 3.338, 2.962, 4.218, 3.797, 3.548, 3.464, 3.509, 3.239, 3.483, 3.317, 3.572, 4.399, 3.657, 3.54, 3.413, 3.312, 3.604, 4.085, 3.765, 4.026, 4.296, 4.071, 4.391, 3.912, 4.173, 3.968, 4.13, 3.477, 3.778, 3.548, 3.712, 3.951, 3.829, 3.478, 3.673, 3.557, 3.507, 3.464, 3.591, 3.616, 3.432, 3.875, 3.649, 3.675, 3.238, 3.871, 3.228, 3.966, 3.677, 3.847, 3.774, 3.944, 3.454, 4.409, 4.073, 3.093, 3.334, 3.33, 3.724, 3.793, 3.421, 3.576, 3.179, 2.777, 3.142, 3.532, 3.654, 3.585, 4.01, 3.456, 3.781, 3.771, 3.698, 3.762, 3.728, 3.921, 3.536, 3.987, 3.27, 4.021, 3.931, 3.643, 4.165, 3.606, 3.778, 3.422, 3.772, 3.31, 3.867, 3.29, 3.56, 3.595, 3.116, 3.607, 3.711, 3.333, 3.816, 3.971, 3.571, 3.787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 49.30800, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 49.30800 to 49.13982, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 49.13982 to 48.93711, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 48.93711 to 48.72169, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 48.72169 to 48.44258, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 48.44258 to 48.15615, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 48.15615 to 47.85286, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 47.85286 to 47.51949, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 47.51949 to 47.22303, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 47.22303 to 46.93454, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 46.93454 to 46.58833, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 46.58833 to 46.20520, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 46.20520 to 45.82290, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 45.82290 to 45.55354, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 45.55354 to 44.92210, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 44.92210 to 44.28797, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 44.28797 to 43.92899, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 43.92899 to 43.36927, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 43.36927 to 43.00835, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 43.00835 to 42.60265, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 42.60265 to 41.23950, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 41.23950 to 41.13224, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 41.13224 to 39.97888, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 39.97888 to 39.40378, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 39.40378 to 39.38122, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 39.38122 to 37.90979, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 37.90979 to 35.49801, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 35.49801 to 33.79911, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 33.79911 to 32.40185, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 32.40185 to 32.04148, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00031: val_mae improved from 32.04148 to 29.19017, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00032: val_mae improved from 29.19017 to 26.68923, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 26.68923 to 25.66918, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00034: val_mae improved from 25.66918 to 25.65765, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00035: val_mae improved from 25.65765 to 20.25196, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 20.25196\n",
      "\n",
      "Epoch 00037: val_mae improved from 20.25196 to 19.88931, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00038: val_mae improved from 19.88931 to 15.63621, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 15.63621\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 15.63621\n",
      "\n",
      "Epoch 00041: val_mae improved from 15.63621 to 13.79795, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00042: val_mae improved from 13.79795 to 13.45338, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00043: val_mae improved from 13.45338 to 12.64973, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 12.64973\n",
      "\n",
      "Epoch 00080: val_mae improved from 12.64973 to 11.55567, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 11.55567\n",
      "\n",
      "Epoch 00082: val_mae improved from 11.55567 to 11.37270, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 11.37270\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 11.37270\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 11.37270\n",
      "\n",
      "Epoch 00086: val_mae improved from 11.37270 to 11.10787, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 11.10787\n",
      "\n",
      "Epoch 00088: val_mae improved from 11.10787 to 10.58937, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 10.58937\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 10.58937\n",
      "\n",
      "Epoch 00091: val_mae improved from 10.58937 to 9.97055, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 9.97055\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 9.97055\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 9.97055\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 9.97055\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 9.97055\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 9.97055\n",
      "\n",
      "Epoch 00098: val_mae improved from 9.97055 to 9.52334, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 9.52334\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 9.52334\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 9.52334\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 9.52334\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 9.52334\n",
      "\n",
      "Epoch 00104: val_mae improved from 9.52334 to 9.04730, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 9.04730\n",
      "\n",
      "Epoch 00106: val_mae improved from 9.04730 to 8.06144, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 8.06144\n",
      "\n",
      "Epoch 00108: val_mae improved from 8.06144 to 8.02132, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 8.02132\n",
      "\n",
      "Epoch 00141: val_mae improved from 8.02132 to 7.57239, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 7.57239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00164: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 7.57239\n",
      "\n",
      "Epoch 00207: val_mae improved from 7.57239 to 7.44085, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 7.44085\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 7.44085\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 7.44085\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 7.44085\n",
      "\n",
      "Epoch 00212: val_mae improved from 7.44085 to 7.38392, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 7.38392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00319: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 7.38392\n",
      "\n",
      "Epoch 00370: val_mae improved from 7.38392 to 7.19495, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 7.19495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00477: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 7.19495\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 7.19495\n",
      "\n",
      "Lambda: 0.1 , Time: 0:03:58\n",
      "Train Error(all epochs): 0.9352443814277649 \n",
      " [49.247, 49.143, 49.057, 48.964, 48.862, 48.742, 48.606, 48.449, 48.264, 48.039, 47.781, 47.494, 47.161, 46.788, 46.367, 45.896, 45.372, 44.802, 44.14, 43.417, 42.622, 41.764, 40.83, 39.83, 38.778, 37.666, 36.463, 35.2, 33.889, 32.54, 31.162, 29.726, 28.276, 26.79, 25.33, 23.914, 22.455, 21.033, 19.734, 18.361, 17.117, 15.923, 14.889, 13.946, 13.009, 12.311, 11.608, 11.103, 10.631, 10.145, 9.858, 9.454, 9.093, 8.899, 8.564, 8.395, 8.286, 8.105, 7.85, 7.565, 7.413, 7.228, 7.041, 6.901, 6.619, 6.715, 6.478, 6.166, 6.074, 5.825, 5.617, 5.67, 5.51, 5.231, 5.255, 4.918, 4.989, 4.829, 4.652, 4.558, 4.429, 4.425, 4.231, 4.112, 4.059, 4.01, 3.927, 4.003, 3.836, 3.925, 3.944, 3.792, 3.787, 3.858, 3.967, 3.976, 3.871, 4.093, 4.151, 3.807, 3.544, 3.578, 3.464, 3.594, 3.564, 3.633, 3.555, 3.43, 3.4, 3.221, 3.328, 3.292, 3.206, 3.11, 3.087, 3.35, 3.261, 3.496, 3.391, 3.316, 3.3, 3.407, 3.411, 3.549, 3.49, 3.269, 3.257, 3.053, 3.102, 2.962, 2.893, 2.777, 2.682, 2.509, 2.496, 2.619, 2.614, 2.494, 2.534, 2.571, 2.836, 2.839, 3.062, 2.871, 2.939, 2.752, 2.791, 2.614, 2.569, 2.411, 2.448, 2.353, 2.575, 2.757, 2.72, 2.883, 2.853, 2.764, 2.662, 2.46, 2.359, 2.411, 2.311, 2.302, 2.222, 2.06, 1.981, 2.025, 2.101, 2.204, 2.312, 2.436, 2.518, 2.481, 2.272, 2.372, 2.285, 2.274, 2.246, 2.08, 1.994, 2.145, 2.326, 2.522, 2.656, 2.725, 2.635, 2.478, 2.388, 2.201, 2.111, 2.061, 1.997, 2.097, 2.318, 2.38, 2.347, 2.217, 2.028, 1.951, 1.799, 1.707, 1.779, 1.959, 2.099, 2.16, 2.12, 2.079, 2.061, 2.01, 2.007, 2.024, 2.017, 2.184, 2.143, 2.028, 1.879, 1.826, 1.67, 1.756, 1.73, 1.673, 1.655, 1.649, 1.63, 1.749, 1.951, 1.982, 2.105, 2.25, 2.205, 2.016, 1.943, 1.734, 1.715, 1.546, 1.569, 1.706, 1.77, 1.77, 1.775, 1.776, 1.743, 1.742, 1.631, 1.45, 1.336, 1.395, 1.382, 1.449, 1.502, 1.549, 1.735, 1.796, 2.108, 2.283, 2.379, 2.208, 2.088, 1.946, 1.911, 1.706, 1.584, 1.581, 1.443, 1.418, 1.317, 1.309, 1.311, 1.331, 1.374, 1.444, 1.519, 1.66, 1.926, 1.975, 1.874, 1.828, 1.727, 1.759, 1.572, 1.404, 1.221, 1.223, 1.131, 1.189, 1.363, 1.536, 1.771, 1.957, 1.835, 1.716, 1.71, 1.729, 1.737, 1.714, 1.652, 1.508, 1.332, 1.261, 1.189, 1.109, 1.222, 1.23, 1.433, 1.588, 1.66, 1.759, 1.8, 1.725, 1.691, 1.522, 1.317, 1.254, 1.261, 1.218, 1.126, 1.118, 1.119, 1.179, 1.328, 1.574, 1.776, 1.891, 1.944, 1.867, 1.828, 2.006, 1.789, 1.781, 1.726, 1.846, 1.843, 1.835, 1.779, 1.707, 1.466, 1.411, 1.384, 1.441, 1.442, 1.506, 1.538, 1.513, 1.454, 1.491, 1.532, 1.485, 1.641, 1.625, 1.588, 1.58, 1.608, 1.65, 1.578, 1.508, 1.393, 1.275, 1.188, 1.128, 1.049, 1.024, 1.009, 1.058, 1.073, 1.12, 1.143, 1.132, 1.19, 1.324, 1.461, 1.483, 1.386, 1.272, 1.216, 1.197, 1.164, 1.232, 1.331, 1.448, 1.599, 1.863, 1.908, 1.966, 1.942, 1.818, 1.725, 1.645, 1.529, 1.446, 1.441, 1.335, 1.308, 1.305, 1.421, 1.676, 1.786, 1.876, 1.92, 1.735, 1.66, 1.518, 1.486, 1.447, 1.52, 1.477, 1.438, 1.303, 1.27, 1.15, 1.159, 1.122, 1.089, 1.068, 1.161, 1.261, 1.324, 1.31, 1.296, 1.274, 1.342, 1.379, 1.366, 1.465, 1.48, 1.616, 1.771, 1.798, 1.793, 1.593, 1.722, 1.566, 1.5, 1.46, 1.393, 1.363, 1.473, 1.386, 1.358, 1.256, 1.33, 1.504, 1.541, 1.59, 1.548, 1.583, 1.565, 1.602, 1.583, 1.569, 1.497, 1.426, 1.408, 1.478, 1.564, 1.516, 1.418, 1.465, 1.387, 1.348, 1.274, 1.2, 1.261, 1.134, 1.02, 0.935, 0.986, 1.027, 1.032, 1.021, 1.041, 1.043, 1.134, 1.271, 1.473, 1.566, 1.506, 1.536, 1.429, 1.355, 1.388, 1.398, 1.374, 1.335, 1.377, 1.405, 1.368, 1.422, 1.376, 1.3, 1.282, 1.342, 1.431, 1.586, 1.63, 1.672, 1.57, 1.585, 1.503, 1.43]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.015, 0.023, 0.026, 0.03, 0.094, 0.139, 0.174, 0.267, 0.335, 0.453, 0.558, 0.605, 0.743, 0.795, 0.806, 0.918, 0.925, 0.997, 1.075, 1.14, 1.092, 1.112, 1.119, 1.079, 1.158, 1.142, 1.138, 1.256, 1.224, 1.162, 1.198, 1.14, 1.154, 1.227, 1.265, 1.19, 1.214, 1.184, 1.248, 1.223, 1.247, 1.21, 1.205, 1.222, 1.165, 1.166, 1.141, 1.181, 1.167, 1.272, 1.22, 1.283, 1.325, 1.304, 1.29, 1.363, 1.431, 1.407, 1.438, 1.521, 1.728, 1.458, 1.242, 1.351, 1.271, 1.423, 1.348, 1.394, 1.459, 1.266, 1.366, 1.214, 1.277, 1.308, 1.233, 1.194, 1.208, 1.385, 1.345, 1.399, 1.476, 1.261, 1.299, 1.386, 1.427, 1.495, 1.501, 1.322, 1.382, 1.263, 1.189, 1.256, 1.165, 1.106, 1.022, 0.982, 0.956, 1.045, 1.07, 0.975, 1.003, 1.072, 1.16, 1.186, 1.38, 1.201, 1.271, 1.143, 1.173, 1.123, 0.999, 0.982, 0.999, 0.961, 1.025, 1.222, 1.105, 1.357, 1.275, 1.199, 1.136, 1.039, 0.917, 1.077, 0.944, 0.865, 0.998, 0.815, 0.773, 0.853, 0.796, 0.957, 0.984, 1.062, 1.109, 1.122, 0.986, 0.999, 1.093, 0.827, 1.069, 0.943, 0.753, 0.991, 0.972, 1.144, 1.219, 1.307, 1.123, 0.985, 1.142, 0.963, 0.859, 0.92, 0.781, 0.909, 1.112, 0.941, 1.174, 0.96, 0.846, 0.884, 0.74, 0.699, 0.836, 0.857, 0.895, 1.082, 0.897, 0.969, 0.86, 0.92, 0.806, 1.0, 0.834, 1.017, 1.045, 0.774, 0.875, 0.885, 0.683, 0.791, 0.808, 0.663, 0.821, 0.691, 0.743, 0.713, 0.983, 0.81, 0.983, 1.135, 0.941, 1.047, 0.792, 0.824, 0.77, 0.636, 0.668, 0.842, 0.745, 0.866, 0.759, 0.847, 0.752, 0.807, 0.697, 0.657, 0.587, 0.619, 0.592, 0.685, 0.648, 0.693, 0.867, 0.717, 1.087, 1.125, 1.051, 1.092, 0.883, 0.966, 0.821, 0.818, 0.69, 0.703, 0.624, 0.656, 0.611, 0.522, 0.663, 0.579, 0.564, 0.735, 0.653, 0.772, 0.981, 0.841, 0.939, 0.888, 0.657, 0.884, 0.699, 0.65, 0.547, 0.529, 0.478, 0.574, 0.577, 0.754, 0.89, 0.83, 0.989, 0.713, 0.794, 0.854, 0.723, 0.849, 0.752, 0.681, 0.616, 0.555, 0.547, 0.457, 0.598, 0.496, 0.694, 0.812, 0.647, 0.998, 0.688, 0.906, 0.792, 0.656, 0.617, 0.565, 0.609, 0.525, 0.507, 0.545, 0.466, 0.534, 0.679, 0.716, 0.839, 0.972, 0.87, 0.907, 0.874, 0.864, 0.861, 0.845, 0.727, 0.93, 0.838, 0.837, 0.861, 0.767, 0.62, 0.755, 0.534, 0.726, 0.713, 0.619, 0.798, 0.666, 0.627, 0.722, 0.779, 0.57, 0.901, 0.755, 0.655, 0.865, 0.635, 0.858, 0.731, 0.614, 0.751, 0.48, 0.609, 0.458, 0.493, 0.477, 0.451, 0.502, 0.447, 0.616, 0.431, 0.617, 0.526, 0.554, 0.801, 0.614, 0.653, 0.679, 0.456, 0.624, 0.493, 0.577, 0.628, 0.659, 0.831, 0.775, 0.982, 0.894, 0.884, 0.972, 0.667, 0.884, 0.653, 0.642, 0.715, 0.621, 0.585, 0.616, 0.672, 0.735, 0.993, 0.718, 1.003, 0.847, 0.676, 0.779, 0.706, 0.63, 0.712, 0.685, 0.667, 0.62, 0.592, 0.467, 0.595, 0.458, 0.551, 0.451, 0.562, 0.628, 0.55, 0.688, 0.556, 0.632, 0.609, 0.641, 0.64, 0.722, 0.699, 0.753, 0.88, 0.835, 0.841, 0.771, 0.778, 0.669, 0.783, 0.63, 0.658, 0.644, 0.688, 0.626, 0.697, 0.474, 0.724, 0.68, 0.676, 0.874, 0.622, 0.763, 0.807, 0.624, 0.817, 0.746, 0.653, 0.749, 0.564, 0.793, 0.676, 0.743, 0.625, 0.771, 0.582, 0.651, 0.636, 0.426, 0.733, 0.407, 0.534, 0.446, 0.393, 0.545, 0.435, 0.487, 0.502, 0.496, 0.518, 0.629, 0.708, 0.704, 0.799, 0.618, 0.733, 0.603, 0.621, 0.729, 0.651, 0.545, 0.726, 0.649, 0.63, 0.676, 0.644, 0.586, 0.638, 0.605, 0.622, 0.842, 0.667, 0.844, 0.766, 0.713, 0.706, 0.68]\n",
      "Val Error(all epochs): 7.194948673248291 \n",
      " [49.308, 49.14, 48.937, 48.722, 48.443, 48.156, 47.853, 47.519, 47.223, 46.935, 46.588, 46.205, 45.823, 45.554, 44.922, 44.288, 43.929, 43.369, 43.008, 42.603, 41.239, 41.132, 39.979, 39.404, 39.381, 37.91, 35.498, 33.799, 32.402, 32.041, 29.19, 26.689, 25.669, 25.658, 20.252, 23.757, 19.889, 15.636, 17.141, 16.088, 13.798, 13.453, 12.65, 13.141, 13.1, 13.988, 14.492, 14.428, 14.97, 16.011, 15.77, 15.874, 15.708, 15.582, 15.328, 15.943, 16.339, 16.087, 17.512, 16.262, 15.927, 15.777, 15.886, 16.496, 15.348, 14.757, 16.68, 14.22, 14.329, 15.313, 15.879, 13.95, 15.307, 13.707, 12.757, 14.225, 12.662, 13.232, 13.494, 11.556, 12.746, 11.373, 11.72, 12.251, 12.097, 11.108, 12.198, 10.589, 11.123, 10.926, 9.971, 10.447, 10.389, 11.172, 10.583, 10.181, 10.321, 9.523, 11.058, 10.216, 10.684, 10.109, 9.949, 9.047, 9.89, 8.061, 8.118, 8.021, 9.155, 8.976, 9.178, 9.206, 8.866, 9.063, 9.429, 9.084, 9.164, 8.684, 8.648, 8.935, 9.397, 9.777, 8.665, 9.686, 8.767, 9.356, 9.071, 10.295, 8.65, 8.995, 8.999, 9.104, 8.796, 9.305, 8.731, 9.276, 8.329, 8.531, 8.291, 8.616, 7.572, 9.092, 7.74, 8.65, 8.187, 9.37, 8.826, 9.394, 9.357, 9.252, 8.618, 9.047, 9.121, 8.677, 8.709, 8.85, 8.509, 7.993, 8.842, 7.951, 9.183, 8.233, 8.408, 8.223, 8.103, 8.011, 8.246, 8.738, 8.03, 8.597, 7.943, 8.026, 8.122, 8.066, 8.551, 8.447, 7.996, 8.615, 8.58, 8.105, 8.694, 7.747, 8.984, 7.955, 8.343, 8.282, 8.855, 8.908, 8.967, 9.002, 9.456, 8.67, 8.625, 8.593, 8.678, 8.103, 8.229, 8.169, 7.923, 7.758, 7.724, 8.05, 7.748, 8.29, 7.684, 7.821, 7.441, 7.934, 8.2, 7.48, 8.475, 7.384, 7.895, 7.635, 7.667, 7.493, 7.793, 7.742, 7.408, 8.258, 7.633, 8.147, 7.826, 8.217, 7.783, 8.221, 7.59, 8.073, 7.671, 8.086, 7.659, 8.191, 7.836, 8.223, 8.309, 8.18, 8.247, 8.151, 8.11, 8.118, 7.867, 7.807, 8.069, 7.643, 8.393, 7.861, 7.903, 7.766, 7.997, 7.772, 7.719, 8.081, 7.671, 7.864, 7.827, 7.921, 7.525, 8.109, 7.624, 7.759, 8.238, 7.75, 7.809, 8.005, 7.686, 7.843, 8.145, 7.779, 8.104, 7.95, 8.044, 7.806, 8.292, 7.554, 8.147, 7.51, 7.916, 7.963, 7.866, 7.939, 7.646, 7.593, 7.909, 7.652, 7.749, 7.827, 7.623, 7.683, 7.742, 7.838, 7.706, 8.16, 7.579, 8.298, 7.874, 8.149, 7.66, 7.935, 7.565, 7.863, 7.667, 7.978, 7.773, 7.799, 7.655, 7.822, 7.6, 7.818, 7.886, 7.443, 7.591, 7.703, 7.614, 7.725, 7.718, 7.585, 7.655, 7.853, 7.503, 7.581, 7.768, 7.756, 7.487, 8.409, 7.561, 8.096, 7.807, 7.901, 7.49, 7.567, 7.488, 7.684, 7.785, 7.607, 7.919, 7.595, 8.093, 8.04, 8.141, 7.775, 8.053, 7.891, 7.986, 7.832, 8.05, 7.768, 7.865, 7.812, 7.916, 7.593, 7.985, 7.83, 8.012, 7.969, 7.835, 8.0, 7.943, 7.693, 8.059, 7.732, 7.725, 7.647, 7.908, 7.507, 7.709, 7.514, 7.566, 7.616, 7.612, 7.195, 7.892, 7.615, 7.63, 7.753, 7.761, 7.471, 7.865, 7.449, 7.695, 7.643, 7.677, 7.414, 8.041, 7.775, 7.891, 8.048, 7.655, 8.179, 8.422, 7.36, 8.112, 7.464, 7.701, 7.844, 7.592, 7.521, 8.01, 7.629, 8.012, 7.836, 8.12, 7.665, 7.746, 7.811, 8.023, 7.892, 8.006, 7.898, 7.759, 7.762, 7.712, 7.758, 7.544, 7.883, 7.645, 7.824, 7.596, 7.996, 7.816, 7.952, 8.071, 7.789, 8.039, 7.843, 7.985, 7.523, 8.266, 7.738, 8.107, 8.079, 8.058, 7.85, 8.231, 8.148, 7.661, 8.349, 7.579, 8.115, 7.964, 7.779, 8.283, 7.763, 7.869, 7.854, 7.6, 7.787, 7.981, 7.439, 7.821, 7.407, 7.779, 7.512, 7.744, 7.255, 7.772, 7.571, 7.889, 7.484, 7.832, 7.785, 7.701, 7.591, 7.681, 7.593, 7.594, 7.602, 7.467, 7.736, 7.482, 7.587, 7.649, 7.721, 7.513, 7.986, 7.378, 8.022, 7.732, 7.763, 7.294, 7.724, 7.403, 8.123, 7.251, 7.599, 8.054, 7.673, 7.373, 7.913, 7.673, 7.459, 7.601, 7.698, 7.343, 8.06, 7.462, 7.918, 7.858, 7.914, 7.964, 7.923]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.051, 0.092, 0.102, 0.293, 0.198, 0.368, 0.732, 0.637, 0.796, 1.315, 1.951, 3.964, 5.624, 4.189, 6.429, 7.6, 7.254, 7.686, 9.586, 8.838, 9.107, 8.173, 7.957, 8.061, 8.147, 8.921, 9.412, 10.173, 9.041, 8.311, 7.716, 8.078, 8.927, 7.298, 7.177, 9.176, 6.735, 6.758, 8.253, 8.843, 7.178, 8.551, 7.515, 6.037, 8.083, 6.489, 7.537, 7.406, 6.248, 6.93, 6.058, 6.259, 6.721, 6.908, 6.055, 6.956, 5.575, 6.026, 5.906, 5.564, 5.739, 5.916, 7.128, 6.376, 6.228, 6.749, 6.0, 7.544, 6.541, 6.854, 6.353, 6.677, 4.839, 6.226, 4.517, 4.345, 3.928, 5.175, 5.459, 5.336, 5.766, 5.313, 5.102, 6.012, 5.569, 5.649, 5.002, 4.807, 5.145, 5.703, 6.658, 5.503, 6.568, 5.937, 6.41, 5.821, 7.25, 5.742, 5.464, 5.624, 5.26, 4.878, 5.647, 4.813, 5.447, 4.394, 4.692, 3.846, 4.867, 3.185, 4.64, 3.927, 4.619, 3.969, 5.433, 5.248, 5.761, 5.369, 5.596, 4.747, 5.359, 5.083, 5.114, 4.973, 5.168, 4.755, 4.092, 5.145, 3.712, 5.497, 4.506, 4.176, 4.408, 4.009, 4.002, 4.326, 4.746, 4.031, 4.889, 3.332, 4.165, 4.033, 4.17, 4.774, 5.035, 4.087, 4.812, 5.15, 4.206, 4.998, 4.225, 4.958, 4.534, 4.958, 4.559, 5.534, 5.708, 5.816, 5.49, 6.339, 5.234, 5.421, 5.063, 5.286, 4.79, 4.336, 4.568, 4.337, 3.61, 4.109, 4.158, 4.084, 4.427, 4.247, 3.64, 3.726, 4.146, 4.572, 4.105, 4.791, 3.876, 4.145, 3.92, 3.378, 3.73, 3.848, 4.163, 3.461, 4.545, 3.827, 4.694, 3.974, 4.795, 4.366, 4.574, 4.147, 4.315, 4.0, 3.575, 4.434, 3.846, 4.683, 4.605, 4.86, 4.788, 4.742, 4.503, 4.534, 4.493, 4.065, 4.234, 4.426, 4.199, 4.806, 4.444, 4.202, 4.205, 4.091, 4.068, 3.869, 4.387, 3.767, 4.604, 3.899, 4.039, 4.002, 3.813, 3.598, 4.078, 4.168, 4.457, 4.149, 4.521, 4.15, 4.462, 4.458, 4.601, 4.381, 4.594, 4.541, 4.203, 4.759, 4.151, 4.4, 4.005, 4.099, 4.106, 4.201, 4.324, 3.972, 3.906, 3.965, 4.169, 3.986, 4.041, 3.962, 3.837, 3.815, 4.434, 4.284, 4.226, 4.323, 4.183, 4.447, 4.023, 3.784, 4.422, 3.418, 4.227, 4.077, 4.516, 4.063, 4.368, 3.953, 4.163, 4.237, 4.05, 3.653, 4.091, 3.604, 4.093, 3.848, 4.239, 3.772, 3.9, 3.895, 4.07, 3.882, 3.959, 3.99, 4.126, 4.203, 3.927, 3.982, 4.045, 4.011, 3.836, 3.177, 3.783, 3.968, 4.093, 4.241, 4.592, 4.955, 4.455, 5.05, 4.753, 5.146, 4.567, 4.271, 4.982, 4.174, 4.501, 4.787, 4.283, 4.42, 4.469, 4.314, 3.904, 4.804, 3.927, 4.772, 4.318, 4.417, 4.659, 4.527, 4.307, 4.868, 4.208, 4.388, 4.063, 4.52, 3.847, 4.186, 3.688, 3.876, 3.672, 3.602, 3.596, 3.767, 3.915, 4.06, 3.797, 4.245, 3.598, 4.071, 3.907, 3.671, 3.756, 3.514, 3.55, 3.562, 3.548, 4.122, 3.27, 3.88, 2.894, 4.058, 3.66, 4.142, 3.878, 4.122, 3.963, 3.925, 4.111, 4.063, 4.648, 4.361, 4.411, 4.783, 3.741, 4.465, 4.158, 4.591, 4.436, 4.57, 4.372, 4.283, 4.221, 4.127, 4.323, 4.063, 4.455, 4.134, 4.204, 4.246, 4.273, 4.551, 4.47, 4.859, 4.414, 4.839, 4.429, 3.944, 4.22, 4.124, 3.856, 4.114, 3.514, 3.492, 3.226, 3.958, 4.347, 3.981, 4.362, 4.271, 4.236, 4.39, 4.351, 4.664, 4.091, 4.345, 3.997, 3.754, 4.211, 4.194, 3.77, 3.872, 3.838, 3.601, 3.241, 3.532, 3.384, 3.625, 4.075, 3.72, 3.825, 4.26, 4.373, 4.215, 4.34, 3.848, 4.256, 4.076, 3.811, 4.02, 3.777, 3.693, 3.955, 3.561, 4.026, 3.844, 3.825, 3.612, 3.419, 3.355, 3.605, 3.438, 3.724, 3.905, 3.996, 3.435, 3.87, 4.221, 3.72, 3.412, 3.462, 3.832, 3.877, 3.665, 3.873, 3.722, 3.635, 3.987, 4.121, 4.31, 4.591, 4.554, 4.147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 49.35950, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 49.35950 to 49.26087, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 49.26087 to 49.14397, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 49.14397 to 49.01436, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 49.01436 to 48.87992, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 48.87992 to 48.72140, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 48.72140 to 48.61079, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 48.61079 to 48.34338, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 48.34338 to 47.99459, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 47.99459 to 47.63654, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 47.63654 to 47.25237, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 47.25237 to 46.78451, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 46.78451 to 46.34279, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 46.34279 to 45.58248, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 45.58248 to 44.58834, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 44.58834 to 43.95723, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 43.95723 to 43.76844, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 43.76844 to 42.40194, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 42.40194 to 40.59064, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 40.59064 to 37.36404, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 37.36404 to 36.42789, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 36.42789 to 34.84950, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 34.84950 to 30.83801, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 30.83801\n",
      "\n",
      "Epoch 00025: val_mae improved from 30.83801 to 28.49755, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 28.49755 to 25.40780, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 25.40780\n",
      "\n",
      "Epoch 00028: val_mae improved from 25.40780 to 24.41350, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 24.41350 to 17.08684, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 17.08684\n",
      "\n",
      "Epoch 00031: val_mae improved from 17.08684 to 14.23931, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00032: val_mae improved from 14.23931 to 13.25086, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 13.25086 to 10.73770, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00034: val_mae improved from 10.73770 to 9.46621, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00035: val_mae improved from 9.46621 to 7.79520, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 7.79520\n",
      "\n",
      "Epoch 00037: val_mae improved from 7.79520 to 7.49020, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 7.49020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00056: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 7.49020\n",
      "\n",
      "Epoch 00216: val_mae improved from 7.49020 to 7.29294, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00217: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 7.29294\n",
      "\n",
      "Epoch 00294: val_mae improved from 7.29294 to 7.28887, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00295: val_mae improved from 7.28887 to 7.25714, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 7.25714\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 7.25714\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 7.25714\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 7.25714\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 7.25714\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 7.25714\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 7.25714\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 7.25714\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 7.25714\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 7.25714\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 7.25714\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 7.25714\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 7.25714\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 7.25714\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 7.25714\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 7.25714\n",
      "\n",
      "Epoch 00312: val_mae improved from 7.25714 to 7.09001, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 7.09001\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 7.09001\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 7.09001\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 7.09001\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 7.09001\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 7.09001\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 7.09001\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 7.09001\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 7.09001\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 7.09001\n",
      "\n",
      "Epoch 00323: val_mae improved from 7.09001 to 7.00701, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 7.00701\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 7.00701\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 7.00701\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 7.00701\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 7.00701\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 7.00701\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 7.00701\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 7.00701\n",
      "\n",
      "Epoch 00332: val_mae improved from 7.00701 to 6.93764, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 6.93764\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 6.93764\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 6.93764\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 6.93764\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 6.93764\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 6.93764\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 6.93764\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 6.93764\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 6.93764\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 6.93764\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 6.93764\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 6.93764\n",
      "\n",
      "Epoch 00345: val_mae improved from 6.93764 to 6.93042, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 6.93042\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 6.93042\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 6.93042\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 6.93042\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 6.93042\n",
      "\n",
      "Epoch 00351: val_mae improved from 6.93042 to 6.63022, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 6.63022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00356: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 6.63022\n",
      "\n",
      "Epoch 00372: val_mae improved from 6.63022 to 6.49971, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 6.49971\n",
      "\n",
      "Epoch 00464: val_mae improved from 6.49971 to 6.34580, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_4/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 6.34580\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 6.34580\n",
      "\n",
      "Lambda: 1 , Time: 0:03:59\n",
      "Train Error(all epochs): 1.9327248334884644 \n",
      " [49.215, 49.092, 49.003, 48.909, 48.8, 48.672, 48.522, 48.344, 48.125, 47.845, 47.522, 47.163, 46.752, 46.291, 45.773, 45.186, 44.547, 43.852, 43.078, 42.243, 41.342, 40.338, 39.29, 38.204, 37.024, 35.804, 34.55, 33.25, 31.931, 30.631, 29.186, 27.808, 26.376, 24.996, 23.604, 22.242, 20.946, 19.617, 18.351, 17.186, 16.153, 15.021, 14.123, 13.152, 12.466, 11.676, 11.0, 10.45, 9.944, 9.489, 9.266, 8.793, 8.252, 8.146, 7.802, 7.611, 7.293, 7.464, 7.299, 7.327, 6.739, 6.644, 6.371, 6.557, 6.233, 6.197, 6.156, 6.316, 6.232, 6.08, 6.126, 5.982, 5.932, 5.792, 5.666, 5.508, 5.248, 5.324, 5.268, 5.235, 5.151, 5.336, 5.251, 5.107, 5.149, 5.072, 4.959, 4.946, 4.949, 4.877, 4.822, 4.84, 4.678, 4.825, 5.122, 4.667, 4.792, 4.914, 4.709, 4.699, 4.525, 4.828, 4.489, 4.571, 4.25, 4.445, 4.503, 4.08, 4.225, 4.157, 4.134, 4.095, 4.131, 4.13, 3.986, 4.028, 4.165, 4.219, 4.169, 4.178, 3.914, 4.023, 4.058, 3.959, 3.856, 3.905, 4.031, 3.923, 3.683, 3.918, 3.987, 3.955, 4.167, 4.053, 4.053, 3.609, 3.768, 3.685, 3.697, 4.041, 4.143, 4.054, 4.171, 3.945, 4.001, 3.558, 3.546, 3.459, 3.411, 3.477, 3.495, 3.385, 3.344, 3.577, 3.75, 4.316, 4.349, 4.108, 3.982, 3.708, 3.776, 3.489, 3.481, 3.263, 3.461, 3.205, 3.249, 3.131, 3.075, 3.175, 3.388, 3.439, 3.542, 3.449, 3.663, 3.944, 3.647, 3.444, 3.273, 3.222, 3.407, 3.552, 3.431, 3.371, 3.618, 3.593, 3.558, 3.627, 3.513, 3.663, 3.609, 3.484, 3.45, 3.408, 3.307, 3.136, 3.031, 3.035, 3.115, 3.189, 3.305, 3.349, 3.208, 3.273, 3.248, 3.151, 3.095, 3.332, 3.383, 3.367, 3.09, 3.209, 3.226, 3.509, 3.62, 3.467, 3.415, 3.155, 3.08, 3.1, 3.074, 3.163, 3.522, 3.976, 3.64, 3.625, 3.383, 3.174, 2.984, 2.885, 2.874, 2.926, 3.089, 3.073, 3.293, 3.386, 3.369, 3.176, 3.023, 3.087, 3.101, 3.298, 3.249, 3.162, 3.095, 2.914, 2.832, 2.894, 2.736, 2.899, 2.955, 2.82, 2.911, 2.948, 2.834, 2.844, 3.067, 2.923, 3.001, 3.048, 3.071, 3.146, 3.189, 3.193, 2.961, 3.178, 3.089, 3.065, 3.166, 2.898, 2.524, 2.525, 2.442, 2.529, 2.744, 3.056, 3.04, 3.236, 3.324, 3.376, 3.602, 3.276, 3.148, 2.849, 3.021, 2.835, 2.679, 2.69, 2.757, 2.752, 2.822, 2.695, 3.065, 2.9, 3.033, 2.789, 2.65, 2.631, 2.375, 2.463, 2.288, 2.368, 2.477, 2.534, 2.782, 3.396, 3.567, 3.334, 2.943, 3.04, 3.177, 2.94, 3.042, 2.889, 2.516, 2.487, 2.384, 2.519, 2.711, 2.808, 2.755, 2.828, 2.909, 2.881, 2.996, 2.99, 2.843, 2.816, 2.709, 2.78, 2.918, 2.679, 2.595, 2.511, 2.508, 2.742, 2.955, 3.373, 3.484, 3.436, 3.265, 3.355, 3.194, 3.192, 2.886, 2.938, 2.835, 2.753, 2.727, 2.666, 2.772, 2.821, 3.148, 3.074, 3.084, 3.003, 2.734, 2.717, 2.546, 2.404, 2.246, 2.178, 2.239, 2.284, 2.639, 2.825, 2.746, 2.53, 2.662, 2.826, 3.102, 2.966, 3.068, 2.995, 3.135, 2.83, 2.848, 2.831, 2.893, 2.881, 2.499, 2.389, 2.32, 2.323, 2.658, 2.767, 2.872, 2.567, 2.822, 2.755, 2.834, 2.565, 2.613, 2.576, 2.708, 2.581, 2.554, 2.41, 2.329, 2.221, 2.295, 2.319, 2.503, 2.383, 2.311, 2.542, 2.516, 2.597, 2.564, 2.975, 2.722, 2.658, 2.762, 2.745, 2.683, 2.917, 3.169, 3.207, 3.08, 2.86, 2.807, 2.484, 2.187, 2.122, 2.14, 2.271, 2.293, 2.413, 2.424, 2.579, 2.74, 2.567, 2.409, 2.324, 2.457, 2.676, 2.805, 2.718, 2.609, 2.621, 2.618, 2.463, 2.477, 2.414, 2.37, 2.297, 2.331, 2.161, 2.216, 2.237, 2.375, 2.616, 2.624, 2.784, 3.014, 2.93, 2.634, 2.655, 2.701, 2.851, 2.592, 2.807, 2.764, 2.571, 2.412, 2.336, 2.241, 2.289, 2.242, 2.399, 2.246, 2.162, 2.083, 2.2, 2.187, 2.492, 2.529, 2.72, 2.857, 2.8, 2.633, 2.564, 2.579, 2.62, 2.423, 2.227, 2.23, 2.101, 2.324, 2.005, 1.999, 1.933, 2.106, 2.173, 2.536, 2.32, 2.383, 2.471, 2.6, 2.516]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.014, 0.019, 0.025, 0.037, 0.095, 0.105, 0.208, 0.249, 0.369, 0.391, 0.522, 0.618, 0.689, 0.819, 1.024, 1.004, 1.013, 1.225, 1.251, 1.325, 1.32, 1.647, 1.641, 1.849, 1.636, 1.711, 1.569, 1.851, 1.687, 1.675, 1.796, 1.998, 1.893, 1.847, 2.01, 1.872, 1.877, 1.906, 1.84, 1.795, 1.646, 1.625, 1.719, 1.77, 1.637, 1.767, 1.682, 1.66, 1.817, 1.766, 1.649, 1.638, 1.761, 1.572, 1.62, 1.727, 1.575, 1.65, 1.866, 1.533, 1.644, 1.891, 1.678, 1.76, 1.602, 1.714, 1.629, 1.671, 1.468, 1.662, 1.71, 1.413, 1.67, 1.58, 1.517, 1.55, 1.569, 1.551, 1.481, 1.5, 1.56, 1.615, 1.587, 1.607, 1.535, 1.608, 1.569, 1.505, 1.467, 1.488, 1.628, 1.473, 1.394, 1.528, 1.59, 1.619, 1.598, 1.614, 1.623, 1.456, 1.492, 1.514, 1.462, 1.788, 1.726, 1.673, 1.583, 1.532, 1.673, 1.412, 1.245, 1.372, 1.326, 1.394, 1.317, 1.438, 1.29, 1.504, 1.532, 1.849, 1.832, 1.752, 1.593, 1.425, 1.542, 1.486, 1.358, 1.311, 1.45, 1.251, 1.324, 1.278, 1.188, 1.209, 1.356, 1.365, 1.417, 1.341, 1.548, 1.65, 1.531, 1.427, 1.296, 1.311, 1.422, 1.412, 1.454, 1.292, 1.576, 1.463, 1.463, 1.552, 1.425, 1.475, 1.511, 1.377, 1.439, 1.416, 1.42, 1.203, 1.197, 1.195, 1.297, 1.28, 1.435, 1.358, 1.364, 1.299, 1.214, 1.314, 1.238, 1.298, 1.417, 1.37, 1.222, 1.383, 1.288, 1.49, 1.477, 1.345, 1.42, 1.209, 1.228, 1.301, 1.286, 1.276, 1.536, 1.634, 1.595, 1.473, 1.324, 1.271, 1.208, 1.109, 1.157, 1.137, 1.297, 1.304, 1.395, 1.488, 1.408, 1.262, 1.214, 1.268, 1.241, 1.352, 1.33, 1.286, 1.311, 1.161, 1.106, 1.147, 1.214, 1.139, 1.181, 1.138, 1.223, 1.273, 1.176, 1.141, 1.271, 1.182, 1.316, 1.259, 1.234, 1.252, 1.349, 1.334, 1.278, 1.293, 1.22, 1.278, 1.318, 1.221, 0.967, 1.016, 0.977, 1.105, 1.095, 1.287, 1.332, 1.331, 1.366, 1.539, 1.684, 1.363, 1.309, 1.054, 1.185, 1.146, 1.08, 1.123, 1.07, 1.196, 1.24, 1.026, 1.408, 1.151, 1.249, 1.117, 1.045, 1.116, 0.97, 0.952, 0.929, 0.992, 1.026, 1.001, 1.186, 1.473, 1.49, 1.454, 1.255, 1.199, 1.379, 1.225, 1.26, 1.213, 0.951, 1.063, 0.92, 1.085, 1.23, 1.173, 1.106, 1.136, 1.226, 1.242, 1.262, 1.273, 1.171, 1.226, 1.089, 1.169, 1.283, 1.017, 1.139, 0.971, 0.954, 1.279, 1.255, 1.495, 1.535, 1.511, 1.239, 1.549, 1.287, 1.38, 1.277, 1.156, 1.237, 1.102, 1.168, 1.099, 1.168, 1.18, 1.384, 1.24, 1.373, 1.319, 1.039, 1.178, 1.042, 0.956, 0.949, 0.925, 0.934, 0.972, 1.155, 1.246, 1.169, 1.01, 1.064, 1.204, 1.428, 1.315, 1.333, 1.165, 1.38, 1.139, 1.293, 1.257, 1.169, 1.392, 0.925, 0.926, 0.968, 0.963, 1.128, 1.19, 1.285, 1.002, 1.298, 1.083, 1.32, 1.079, 1.037, 1.068, 1.287, 1.037, 1.066, 0.981, 0.979, 0.921, 1.016, 0.935, 1.128, 0.982, 0.932, 1.151, 1.046, 1.119, 1.055, 1.416, 1.043, 1.127, 1.273, 1.142, 1.097, 1.308, 1.339, 1.454, 1.3, 1.23, 1.357, 0.942, 0.885, 0.913, 0.848, 1.042, 0.924, 1.059, 1.005, 1.111, 1.252, 1.007, 1.065, 0.923, 1.064, 1.146, 1.296, 1.185, 1.004, 1.095, 1.098, 1.13, 1.048, 1.012, 1.052, 0.909, 1.024, 0.862, 0.966, 0.954, 1.01, 1.241, 1.093, 1.245, 1.352, 1.181, 1.129, 1.157, 1.091, 1.34, 0.932, 1.255, 1.332, 1.069, 1.053, 0.887, 0.999, 1.023, 0.912, 1.11, 0.932, 0.886, 0.875, 0.97, 0.95, 1.102, 1.105, 1.167, 1.279, 1.151, 1.123, 1.153, 1.075, 1.203, 0.965, 0.918, 1.091, 0.853, 1.01, 0.862, 0.832, 0.86, 0.858, 1.001, 1.206, 0.921, 0.953, 1.187, 1.12, 1.035]\n",
      "Val Error(all epochs): 6.345803737640381 \n",
      " [49.36, 49.261, 49.144, 49.014, 48.88, 48.721, 48.611, 48.343, 47.995, 47.637, 47.252, 46.785, 46.343, 45.582, 44.588, 43.957, 43.768, 42.402, 40.591, 37.364, 36.428, 34.849, 30.838, 31.22, 28.498, 25.408, 25.931, 24.413, 17.087, 18.481, 14.239, 13.251, 10.738, 9.466, 7.795, 8.965, 7.49, 8.228, 8.798, 8.151, 9.483, 8.195, 8.567, 10.148, 14.685, 12.755, 12.182, 15.133, 18.506, 13.139, 19.416, 20.879, 15.889, 15.925, 15.862, 13.751, 12.911, 15.214, 13.124, 13.941, 12.419, 11.823, 11.478, 12.112, 11.835, 11.782, 12.576, 11.399, 11.513, 12.842, 11.41, 12.232, 12.974, 11.999, 11.532, 11.401, 11.177, 11.277, 11.549, 11.328, 11.331, 10.704, 10.872, 10.652, 11.094, 10.768, 10.67, 10.078, 10.782, 10.617, 9.452, 10.168, 10.207, 9.169, 8.392, 9.091, 8.848, 8.379, 9.157, 9.202, 10.633, 8.585, 7.79, 8.955, 8.529, 8.577, 9.133, 8.427, 10.4, 10.877, 11.1, 9.474, 9.922, 10.014, 8.877, 8.669, 8.737, 8.013, 7.87, 8.271, 7.937, 7.845, 7.97, 8.161, 9.688, 8.274, 9.341, 8.187, 8.149, 8.317, 7.709, 7.774, 8.301, 7.911, 7.978, 8.522, 8.07, 8.501, 7.817, 8.682, 9.877, 8.173, 8.243, 9.303, 8.072, 7.853, 7.726, 8.027, 7.78, 7.809, 7.717, 8.163, 8.35, 8.301, 8.656, 12.931, 15.49, 16.199, 15.92, 10.362, 11.272, 8.16, 8.173, 8.155, 8.085, 9.034, 8.754, 9.935, 12.56, 12.006, 13.441, 13.347, 10.253, 8.202, 8.038, 7.918, 8.316, 8.307, 9.18, 8.238, 8.551, 8.145, 10.129, 9.011, 8.344, 8.148, 9.009, 7.871, 8.177, 7.989, 7.616, 7.79, 8.027, 8.565, 8.47, 8.004, 9.136, 8.637, 8.065, 8.309, 8.468, 8.724, 8.903, 8.277, 8.16, 8.288, 11.378, 9.537, 11.513, 12.111, 11.788, 13.615, 11.338, 9.19, 8.885, 7.293, 7.976, 9.019, 8.944, 9.043, 8.859, 8.266, 9.428, 8.2, 8.404, 8.334, 8.637, 7.883, 7.649, 7.494, 9.258, 7.74, 8.302, 8.93, 8.287, 8.008, 7.989, 7.714, 8.363, 8.334, 8.405, 9.262, 9.322, 8.071, 9.573, 9.806, 9.86, 11.68, 11.166, 10.196, 9.257, 10.559, 8.879, 9.452, 8.076, 8.492, 8.786, 8.081, 9.504, 8.819, 8.379, 11.464, 10.657, 14.447, 18.111, 18.204, 18.727, 18.677, 12.746, 15.432, 11.316, 12.147, 11.436, 11.562, 11.612, 8.593, 8.873, 9.465, 7.932, 7.716, 7.657, 8.7, 7.476, 7.797, 7.579, 8.158, 8.075, 8.892, 8.494, 9.328, 9.657, 7.688, 8.751, 7.289, 7.257, 8.304, 8.691, 9.822, 9.524, 9.632, 9.771, 10.446, 9.849, 8.395, 7.441, 7.977, 9.387, 9.405, 9.036, 8.545, 8.399, 7.09, 7.291, 7.75, 8.254, 9.143, 8.134, 8.574, 8.074, 8.228, 7.655, 7.551, 7.007, 7.971, 7.098, 7.035, 7.751, 7.359, 7.688, 7.135, 7.397, 6.938, 7.744, 8.759, 7.372, 8.681, 7.059, 7.584, 7.475, 7.573, 7.706, 7.85, 7.378, 7.39, 6.93, 8.887, 7.468, 7.825, 7.47, 7.723, 6.63, 7.093, 6.863, 7.547, 7.729, 7.428, 7.536, 8.546, 7.589, 8.322, 8.19, 10.041, 10.302, 10.338, 9.55, 8.246, 7.933, 8.046, 9.043, 8.08, 9.135, 6.5, 7.627, 7.762, 7.852, 7.379, 7.12, 7.613, 9.039, 8.529, 9.245, 8.041, 8.781, 9.039, 7.78, 7.659, 8.841, 6.879, 9.948, 7.681, 7.441, 7.86, 7.41, 8.602, 7.615, 7.109, 7.324, 6.866, 7.106, 7.434, 7.852, 6.961, 7.98, 9.382, 7.571, 8.569, 8.388, 8.244, 7.558, 7.27, 6.945, 7.226, 8.986, 7.86, 7.95, 8.867, 6.807, 7.022, 7.021, 7.218, 7.218, 7.001, 7.36, 7.564, 7.094, 8.52, 7.491, 8.228, 7.915, 7.293, 8.269, 7.565, 7.195, 6.97, 7.079, 7.103, 7.5, 9.248, 6.824, 7.108, 6.761, 7.913, 8.051, 6.719, 8.16, 8.045, 7.482, 7.86, 8.406, 7.38, 7.563, 8.417, 7.187, 7.336, 7.305, 6.77, 7.227, 7.039, 6.73, 7.482, 7.46, 7.913, 6.908, 6.346, 6.849, 6.535, 6.717, 6.869, 6.498, 7.447, 7.349, 7.031, 8.278, 8.329, 7.512, 7.922, 7.531, 6.659, 7.04, 7.715, 8.311, 7.289, 6.775, 7.139, 7.815, 7.154, 7.837, 6.84, 8.42, 8.196, 8.501, 8.443, 8.887, 7.281, 8.262, 9.297, 6.962, 8.106, 8.326, 8.074]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.019, 0.057, 0.216, 0.2, 0.301, 0.392, 0.626, 0.93, 1.79, 1.169, 3.878, 5.499, 6.591, 4.859, 7.258, 4.41, 5.717, 8.611, 14.301, 12.013, 10.969, 14.399, 18.069, 12.247, 19.039, 20.598, 14.657, 14.537, 14.677, 11.468, 9.988, 13.122, 8.775, 8.908, 7.059, 6.772, 5.99, 7.314, 5.795, 4.901, 5.991, 4.105, 4.671, 6.267, 4.438, 5.411, 5.884, 4.386, 4.081, 3.42, 2.518, 2.36, 1.863, 2.031, 1.63, 2.027, 1.476, 1.418, 1.305, 1.694, 2.361, 1.5, 1.113, 1.668, 1.618, 1.519, 1.634, 1.828, 2.292, 1.437, 1.636, 1.574, 1.596, 1.046, 0.986, 3.02, 3.146, 1.499, 1.936, 1.781, 1.308, 1.755, 1.097, 0.88, 0.93, 1.39, 1.333, 1.141, 1.54, 3.432, 3.725, 3.448, 3.729, 2.215, 2.654, 2.94, 2.295, 2.334, 1.346, 3.135, 4.824, 2.705, 2.312, 3.44, 2.741, 3.914, 3.667, 3.513, 3.982, 3.817, 2.31, 2.907, 3.355, 4.132, 6.23, 5.595, 1.542, 1.894, 2.253, 3.606, 3.046, 3.175, 4.335, 3.292, 3.24, 2.988, 2.86, 3.369, 4.622, 9.712, 12.187, 12.873, 13.015, 6.609, 8.706, 3.839, 3.381, 3.782, 2.946, 1.772, 2.136, 1.806, 0.998, 1.222, 1.084, 0.904, 1.536, 3.069, 3.056, 3.517, 3.748, 4.105, 1.947, 2.164, 2.805, 3.027, 1.416, 1.916, 2.715, 3.133, 2.494, 2.714, 3.59, 2.339, 3.352, 3.799, 4.439, 5.522, 4.297, 3.847, 5.16, 2.565, 2.85, 3.038, 3.293, 4.127, 4.993, 3.267, 2.241, 2.404, 1.176, 1.592, 1.322, 1.15, 1.334, 0.973, 1.248, 2.344, 1.785, 4.526, 3.918, 2.218, 1.989, 2.202, 2.051, 2.36, 2.284, 5.108, 5.79, 5.76, 5.556, 3.917, 4.098, 3.264, 1.939, 2.917, 2.44, 2.417, 3.479, 4.242, 3.3, 3.033, 2.579, 2.299, 2.552, 1.895, 1.688, 2.273, 1.923, 1.598, 1.578, 1.296, 1.621, 1.572, 1.949, 1.344, 2.334, 1.746, 2.757, 2.289, 2.431, 2.257, 2.054, 1.808, 2.225, 1.215, 1.647, 1.122, 0.683, 0.832, 0.745, 0.877, 1.235, 1.024, 1.319, 1.151, 1.184, 1.303, 1.114, 2.25, 1.84, 1.73, 2.461, 2.652, 4.144, 2.28, 2.468, 3.655, 3.072, 2.306, 2.852, 1.803, 2.248, 1.842, 1.909, 2.473, 2.111, 3.075, 3.351, 2.396, 1.718, 1.62, 1.441, 1.646, 1.466, 1.327, 1.575, 2.052, 3.03, 2.283, 1.998, 1.933, 2.014, 2.283, 2.123, 3.28, 4.335, 2.411, 2.003, 2.051, 1.92, 2.098, 1.996, 2.188, 2.266, 2.411, 3.452, 1.976, 4.004, 2.783, 2.526, 2.242, 2.919, 3.739, 3.325, 3.813, 3.003, 2.261, 2.668, 2.086, 3.007, 3.996, 5.309, 2.081, 3.505, 4.186, 3.655, 2.224, 3.741, 7.161, 4.88, 5.547, 4.798, 2.65, 3.486, 4.031, 3.381, 3.22, 2.996, 2.696, 2.501, 1.916, 2.302, 1.968, 2.111, 1.643, 1.585, 1.281, 1.858, 1.68, 2.077, 1.698, 1.493, 1.826, 1.519, 2.944, 2.26, 1.825, 1.964, 2.665, 2.997, 2.642, 1.733, 2.019, 1.625, 1.88, 1.989, 1.673, 2.501, 2.382, 2.054, 2.473, 1.659, 2.01, 3.418, 2.232, 2.48, 1.657, 2.224, 2.686, 3.37, 2.741, 2.906, 1.994, 2.212, 2.676, 2.09, 1.552, 2.214, 1.876, 1.946, 1.915, 2.454, 2.713, 2.775, 2.335, 1.501, 2.076, 2.053, 1.922, 3.774, 3.867, 2.93, 4.276, 3.065, 2.817, 2.372, 2.339, 2.434, 1.862, 2.239, 2.1, 1.98, 2.625, 2.096, 2.076, 3.758, 3.213, 2.969, 2.853, 2.605, 1.593, 2.495, 2.578, 2.815, 1.757, 2.23, 2.27, 1.922, 1.752, 2.266, 1.923, 1.84, 2.22, 1.954, 1.841, 1.955, 2.759, 4.032, 3.97, 4.757, 3.017, 3.746, 2.607, 2.452, 2.014, 2.262, 4.078, 2.807, 3.578, 3.482, 2.698, 3.07, 2.444, 2.041, 2.15, 1.667, 1.748, 1.957, 1.912, 2.284, 3.103, 2.543, 2.157, 1.745, 2.136, 3.012, 2.518, 2.349, 2.068, 2.262, 2.252, 1.988, 1.713, 1.781, 1.737, 1.771, 2.077, 1.953, 1.352, 2.415, 2.028, 1.426, 1.969]\n",
      "\n",
      "#Fold: 4 \n",
      "Trainig set size: 420 , Time: 0:11:55 , best_lambda: 1 , min_  , error: 6.346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test starts:  467 , ends:  518\n",
      "1/1 [==============================] - 0s 637us/step - loss: 92.3621 - mse: 55.8002 - mae: 6.0318 - fp_mae: 2.7309\n",
      "average_error:  6.032 , fp_average_error:  2.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:00<00:00, 244.31it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 214.44it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 207.41it/s]\n",
      "100%|██████████| 107/107 [00:00<00:00, 234.01it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 215.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Fold: 5 , Training Size: 420 , Validation size: 47 , Test Size 52\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 49.67799, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 49.67799 to 49.47866, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 49.47866 to 49.20802, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 49.20802 to 48.94139, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 48.94139 to 48.65630, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 48.65630 to 48.36129, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 48.36129 to 48.01713, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 48.01713 to 47.60030, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 47.60030 to 47.15118, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 47.15118 to 46.60304, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 46.60304 to 45.98726, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 45.98726 to 45.16090, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 45.16090 to 44.64499, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 44.64499 to 43.29113, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 43.29113 to 42.08723, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 42.08723 to 41.47351, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 41.47351 to 39.59285, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 39.59285 to 38.67459, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 38.67459 to 36.81783, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 36.81783 to 35.88067, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 35.88067 to 34.96854, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 34.96854\n",
      "\n",
      "Epoch 00023: val_mae improved from 34.96854 to 33.73985, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 33.73985\n",
      "\n",
      "Epoch 00025: val_mae improved from 33.73985 to 32.40380, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 32.40380\n",
      "\n",
      "Epoch 00027: val_mae improved from 32.40380 to 29.85687, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 29.85687 to 29.49538, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 29.49538 to 26.88812, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 26.88812 to 19.08635, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 19.08635\n",
      "\n",
      "Epoch 00032: val_mae improved from 19.08635 to 15.03333, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 15.03333\n",
      "\n",
      "Epoch 00034: val_mae improved from 15.03333 to 13.26129, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00035: val_mae improved from 13.26129 to 13.22575, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00036: val_mae improved from 13.22575 to 12.51596, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 12.51596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00060: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 12.51596\n",
      "\n",
      "Epoch 00147: val_mae improved from 12.51596 to 12.50966, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 12.50966\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 12.50966\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 12.50966\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 12.50966\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 12.50966\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 12.50966\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 12.50966\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 12.50966\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 12.50966\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 12.50966\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 12.50966\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 12.50966\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 12.50966\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 12.50966\n",
      "\n",
      "Epoch 00162: val_mae improved from 12.50966 to 12.33971, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 12.33971\n",
      "\n",
      "Epoch 00164: val_mae improved from 12.33971 to 11.69656, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 11.69656\n",
      "\n",
      "Epoch 00186: val_mae improved from 11.69656 to 11.58170, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 11.58170\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 11.58170\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 11.58170\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 11.58170\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 11.58170\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 11.58170\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 11.58170\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 11.58170\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 11.58170\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 11.58170\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 11.58170\n",
      "\n",
      "Epoch 00198: val_mae improved from 11.58170 to 11.43878, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 11.43878\n",
      "\n",
      "Epoch 00200: val_mae improved from 11.43878 to 11.39821, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00201: val_mae did not improve from 11.39821\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 11.39821\n",
      "\n",
      "Epoch 00203: val_mae improved from 11.39821 to 11.38337, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 11.38337\n",
      "\n",
      "Epoch 00205: val_mae improved from 11.38337 to 11.18736, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 11.18736\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 11.18736\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 11.18736\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 11.18736\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 11.18736\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 11.18736\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 11.18736\n",
      "\n",
      "Epoch 00213: val_mae improved from 11.18736 to 11.13869, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 11.13869\n",
      "\n",
      "Epoch 00215: val_mae improved from 11.13869 to 11.05362, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 11.05362\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 11.05362\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 11.05362\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 11.05362\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 11.05362\n",
      "\n",
      "Epoch 00221: val_mae improved from 11.05362 to 10.83691, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 10.83691\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 10.83691\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 10.83691\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 10.83691\n",
      "\n",
      "Epoch 00226: val_mae improved from 10.83691 to 10.81309, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 10.81309\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 10.81309\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 10.81309\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 10.81309\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 10.81309\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 10.81309\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 10.81309\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 10.81309\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 10.81309\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 10.81309\n",
      "\n",
      "Epoch 00237: val_mae improved from 10.81309 to 10.70423, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 10.70423\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 10.70423\n",
      "\n",
      "Epoch 00240: val_mae improved from 10.70423 to 10.50410, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 10.50410\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 10.50410\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 10.50410\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 10.50410\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 10.50410\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 10.50410\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 10.50410\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 10.50410\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 10.50410\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 10.50410\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 10.50410\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 10.50410\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 10.50410\n",
      "\n",
      "Epoch 00254: val_mae improved from 10.50410 to 10.42679, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00255: val_mae improved from 10.42679 to 10.31323, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 10.31323\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 10.31323\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 10.31323\n",
      "\n",
      "Epoch 00259: val_mae improved from 10.31323 to 10.24996, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 10.24996\n",
      "\n",
      "Epoch 00261: val_mae improved from 10.24996 to 10.05627, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00262: val_mae improved from 10.05627 to 10.05415, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 10.05415\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 10.05415\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 10.05415\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 10.05415\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 10.05415\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 10.05415\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 10.05415\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 10.05415\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 10.05415\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 10.05415\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 10.05415\n",
      "\n",
      "Epoch 00274: val_mae improved from 10.05415 to 9.90036, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 9.90036\n",
      "\n",
      "Epoch 00276: val_mae improved from 9.90036 to 9.88902, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 9.88902\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 9.88902\n",
      "\n",
      "Epoch 00279: val_mae improved from 9.88902 to 9.81740, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 9.81740\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 9.81740\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 9.81740\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 9.81740\n",
      "\n",
      "Epoch 00284: val_mae improved from 9.81740 to 9.81495, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 9.81495\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 9.81495\n",
      "\n",
      "Epoch 00287: val_mae improved from 9.81495 to 9.74683, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00288: val_mae improved from 9.74683 to 9.71421, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00289: val_mae improved from 9.71421 to 9.66444, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 9.66444\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 9.66444\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 9.66444\n",
      "\n",
      "Epoch 00293: val_mae improved from 9.66444 to 9.62294, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00294: val_mae improved from 9.62294 to 9.62076, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00295: val_mae improved from 9.62076 to 9.60964, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00296: val_mae improved from 9.60964 to 9.47655, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 9.47655\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 9.47655\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 9.47655\n",
      "\n",
      "Epoch 00300: val_mae improved from 9.47655 to 9.35782, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 9.35782\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 9.35782\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 9.35782\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 9.35782\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 9.35782\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 9.35782\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 9.35782\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 9.35782\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 9.35782\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 9.35782\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 9.35782\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 9.35782\n",
      "\n",
      "Epoch 00313: val_mae improved from 9.35782 to 9.32837, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 9.32837\n",
      "\n",
      "Epoch 00315: val_mae improved from 9.32837 to 9.27088, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 9.27088\n",
      "\n",
      "Epoch 00317: val_mae improved from 9.27088 to 9.21457, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 9.21457\n",
      "\n",
      "Epoch 00319: val_mae improved from 9.21457 to 9.17854, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 9.17854\n",
      "\n",
      "Epoch 00321: val_mae improved from 9.17854 to 9.16122, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00322: val_mae improved from 9.16122 to 9.14902, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00323: val_mae improved from 9.14902 to 9.14100, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 9.14100\n",
      "\n",
      "Epoch 00325: val_mae improved from 9.14100 to 9.00963, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 9.00963\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 9.00963\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 9.00963\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 9.00963\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 9.00963\n",
      "\n",
      "Epoch 00331: val_mae improved from 9.00963 to 8.92896, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 8.92896\n",
      "\n",
      "Epoch 00333: val_mae improved from 8.92896 to 8.80478, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 8.80478\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 8.80478\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 8.80478\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 8.80478\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 8.80478\n",
      "\n",
      "Epoch 00339: val_mae improved from 8.80478 to 8.77607, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 8.77607\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 8.77607\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 8.77607\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 8.77607\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 8.77607\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 8.77607\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 8.77607\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 8.77607\n",
      "\n",
      "Epoch 00348: val_mae improved from 8.77607 to 8.77555, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 8.77555\n",
      "\n",
      "Epoch 00350: val_mae improved from 8.77555 to 8.61192, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 8.61192\n",
      "\n",
      "Epoch 00370: val_mae improved from 8.61192 to 8.55324, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00371: val_mae improved from 8.55324 to 8.42514, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 8.42514\n",
      "\n",
      "Epoch 00373: val_mae improved from 8.42514 to 8.36111, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 8.36111\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 8.36111\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 8.36111\n",
      "\n",
      "Epoch 00377: val_mae improved from 8.36111 to 8.34891, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00378: val_mae improved from 8.34891 to 8.32570, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00379: val_mae improved from 8.32570 to 8.31142, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00380: val_mae improved from 8.31142 to 8.28185, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00381: val_mae improved from 8.28185 to 8.21857, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 8.21857\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 8.21857\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 8.21857\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 8.21857\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 8.21857\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 8.21857\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 8.21857\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 8.21857\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 8.21857\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 8.21857\n",
      "\n",
      "Epoch 00392: val_mae improved from 8.21857 to 8.06787, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00393: val_mae improved from 8.06787 to 8.04217, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00394: val_mae improved from 8.04217 to 7.94854, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 7.94854\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 7.94854\n",
      "\n",
      "Epoch 00397: val_mae improved from 7.94854 to 7.83521, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 7.83521\n",
      "\n",
      "Epoch 00399: val_mae improved from 7.83521 to 7.77709, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 7.77709\n",
      "\n",
      "Epoch 00401: val_mae improved from 7.77709 to 7.74775, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 7.74775\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 7.74775\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 7.74775\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 7.74775\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 7.74775\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 7.74775\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 7.74775\n",
      "\n",
      "Epoch 00409: val_mae improved from 7.74775 to 7.67540, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 7.67540\n",
      "\n",
      "Epoch 00411: val_mae improved from 7.67540 to 7.65873, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 7.65873\n",
      "\n",
      "Epoch 00413: val_mae improved from 7.65873 to 7.61393, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 7.61393\n",
      "\n",
      "Epoch 00415: val_mae improved from 7.61393 to 7.51930, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 7.51930\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 7.51930\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 7.51930\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 7.51930\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 7.51930\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 7.51930\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 7.51930\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 7.51930\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 7.51930\n",
      "\n",
      "Epoch 00425: val_mae improved from 7.51930 to 7.46680, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 7.46680\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 7.46680\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 7.46680\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 7.46680\n",
      "\n",
      "Epoch 00430: val_mae improved from 7.46680 to 7.25375, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 7.25375\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 7.25375\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 7.25375\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 7.25375\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 7.25375\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 7.25375\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 7.25375\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 7.25375\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 7.25375\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 7.25375\n",
      "\n",
      "Epoch 00441: val_mae improved from 7.25375 to 7.22129, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 7.22129\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 7.22129\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 7.22129\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 7.22129\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 7.22129\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 7.22129\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 7.22129\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 7.22129\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 7.22129\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 7.22129\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 7.22129\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 7.22129\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 7.22129\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 7.22129\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 7.22129\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 7.22129\n",
      "\n",
      "Epoch 00458: val_mae improved from 7.22129 to 7.21412, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 7.21412\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 7.21412\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 7.21412\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 7.21412\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 7.21412\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 7.21412\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 7.21412\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 7.21412\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 7.21412\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 7.21412\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 7.21412\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 7.21412\n",
      "\n",
      "Epoch 00471: val_mae improved from 7.21412 to 7.03235, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 7.03235\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 7.03235\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 7.03235\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 7.03235\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 7.03235\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 7.03235\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 7.03235\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 7.03235\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 7.03235\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 7.03235\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 7.03235\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 7.03235\n",
      "\n",
      "Epoch 00484: val_mae improved from 7.03235 to 7.01950, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00485: val_mae improved from 7.01950 to 6.92322, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00486: val_mae improved from 6.92322 to 6.87225, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00487: val_mae improved from 6.87225 to 6.77819, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 6.77819\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 6.77819\n",
      "\n",
      "Epoch 00490: val_mae improved from 6.77819 to 6.70932, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 6.70932\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 6.70932\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 6.70932\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 6.70932\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 6.70932\n",
      "\n",
      "Epoch 00496: val_mae improved from 6.70932 to 6.70826, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 6.70826\n",
      "\n",
      "Epoch 00498: val_mae improved from 6.70826 to 6.67000, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 6.67000\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 6.67000\n",
      "\n",
      "Lambda: 0.01 , Time: 0:03:59\n",
      "Train Error(all epochs): 1.9743704795837402 \n",
      " [49.049, 48.907, 48.809, 48.709, 48.595, 48.46, 48.302, 48.12, 47.905, 47.658, 47.367, 47.041, 46.627, 46.143, 45.597, 44.985, 44.34, 43.547, 42.758, 41.846, 40.872, 39.781, 38.632, 37.371, 36.031, 34.61, 33.198, 31.692, 30.367, 28.795, 27.202, 25.706, 24.123, 22.842, 21.422, 20.374, 18.921, 17.973, 17.115, 16.433, 15.67, 15.228, 14.959, 14.495, 14.359, 14.214, 13.98, 13.896, 13.952, 13.896, 13.96, 13.664, 13.446, 13.427, 13.294, 13.308, 13.102, 13.204, 13.149, 13.213, 13.27, 13.2, 12.925, 12.929, 12.782, 12.758, 12.699, 12.673, 12.447, 12.405, 12.187, 12.129, 12.058, 12.029, 11.932, 11.992, 11.939, 11.93, 11.963, 11.949, 12.035, 12.053, 11.893, 11.644, 11.614, 11.478, 11.43, 11.251, 11.229, 11.2, 11.447, 11.435, 11.697, 11.417, 11.772, 11.42, 11.077, 10.691, 10.522, 10.084, 9.948, 9.558, 9.499, 9.153, 9.171, 8.928, 9.09, 9.033, 8.955, 8.96, 8.774, 8.669, 8.416, 8.364, 8.304, 8.335, 8.349, 8.249, 8.074, 8.245, 8.181, 8.067, 8.052, 8.028, 8.154, 8.027, 8.019, 7.923, 7.921, 7.872, 7.804, 7.764, 7.775, 7.844, 7.902, 7.787, 7.627, 7.508, 7.371, 7.293, 7.234, 7.299, 7.366, 7.382, 7.441, 7.502, 7.782, 7.665, 7.614, 7.503, 7.408, 7.309, 7.241, 7.219, 7.179, 7.043, 6.967, 7.012, 7.0, 7.023, 7.064, 7.194, 7.403, 7.617, 7.178, 7.045, 6.943, 6.827, 6.788, 6.755, 6.789, 6.783, 6.796, 6.71, 6.607, 6.587, 6.648, 6.804, 6.751, 6.768, 6.69, 6.717, 6.635, 6.646, 6.488, 6.398, 6.275, 6.197, 6.2, 6.257, 6.442, 6.501, 6.526, 6.525, 6.441, 6.45, 6.463, 6.346, 6.334, 6.164, 6.101, 6.145, 6.184, 6.226, 6.215, 6.23, 6.149, 6.096, 5.974, 5.897, 5.949, 5.871, 5.87, 5.834, 5.881, 6.043, 6.123, 6.066, 6.059, 6.087, 5.946, 5.889, 5.738, 5.683, 5.558, 5.511, 5.51, 5.597, 5.636, 5.721, 5.674, 5.67, 5.614, 5.556, 5.466, 5.468, 5.666, 5.675, 5.79, 5.785, 5.73, 5.704, 5.495, 5.367, 5.399, 5.347, 5.281, 5.142, 5.132, 5.062, 5.041, 4.981, 4.965, 4.984, 5.122, 5.106, 5.134, 5.132, 5.26, 5.453, 5.429, 5.35, 5.107, 4.986, 4.914, 4.923, 4.981, 4.99, 5.057, 5.05, 5.118, 5.04, 4.92, 4.876, 4.818, 4.751, 4.73, 4.717, 4.726, 4.76, 4.741, 4.691, 4.636, 4.558, 4.628, 4.621, 4.595, 4.635, 4.566, 4.552, 4.465, 4.54, 4.668, 4.798, 4.858, 4.736, 4.858, 4.651, 4.505, 4.406, 4.317, 4.255, 4.226, 4.246, 4.279, 4.311, 4.349, 4.359, 4.295, 4.266, 4.219, 4.15, 4.139, 4.172, 4.118, 4.119, 4.106, 4.105, 4.101, 4.117, 4.074, 4.134, 4.136, 4.176, 4.157, 4.099, 3.987, 3.849, 3.82, 3.929, 3.978, 4.019, 4.045, 4.04, 4.023, 3.909, 3.809, 3.831, 3.907, 3.834, 3.815, 3.744, 3.771, 3.767, 3.783, 3.75, 3.772, 3.777, 3.773, 3.828, 3.859, 3.918, 3.889, 3.804, 3.795, 3.766, 3.856, 3.853, 3.948, 3.845, 3.9, 3.985, 3.964, 3.819, 3.753, 3.623, 3.639, 3.55, 3.463, 3.403, 3.305, 3.285, 3.235, 3.21, 3.198, 3.233, 3.341, 3.349, 3.33, 3.358, 3.399, 3.556, 3.532, 3.597, 3.55, 3.514, 3.493, 3.392, 3.486, 3.553, 3.483, 3.375, 3.365, 3.288, 3.238, 3.193, 3.106, 3.171, 3.109, 3.037, 3.013, 3.062, 3.127, 3.161, 3.109, 2.991, 2.973, 3.016, 3.041, 3.056, 2.958, 2.85, 2.794, 2.813, 2.945, 2.993, 2.998, 2.964, 2.934, 2.8, 2.826, 2.819, 2.83, 2.79, 2.804, 2.777, 2.776, 2.739, 2.699, 2.793, 3.006, 3.014, 2.957, 2.921, 2.857, 2.749, 2.764, 2.76, 2.827, 2.865, 2.721, 2.623, 2.607, 2.632, 2.759, 2.769, 2.777, 2.713, 2.637, 2.581, 2.61, 2.548, 2.547, 2.471, 2.468, 2.506, 2.553, 2.568, 2.64, 2.582, 2.661, 2.721, 2.777, 2.741, 2.693, 2.746, 2.645, 2.502, 2.441, 2.424, 2.452, 2.424, 2.438, 2.459, 2.566, 2.482, 2.486, 2.428, 2.37, 2.406, 2.353, 2.249, 2.131, 2.118, 2.112, 2.11, 2.049, 1.998, 1.994, 2.001, 2.105, 2.148, 2.078, 2.037, 2.012, 2.009, 1.974, 1.987, 2.224, 2.564]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.005, 0.018, 0.071, 0.092, 0.064, 0.153, 0.231, 0.339, 0.335, 0.459, 0.601, 0.583, 0.74, 0.811, 0.793, 0.894, 0.998, 1.041, 1.137, 1.018, 0.947, 0.969, 0.947, 1.008, 0.921, 1.005, 0.943, 1.049, 1.067, 1.148, 0.951, 1.017, 0.875, 0.961, 0.901, 0.965, 0.877, 0.795, 0.738, 0.738, 0.691, 0.756, 0.674, 0.767, 0.698, 0.808, 0.73, 0.862, 0.893, 0.897, 0.838, 0.694, 0.651, 0.691, 0.606, 0.636, 0.55, 0.606, 0.76, 0.725, 0.928, 0.605, 0.593, 0.959, 1.329, 1.293, 1.397, 1.231, 1.252, 0.957, 0.999, 0.863, 0.881, 0.85, 0.877, 1.03, 0.826, 1.026, 0.778, 0.845, 0.65, 0.691, 0.653, 0.757, 0.658, 0.783, 0.532, 0.761, 0.693, 0.599, 0.709, 0.641, 0.712, 0.715, 0.606, 0.668, 0.607, 0.696, 0.575, 0.59, 0.634, 0.65, 0.637, 0.688, 0.49, 0.568, 0.405, 0.45, 0.386, 0.454, 0.525, 0.503, 0.64, 0.531, 0.847, 0.584, 0.71, 0.597, 0.549, 0.561, 0.512, 0.471, 0.527, 0.412, 0.437, 0.393, 0.458, 0.479, 0.479, 0.664, 0.657, 0.828, 0.663, 0.432, 0.475, 0.395, 0.429, 0.391, 0.44, 0.467, 0.427, 0.488, 0.303, 0.42, 0.379, 0.528, 0.488, 0.494, 0.511, 0.459, 0.475, 0.46, 0.394, 0.345, 0.325, 0.23, 0.325, 0.294, 0.467, 0.473, 0.451, 0.596, 0.413, 0.538, 0.47, 0.456, 0.334, 0.404, 0.319, 0.368, 0.415, 0.413, 0.461, 0.424, 0.424, 0.378, 0.343, 0.316, 0.333, 0.337, 0.289, 0.351, 0.335, 0.47, 0.484, 0.476, 0.455, 0.469, 0.393, 0.426, 0.301, 0.324, 0.244, 0.246, 0.25, 0.305, 0.36, 0.38, 0.388, 0.363, 0.344, 0.328, 0.346, 0.232, 0.482, 0.431, 0.498, 0.517, 0.478, 0.423, 0.397, 0.319, 0.324, 0.332, 0.287, 0.259, 0.221, 0.21, 0.212, 0.184, 0.216, 0.21, 0.327, 0.331, 0.294, 0.325, 0.407, 0.42, 0.549, 0.423, 0.356, 0.25, 0.232, 0.277, 0.358, 0.337, 0.374, 0.356, 0.434, 0.438, 0.285, 0.274, 0.266, 0.279, 0.247, 0.266, 0.288, 0.286, 0.322, 0.247, 0.296, 0.174, 0.312, 0.23, 0.307, 0.267, 0.291, 0.286, 0.218, 0.296, 0.346, 0.445, 0.471, 0.406, 0.499, 0.437, 0.244, 0.263, 0.225, 0.224, 0.209, 0.218, 0.276, 0.271, 0.314, 0.303, 0.285, 0.29, 0.261, 0.234, 0.254, 0.267, 0.268, 0.232, 0.253, 0.271, 0.261, 0.27, 0.27, 0.318, 0.312, 0.327, 0.345, 0.343, 0.272, 0.193, 0.178, 0.281, 0.294, 0.34, 0.338, 0.36, 0.316, 0.321, 0.225, 0.246, 0.364, 0.253, 0.327, 0.258, 0.293, 0.274, 0.332, 0.251, 0.35, 0.273, 0.354, 0.343, 0.429, 0.376, 0.446, 0.349, 0.384, 0.344, 0.443, 0.392, 0.562, 0.418, 0.479, 0.541, 0.563, 0.437, 0.446, 0.398, 0.403, 0.363, 0.262, 0.339, 0.22, 0.258, 0.215, 0.208, 0.249, 0.22, 0.341, 0.315, 0.296, 0.367, 0.313, 0.536, 0.397, 0.486, 0.449, 0.47, 0.491, 0.331, 0.464, 0.529, 0.482, 0.427, 0.39, 0.419, 0.351, 0.396, 0.329, 0.396, 0.329, 0.279, 0.327, 0.34, 0.377, 0.405, 0.346, 0.323, 0.304, 0.346, 0.366, 0.35, 0.354, 0.242, 0.254, 0.25, 0.414, 0.394, 0.324, 0.383, 0.415, 0.266, 0.36, 0.315, 0.371, 0.267, 0.365, 0.26, 0.355, 0.305, 0.319, 0.315, 0.515, 0.438, 0.525, 0.361, 0.43, 0.345, 0.405, 0.4, 0.354, 0.524, 0.35, 0.334, 0.332, 0.359, 0.409, 0.402, 0.441, 0.425, 0.364, 0.309, 0.406, 0.347, 0.41, 0.306, 0.326, 0.343, 0.395, 0.374, 0.464, 0.374, 0.518, 0.448, 0.592, 0.488, 0.44, 0.487, 0.519, 0.323, 0.434, 0.317, 0.423, 0.35, 0.426, 0.361, 0.559, 0.329, 0.507, 0.387, 0.364, 0.407, 0.415, 0.327, 0.262, 0.286, 0.283, 0.27, 0.312, 0.205, 0.25, 0.224, 0.313, 0.346, 0.271, 0.282, 0.26, 0.295, 0.268, 0.276, 0.391, 0.536]\n",
      "Val Error(all epochs): 6.670004844665527 \n",
      " [49.678, 49.479, 49.208, 48.941, 48.656, 48.361, 48.017, 47.6, 47.151, 46.603, 45.987, 45.161, 44.645, 43.291, 42.087, 41.474, 39.593, 38.675, 36.818, 35.881, 34.969, 35.092, 33.74, 33.744, 32.404, 32.549, 29.857, 29.495, 26.888, 19.086, 20.636, 15.033, 18.224, 13.261, 13.226, 12.516, 13.192, 13.514, 14.822, 14.328, 14.459, 15.491, 15.141, 15.562, 15.968, 15.446, 15.089, 15.623, 15.443, 15.126, 15.043, 15.339, 15.07, 15.143, 14.428, 15.412, 15.354, 15.705, 15.181, 16.168, 16.226, 16.657, 16.331, 16.228, 16.23, 16.133, 16.23, 16.646, 16.109, 16.184, 16.227, 16.581, 16.204, 16.478, 16.364, 16.605, 16.266, 16.4, 16.484, 16.309, 17.089, 15.965, 16.401, 16.618, 16.237, 16.451, 16.313, 16.219, 16.789, 15.886, 16.457, 15.875, 15.768, 17.037, 14.999, 18.632, 22.443, 24.325, 22.155, 21.816, 21.087, 20.213, 21.386, 20.562, 21.012, 19.493, 19.402, 19.156, 19.152, 18.247, 16.705, 15.745, 14.658, 13.934, 14.37, 13.337, 13.449, 13.076, 13.164, 12.812, 13.101, 12.664, 13.358, 12.971, 13.105, 13.278, 13.201, 13.21, 14.137, 13.024, 14.065, 13.35, 13.838, 13.642, 13.146, 13.418, 13.289, 13.505, 13.89, 13.541, 13.906, 13.688, 14.04, 13.58, 14.22, 13.509, 12.51, 13.368, 13.36, 13.56, 13.788, 13.394, 13.415, 13.253, 13.453, 12.783, 13.04, 13.376, 12.738, 13.042, 12.822, 12.34, 12.553, 11.697, 12.422, 11.952, 12.253, 12.249, 12.245, 12.099, 11.955, 12.221, 12.181, 12.104, 12.25, 12.044, 12.115, 11.872, 11.817, 11.704, 11.738, 11.771, 11.76, 12.108, 11.847, 11.582, 11.897, 11.672, 11.803, 11.831, 11.742, 11.964, 11.595, 11.889, 11.757, 11.953, 11.928, 11.439, 11.573, 11.398, 11.667, 11.619, 11.383, 11.762, 11.187, 11.941, 11.265, 12.111, 11.957, 11.808, 11.74, 11.307, 11.139, 11.179, 11.054, 11.355, 11.723, 11.268, 11.191, 11.791, 10.837, 11.177, 10.857, 10.861, 10.85, 10.813, 10.919, 10.944, 10.862, 11.055, 11.067, 11.223, 11.275, 11.003, 10.826, 10.959, 10.704, 10.765, 10.829, 10.504, 11.071, 10.69, 10.938, 10.761, 10.599, 10.527, 10.698, 10.556, 10.538, 10.615, 10.563, 10.567, 10.517, 10.427, 10.313, 10.504, 10.333, 10.351, 10.25, 10.254, 10.056, 10.054, 10.319, 10.32, 10.148, 10.309, 10.263, 10.417, 10.461, 10.203, 10.171, 10.349, 10.185, 9.9, 10.159, 9.889, 10.034, 9.945, 9.817, 10.002, 9.848, 10.125, 10.007, 9.815, 10.032, 9.835, 9.747, 9.714, 9.664, 9.802, 9.88, 9.889, 9.623, 9.621, 9.61, 9.477, 9.502, 9.818, 9.619, 9.358, 9.459, 9.474, 9.559, 9.554, 9.391, 9.587, 9.454, 9.802, 9.834, 9.788, 9.711, 9.602, 9.328, 9.592, 9.271, 9.552, 9.215, 9.382, 9.179, 9.183, 9.161, 9.149, 9.141, 9.209, 9.01, 9.058, 9.212, 9.045, 9.031, 9.021, 8.929, 9.145, 8.805, 9.193, 8.882, 9.017, 8.936, 8.986, 8.776, 8.944, 8.94, 8.892, 8.802, 8.799, 8.901, 8.884, 8.87, 8.776, 8.934, 8.612, 8.982, 9.08, 8.975, 9.076, 10.022, 10.476, 10.115, 9.702, 9.89, 9.837, 9.545, 9.457, 9.122, 9.232, 9.277, 8.801, 9.415, 9.047, 8.76, 8.553, 8.425, 8.597, 8.361, 8.487, 8.373, 8.524, 8.349, 8.326, 8.311, 8.282, 8.219, 8.302, 8.263, 8.371, 8.226, 8.305, 8.335, 8.34, 8.257, 8.274, 8.282, 8.068, 8.042, 7.949, 7.986, 8.081, 7.835, 7.929, 7.777, 7.926, 7.748, 7.962, 7.974, 7.938, 7.825, 7.937, 7.783, 8.0, 7.675, 7.934, 7.659, 7.821, 7.614, 7.721, 7.519, 7.97, 7.7, 7.791, 7.613, 7.757, 7.643, 7.568, 7.585, 7.683, 7.467, 7.599, 7.645, 7.467, 7.557, 7.254, 7.509, 7.551, 7.732, 7.54, 7.598, 7.434, 7.545, 7.28, 7.307, 7.335, 7.221, 7.519, 7.247, 7.453, 7.375, 7.305, 7.368, 7.445, 7.348, 7.37, 7.369, 7.279, 7.267, 7.298, 7.3, 7.224, 7.318, 7.214, 7.491, 8.412, 8.065, 8.827, 8.388, 7.866, 7.375, 8.67, 8.211, 7.99, 7.451, 7.303, 7.032, 7.439, 7.192, 7.444, 7.437, 7.571, 7.754, 7.458, 7.296, 7.98, 7.774, 7.522, 7.101, 7.02, 6.923, 6.872, 6.778, 6.792, 6.817, 6.709, 6.868, 6.784, 6.994, 6.77, 6.88, 6.708, 6.763, 6.67, 6.767, 6.97]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29, 0.079, 1.61, 2.12, 3.012, 6.139, 6.397, 5.626, 4.535, 3.71, 3.06, 2.682, 2.446, 1.837, 1.975, 2.327, 2.007, 2.592, 1.997, 4.262, 2.955, 2.57, 1.798, 2.386, 2.484, 3.2, 1.513, 2.625, 1.688, 4.394, 4.312, 4.529, 3.45, 3.619, 2.391, 2.375, 3.633, 2.41, 2.175, 2.091, 1.357, 1.87, 1.4, 1.591, 1.209, 1.83, 1.317, 1.375, 1.5, 0.584, 2.197, 1.822, 0.905, 1.88, 1.387, 1.477, 1.397, 0.943, 1.815, 1.302, 1.862, 1.941, 1.266, 0.649, 10.398, 14.287, 16.238, 14.027, 13.75, 12.95, 11.99, 13.246, 12.386, 12.832, 11.331, 11.054, 11.047, 10.79, 9.97, 8.109, 7.022, 5.782, 4.828, 5.36, 3.972, 4.148, 3.661, 3.518, 3.39, 1.918, 1.986, 1.249, 1.862, 1.475, 1.18, 1.415, 1.272, 0.668, 1.24, 0.712, 0.848, 0.86, 0.617, 1.394, 1.046, 0.905, 0.975, 0.537, 0.87, 0.534, 0.63, 0.553, 0.866, 0.374, 0.971, 1.437, 0.692, 0.625, 0.852, 0.514, 0.772, 0.742, 0.83, 0.578, 1.157, 0.863, 0.751, 1.02, 1.029, 0.965, 1.328, 1.573, 1.933, 1.607, 1.495, 1.723, 1.173, 1.51, 1.658, 1.586, 1.607, 1.258, 1.532, 1.263, 1.553, 1.225, 1.704, 1.671, 1.948, 2.191, 1.483, 2.251, 1.228, 1.76, 1.498, 1.551, 1.361, 1.489, 1.288, 1.431, 1.1, 1.705, 1.272, 1.573, 1.07, 1.387, 1.478, 1.559, 1.695, 1.277, 1.461, 1.345, 1.132, 1.786, 0.98, 1.426, 0.834, 0.856, 0.972, 1.07, 1.403, 1.752, 1.976, 1.802, 1.583, 0.994, 1.156, 1.532, 0.645, 2.076, 1.456, 1.652, 1.873, 1.689, 1.877, 1.433, 1.537, 1.687, 1.358, 1.476, 1.178, 1.263, 1.407, 1.497, 1.467, 1.498, 1.601, 1.266, 2.019, 1.088, 1.594, 1.259, 1.3, 1.295, 1.652, 1.36, 1.555, 1.551, 1.599, 1.416, 1.721, 1.49, 1.942, 1.755, 1.871, 2.008, 2.577, 1.539, 2.334, 2.107, 1.978, 1.74, 1.476, 2.178, 1.403, 1.448, 1.321, 1.236, 1.837, 1.399, 1.476, 1.206, 2.005, 1.323, 2.09, 1.552, 1.912, 1.934, 1.64, 1.748, 1.403, 1.314, 1.751, 1.162, 1.587, 1.95, 1.72, 2.127, 1.714, 1.58, 1.19, 2.019, 1.75, 2.113, 2.652, 1.517, 1.426, 1.607, 1.958, 1.58, 1.655, 1.285, 1.477, 1.547, 1.3, 1.423, 1.127, 0.902, 1.103, 1.083, 1.139, 1.576, 1.092, 1.572, 1.124, 1.536, 1.36, 1.4, 1.74, 1.691, 1.861, 1.484, 1.532, 1.688, 1.567, 1.559, 1.501, 2.142, 1.639, 1.925, 1.705, 2.166, 1.73, 1.771, 2.164, 1.828, 2.109, 1.807, 1.985, 1.303, 1.618, 1.467, 1.797, 1.58, 1.642, 1.691, 1.67, 1.616, 1.777, 1.454, 1.375, 1.356, 1.058, 0.631, 0.467, 0.543, 0.881, 0.665, 0.837, 0.755, 0.731, 0.746, 0.899, 0.878, 0.973, 1.048, 0.826, 1.193, 1.204, 1.466, 1.279, 1.427, 1.439, 1.417, 1.413, 1.482, 1.62, 1.673, 1.643, 1.711, 1.791, 1.807, 1.467, 1.736, 1.774, 1.452, 1.51, 1.71, 1.931, 1.88, 2.061, 1.993, 2.094, 2.163, 2.102, 2.229, 1.847, 1.847, 1.853, 1.9, 1.761, 1.763, 1.825, 1.74, 1.726, 1.958, 1.868, 2.001, 1.889, 1.694, 1.825, 1.85, 1.755, 1.859, 1.578, 1.641, 1.959, 1.774, 1.785, 1.815, 1.844, 1.721, 1.628, 1.759, 1.899, 1.619, 1.901, 1.645, 2.181, 1.679, 1.575, 1.55, 1.729, 1.668, 1.621, 1.8, 1.635, 1.953, 1.675, 1.882, 1.654, 1.818, 1.761, 1.815, 1.558, 1.831, 1.555, 1.698, 1.674, 1.823, 1.594, 1.843, 1.653, 1.708, 1.577, 1.835, 1.584, 1.627, 0.709, 0.97, 0.419, 1.085, 0.675, 1.695, 0.554, 0.86, 0.859, 1.414, 1.176, 1.917, 1.119, 1.582, 1.129, 1.511, 0.987, 1.076, 1.028, 1.536, 0.642, 1.025, 1.113, 1.492, 1.566, 1.785, 1.879, 2.055, 2.393, 1.929, 2.119, 1.635, 1.807, 1.432, 1.865, 1.681, 2.039, 1.844, 2.11, 1.853, 2.587]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 49.65973, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 49.65973 to 49.58510, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 49.58510 to 49.48364, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 49.48364 to 49.41264, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 49.41264 to 49.31953, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 49.31953 to 49.24740, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 49.24740 to 49.14956, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 49.14956 to 49.12366, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 49.12366 to 48.88192, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00010: val_mae did not improve from 48.88192\n",
      "\n",
      "Epoch 00011: val_mae improved from 48.88192 to 48.41215, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00012: val_mae did not improve from 48.41215\n",
      "\n",
      "Epoch 00013: val_mae improved from 48.41215 to 47.75066, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00014: val_mae did not improve from 47.75066\n",
      "\n",
      "Epoch 00015: val_mae improved from 47.75066 to 46.79343, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 46.79343 to 46.78011, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 46.78011 to 45.92989, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 45.92989 to 45.52964, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 45.52964 to 44.48729, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 44.48729 to 43.81350, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 43.81350 to 43.01876, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 43.01876 to 42.20507, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 42.20507 to 41.04264, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 41.04264 to 40.13318, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 40.13318 to 39.33600, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 39.33600 to 38.41286, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 38.41286 to 37.11479, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 37.11479 to 36.45418, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 36.45418 to 36.24083, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 36.24083 to 34.24934, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 34.24934\n",
      "\n",
      "Epoch 00032: val_mae improved from 34.24934 to 32.89389, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 32.89389 to 32.52532, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00034: val_mae improved from 32.52532 to 29.43864, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 29.43864\n",
      "\n",
      "Epoch 00036: val_mae improved from 29.43864 to 28.51866, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00037: val_mae improved from 28.51866 to 28.25049, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00038: val_mae improved from 28.25049 to 26.70116, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00039: val_mae improved from 26.70116 to 25.34099, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00040: val_mae improved from 25.34099 to 24.30867, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 24.30867\n",
      "\n",
      "Epoch 00042: val_mae improved from 24.30867 to 22.90958, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 22.90958\n",
      "\n",
      "Epoch 00044: val_mae improved from 22.90958 to 19.89454, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00045: val_mae improved from 19.89454 to 17.54720, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00046: val_mae improved from 17.54720 to 15.25023, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00047: val_mae improved from 15.25023 to 14.11033, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 14.11033\n",
      "\n",
      "Epoch 00070: val_mae improved from 14.11033 to 13.75519, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 13.75519\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 13.75519\n",
      "\n",
      "Epoch 00073: val_mae improved from 13.75519 to 13.05971, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 13.05971\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 13.05971\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 13.05971\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 13.05971\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 13.05971\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 13.05971\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 13.05971\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 13.05971\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 13.05971\n",
      "\n",
      "Epoch 00083: val_mae improved from 13.05971 to 11.71013, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 11.71013\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 11.71013\n",
      "\n",
      "Epoch 00086: val_mae improved from 11.71013 to 11.49567, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00087: val_mae improved from 11.49567 to 10.29899, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 10.29899\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 10.29899\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 10.29899\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 10.29899\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 10.29899\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 10.29899\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 10.29899\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 10.29899\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 10.29899\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 10.29899\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 10.29899\n",
      "\n",
      "Epoch 00099: val_mae improved from 10.29899 to 8.94209, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 8.94209\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 8.94209\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 8.94209\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 8.94209\n",
      "\n",
      "Epoch 00104: val_mae improved from 8.94209 to 8.28616, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00105: val_mae improved from 8.28616 to 7.60299, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 7.60299\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 7.60299\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 7.60299\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 7.60299\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 7.60299\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 7.60299\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 7.60299\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 7.60299\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 7.60299\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 7.60299\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 7.60299\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 7.60299\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 7.60299\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 7.60299\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 7.60299\n",
      "\n",
      "Epoch 00121: val_mae improved from 7.60299 to 7.58137, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 7.58137\n",
      "\n",
      "Epoch 00123: val_mae improved from 7.58137 to 6.72857, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 6.72857\n",
      "\n",
      "Epoch 00148: val_mae improved from 6.72857 to 6.65228, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 6.65228\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 6.65228\n",
      "\n",
      "Epoch 00151: val_mae improved from 6.65228 to 6.59445, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 6.59445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00157: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 6.59445\n",
      "\n",
      "Epoch 00182: val_mae improved from 6.59445 to 6.55216, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 6.55216\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 6.55216\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 6.55216\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 6.55216\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 6.55216\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 6.55216\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 6.55216\n",
      "\n",
      "Epoch 00190: val_mae improved from 6.55216 to 6.50850, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 6.50850\n",
      "\n",
      "Epoch 00270: val_mae improved from 6.50850 to 6.48696, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 6.48696\n",
      "\n",
      "Epoch 00307: val_mae improved from 6.48696 to 6.48413, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00308: val_mae improved from 6.48413 to 6.45598, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 6.45598\n",
      "\n",
      "Epoch 00310: val_mae improved from 6.45598 to 6.32611, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 6.32611\n",
      "\n",
      "Epoch 00375: val_mae improved from 6.32611 to 6.21859, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 6.21859\n",
      "\n",
      "Epoch 00447: val_mae improved from 6.21859 to 6.19296, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 6.19296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00456: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 6.19296\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 6.19296\n",
      "\n",
      "Lambda: 0.1 , Time: 0:03:57\n",
      "Train Error(all epochs): 0.8009127378463745 \n",
      " [49.109, 49.045, 48.986, 48.919, 48.842, 48.745, 48.63, 48.493, 48.322, 48.12, 47.871, 47.59, 47.24, 46.85, 46.404, 45.891, 45.264, 44.584, 43.851, 43.068, 42.242, 41.375, 40.478, 39.549, 38.58, 37.577, 36.564, 35.52, 34.457, 33.384, 32.297, 31.181, 30.058, 28.912, 27.758, 26.597, 25.424, 24.258, 23.068, 21.931, 20.741, 19.609, 18.463, 17.384, 16.3, 15.261, 14.223, 13.187, 12.305, 11.402, 10.601, 9.895, 9.159, 8.589, 7.993, 7.352, 6.837, 6.299, 5.925, 5.705, 5.388, 5.308, 5.396, 4.971, 4.757, 4.524, 4.364, 4.41, 4.366, 4.497, 4.837, 4.506, 4.426, 4.389, 4.508, 4.598, 4.192, 4.039, 4.082, 3.956, 3.852, 3.781, 3.722, 3.549, 3.748, 3.672, 3.919, 4.063, 3.88, 3.831, 3.49, 3.513, 3.386, 3.477, 3.403, 3.399, 3.396, 3.341, 3.488, 3.464, 3.469, 3.69, 3.417, 3.399, 3.342, 3.395, 3.57, 3.541, 3.387, 3.136, 3.145, 3.087, 2.93, 2.928, 2.763, 2.765, 2.535, 2.56, 2.407, 2.512, 2.584, 2.58, 2.718, 2.668, 2.481, 2.495, 2.47, 2.595, 2.601, 2.684, 2.979, 3.177, 2.875, 2.792, 2.55, 2.435, 2.53, 2.51, 2.592, 2.403, 2.379, 2.274, 2.334, 2.529, 2.507, 2.594, 2.509, 2.531, 2.302, 2.096, 1.983, 2.05, 2.039, 2.237, 2.384, 2.326, 2.388, 2.21, 2.058, 2.024, 1.869, 1.817, 1.785, 1.736, 1.679, 1.831, 1.992, 2.178, 2.212, 1.935, 1.854, 1.723, 1.75, 1.808, 1.714, 1.887, 1.833, 1.914, 2.016, 2.01, 2.109, 2.05, 1.929, 1.781, 1.856, 1.807, 1.84, 1.988, 1.941, 1.934, 2.079, 1.823, 1.794, 1.616, 1.633, 1.505, 1.67, 1.766, 1.807, 1.902, 1.881, 1.863, 1.994, 1.854, 1.739, 1.727, 1.619, 1.623, 1.764, 1.856, 1.858, 1.968, 1.882, 1.733, 1.609, 1.573, 1.604, 1.614, 1.536, 1.566, 1.563, 1.841, 1.759, 1.962, 1.853, 1.843, 1.588, 1.557, 1.628, 1.552, 1.711, 1.557, 1.45, 1.383, 1.496, 1.626, 1.708, 1.765, 1.925, 1.971, 2.155, 2.005, 2.045, 2.134, 2.076, 2.055, 1.742, 1.641, 1.399, 1.312, 1.326, 1.164, 1.18, 1.186, 1.282, 1.256, 1.42, 1.608, 1.906, 2.064, 1.841, 1.662, 1.657, 1.715, 1.595, 1.501, 1.457, 1.539, 1.585, 1.471, 1.384, 1.292, 1.244, 1.227, 1.4, 1.395, 1.45, 1.46, 1.437, 1.491, 1.35, 1.39, 1.487, 1.51, 1.636, 1.577, 1.589, 1.658, 1.709, 1.654, 1.638, 1.56, 1.683, 1.714, 1.545, 1.581, 1.444, 1.58, 1.718, 1.603, 1.633, 1.487, 1.34, 1.324, 1.429, 1.469, 1.556, 1.594, 1.446, 1.527, 1.493, 1.351, 1.319, 1.283, 1.206, 1.174, 1.15, 1.096, 1.07, 1.074, 1.037, 1.108, 1.145, 1.407, 1.54, 1.748, 1.687, 1.472, 1.504, 1.557, 1.647, 1.907, 2.064, 2.336, 2.194, 2.275, 1.868, 1.782, 1.577, 1.6, 1.58, 1.384, 1.32, 1.322, 1.391, 1.465, 1.376, 1.386, 1.291, 1.348, 1.296, 1.35, 1.48, 1.467, 1.494, 1.477, 1.415, 1.195, 1.177, 1.145, 1.127, 1.256, 1.25, 1.351, 1.331, 1.422, 1.344, 1.345, 1.399, 1.343, 1.294, 1.319, 1.415, 1.408, 1.397, 1.369, 1.289, 1.379, 1.329, 1.451, 1.606, 1.661, 1.872, 1.777, 1.792, 1.917, 1.97, 1.765, 1.631, 1.521, 1.445, 1.369, 1.434, 1.435, 1.535, 1.36, 1.492, 1.429, 1.413, 1.536, 1.679, 1.768, 1.667, 1.724, 1.675, 1.624, 1.498, 1.413, 1.54, 1.393, 1.463, 1.247, 1.153, 1.087, 1.009, 0.978, 0.832, 0.803, 0.801, 0.9, 1.014, 1.153, 1.143, 1.291, 1.445, 1.525, 1.614, 1.469, 1.482, 1.427, 1.547, 1.428, 1.399, 1.338, 1.543, 1.702, 1.731, 1.908, 1.725, 1.751, 1.625, 1.83, 1.705, 1.579, 1.6, 1.536, 1.442, 1.309, 1.27, 1.163, 1.156, 1.113, 1.099, 1.189, 1.192, 1.348, 1.263, 1.314, 1.256, 1.258, 1.284, 1.173, 1.213, 1.163, 1.21, 1.198, 1.256, 1.481, 1.481, 1.598, 1.634, 1.773, 1.829, 1.864, 1.575, 1.766, 1.728, 1.697, 1.567, 1.566, 1.527, 1.595, 1.304, 1.276, 1.229, 1.268, 1.297, 1.367, 1.247, 1.291, 1.193, 1.204, 1.182, 1.239, 1.251, 1.201, 1.246, 1.481, 1.657, 1.695]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.0, 0.003, 0.001, 0.007, 0.012, 0.02, 0.04, 0.06, 0.067, 0.077, 0.111, 0.13, 0.221, 0.258, 0.359, 0.427, 0.437, 0.457, 0.483, 0.574, 0.66, 0.772, 0.921, 1.095, 1.049, 1.124, 1.094, 1.157, 1.283, 1.342, 1.484, 1.817, 1.603, 1.68, 1.543, 1.766, 1.793, 1.649, 1.565, 1.579, 1.552, 1.424, 1.521, 1.469, 1.355, 1.517, 1.415, 1.627, 1.708, 1.569, 1.658, 1.4, 1.391, 1.318, 1.407, 1.388, 1.304, 1.396, 1.348, 1.327, 1.465, 1.513, 1.582, 1.378, 1.415, 1.33, 1.591, 1.492, 1.57, 1.382, 1.237, 1.207, 1.306, 1.153, 1.15, 1.044, 1.216, 0.936, 1.038, 0.863, 1.06, 1.012, 1.092, 1.094, 1.176, 0.932, 1.025, 1.003, 1.131, 1.108, 1.193, 1.284, 1.535, 1.158, 1.304, 0.99, 1.064, 0.99, 1.084, 1.085, 0.991, 1.011, 0.928, 0.969, 1.147, 1.073, 1.096, 1.212, 1.012, 1.1, 0.911, 0.712, 0.985, 0.72, 1.091, 1.075, 0.93, 1.202, 0.86, 0.898, 0.898, 0.747, 0.774, 0.747, 0.76, 0.677, 0.878, 0.836, 1.046, 1.001, 0.811, 0.828, 0.767, 0.684, 0.927, 0.64, 0.956, 0.733, 0.887, 0.931, 0.875, 0.977, 0.897, 0.913, 0.718, 0.841, 0.872, 0.767, 0.989, 0.842, 0.82, 1.076, 0.708, 0.901, 0.675, 0.709, 0.734, 0.654, 0.927, 0.755, 0.883, 0.973, 0.702, 1.034, 0.768, 0.795, 0.768, 0.763, 0.675, 0.841, 0.911, 0.73, 1.038, 0.812, 0.822, 0.747, 0.7, 0.746, 0.737, 0.678, 0.743, 0.695, 0.87, 0.821, 0.904, 0.826, 0.905, 0.69, 0.687, 0.761, 0.625, 0.895, 0.596, 0.705, 0.661, 0.624, 0.837, 0.801, 0.733, 1.002, 0.798, 1.175, 0.896, 0.93, 1.111, 0.815, 1.137, 0.638, 0.78, 0.582, 0.572, 0.649, 0.467, 0.556, 0.524, 0.572, 0.626, 0.621, 0.823, 0.912, 0.929, 0.982, 0.686, 0.748, 0.808, 0.768, 0.66, 0.677, 0.755, 0.688, 0.742, 0.574, 0.6, 0.621, 0.561, 0.652, 0.673, 0.63, 0.742, 0.613, 0.699, 0.667, 0.576, 0.734, 0.712, 0.732, 0.816, 0.681, 0.752, 0.927, 0.613, 0.902, 0.739, 0.699, 0.919, 0.617, 0.829, 0.564, 0.759, 0.843, 0.687, 0.758, 0.704, 0.609, 0.567, 0.727, 0.717, 0.707, 0.844, 0.599, 0.744, 0.71, 0.629, 0.586, 0.626, 0.498, 0.578, 0.496, 0.515, 0.478, 0.504, 0.455, 0.524, 0.536, 0.723, 0.744, 0.871, 0.779, 0.691, 0.68, 0.718, 0.827, 0.868, 1.046, 1.175, 0.87, 1.204, 0.787, 0.813, 0.801, 0.586, 0.898, 0.509, 0.604, 0.653, 0.543, 0.812, 0.563, 0.7, 0.573, 0.655, 0.582, 0.655, 0.665, 0.699, 0.737, 0.604, 0.773, 0.43, 0.584, 0.55, 0.443, 0.694, 0.446, 0.759, 0.526, 0.716, 0.624, 0.664, 0.632, 0.685, 0.543, 0.667, 0.615, 0.655, 0.664, 0.668, 0.519, 0.733, 0.553, 0.73, 0.785, 0.712, 1.035, 0.704, 0.951, 0.813, 0.928, 0.89, 0.611, 0.807, 0.639, 0.619, 0.724, 0.58, 0.86, 0.517, 0.772, 0.624, 0.731, 0.652, 0.84, 0.85, 0.693, 0.967, 0.631, 0.933, 0.626, 0.645, 0.792, 0.537, 0.765, 0.509, 0.544, 0.506, 0.431, 0.487, 0.342, 0.391, 0.354, 0.427, 0.476, 0.541, 0.574, 0.529, 0.807, 0.57, 0.916, 0.562, 0.801, 0.643, 0.726, 0.709, 0.602, 0.687, 0.699, 0.844, 0.798, 0.898, 0.823, 0.87, 0.647, 0.99, 0.781, 0.71, 0.806, 0.609, 0.773, 0.534, 0.619, 0.532, 0.549, 0.489, 0.531, 0.571, 0.514, 0.729, 0.523, 0.651, 0.615, 0.469, 0.774, 0.36, 0.736, 0.417, 0.631, 0.566, 0.517, 0.793, 0.618, 0.837, 0.705, 0.835, 0.85, 0.867, 0.72, 0.836, 0.842, 0.766, 0.738, 0.778, 0.666, 0.815, 0.583, 0.616, 0.531, 0.664, 0.502, 0.747, 0.486, 0.646, 0.586, 0.501, 0.591, 0.6, 0.484, 0.691, 0.511, 0.722, 0.838, 0.727]\n",
      "Val Error(all epochs): 6.192959308624268 \n",
      " [49.66, 49.585, 49.484, 49.413, 49.32, 49.247, 49.15, 49.124, 48.882, 48.949, 48.412, 48.467, 47.751, 48.0, 46.793, 46.78, 45.93, 45.53, 44.487, 43.814, 43.019, 42.205, 41.043, 40.133, 39.336, 38.413, 37.115, 36.454, 36.241, 34.249, 34.955, 32.894, 32.525, 29.439, 30.471, 28.519, 28.25, 26.701, 25.341, 24.309, 24.479, 22.91, 23.575, 19.895, 17.547, 15.25, 14.11, 14.21, 15.236, 14.895, 15.485, 14.578, 15.079, 14.94, 15.276, 15.584, 15.929, 15.198, 14.928, 15.413, 14.657, 15.55, 15.261, 14.567, 14.429, 14.594, 15.075, 14.276, 15.283, 13.755, 15.344, 15.728, 13.06, 13.518, 14.539, 16.059, 16.263, 17.113, 16.314, 15.165, 13.732, 13.092, 11.71, 11.919, 11.878, 11.496, 10.299, 10.503, 10.693, 11.615, 12.156, 11.416, 12.001, 10.474, 11.243, 11.129, 11.38, 10.9, 8.942, 11.014, 11.903, 12.428, 10.907, 8.286, 7.603, 7.785, 9.458, 10.888, 8.978, 8.728, 9.322, 9.255, 8.795, 8.644, 7.949, 8.083, 8.105, 7.809, 7.897, 7.837, 7.581, 7.749, 6.729, 7.1, 7.266, 7.056, 7.611, 7.124, 7.28, 7.396, 7.44, 8.128, 7.687, 7.658, 8.632, 8.01, 8.075, 7.39, 7.575, 8.062, 8.568, 9.287, 7.559, 7.402, 7.306, 6.946, 7.315, 6.652, 7.063, 6.958, 6.594, 7.281, 7.078, 7.469, 7.578, 7.131, 7.957, 9.16, 7.238, 7.133, 6.93, 7.087, 7.104, 7.18, 7.293, 7.35, 7.485, 6.907, 7.352, 7.108, 7.299, 7.415, 7.128, 7.28, 7.191, 7.203, 7.348, 7.317, 7.058, 6.88, 6.952, 6.552, 6.687, 6.696, 6.898, 6.927, 6.758, 6.833, 7.387, 6.508, 6.804, 6.741, 6.646, 7.313, 6.744, 7.435, 6.597, 7.208, 6.924, 6.911, 7.202, 7.375, 7.301, 7.934, 8.245, 8.465, 8.449, 7.433, 7.078, 7.012, 6.928, 7.326, 7.508, 7.011, 7.366, 7.064, 6.836, 6.846, 6.66, 6.963, 6.883, 6.842, 7.24, 6.982, 7.293, 7.111, 7.289, 7.238, 7.271, 7.112, 7.323, 7.229, 7.015, 7.032, 6.694, 7.091, 7.154, 6.846, 6.752, 6.799, 6.534, 7.996, 7.142, 7.345, 6.938, 6.756, 7.561, 6.875, 7.15, 6.879, 7.076, 6.864, 7.142, 7.124, 7.073, 7.236, 6.95, 7.156, 6.788, 6.708, 7.081, 7.677, 6.896, 6.795, 6.755, 6.872, 7.202, 6.872, 6.836, 6.487, 7.167, 6.939, 7.24, 6.9, 7.133, 7.033, 6.927, 7.139, 7.017, 6.94, 7.355, 6.883, 7.055, 7.002, 6.561, 7.038, 6.866, 6.731, 7.145, 6.809, 6.991, 7.57, 7.555, 7.834, 7.181, 7.542, 8.29, 7.213, 7.639, 7.827, 7.223, 6.757, 6.713, 6.95, 6.717, 6.685, 6.484, 6.456, 6.743, 6.326, 7.188, 6.447, 7.0, 6.652, 6.997, 7.303, 7.093, 7.084, 7.2, 7.253, 7.178, 7.259, 7.336, 7.185, 7.158, 6.806, 6.97, 6.903, 6.78, 6.899, 6.61, 6.981, 6.761, 6.783, 6.735, 7.214, 6.87, 7.142, 7.01, 6.675, 6.468, 6.419, 6.614, 6.682, 6.684, 6.614, 6.87, 6.538, 6.836, 6.725, 6.838, 6.817, 6.951, 6.758, 6.912, 6.525, 6.972, 6.93, 6.912, 7.06, 6.94, 7.047, 6.979, 7.004, 6.976, 6.769, 7.115, 6.527, 6.951, 6.58, 6.595, 6.689, 6.852, 6.763, 6.219, 6.721, 6.467, 6.939, 7.018, 6.751, 7.026, 6.63, 6.594, 7.32, 6.846, 6.71, 6.531, 6.9, 7.165, 6.903, 6.847, 7.169, 6.837, 7.058, 6.756, 6.93, 6.485, 6.761, 7.102, 6.951, 6.693, 6.504, 6.637, 6.798, 6.819, 6.903, 6.938, 6.715, 6.854, 6.517, 6.313, 6.64, 6.57, 6.746, 6.679, 6.88, 6.873, 6.93, 6.915, 7.016, 6.755, 7.191, 6.901, 7.27, 7.312, 7.021, 7.273, 7.427, 6.938, 7.345, 6.431, 6.796, 6.735, 6.906, 6.841, 7.082, 6.852, 6.963, 6.678, 6.754, 7.197, 6.549, 7.281, 6.822, 6.398, 6.682, 6.193, 6.909, 6.88, 6.75, 7.075, 6.949, 7.141, 7.13, 6.916, 7.2, 6.755, 6.947, 6.973, 6.898, 6.979, 7.258, 7.039, 7.503, 7.079, 7.588, 7.098, 6.845, 7.147, 6.909, 7.05, 7.344, 6.916, 7.278, 7.071, 7.843, 6.935, 6.801, 7.105, 7.906, 7.299, 7.557, 7.306, 7.663, 7.203, 7.786, 7.01, 7.645, 6.953, 6.872, 7.046, 7.028, 7.156, 7.019, 7.15, 7.022, 7.198, 7.079, 6.918, 6.791]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.108, 0.625, 1.001, 1.384, 0.851, 1.07, 1.05, 2.781, 2.829, 3.512, 4.153, 4.32, 5.253, 3.805, 3.639, 4.289, 2.872, 4.357, 3.796, 3.44, 3.187, 3.586, 3.909, 2.193, 4.194, 3.514, 5.329, 5.98, 3.736, 5.377, 6.523, 7.683, 7.633, 8.415, 7.739, 7.074, 5.818, 4.92, 3.394, 3.963, 3.657, 3.686, 2.552, 2.594, 3.806, 5.214, 4.963, 4.497, 4.851, 3.967, 4.732, 4.436, 4.445, 4.589, 3.324, 5.497, 5.941, 7.03, 5.705, 3.692, 3.28, 3.349, 5.457, 5.761, 3.469, 2.644, 4.165, 4.062, 3.808, 3.044, 2.508, 2.21, 2.302, 1.676, 1.374, 1.505, 1.293, 1.64, 1.575, 1.275, 1.782, 1.417, 1.95, 1.996, 1.52, 1.978, 2.608, 4.011, 3.186, 3.19, 3.688, 3.193, 2.932, 2.527, 2.564, 3.694, 3.789, 4.239, 2.954, 2.757, 2.713, 2.662, 2.649, 2.397, 2.081, 1.429, 2.11, 1.785, 2.2, 2.569, 2.537, 2.802, 3.489, 4.221, 2.876, 2.196, 2.285, 1.72, 1.71, 1.822, 1.848, 1.935, 1.936, 1.942, 1.694, 1.69, 1.661, 1.629, 1.68, 1.507, 1.832, 1.754, 2.207, 1.49, 1.66, 2.393, 1.412, 1.878, 1.653, 1.648, 1.588, 2.02, 2.101, 2.308, 2.318, 2.419, 1.413, 1.584, 2.105, 2.482, 1.884, 2.02, 1.728, 2.049, 1.435, 1.955, 1.661, 2.602, 2.539, 3.335, 3.442, 3.74, 3.736, 2.643, 2.342, 2.551, 2.265, 3.088, 3.123, 3.078, 2.75, 2.342, 2.31, 1.773, 1.828, 1.65, 1.776, 1.769, 1.941, 2.004, 2.323, 2.226, 2.164, 2.315, 2.021, 2.015, 2.378, 2.3, 1.856, 1.479, 1.909, 1.366, 1.406, 1.448, 1.999, 2.334, 2.737, 3.323, 3.21, 1.939, 2.475, 2.419, 3.016, 2.226, 1.934, 2.136, 1.56, 1.84, 1.597, 1.7, 1.766, 1.606, 1.635, 2.128, 1.797, 2.078, 2.609, 2.971, 2.256, 1.576, 1.662, 1.49, 1.253, 1.478, 1.637, 2.19, 1.952, 1.567, 1.443, 1.994, 1.587, 1.815, 1.964, 1.677, 2.086, 2.074, 2.091, 1.931, 1.454, 1.578, 1.96, 2.056, 2.14, 2.472, 2.365, 1.879, 2.426, 2.803, 3.105, 3.02, 2.609, 2.836, 3.152, 2.742, 3.085, 3.568, 3.08, 2.529, 2.22, 2.339, 1.711, 1.772, 1.732, 1.755, 1.631, 1.643, 1.31, 1.405, 1.399, 1.393, 1.359, 1.132, 1.282, 1.269, 1.163, 1.135, 1.194, 1.214, 1.218, 1.213, 1.339, 2.093, 2.376, 2.159, 1.81, 1.533, 2.16, 1.625, 1.371, 1.532, 2.315, 1.333, 1.466, 1.695, 1.97, 1.765, 1.79, 1.442, 1.567, 1.469, 1.682, 1.503, 1.542, 1.551, 1.592, 1.32, 1.337, 1.237, 1.295, 1.326, 1.381, 2.027, 1.697, 1.972, 1.848, 1.698, 1.72, 1.534, 1.653, 1.476, 1.543, 1.433, 1.456, 1.493, 1.557, 1.603, 1.6, 1.81, 1.781, 1.922, 1.795, 1.837, 1.709, 1.341, 1.342, 1.683, 1.472, 1.75, 2.115, 2.783, 2.226, 1.547, 2.101, 1.99, 1.687, 1.434, 1.339, 1.338, 1.371, 1.699, 1.573, 1.689, 1.6, 1.313, 1.27, 1.172, 1.773, 2.03, 2.385, 2.485, 2.903, 3.022, 2.342, 2.126, 2.436, 2.213, 2.027, 1.704, 1.78, 1.546, 1.494, 1.419, 1.303, 1.48, 1.3, 1.381, 1.25, 1.205, 1.087, 1.157, 1.049, 1.177, 1.076, 0.923, 1.428, 1.164, 1.464, 1.842, 2.027, 1.961, 1.801, 2.034, 2.673, 2.569, 2.822, 1.982, 2.529, 2.106, 2.138, 2.125, 2.155, 1.691, 1.624, 1.468, 1.43, 1.467, 1.593, 1.483, 1.446, 1.211, 1.44, 1.381, 1.555, 1.56, 1.55, 1.425, 1.414, 1.318, 1.291, 1.231, 1.256, 1.046, 1.472, 1.352, 1.289, 1.152, 1.148, 1.285, 1.123, 1.062, 1.163, 1.058, 1.01, 1.342, 1.137, 0.949, 1.128, 1.055, 1.108, 1.035, 1.117, 0.978, 1.284, 1.071, 1.499, 1.293, 1.728, 1.24, 1.537, 1.484, 1.449, 1.33, 1.28, 1.426, 1.521, 1.813]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 49.67341, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 49.67341 to 49.60590, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 49.60590 to 49.53029, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 49.53029 to 49.43430, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 49.43430 to 49.31285, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 49.31285 to 49.14606, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 49.14606 to 49.00328, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 49.00328 to 48.87200, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 48.87200 to 48.67843, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 48.67843 to 48.49541, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 48.49541 to 48.17411, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 48.17411 to 48.00507, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 48.00507 to 47.38697, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 47.38697 to 47.02279, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 47.02279 to 46.27697, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 46.27697 to 46.26436, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 46.26436 to 44.89357, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 44.89357 to 44.76386, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 44.76386 to 43.93986, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 43.93986 to 43.28510, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 43.28510 to 42.69960, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 42.69960 to 41.95613, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 41.95613 to 41.95067, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 41.95067 to 39.51296, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 39.51296\n",
      "\n",
      "Epoch 00026: val_mae improved from 39.51296 to 36.17494, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 36.17494\n",
      "\n",
      "Epoch 00028: val_mae improved from 36.17494 to 31.92013, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 31.92013\n",
      "\n",
      "Epoch 00030: val_mae improved from 31.92013 to 29.49302, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00031: val_mae improved from 29.49302 to 29.10940, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00032: val_mae improved from 29.10940 to 27.41104, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 27.41104 to 22.86263, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 22.86263\n",
      "\n",
      "Epoch 00035: val_mae improved from 22.86263 to 17.11328, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00036: val_mae improved from 17.11328 to 14.84150, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00037: val_mae improved from 14.84150 to 13.49937, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 13.49937\n",
      "\n",
      "Epoch 00039: val_mae improved from 13.49937 to 10.59027, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 10.59027\n",
      "\n",
      "Epoch 00041: val_mae improved from 10.59027 to 10.38561, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00042: val_mae improved from 10.38561 to 10.13952, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00043: val_mae improved from 10.13952 to 8.39623, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00044: val_mae improved from 8.39623 to 6.95726, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 6.95726\n",
      "\n",
      "Epoch 00127: val_mae improved from 6.95726 to 6.87036, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00128: val_mae improved from 6.87036 to 6.69826, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 6.69826\n",
      "\n",
      "Epoch 00167: val_mae improved from 6.69826 to 6.64081, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 6.64081\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 6.64081\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 6.64081\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 6.64081\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 6.64081\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 6.64081\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 6.64081\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 6.64081\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 6.64081\n",
      "\n",
      "Epoch 00177: val_mae improved from 6.64081 to 6.58645, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 6.58645\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 6.58645\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 6.58645\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 6.58645\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 6.58645\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 6.58645\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 6.58645\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 6.58645\n",
      "\n",
      "Epoch 00186: val_mae improved from 6.58645 to 6.57418, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00187: val_mae did not improve from 6.57418\n",
      "\n",
      "Epoch 00188: val_mae improved from 6.57418 to 6.24413, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 6.24413\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 6.24413\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 6.24413\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 6.24413\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 6.24413\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 6.24413\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 6.24413\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 6.24413\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 6.24413\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 6.24413\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 6.24413\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 6.24413\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 6.24413\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 6.24413\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 6.24413\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 6.24413\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 6.24413\n",
      "\n",
      "Epoch 00206: val_mae improved from 6.24413 to 6.19513, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 6.19513\n",
      "\n",
      "Epoch 00248: val_mae improved from 6.19513 to 5.71328, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 5.71328\n",
      "\n",
      "Epoch 00308: val_mae improved from 5.71328 to 5.65995, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 5.65995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00335: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 5.65995\n",
      "\n",
      "Epoch 00410: val_mae improved from 5.65995 to 5.45560, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_5/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 5.45560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00493: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 5.45560\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 5.45560\n",
      "\n",
      "Lambda: 1 , Time: 0:03:56\n",
      "Train Error(all epochs): 1.4728178977966309 \n",
      " [49.078, 48.958, 48.871, 48.781, 48.681, 48.56, 48.421, 48.257, 48.066, 47.837, 47.586, 47.277, 46.928, 46.516, 46.072, 45.573, 45.038, 44.423, 43.783, 43.049, 42.269, 41.407, 40.474, 39.446, 38.348, 37.245, 36.0, 34.787, 33.413, 32.06, 30.599, 29.114, 27.643, 26.074, 24.545, 22.992, 21.398, 19.919, 18.345, 16.933, 15.485, 14.156, 12.96, 11.829, 10.868, 9.781, 9.18, 8.43, 8.02, 6.92, 6.584, 5.953, 5.72, 5.403, 5.063, 4.957, 4.932, 4.947, 4.657, 4.835, 4.727, 4.531, 4.563, 4.508, 4.429, 4.35, 4.002, 3.841, 3.63, 3.699, 3.623, 3.938, 3.962, 4.102, 4.12, 4.141, 3.894, 3.784, 3.765, 3.717, 3.537, 3.313, 3.246, 3.305, 3.524, 3.509, 3.569, 3.76, 3.543, 3.695, 3.786, 3.728, 3.464, 3.867, 3.63, 3.818, 3.688, 3.778, 3.412, 3.473, 3.341, 3.279, 3.395, 3.306, 3.359, 3.45, 3.331, 3.262, 3.39, 3.223, 3.251, 3.091, 3.053, 3.03, 3.008, 3.168, 3.287, 3.172, 3.078, 3.063, 2.837, 2.889, 3.299, 3.229, 3.211, 3.34, 3.305, 3.578, 3.455, 3.292, 3.327, 3.266, 3.052, 2.941, 2.953, 2.816, 3.352, 3.266, 2.918, 2.825, 2.704, 2.787, 2.685, 2.882, 2.94, 2.936, 2.923, 3.187, 2.859, 2.709, 2.672, 2.662, 2.543, 2.752, 2.805, 3.086, 3.021, 2.878, 3.217, 2.829, 2.697, 2.829, 2.927, 2.772, 2.895, 2.705, 2.581, 2.736, 2.922, 2.958, 3.119, 3.517, 3.147, 3.27, 3.182, 2.935, 2.924, 3.136, 3.374, 3.394, 3.501, 3.318, 3.218, 2.89, 2.87, 2.818, 3.339, 3.372, 3.037, 2.913, 2.745, 2.537, 2.777, 2.65, 2.63, 2.552, 2.218, 2.149, 2.375, 2.363, 2.393, 2.358, 2.535, 2.942, 2.876, 2.697, 2.635, 2.356, 2.327, 2.413, 2.647, 2.579, 2.677, 2.85, 3.052, 3.143, 3.34, 3.005, 3.144, 2.94, 2.72, 2.432, 2.164, 2.14, 2.265, 2.301, 2.049, 2.152, 2.185, 2.223, 2.45, 2.701, 2.838, 2.538, 2.326, 2.094, 2.062, 2.269, 2.474, 2.74, 3.079, 3.167, 3.026, 3.028, 3.036, 2.826, 2.632, 2.791, 2.772, 2.633, 2.436, 2.891, 2.744, 2.579, 2.41, 2.057, 2.088, 2.001, 2.133, 2.375, 2.556, 2.813, 2.977, 3.343, 3.427, 3.441, 3.143, 3.145, 2.801, 2.998, 3.0, 2.6, 2.416, 2.21, 2.143, 2.128, 2.083, 2.173, 2.24, 2.262, 2.273, 2.225, 2.261, 2.052, 2.058, 1.903, 2.115, 2.116, 2.198, 1.891, 1.93, 1.938, 2.025, 2.003, 2.064, 2.415, 2.788, 2.72, 2.575, 2.369, 2.604, 2.618, 2.886, 2.891, 2.815, 2.857, 2.711, 2.758, 2.908, 2.728, 2.671, 2.506, 2.388, 2.114, 2.038, 1.89, 1.896, 1.961, 1.993, 2.197, 2.087, 2.154, 2.163, 2.109, 2.02, 2.012, 2.058, 2.168, 2.219, 2.481, 2.613, 2.43, 2.596, 2.484, 2.625, 2.38, 2.206, 1.974, 1.871, 1.787, 1.688, 1.726, 1.793, 1.856, 1.966, 2.042, 2.268, 2.347, 2.68, 2.554, 2.596, 3.02, 3.008, 2.856, 2.675, 2.493, 2.561, 2.666, 2.627, 2.567, 2.536, 2.697, 2.671, 2.402, 2.051, 1.997, 1.991, 1.966, 2.035, 2.159, 1.888, 1.91, 1.847, 1.924, 1.921, 1.884, 1.925, 1.941, 1.956, 1.904, 1.936, 1.928, 2.254, 2.491, 2.457, 2.39, 2.546, 2.464, 2.297, 2.47, 2.438, 2.505, 2.896, 2.856, 2.779, 2.603, 2.482, 2.422, 2.471, 2.511, 2.56, 2.648, 2.749, 2.879, 2.644, 2.431, 2.06, 1.97, 1.751, 1.751, 1.822, 1.727, 1.753, 1.924, 1.843, 1.755, 1.615, 1.678, 1.812, 1.726, 1.905, 2.008, 2.277, 2.48, 2.358, 2.247, 2.284, 2.267, 2.337, 2.488, 2.397, 2.252, 2.172, 1.956, 1.859, 1.961, 2.094, 2.323, 2.601, 2.325, 2.242, 2.198, 2.299, 2.449, 2.225, 2.155, 2.182, 2.014, 1.968, 1.862, 1.656, 1.616, 1.526, 1.473, 1.504, 1.587, 1.787, 2.05, 2.249, 2.262, 2.462, 2.2, 2.331, 2.621, 2.481, 2.282, 2.546, 2.569, 2.775, 2.777, 2.899, 2.921, 2.633, 2.333, 2.43, 2.458, 2.292, 2.107, 1.936, 1.987, 1.726, 1.68, 1.66, 1.6, 1.672, 1.644, 1.74, 1.797, 1.796, 1.801, 1.829, 1.959, 1.995, 2.061, 2.006, 1.96, 2.001, 1.995, 2.29, 2.503]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.004, 0.0, 0.015, 0.005, 0.013, 0.038, 0.054, 0.104, 0.178, 0.249, 0.278, 0.506, 0.463, 0.84, 0.613, 0.747, 0.757, 0.929, 0.996, 1.023, 1.126, 1.275, 1.355, 1.413, 1.438, 1.517, 1.628, 1.564, 1.582, 1.516, 1.575, 1.45, 1.326, 1.258, 1.296, 1.215, 1.491, 1.533, 1.533, 1.526, 1.621, 1.512, 1.424, 1.425, 1.433, 1.295, 1.236, 1.214, 1.215, 1.445, 1.293, 1.365, 1.547, 1.336, 1.454, 1.56, 1.401, 1.339, 1.565, 1.422, 1.513, 1.465, 1.537, 1.378, 1.306, 1.334, 1.243, 1.335, 1.302, 1.288, 1.431, 1.265, 1.258, 1.353, 1.21, 1.322, 1.148, 1.173, 1.177, 1.173, 1.238, 1.34, 1.24, 1.189, 1.213, 1.06, 1.066, 1.368, 1.256, 1.274, 1.38, 1.344, 1.433, 1.363, 1.312, 1.249, 1.357, 1.156, 1.113, 1.167, 0.998, 1.425, 1.315, 1.099, 1.071, 0.965, 1.104, 1.009, 1.168, 1.126, 1.2, 1.036, 1.384, 1.113, 1.081, 1.076, 0.952, 0.952, 1.084, 1.099, 1.24, 1.21, 1.052, 1.409, 1.093, 0.984, 1.103, 1.148, 1.102, 1.198, 1.037, 1.007, 1.06, 1.152, 1.154, 1.263, 1.542, 1.264, 1.285, 1.29, 1.144, 1.158, 1.306, 1.352, 1.438, 1.41, 1.355, 1.237, 1.128, 1.132, 1.049, 1.515, 1.392, 1.158, 1.119, 1.1, 0.987, 1.07, 1.036, 1.027, 1.014, 0.834, 0.791, 0.9, 0.931, 0.907, 0.947, 1.019, 1.213, 1.241, 1.084, 0.985, 0.874, 0.868, 0.872, 1.071, 1.077, 1.048, 1.074, 1.364, 1.304, 1.401, 1.253, 1.182, 1.242, 1.038, 0.946, 0.792, 0.816, 0.861, 0.921, 0.766, 0.789, 0.875, 0.847, 1.031, 1.085, 1.215, 0.963, 0.948, 0.755, 0.781, 0.871, 1.046, 1.076, 1.324, 1.407, 1.128, 1.267, 1.112, 1.194, 1.068, 1.038, 1.306, 0.958, 0.97, 1.191, 1.128, 1.03, 0.999, 0.712, 0.781, 0.814, 0.808, 1.039, 1.058, 1.11, 1.377, 1.39, 1.454, 1.469, 1.262, 1.318, 1.104, 1.278, 1.189, 0.97, 0.919, 0.96, 0.876, 0.822, 0.831, 0.882, 0.856, 0.956, 0.923, 0.849, 0.988, 0.702, 0.876, 0.7, 0.826, 0.858, 0.918, 0.709, 0.749, 0.772, 0.806, 0.78, 0.865, 0.909, 1.295, 1.118, 1.024, 0.979, 0.992, 1.122, 1.26, 1.137, 1.152, 1.2, 1.219, 1.092, 1.35, 1.098, 1.095, 1.018, 0.935, 0.838, 0.81, 0.722, 0.861, 0.763, 0.818, 0.904, 0.782, 0.836, 0.886, 0.854, 0.833, 0.779, 0.83, 0.876, 0.932, 1.084, 1.168, 0.794, 1.185, 0.957, 1.127, 1.061, 0.854, 0.769, 0.693, 0.684, 0.685, 0.686, 0.724, 0.721, 0.836, 0.828, 0.913, 1.034, 1.112, 1.037, 0.98, 1.447, 1.232, 1.219, 1.074, 0.979, 1.146, 1.125, 1.172, 1.119, 0.935, 1.249, 1.104, 0.994, 0.801, 0.779, 0.82, 0.827, 0.824, 0.965, 0.717, 0.783, 0.784, 0.684, 0.864, 0.725, 0.835, 0.732, 0.863, 0.751, 0.804, 0.791, 0.904, 1.103, 1.019, 0.953, 1.075, 1.116, 0.904, 1.014, 1.093, 1.062, 1.356, 1.228, 1.099, 1.086, 1.132, 1.008, 0.996, 1.104, 1.088, 1.2, 1.032, 1.245, 1.208, 0.944, 0.842, 0.858, 0.682, 0.689, 0.798, 0.732, 0.657, 0.826, 0.779, 0.683, 0.627, 0.675, 0.759, 0.682, 0.812, 0.899, 0.973, 1.009, 1.063, 0.904, 1.018, 0.915, 1.026, 1.081, 1.095, 0.841, 0.976, 0.792, 0.794, 0.788, 0.939, 0.98, 1.174, 0.989, 0.891, 0.999, 0.881, 1.225, 0.838, 0.931, 0.977, 0.852, 0.777, 0.801, 0.652, 0.695, 0.597, 0.596, 0.622, 0.652, 0.744, 0.906, 1.038, 0.855, 1.152, 0.854, 1.001, 1.075, 1.149, 1.032, 0.941, 1.117, 1.345, 1.247, 1.196, 1.295, 1.199, 1.013, 1.079, 1.056, 1.006, 0.911, 0.736, 0.859, 0.734, 0.713, 0.751, 0.577, 0.795, 0.629, 0.8, 0.704, 0.793, 0.718, 0.867, 0.775, 0.857, 0.911, 0.817, 0.855, 0.826, 0.883, 1.021, 1.111]\n",
      "Val Error(all epochs): 5.455600261688232 \n",
      " [49.673, 49.606, 49.53, 49.434, 49.313, 49.146, 49.003, 48.872, 48.678, 48.495, 48.174, 48.005, 47.387, 47.023, 46.277, 46.264, 44.894, 44.764, 43.94, 43.285, 42.7, 41.956, 41.951, 39.513, 39.767, 36.175, 37.18, 31.92, 33.398, 29.493, 29.109, 27.411, 22.863, 24.062, 17.113, 14.842, 13.499, 15.226, 10.59, 11.41, 10.386, 10.14, 8.396, 6.957, 7.235, 7.439, 8.128, 12.563, 11.616, 11.217, 11.201, 10.597, 10.393, 10.544, 10.912, 10.602, 9.768, 11.696, 11.6, 10.989, 11.671, 11.99, 11.904, 11.39, 12.639, 12.159, 12.483, 12.667, 11.948, 11.971, 11.81, 11.787, 11.732, 11.767, 11.652, 11.632, 11.359, 10.95, 11.249, 10.011, 10.98, 10.563, 10.677, 10.534, 10.187, 10.546, 10.017, 10.007, 11.041, 9.712, 9.597, 9.238, 9.703, 10.226, 8.477, 7.632, 8.081, 7.727, 7.675, 8.187, 7.817, 8.56, 8.173, 8.2, 7.268, 8.034, 7.465, 8.099, 7.698, 8.35, 8.075, 8.498, 7.913, 8.149, 7.996, 7.933, 8.473, 8.273, 8.484, 8.634, 8.529, 8.656, 8.585, 8.445, 7.41, 8.599, 6.87, 6.698, 6.888, 7.078, 7.3, 6.818, 7.096, 6.924, 7.758, 7.978, 6.831, 6.952, 7.394, 7.453, 7.138, 7.501, 7.239, 6.735, 6.741, 7.224, 6.798, 6.995, 7.569, 7.491, 7.49, 7.353, 7.628, 7.949, 7.003, 8.497, 7.537, 7.735, 6.795, 7.274, 7.475, 7.379, 6.814, 7.149, 7.976, 6.829, 6.641, 7.209, 8.938, 6.863, 6.642, 7.088, 6.892, 6.985, 6.873, 6.831, 6.586, 7.555, 6.66, 6.748, 7.094, 7.109, 6.837, 7.228, 7.077, 6.574, 6.683, 6.244, 6.898, 6.254, 7.058, 7.166, 7.477, 7.513, 6.686, 7.457, 7.765, 7.817, 7.713, 8.241, 7.887, 8.043, 7.567, 6.454, 6.797, 6.195, 6.725, 6.671, 7.322, 7.158, 6.808, 6.9, 7.142, 6.806, 6.36, 6.684, 8.05, 6.787, 6.269, 7.015, 6.233, 6.512, 7.231, 7.41, 6.89, 7.895, 7.39, 8.461, 8.177, 7.617, 7.586, 6.873, 7.201, 7.251, 7.497, 7.462, 7.867, 8.128, 7.919, 7.15, 6.943, 6.939, 7.225, 6.859, 6.759, 6.423, 6.867, 5.713, 6.648, 7.231, 6.495, 6.48, 6.403, 6.06, 6.767, 6.771, 6.859, 7.315, 6.822, 6.767, 7.203, 7.283, 6.654, 8.231, 6.039, 5.977, 6.287, 6.219, 7.221, 6.776, 6.761, 6.538, 6.405, 6.79, 6.866, 7.059, 7.258, 7.371, 6.63, 7.288, 6.492, 6.896, 7.393, 6.769, 7.301, 7.762, 7.714, 7.595, 7.231, 7.934, 7.809, 8.062, 7.54, 8.334, 7.708, 7.847, 6.932, 6.648, 7.57, 7.762, 7.289, 7.311, 7.866, 6.127, 5.963, 6.41, 6.679, 5.66, 6.347, 5.902, 6.652, 6.573, 7.111, 7.341, 7.258, 6.578, 7.034, 7.04, 6.328, 6.931, 7.648, 6.724, 7.31, 6.559, 7.88, 7.103, 7.888, 7.367, 7.256, 7.477, 7.206, 6.549, 7.303, 7.297, 6.641, 7.422, 6.716, 7.019, 7.772, 7.659, 7.635, 7.945, 7.859, 7.867, 8.183, 8.211, 8.072, 7.363, 7.341, 7.05, 6.066, 7.604, 7.143, 6.217, 6.683, 6.166, 6.058, 6.698, 6.186, 7.328, 5.907, 6.081, 6.634, 6.096, 6.15, 6.496, 6.628, 6.71, 6.862, 6.836, 6.637, 6.755, 7.732, 6.745, 7.982, 7.374, 7.767, 7.698, 7.785, 7.376, 7.95, 7.455, 7.159, 6.763, 7.329, 6.916, 6.47, 6.283, 7.129, 6.041, 6.664, 5.974, 6.682, 6.381, 6.993, 5.928, 7.182, 6.054, 6.021, 6.086, 6.17, 6.205, 6.373, 7.417, 7.827, 6.558, 6.419, 6.395, 6.276, 5.456, 6.536, 6.188, 6.189, 6.406, 6.27, 7.144, 6.707, 7.119, 7.318, 7.788, 7.526, 6.78, 6.97, 7.132, 6.413, 6.298, 6.723, 7.019, 6.033, 6.672, 6.711, 6.455, 6.245, 5.945, 6.15, 6.819, 6.709, 5.792, 7.118, 6.438, 6.531, 6.806, 6.459, 7.301, 6.73, 6.728, 6.525, 6.562, 6.775, 6.684, 6.452, 7.016, 6.545, 7.219, 7.3, 7.468, 7.434, 7.932, 7.285, 6.459, 7.756, 6.778, 6.126, 6.187, 6.332, 6.795, 5.652, 5.597, 5.511, 6.227, 6.968, 7.651, 6.665, 7.076, 7.255, 7.86, 7.506, 6.683, 6.956, 6.726, 6.038, 6.326, 6.908, 6.437, 7.56, 7.04, 7.304, 7.387, 6.931, 7.153, 6.984, 7.558, 7.317, 7.039, 6.68, 6.848, 6.675, 6.044, 6.059, 6.017]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.009, 0.078, 0.013, 0.437, 0.368, 0.484, 0.547, 1.074, 1.918, 2.352, 3.415, 3.747, 11.109, 6.721, 5.453, 5.85, 4.882, 5.853, 3.94, 4.317, 4.044, 4.344, 6.545, 3.26, 4.502, 2.789, 3.838, 5.625, 4.376, 6.616, 4.689, 5.708, 6.404, 4.805, 5.124, 4.324, 4.051, 3.081, 4.445, 2.766, 4.563, 3.513, 3.5, 4.528, 3.593, 4.569, 4.109, 3.27, 2.882, 2.473, 2.39, 2.927, 1.802, 3.914, 2.748, 2.284, 2.801, 2.858, 2.547, 3.07, 3.259, 3.447, 2.745, 3.013, 2.228, 2.393, 2.668, 2.245, 2.604, 1.725, 2.159, 1.986, 1.108, 2.064, 1.626, 1.573, 2.078, 1.926, 1.531, 1.17, 1.783, 1.486, 1.433, 2.104, 1.018, 1.672, 1.53, 1.286, 1.572, 1.619, 1.166, 1.893, 2.516, 2.399, 1.326, 1.619, 1.344, 2.51, 1.428, 1.704, 1.111, 1.041, 1.104, 1.289, 1.593, 1.16, 1.941, 1.543, 1.657, 1.566, 2.309, 2.21, 1.842, 2.04, 2.19, 1.489, 2.167, 1.306, 0.879, 1.745, 0.856, 1.37, 2.252, 0.876, 1.475, 1.369, 1.728, 1.273, 1.993, 1.198, 1.971, 1.16, 1.161, 0.727, 1.024, 1.89, 1.427, 1.392, 1.858, 1.969, 1.158, 2.153, 1.088, 2.428, 1.578, 1.819, 1.147, 1.418, 1.416, 1.342, 1.957, 1.624, 2.046, 2.159, 1.616, 2.986, 1.413, 2.547, 1.562, 1.769, 1.736, 1.564, 1.698, 1.181, 1.755, 1.57, 1.581, 1.406, 1.369, 2.087, 1.272, 2.23, 1.969, 1.489, 2.178, 1.797, 1.607, 1.725, 1.632, 1.914, 1.407, 1.661, 1.695, 2.616, 2.551, 2.127, 1.851, 1.546, 1.411, 1.666, 1.468, 1.321, 1.292, 1.185, 1.164, 1.44, 1.365, 1.506, 1.695, 1.853, 1.416, 2.208, 1.164, 1.843, 1.501, 2.653, 1.486, 2.457, 1.887, 2.885, 2.476, 2.751, 2.146, 2.292, 1.125, 2.06, 2.014, 3.16, 2.237, 2.192, 1.502, 2.038, 1.438, 1.322, 1.631, 1.386, 1.33, 1.323, 0.762, 2.151, 3.008, 3.161, 3.962, 4.104, 5.287, 4.927, 3.26, 3.474, 3.223, 2.227, 2.686, 1.802, 2.496, 1.733, 2.43, 1.518, 1.953, 1.717, 1.841, 1.831, 1.493, 1.655, 1.653, 1.94, 1.495, 1.525, 1.279, 1.192, 1.633, 0.997, 1.31, 1.505, 1.772, 0.967, 0.758, 1.277, 1.205, 1.143, 2.482, 1.977, 2.395, 1.821, 2.327, 1.96, 2.095, 1.813, 1.829, 1.203, 1.53, 1.204, 2.138, 1.739, 1.814, 2.05, 1.938, 1.825, 1.921, 2.495, 1.712, 1.901, 1.468, 1.248, 1.7, 1.432, 1.792, 1.019, 1.809, 1.421, 1.183, 1.415, 1.577, 1.715, 1.377, 0.925, 1.033, 1.004, 1.036, 0.979, 1.042, 0.964, 0.781, 1.046, 1.084, 0.894, 1.652, 2.16, 1.255, 0.988, 1.565, 1.93, 1.792, 1.66, 1.194, 1.779, 1.375, 2.551, 1.925, 2.814, 1.959, 2.192, 2.099, 1.762, 1.888, 1.414, 1.819, 1.599, 1.757, 1.085, 1.77, 0.966, 1.416, 1.029, 1.201, 0.959, 1.215, 1.427, 1.027, 1.672, 1.706, 1.367, 1.694, 1.794, 1.649, 1.901, 1.833, 2.025, 1.562, 1.957, 2.055, 5.33, 3.262, 2.35, 1.739, 3.639, 2.413, 2.661, 3.141, 3.965, 5.625, 5.879, 4.036, 4.525, 4.065, 3.689, 2.522, 3.247, 1.62, 2.497, 1.821, 2.352, 1.157, 1.524, 1.008, 1.431, 0.929, 0.895, 1.331, 1.112, 1.178, 1.528, 1.488, 2.055, 1.316, 1.499, 1.697, 1.493, 1.101, 1.928, 1.512, 2.191, 1.293, 1.48, 1.532, 1.365, 1.536, 1.957, 1.328, 1.9, 1.277, 2.032, 1.439, 2.066, 1.605, 1.928, 1.723, 1.759, 1.713, 1.82, 1.47, 1.331, 1.4, 1.371, 0.985, 1.313, 1.316, 1.394, 1.461, 2.02, 1.538, 2.023, 1.922, 3.483, 2.407, 2.933, 3.31, 4.801, 4.859, 3.121, 1.851, 2.106, 1.722, 2.164, 1.503, 1.431, 1.422, 1.53, 1.407, 1.279, 1.143, 1.03, 0.935, 1.037, 0.947, 1.223, 0.868, 1.725, 0.706, 1.326, 0.936, 1.627, 1.067, 1.899, 1.181, 2.534, 1.715]\n",
      "\n",
      "#Fold: 5 \n",
      "Trainig set size: 420 , Time: 0:11:54 , best_lambda: 1 , min_  , error: 5.456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test starts:  467 , ends:  518\n",
      "1/1 [==============================] - 0s 688us/step - loss: 131.1665 - mse: 93.7766 - mae: 7.5833 - fp_mae: 3.9735\n",
      "average_error:  7.583 , fp_average_error:  3.974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:00<00:00, 228.43it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 226.71it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 228.27it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 230.00it/s]\n",
      "100%|██████████| 107/107 [00:00<00:00, 237.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Fold: 6 , Training Size: 420 , Validation size: 47 , Test Size 52\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 47.13798, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 47.13798 to 47.08503, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 47.08503 to 46.97155, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 46.97155 to 46.90092, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 46.90092 to 46.75527, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 46.75527 to 46.55897, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 46.55897 to 46.37766, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 46.37766 to 46.06257, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 46.06257 to 45.69333, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 45.69333 to 45.36230, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 45.36230 to 44.99973, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 44.99973 to 44.60249, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 44.60249 to 44.22872, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 44.22872 to 43.59706, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 43.59706 to 43.09922, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 43.09922 to 42.40178, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 42.40178\n",
      "\n",
      "Epoch 00018: val_mae improved from 42.40178 to 42.30410, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 42.30410 to 41.33200, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 41.33200 to 41.19320, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 41.19320 to 40.21015, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 40.21015 to 39.06334, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 39.06334 to 38.41394, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 38.41394 to 37.63705, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 37.63705 to 36.27915, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 36.27915 to 36.18484, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 36.18484 to 35.55309, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 35.55309 to 34.45106, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 34.45106 to 34.12211, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 34.12211 to 32.33744, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00031: val_mae improved from 32.33744 to 31.49156, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00032: val_mae improved from 31.49156 to 31.17213, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 31.17213 to 28.25814, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 28.25814\n",
      "\n",
      "Epoch 00035: val_mae improved from 28.25814 to 26.19423, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00036: val_mae improved from 26.19423 to 25.77837, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00037: val_mae improved from 25.77837 to 23.44857, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00038: val_mae improved from 23.44857 to 22.93554, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00039: val_mae improved from 22.93554 to 21.20333, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00040: val_mae improved from 21.20333 to 21.06688, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00041: val_mae improved from 21.06688 to 20.44081, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00042: val_mae improved from 20.44081 to 18.71812, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00043: val_mae improved from 18.71812 to 18.69119, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00044: val_mae improved from 18.69119 to 16.41266, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00045: val_mae improved from 16.41266 to 15.64405, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 15.64405\n",
      "\n",
      "Epoch 00047: val_mae improved from 15.64405 to 14.98389, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 14.98389\n",
      "\n",
      "Epoch 00049: val_mae improved from 14.98389 to 14.71647, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00050: val_mae improved from 14.71647 to 13.67455, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00051: val_mae improved from 13.67455 to 12.08046, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 12.08046\n",
      "\n",
      "Epoch 00053: val_mae improved from 12.08046 to 11.69455, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 11.69455\n",
      "\n",
      "Epoch 00055: val_mae improved from 11.69455 to 11.22172, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00056: val_mae improved from 11.22172 to 11.02823, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00057: val_mae improved from 11.02823 to 10.62932, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 10.62932\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 10.62932\n",
      "\n",
      "Epoch 00060: val_mae improved from 10.62932 to 10.45479, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 10.45479\n",
      "\n",
      "Epoch 00062: val_mae improved from 10.45479 to 10.15495, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00063: val_mae improved from 10.15495 to 10.14280, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00064: val_mae improved from 10.14280 to 9.98134, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00065: val_mae improved from 9.98134 to 9.03945, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 9.03945\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 9.03945\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 9.03945\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 9.03945\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 9.03945\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 9.03945\n",
      "\n",
      "Epoch 00072: val_mae improved from 9.03945 to 8.42638, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00073: val_mae improved from 8.42638 to 8.30544, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 8.30544\n",
      "\n",
      "Epoch 00075: val_mae improved from 8.30544 to 8.21772, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00076: val_mae improved from 8.21772 to 8.15313, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 8.15313\n",
      "\n",
      "Epoch 00078: val_mae improved from 8.15313 to 7.61871, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 7.61871\n",
      "\n",
      "Epoch 00080: val_mae improved from 7.61871 to 7.40902, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 7.40902\n",
      "\n",
      "Epoch 00082: val_mae improved from 7.40902 to 6.91094, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 6.91094\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 6.91094\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 6.91094\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 6.91094\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 6.91094\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 6.91094\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 6.91094\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 6.91094\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 6.91094\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 6.91094\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 6.91094\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 6.91094\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 6.91094\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 6.91094\n",
      "\n",
      "Epoch 00097: val_mae improved from 6.91094 to 6.86320, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 6.86320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00118: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 6.86320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00279: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 6.86320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00440: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 6.86320\n",
      "\n",
      "Epoch 00495: val_mae improved from 6.86320 to 6.82353, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 6.82353\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 6.82353\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 6.82353\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 6.82353\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 6.82353\n",
      "\n",
      "Lambda: 0.01 , Time: 0:03:58\n",
      "Train Error(all epochs): 0.8324481248855591 \n",
      " [49.226, 49.114, 49.026, 48.929, 48.82, 48.696, 48.547, 48.374, 48.176, 47.938, 47.673, 47.363, 47.005, 46.61, 46.169, 45.669, 45.131, 44.512, 43.86, 43.153, 42.381, 41.586, 40.736, 39.896, 38.97, 38.028, 37.047, 36.018, 34.979, 33.919, 32.84, 31.738, 30.612, 29.389, 28.184, 26.993, 25.769, 24.559, 23.377, 22.166, 21.016, 19.831, 18.761, 17.595, 16.73, 15.561, 14.695, 13.587, 12.631, 11.813, 10.973, 10.283, 9.529, 8.976, 8.19, 7.847, 7.002, 6.633, 6.025, 5.655, 5.463, 5.043, 5.044, 4.562, 4.608, 4.25, 4.142, 4.049, 3.847, 4.026, 3.676, 3.8, 3.662, 3.63, 3.534, 3.635, 3.531, 3.69, 3.585, 3.837, 3.666, 3.832, 3.467, 3.47, 3.11, 3.131, 2.965, 2.974, 2.908, 3.115, 3.192, 3.392, 3.27, 3.422, 3.067, 3.19, 3.001, 2.996, 2.779, 2.848, 2.8, 2.918, 2.904, 2.886, 2.833, 2.552, 2.448, 2.326, 2.331, 2.286, 2.291, 2.247, 2.421, 2.468, 2.442, 2.594, 2.528, 2.631, 2.783, 2.769, 2.745, 2.881, 2.602, 2.733, 2.541, 2.59, 2.448, 2.293, 2.237, 2.197, 2.141, 2.208, 2.215, 2.211, 2.352, 2.486, 2.432, 2.473, 2.352, 2.369, 2.416, 2.343, 2.365, 2.456, 2.409, 2.336, 2.207, 2.142, 2.149, 2.241, 2.27, 2.253, 2.247, 2.129, 2.224, 2.067, 2.016, 1.988, 1.88, 1.852, 1.81, 1.826, 1.981, 1.91, 2.154, 2.142, 2.31, 2.107, 2.185, 1.971, 2.033, 1.938, 1.84, 1.808, 1.817, 1.815, 1.999, 2.13, 2.161, 2.371, 2.322, 2.104, 2.01, 1.942, 1.734, 1.689, 1.655, 1.671, 1.77, 1.773, 1.886, 1.806, 1.813, 1.839, 1.866, 1.988, 1.98, 2.007, 1.759, 1.876, 1.863, 2.016, 1.923, 2.095, 2.063, 2.108, 2.06, 1.927, 1.882, 1.93, 1.827, 1.704, 1.683, 1.597, 1.564, 1.494, 1.41, 1.37, 1.351, 1.319, 1.359, 1.523, 1.743, 1.743, 1.981, 1.994, 1.998, 1.986, 2.102, 2.076, 2.18, 1.994, 1.99, 1.859, 1.828, 1.802, 1.765, 1.789, 1.811, 1.773, 1.683, 1.611, 1.742, 1.692, 1.594, 1.487, 1.408, 1.389, 1.376, 1.411, 1.464, 1.581, 1.598, 1.641, 1.593, 1.612, 1.651, 1.556, 1.46, 1.358, 1.343, 1.346, 1.412, 1.416, 1.471, 1.526, 1.575, 1.541, 1.587, 1.637, 1.595, 1.579, 1.594, 1.672, 1.55, 1.417, 1.285, 1.235, 1.197, 1.206, 1.169, 1.239, 1.324, 1.403, 1.469, 1.43, 1.473, 1.539, 1.509, 1.541, 1.501, 1.57, 1.582, 1.665, 1.622, 1.596, 1.53, 1.493, 1.458, 1.434, 1.343, 1.342, 1.293, 1.321, 1.361, 1.402, 1.363, 1.325, 1.258, 1.192, 1.122, 1.108, 1.099, 1.068, 1.108, 1.219, 1.345, 1.369, 1.3, 1.229, 1.252, 1.351, 1.336, 1.378, 1.385, 1.362, 1.479, 1.449, 1.425, 1.473, 1.454, 1.438, 1.534, 1.553, 1.615, 1.509, 1.344, 1.288, 1.273, 1.331, 1.458, 1.508, 1.622, 1.693, 1.602, 1.525, 1.517, 1.48, 1.466, 1.476, 1.507, 1.432, 1.454, 1.489, 1.629, 1.619, 1.559, 1.539, 1.438, 1.4, 1.351, 1.384, 1.407, 1.369, 1.36, 1.274, 1.258, 1.203, 1.173, 1.11, 1.081, 1.068, 1.053, 1.106, 1.125, 1.291, 1.357, 1.472, 1.508, 1.598, 1.518, 1.603, 1.555, 1.535, 1.407, 1.391, 1.295, 1.274, 1.141, 1.064, 1.018, 0.993, 0.982, 0.979, 0.967, 0.966, 0.922, 0.913, 0.94, 0.954, 1.059, 1.119, 1.232, 1.328, 1.198, 1.184, 1.205, 1.237, 1.245, 1.248, 1.249, 1.28, 1.258, 1.262, 1.18, 1.216, 1.175, 1.203, 1.205, 1.263, 1.244, 1.317, 1.345, 1.394, 1.388, 1.358, 1.247, 1.191, 1.119, 1.043, 1.012, 1.007, 1.058, 0.995, 1.01, 0.945, 0.926, 0.912, 0.911, 0.883, 0.908, 0.948, 1.054, 1.269, 1.337, 1.372, 1.444, 1.379, 1.44, 1.373, 1.36, 1.371, 1.406, 1.523, 1.689, 1.674, 1.659, 1.603, 1.68, 1.747, 1.845, 2.024, 2.016, 1.94, 1.939, 1.683, 1.61, 1.549, 1.512, 1.505, 1.497, 1.264, 1.135, 0.978, 0.908, 0.887, 0.842, 0.908, 0.902, 0.929, 0.883, 0.838, 0.832, 0.849, 0.901, 0.989, 1.062, 1.13, 1.084, 1.069, 1.015, 1.042, 1.096, 1.162, 1.169, 1.211, 1.267, 1.391, 1.397, 1.422]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.004, 0.012, 0.011, 0.096, 0.051, 0.122, 0.071, 0.121, 0.154, 0.195, 0.298, 0.303, 0.441, 0.388, 0.559, 0.443, 0.558, 0.538, 0.625, 0.732, 0.802, 1.001, 0.895, 1.094, 1.007, 1.111, 1.15, 1.187, 1.339, 1.231, 1.399, 1.362, 1.381, 1.349, 1.488, 1.447, 1.536, 1.543, 1.63, 1.615, 1.676, 1.507, 1.517, 1.377, 1.342, 1.291, 1.332, 1.292, 1.389, 1.512, 1.516, 1.487, 1.585, 1.35, 1.435, 1.373, 1.376, 1.21, 1.342, 1.223, 1.384, 1.362, 1.361, 1.314, 1.156, 1.067, 1.06, 1.068, 1.005, 1.121, 0.985, 1.134, 1.193, 1.079, 1.225, 1.215, 1.207, 1.324, 1.298, 1.3, 1.318, 1.248, 1.283, 1.238, 1.169, 1.13, 1.016, 1.041, 0.991, 0.983, 1.03, 1.057, 1.001, 1.127, 1.177, 1.199, 1.189, 1.107, 1.075, 1.157, 1.14, 1.069, 1.196, 1.184, 1.072, 1.13, 0.991, 1.013, 1.068, 1.04, 1.102, 1.099, 0.979, 1.126, 0.982, 0.972, 0.921, 0.884, 0.841, 0.899, 0.803, 0.991, 0.894, 1.046, 1.035, 1.165, 0.949, 1.11, 0.894, 1.001, 0.94, 0.852, 0.892, 0.863, 0.833, 1.005, 1.093, 0.958, 1.262, 1.114, 1.028, 0.972, 0.889, 0.811, 0.779, 0.781, 0.77, 0.824, 0.884, 0.911, 0.85, 0.928, 0.855, 0.897, 1.011, 0.906, 1.035, 0.842, 0.911, 0.911, 1.021, 0.878, 1.082, 1.008, 0.976, 1.074, 0.944, 0.847, 0.894, 0.895, 0.786, 0.8, 0.77, 0.75, 0.74, 0.682, 0.665, 0.658, 0.646, 0.648, 0.751, 0.898, 0.831, 1.053, 0.944, 0.937, 0.988, 1.008, 1.021, 1.075, 0.965, 0.941, 0.912, 0.874, 0.817, 0.874, 0.859, 0.867, 0.878, 0.821, 0.798, 0.832, 0.817, 0.758, 0.714, 0.693, 0.644, 0.678, 0.683, 0.695, 0.802, 0.792, 0.789, 0.824, 0.763, 0.822, 0.766, 0.73, 0.636, 0.687, 0.602, 0.719, 0.686, 0.718, 0.756, 0.783, 0.717, 0.823, 0.806, 0.78, 0.783, 0.761, 0.831, 0.778, 0.714, 0.596, 0.646, 0.541, 0.599, 0.566, 0.603, 0.642, 0.716, 0.695, 0.741, 0.692, 0.767, 0.779, 0.745, 0.734, 0.786, 0.752, 0.815, 0.796, 0.8, 0.743, 0.7, 0.713, 0.697, 0.679, 0.641, 0.632, 0.678, 0.65, 0.728, 0.655, 0.646, 0.602, 0.574, 0.542, 0.543, 0.518, 0.544, 0.528, 0.615, 0.7, 0.658, 0.665, 0.578, 0.592, 0.725, 0.658, 0.7, 0.678, 0.642, 0.768, 0.708, 0.7, 0.704, 0.771, 0.695, 0.772, 0.767, 0.813, 0.739, 0.662, 0.622, 0.635, 0.631, 0.743, 0.741, 0.838, 0.817, 0.795, 0.73, 0.742, 0.733, 0.712, 0.71, 0.732, 0.697, 0.704, 0.715, 0.829, 0.806, 0.74, 0.755, 0.681, 0.675, 0.653, 0.722, 0.672, 0.68, 0.663, 0.59, 0.645, 0.575, 0.575, 0.537, 0.524, 0.523, 0.495, 0.572, 0.54, 0.677, 0.675, 0.746, 0.777, 0.79, 0.762, 0.775, 0.787, 0.752, 0.64, 0.717, 0.584, 0.63, 0.545, 0.519, 0.489, 0.481, 0.484, 0.5, 0.45, 0.498, 0.44, 0.456, 0.465, 0.459, 0.522, 0.586, 0.572, 0.699, 0.588, 0.583, 0.606, 0.573, 0.617, 0.632, 0.621, 0.635, 0.613, 0.636, 0.544, 0.647, 0.539, 0.633, 0.575, 0.637, 0.618, 0.628, 0.699, 0.663, 0.677, 0.694, 0.579, 0.578, 0.554, 0.502, 0.512, 0.472, 0.553, 0.467, 0.504, 0.457, 0.431, 0.448, 0.452, 0.415, 0.473, 0.465, 0.513, 0.66, 0.669, 0.694, 0.723, 0.681, 0.701, 0.704, 0.597, 0.726, 0.697, 0.763, 0.852, 0.782, 0.838, 0.732, 0.874, 0.843, 0.898, 1.025, 0.963, 0.883, 0.979, 0.782, 0.77, 0.815, 0.751, 0.698, 0.748, 0.619, 0.566, 0.461, 0.427, 0.454, 0.378, 0.478, 0.406, 0.487, 0.421, 0.412, 0.431, 0.388, 0.467, 0.481, 0.51, 0.595, 0.492, 0.562, 0.478, 0.549, 0.528, 0.574, 0.554, 0.613, 0.58, 0.707, 0.659, 0.73]\n",
      "Val Error(all epochs): 6.8235344886779785 \n",
      " [47.138, 47.085, 46.972, 46.901, 46.755, 46.559, 46.378, 46.063, 45.693, 45.362, 45.0, 44.602, 44.229, 43.597, 43.099, 42.402, 42.643, 42.304, 41.332, 41.193, 40.21, 39.063, 38.414, 37.637, 36.279, 36.185, 35.553, 34.451, 34.122, 32.337, 31.492, 31.172, 28.258, 28.597, 26.194, 25.778, 23.449, 22.936, 21.203, 21.067, 20.441, 18.718, 18.691, 16.413, 15.644, 16.068, 14.984, 16.416, 14.716, 13.675, 12.08, 12.6, 11.695, 11.937, 11.222, 11.028, 10.629, 10.759, 10.857, 10.455, 10.902, 10.155, 10.143, 9.981, 9.039, 9.685, 9.646, 9.266, 9.942, 9.298, 9.321, 8.426, 8.305, 8.365, 8.218, 8.153, 8.589, 7.619, 8.056, 7.409, 7.635, 6.911, 7.93, 7.382, 7.998, 7.474, 7.785, 7.725, 7.375, 8.487, 6.975, 7.616, 7.57, 7.828, 7.422, 7.568, 6.863, 7.554, 6.892, 7.225, 6.936, 7.057, 6.966, 7.047, 7.147, 7.405, 7.273, 7.51, 7.429, 7.694, 7.472, 7.976, 7.085, 8.009, 7.551, 7.14, 7.747, 7.321, 7.791, 7.918, 9.065, 7.549, 8.149, 8.198, 8.733, 7.836, 8.073, 7.143, 8.106, 7.763, 8.462, 8.257, 8.806, 8.046, 8.248, 7.889, 8.515, 8.025, 9.081, 7.764, 9.548, 8.176, 8.948, 7.862, 8.932, 8.187, 8.973, 8.364, 8.718, 8.088, 8.706, 8.398, 8.483, 8.383, 8.581, 7.992, 8.011, 7.604, 7.905, 7.792, 8.326, 7.65, 8.272, 7.662, 7.746, 7.769, 7.792, 8.11, 8.383, 8.525, 8.09, 8.755, 7.663, 8.543, 7.741, 8.336, 7.275, 8.843, 7.524, 8.551, 8.082, 8.664, 7.398, 8.192, 7.97, 8.944, 7.924, 8.869, 7.688, 8.425, 7.784, 8.574, 7.767, 9.266, 8.308, 9.117, 9.17, 9.243, 9.602, 9.034, 9.137, 9.467, 8.536, 9.307, 8.844, 8.086, 8.378, 7.925, 8.419, 7.782, 8.728, 8.117, 8.656, 8.394, 8.769, 8.316, 8.856, 8.275, 8.952, 8.521, 8.452, 8.129, 7.976, 7.78, 7.475, 7.891, 7.717, 7.875, 7.705, 9.259, 7.994, 9.13, 8.191, 9.63, 7.575, 9.345, 8.076, 8.605, 8.293, 8.482, 8.645, 8.455, 8.253, 9.013, 8.898, 9.203, 9.06, 8.878, 8.961, 8.231, 9.119, 8.435, 9.631, 8.5, 9.228, 8.533, 8.841, 8.169, 8.581, 8.389, 8.335, 8.292, 8.153, 8.295, 8.027, 8.31, 7.874, 8.028, 7.637, 8.309, 7.622, 8.619, 7.831, 8.738, 8.336, 8.673, 8.31, 8.555, 8.163, 8.184, 7.932, 8.048, 7.688, 7.993, 7.671, 8.003, 7.656, 7.685, 7.802, 7.531, 8.383, 7.73, 8.211, 7.667, 8.331, 7.64, 7.866, 7.653, 7.457, 7.673, 7.448, 7.403, 7.686, 7.386, 8.06, 7.909, 8.415, 8.356, 7.976, 8.428, 7.91, 8.433, 7.933, 8.305, 8.113, 8.092, 8.189, 7.833, 8.038, 7.813, 8.012, 7.818, 8.533, 7.992, 8.239, 8.428, 7.875, 8.401, 7.687, 7.977, 8.255, 7.917, 8.247, 7.767, 8.087, 8.0, 8.103, 7.715, 7.91, 7.69, 7.876, 7.893, 8.143, 7.617, 8.058, 7.874, 9.121, 8.456, 9.206, 9.617, 8.761, 10.102, 8.665, 9.819, 8.774, 9.374, 8.624, 9.038, 8.539, 8.359, 8.498, 8.003, 8.494, 8.442, 8.621, 8.464, 8.313, 8.54, 8.157, 8.108, 8.307, 7.861, 8.556, 7.561, 8.546, 7.502, 8.367, 7.739, 8.031, 7.473, 8.046, 7.275, 8.861, 7.388, 9.131, 7.931, 8.9, 8.267, 8.413, 8.184, 8.124, 8.197, 7.87, 8.028, 7.88, 7.814, 8.028, 7.589, 7.803, 7.545, 7.406, 7.423, 7.439, 7.127, 7.523, 7.271, 7.699, 7.555, 7.686, 7.39, 7.825, 7.214, 8.064, 7.55, 8.28, 7.301, 7.85, 7.217, 7.474, 7.016, 7.424, 7.227, 7.258, 7.543, 7.537, 7.872, 7.781, 7.667, 7.511, 7.51, 7.334, 7.336, 7.238, 7.378, 7.054, 7.338, 7.061, 7.301, 7.27, 7.415, 7.327, 7.505, 7.57, 7.435, 8.063, 7.343, 8.083, 7.309, 7.851, 7.761, 7.693, 7.538, 7.173, 7.409, 7.558, 7.534, 7.229, 8.057, 7.551, 8.794, 8.555, 8.919, 8.434, 8.563, 8.407, 8.404, 9.101, 9.76, 9.474, 9.75, 9.015, 10.258, 9.179, 9.641, 8.867, 9.038, 8.298, 8.621, 7.968, 8.38, 7.866, 8.135, 7.712, 8.238, 7.494, 8.021, 7.472, 7.65, 7.325, 7.659, 7.271, 7.69, 6.938, 7.747, 6.824, 7.42, 7.288, 7.178, 7.381, 7.015]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.254, 0.01, 0.157, 0.102, 0.029, 0.472, 0.952, 0.89, 1.142, 0.934, 1.636, 1.016, 0.942, 0.724, 0.712, 0.888, 0.802, 1.037, 1.085, 1.317, 1.802, 1.937, 1.915, 1.206, 1.087, 0.999, 1.016, 1.481, 1.155, 1.337, 1.32, 1.011, 2.098, 1.831, 2.348, 2.251, 3.192, 2.645, 3.213, 2.297, 2.752, 2.453, 2.113, 1.662, 1.857, 1.208, 1.824, 2.464, 2.925, 2.48, 2.245, 3.127, 2.3, 2.978, 2.702, 2.547, 3.082, 2.907, 2.947, 3.062, 2.672, 2.924, 2.621, 2.392, 2.605, 2.579, 2.306, 3.26, 2.412, 3.26, 3.145, 2.973, 3.816, 2.864, 4.302, 4.515, 5.273, 4.231, 5.063, 5.759, 6.353, 5.589, 5.667, 4.402, 5.0, 4.33, 4.856, 4.574, 4.666, 4.623, 4.436, 4.589, 4.912, 4.438, 5.339, 4.226, 6.145, 5.019, 5.778, 4.66, 5.36, 4.769, 5.059, 4.74, 4.561, 4.144, 5.028, 4.637, 5.401, 4.988, 5.015, 4.846, 4.316, 4.146, 3.903, 4.23, 3.982, 4.158, 4.0, 3.774, 3.796, 3.865, 3.953, 4.283, 4.409, 4.449, 3.721, 4.711, 3.446, 4.094, 3.524, 4.229, 3.019, 4.776, 4.168, 4.54, 5.445, 5.164, 4.194, 4.742, 3.631, 5.029, 3.627, 4.382, 3.479, 3.799, 3.617, 4.481, 3.527, 5.169, 4.555, 5.193, 5.502, 5.188, 5.292, 4.536, 4.466, 4.922, 4.599, 4.871, 5.195, 4.365, 4.612, 4.444, 4.428, 4.279, 4.68, 4.183, 4.27, 4.142, 4.36, 3.834, 4.217, 4.002, 4.208, 4.151, 4.3, 3.387, 4.508, 4.131, 3.783, 4.881, 4.676, 5.082, 4.717, 6.201, 4.941, 5.669, 4.567, 6.28, 3.639, 5.363, 3.954, 4.348, 4.31, 4.262, 4.668, 4.479, 3.774, 5.129, 4.448, 5.322, 4.881, 4.804, 5.044, 3.93, 4.953, 4.151, 5.278, 4.353, 4.844, 4.984, 4.768, 4.555, 4.856, 4.396, 4.706, 4.142, 4.345, 4.141, 4.051, 4.341, 3.729, 4.379, 3.363, 5.142, 3.744, 4.969, 4.221, 5.09, 4.726, 5.068, 4.448, 4.964, 4.156, 4.409, 3.672, 4.285, 3.516, 4.478, 3.95, 4.069, 4.005, 3.88, 4.369, 3.81, 4.759, 3.868, 4.246, 3.774, 4.394, 3.916, 4.327, 4.249, 3.839, 4.098, 3.425, 3.741, 3.436, 3.835, 3.942, 4.385, 4.305, 4.752, 3.863, 4.547, 3.845, 4.351, 3.796, 3.976, 3.931, 3.901, 4.215, 4.04, 3.949, 4.006, 4.299, 3.802, 4.58, 4.497, 4.308, 4.895, 4.201, 4.608, 4.239, 4.257, 4.468, 4.21, 4.373, 4.217, 4.415, 4.475, 4.413, 3.91, 3.958, 4.163, 4.125, 4.665, 4.865, 4.276, 4.476, 4.493, 5.344, 5.395, 5.288, 6.242, 5.101, 6.325, 5.231, 5.912, 5.344, 5.75, 5.134, 5.172, 4.674, 4.363, 4.631, 4.247, 4.497, 4.985, 4.946, 4.827, 4.729, 4.624, 4.452, 4.351, 4.637, 3.959, 4.995, 3.591, 4.952, 3.359, 4.633, 3.59, 4.676, 3.381, 4.59, 3.245, 5.32, 3.381, 5.597, 4.032, 5.157, 4.486, 4.57, 4.362, 4.135, 4.107, 4.188, 3.978, 4.22, 3.783, 4.288, 3.887, 3.773, 3.939, 3.671, 3.473, 4.11, 3.311, 4.019, 3.524, 4.074, 3.636, 3.946, 3.673, 4.18, 3.553, 4.669, 3.908, 4.823, 3.713, 4.446, 3.589, 3.847, 3.05, 3.772, 3.505, 3.731, 3.799, 3.847, 4.468, 4.104, 4.146, 3.712, 3.935, 3.297, 3.969, 3.085, 3.829, 3.212, 3.463, 3.141, 3.388, 3.115, 3.545, 3.184, 3.601, 3.9, 3.894, 4.345, 4.226, 4.151, 4.398, 4.197, 4.285, 4.143, 3.807, 3.74, 3.821, 4.453, 4.136, 3.695, 4.763, 3.46, 5.692, 4.883, 6.252, 4.89, 5.304, 4.934, 4.743, 5.667, 5.974, 5.946, 6.445, 5.735, 7.396, 5.929, 6.473, 5.551, 5.545, 4.781, 4.933, 4.385, 4.418, 4.189, 4.312, 3.71, 4.213, 3.581, 3.874, 3.908, 3.366, 3.561, 3.45, 3.492, 3.461, 3.2, 3.566, 3.262, 3.583, 3.636, 3.117, 4.058, 3.058]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 47.03357, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 47.03357 to 46.92846, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 46.92846 to 46.85093, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 46.85093 to 46.80264, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 46.80264 to 46.73370, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 46.73370 to 46.62801, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 46.62801 to 46.50335, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 46.50335 to 46.38609, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 46.38609 to 46.11829, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 46.11829 to 45.89692, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 45.89692 to 45.31242, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 45.31242 to 44.66752, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 44.66752 to 44.09458, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 44.09458 to 43.50089, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 43.50089 to 43.08984, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 43.08984 to 42.45216, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 42.45216 to 42.31424, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 42.31424 to 41.75307, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 41.75307 to 41.08207, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 41.08207 to 40.38677, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 40.38677\n",
      "\n",
      "Epoch 00022: val_mae improved from 40.38677 to 40.26289, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 40.26289 to 39.79249, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 39.79249 to 37.60009, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 37.60009 to 37.59410, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 37.59410 to 37.53358, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 37.53358 to 36.02778, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 36.02778\n",
      "\n",
      "Epoch 00029: val_mae improved from 36.02778 to 34.43776, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 34.43776 to 33.31497, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 33.31497\n",
      "\n",
      "Epoch 00032: val_mae improved from 33.31497 to 30.36951, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 30.36951\n",
      "\n",
      "Epoch 00034: val_mae improved from 30.36951 to 28.98138, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 28.98138\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 28.98138\n",
      "\n",
      "Epoch 00037: val_mae improved from 28.98138 to 26.27145, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00038: val_mae improved from 26.27145 to 26.16105, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00039: val_mae improved from 26.16105 to 24.02264, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 24.02264\n",
      "\n",
      "Epoch 00041: val_mae improved from 24.02264 to 23.75799, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 23.75799\n",
      "\n",
      "Epoch 00043: val_mae improved from 23.75799 to 18.03493, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 18.03493\n",
      "\n",
      "Epoch 00045: val_mae improved from 18.03493 to 16.11540, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00046: val_mae did not improve from 16.11540\n",
      "\n",
      "Epoch 00047: val_mae improved from 16.11540 to 15.27056, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 15.27056\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 15.27056\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 15.27056\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 15.27056\n",
      "\n",
      "Epoch 00052: val_mae improved from 15.27056 to 14.46859, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 14.46859\n",
      "\n",
      "Epoch 00054: val_mae improved from 14.46859 to 13.92432, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00055: val_mae improved from 13.92432 to 13.35118, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00056: val_mae improved from 13.35118 to 12.48098, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 12.48098\n",
      "\n",
      "Epoch 00058: val_mae improved from 12.48098 to 11.41733, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 11.41733\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 11.41733\n",
      "\n",
      "Epoch 00061: val_mae improved from 11.41733 to 11.02223, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 11.02223\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 11.02223\n",
      "\n",
      "Epoch 00064: val_mae improved from 11.02223 to 10.49763, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 10.49763\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 10.49763\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 10.49763\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 10.49763\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 10.49763\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 10.49763\n",
      "\n",
      "Epoch 00071: val_mae improved from 10.49763 to 9.95112, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00072: val_mae improved from 9.95112 to 9.70949, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 9.70949\n",
      "\n",
      "Epoch 00074: val_mae improved from 9.70949 to 9.60490, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 9.60490\n",
      "\n",
      "Epoch 00076: val_mae improved from 9.60490 to 9.14168, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00077: val_mae improved from 9.14168 to 8.30626, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 8.30626\n",
      "\n",
      "Epoch 00079: val_mae improved from 8.30626 to 7.74158, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 7.74158\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 7.74158\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 7.74158\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 7.74158\n",
      "\n",
      "Epoch 00084: val_mae improved from 7.74158 to 7.63636, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00085: val_mae improved from 7.63636 to 7.04784, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 7.04784\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 7.04784\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 7.04784\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 7.04784\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 7.04784\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 7.04784\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 7.04784\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 7.04784\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 7.04784\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 7.04784\n",
      "\n",
      "Epoch 00096: val_mae improved from 7.04784 to 6.83269, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 6.83269\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 6.83269\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 6.83269\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 6.83269\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 6.83269\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 6.83269\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 6.83269\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 6.83269\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 6.83269\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 6.83269\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 6.83269\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 6.83269\n",
      "\n",
      "Epoch 00109: val_mae improved from 6.83269 to 6.70557, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 6.70557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00149: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 6.70557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00310: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 6.70557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00471: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 6.70557\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 6.70557\n",
      "\n",
      "Lambda: 0.1 , Time: 0:03:57\n",
      "Train Error(all epochs): 0.8301255106925964 \n",
      " [49.179, 49.02, 48.927, 48.828, 48.71, 48.567, 48.394, 48.187, 47.937, 47.638, 47.272, 46.842, 46.363, 45.835, 45.25, 44.606, 43.901, 43.142, 42.303, 41.415, 40.467, 39.464, 38.408, 37.291, 36.131, 34.933, 33.701, 32.451, 31.142, 29.855, 28.523, 27.176, 25.825, 24.456, 23.13, 21.804, 20.5, 19.285, 18.047, 16.924, 15.817, 14.843, 13.944, 13.187, 12.329, 11.693, 10.951, 10.417, 9.849, 9.515, 9.17, 8.68, 8.592, 8.185, 7.977, 7.685, 7.286, 7.231, 6.707, 6.403, 6.28, 6.094, 5.968, 5.854, 5.799, 5.466, 5.286, 5.168, 5.098, 4.945, 4.927, 4.886, 4.889, 4.722, 4.786, 4.793, 4.93, 4.813, 4.652, 4.717, 4.555, 4.62, 4.554, 4.648, 4.548, 4.531, 4.331, 4.393, 4.489, 4.262, 4.241, 4.143, 4.158, 4.099, 3.888, 4.077, 3.9, 3.912, 3.947, 3.91, 3.947, 3.944, 3.835, 3.87, 3.661, 3.709, 3.682, 3.77, 3.756, 3.739, 3.731, 4.17, 3.919, 3.949, 3.92, 3.854, 3.572, 3.476, 3.554, 3.474, 3.556, 3.241, 3.264, 3.197, 3.336, 3.292, 3.156, 3.225, 3.007, 3.158, 2.918, 2.886, 2.841, 2.788, 2.867, 2.832, 2.887, 2.936, 3.039, 2.86, 2.933, 2.952, 2.767, 2.93, 2.846, 2.818, 2.644, 2.615, 2.56, 2.538, 2.628, 2.734, 2.741, 2.734, 2.845, 3.138, 3.175, 3.078, 2.798, 2.822, 2.821, 2.97, 2.719, 2.662, 2.54, 2.438, 2.421, 2.496, 2.546, 2.503, 2.533, 2.584, 2.479, 2.418, 2.298, 2.384, 2.304, 2.186, 2.211, 2.045, 2.018, 2.095, 2.015, 1.981, 1.972, 2.004, 2.164, 2.165, 2.494, 2.606, 2.382, 2.325, 2.157, 2.175, 2.061, 2.045, 2.045, 2.069, 2.071, 2.018, 1.846, 1.835, 1.789, 1.761, 1.818, 1.847, 2.221, 2.419, 2.329, 2.384, 2.339, 2.378, 2.283, 2.53, 2.86, 2.919, 2.594, 2.499, 2.387, 2.215, 1.884, 1.894, 1.803, 1.601, 1.495, 1.451, 1.468, 1.592, 1.607, 1.552, 1.453, 1.419, 1.392, 1.587, 1.83, 2.058, 2.04, 2.004, 1.981, 2.084, 2.293, 2.347, 2.239, 2.038, 1.803, 1.646, 1.676, 1.671, 1.86, 1.867, 1.872, 1.783, 1.828, 1.882, 1.682, 1.642, 1.493, 1.473, 1.42, 1.448, 1.378, 1.41, 1.474, 1.488, 1.581, 1.579, 1.633, 1.617, 1.637, 1.762, 1.75, 1.61, 1.577, 1.705, 1.842, 1.799, 1.769, 1.539, 1.67, 1.657, 1.874, 1.932, 1.972, 1.782, 1.724, 1.586, 1.496, 1.441, 1.387, 1.377, 1.322, 1.309, 1.312, 1.362, 1.277, 1.265, 1.271, 1.219, 1.288, 1.379, 1.55, 1.67, 1.752, 2.015, 2.093, 2.134, 2.082, 1.993, 1.947, 1.799, 1.615, 1.614, 1.58, 1.43, 1.47, 1.463, 1.558, 1.67, 1.767, 1.765, 1.623, 1.467, 1.329, 1.385, 1.368, 1.433, 1.383, 1.401, 1.474, 1.674, 1.631, 1.635, 1.567, 1.561, 1.7, 1.711, 1.724, 1.562, 1.371, 1.284, 1.159, 1.041, 0.962, 0.948, 0.953, 1.059, 1.129, 1.333, 1.299, 1.358, 1.314, 1.344, 1.296, 1.247, 1.38, 1.428, 1.493, 1.433, 1.308, 1.209, 1.22, 1.191, 1.215, 1.183, 1.24, 1.192, 1.198, 1.192, 1.346, 1.342, 1.365, 1.462, 1.63, 1.592, 1.56, 1.555, 1.582, 1.816, 2.073, 2.103, 2.154, 2.062, 2.207, 1.957, 1.664, 1.745, 1.67, 1.62, 1.667, 1.669, 1.592, 1.448, 1.372, 1.293, 1.402, 1.471, 1.681, 1.662, 1.603, 1.752, 1.723, 1.813, 1.765, 1.809, 1.821, 1.752, 1.735, 1.77, 1.805, 1.645, 1.505, 1.275, 1.205, 1.142, 1.187, 1.204, 1.32, 1.254, 1.194, 1.091, 0.966, 0.937, 0.83, 0.889, 0.912, 0.997, 1.054, 1.064, 1.005, 0.96, 1.079, 1.071, 1.126, 1.092, 1.217, 1.291, 1.416, 1.558, 1.553, 1.496, 1.401, 1.375, 1.34, 1.362, 1.279, 1.254, 1.179, 1.038, 1.032, 1.065, 1.148, 1.127, 1.224, 1.35, 1.533, 1.723, 1.7, 1.814, 1.884, 1.779, 1.841, 1.856, 1.83, 1.92, 1.777, 1.804, 1.893, 1.99, 2.076, 2.008, 1.924, 1.688, 1.796, 1.616, 1.541, 1.341, 1.247, 1.173, 1.13, 1.197, 1.203, 1.247, 1.232, 1.152, 1.06, 0.974, 1.042, 0.993, 1.041, 1.094, 1.101, 1.042, 0.936, 1.038, 1.112, 1.29, 1.339, 1.301, 1.158, 1.007]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.009, 0.019, 0.039, 0.044, 0.103, 0.097, 0.184, 0.239, 0.376, 0.391, 0.507, 0.531, 0.652, 0.721, 0.841, 0.987, 0.99, 1.151, 1.198, 1.3, 1.317, 1.312, 1.464, 1.326, 1.342, 1.389, 1.442, 1.483, 1.535, 1.625, 1.511, 1.523, 1.534, 1.592, 1.555, 1.59, 1.661, 1.686, 1.669, 1.711, 1.779, 1.911, 1.817, 1.769, 1.858, 1.781, 1.787, 1.844, 1.797, 1.844, 1.923, 1.673, 1.683, 1.903, 1.626, 1.589, 1.615, 1.543, 1.655, 1.396, 1.573, 1.533, 1.447, 1.499, 1.635, 1.467, 1.572, 1.465, 1.494, 1.482, 1.37, 1.53, 1.421, 1.597, 1.515, 1.428, 1.786, 1.809, 1.572, 1.543, 1.664, 1.26, 1.45, 1.474, 1.362, 1.561, 1.228, 1.306, 1.285, 1.383, 1.347, 1.204, 1.371, 1.173, 1.253, 1.167, 1.141, 1.11, 1.092, 1.182, 1.098, 1.251, 1.136, 1.319, 1.134, 1.199, 1.244, 1.079, 1.249, 1.216, 1.061, 1.184, 0.959, 1.088, 1.007, 1.027, 1.232, 1.091, 1.168, 1.233, 1.37, 1.495, 1.33, 1.132, 1.164, 1.269, 1.246, 1.178, 1.095, 1.11, 0.91, 1.028, 1.106, 0.997, 1.184, 0.964, 1.243, 0.951, 1.013, 0.945, 1.003, 0.965, 0.916, 0.961, 0.808, 0.795, 0.928, 0.783, 0.86, 0.807, 0.887, 0.911, 0.96, 1.125, 1.162, 1.032, 0.983, 0.914, 0.97, 0.881, 0.771, 1.007, 0.783, 0.944, 0.859, 0.714, 0.846, 0.705, 0.759, 0.777, 0.77, 1.089, 1.041, 1.11, 0.97, 1.126, 0.952, 1.072, 1.11, 1.373, 1.283, 1.352, 0.989, 1.124, 0.941, 0.706, 0.894, 0.653, 0.783, 0.542, 0.627, 0.633, 0.66, 0.722, 0.646, 0.629, 0.59, 0.614, 0.668, 0.869, 0.925, 0.955, 0.924, 0.833, 0.954, 1.094, 1.089, 0.997, 0.823, 0.958, 0.528, 0.891, 0.701, 0.892, 0.793, 0.87, 0.76, 0.839, 0.857, 0.763, 0.762, 0.609, 0.648, 0.668, 0.543, 0.727, 0.461, 0.802, 0.548, 0.773, 0.667, 0.781, 0.662, 0.844, 0.74, 0.864, 0.688, 0.708, 0.772, 0.908, 0.738, 0.901, 0.577, 0.895, 0.629, 0.969, 0.83, 0.955, 0.733, 0.855, 0.627, 0.778, 0.55, 0.681, 0.56, 0.671, 0.491, 0.67, 0.538, 0.642, 0.519, 0.606, 0.505, 0.615, 0.578, 0.771, 0.811, 0.718, 1.14, 0.74, 1.178, 0.914, 0.944, 0.869, 0.828, 0.718, 0.688, 0.794, 0.581, 0.736, 0.609, 0.762, 0.766, 0.804, 0.839, 0.675, 0.757, 0.497, 0.757, 0.498, 0.791, 0.524, 0.712, 0.696, 0.751, 0.85, 0.633, 0.868, 0.572, 0.952, 0.747, 0.781, 0.792, 0.522, 0.671, 0.439, 0.527, 0.392, 0.405, 0.459, 0.442, 0.57, 0.582, 0.653, 0.625, 0.588, 0.64, 0.594, 0.595, 0.668, 0.633, 0.707, 0.688, 0.568, 0.585, 0.538, 0.56, 0.539, 0.56, 0.594, 0.524, 0.581, 0.53, 0.638, 0.623, 0.711, 0.594, 0.861, 0.694, 0.767, 0.757, 0.666, 0.936, 0.942, 1.006, 0.996, 0.951, 1.028, 0.976, 0.602, 0.971, 0.631, 0.877, 0.751, 0.764, 0.788, 0.607, 0.717, 0.527, 0.688, 0.683, 0.748, 0.833, 0.625, 0.973, 0.608, 1.032, 0.658, 1.067, 0.749, 0.933, 0.717, 0.912, 0.792, 0.728, 0.765, 0.505, 0.62, 0.476, 0.589, 0.54, 0.592, 0.617, 0.496, 0.562, 0.38, 0.503, 0.295, 0.468, 0.402, 0.432, 0.556, 0.42, 0.542, 0.383, 0.539, 0.488, 0.529, 0.493, 0.56, 0.64, 0.652, 0.769, 0.685, 0.759, 0.579, 0.694, 0.661, 0.581, 0.647, 0.512, 0.628, 0.411, 0.535, 0.443, 0.58, 0.516, 0.593, 0.613, 0.777, 0.808, 0.888, 0.712, 1.065, 0.683, 0.867, 0.932, 0.721, 0.964, 0.775, 0.849, 0.941, 0.995, 0.929, 1.029, 0.776, 0.778, 0.887, 0.698, 0.783, 0.508, 0.676, 0.475, 0.576, 0.536, 0.607, 0.551, 0.624, 0.48, 0.562, 0.406, 0.498, 0.468, 0.428, 0.598, 0.405, 0.613, 0.341, 0.546, 0.484, 0.604, 0.667, 0.57, 0.545, 0.471]\n",
      "Val Error(all epochs): 6.705567836761475 \n",
      " [47.034, 46.928, 46.851, 46.803, 46.734, 46.628, 46.503, 46.386, 46.118, 45.897, 45.312, 44.668, 44.095, 43.501, 43.09, 42.452, 42.314, 41.753, 41.082, 40.387, 40.603, 40.263, 39.792, 37.6, 37.594, 37.534, 36.028, 36.177, 34.438, 33.315, 34.519, 30.37, 30.775, 28.981, 30.834, 29.498, 26.271, 26.161, 24.023, 25.82, 23.758, 25.669, 18.035, 18.377, 16.115, 18.078, 15.271, 16.314, 18.378, 16.822, 17.428, 14.469, 14.774, 13.924, 13.351, 12.481, 12.972, 11.417, 12.539, 12.212, 11.022, 11.367, 11.224, 10.498, 10.83, 10.964, 10.939, 10.635, 10.757, 10.797, 9.951, 9.709, 10.39, 9.605, 9.681, 9.142, 8.306, 8.745, 7.742, 7.89, 8.315, 7.844, 7.869, 7.636, 7.048, 8.201, 7.719, 9.227, 7.445, 9.061, 9.468, 8.949, 8.652, 8.925, 8.258, 6.833, 7.844, 7.219, 8.821, 7.003, 8.369, 8.6, 7.541, 7.628, 7.501, 7.424, 6.961, 7.81, 6.706, 7.634, 7.054, 7.565, 7.205, 7.786, 8.074, 7.339, 7.037, 7.289, 7.259, 7.284, 6.888, 7.381, 6.762, 7.721, 7.039, 7.049, 7.436, 7.106, 7.506, 7.229, 7.296, 7.395, 7.112, 7.37, 7.466, 7.494, 7.379, 8.355, 7.472, 8.371, 7.753, 7.592, 8.063, 7.951, 7.94, 7.428, 8.305, 8.299, 7.793, 9.148, 7.173, 9.72, 8.44, 8.416, 9.153, 9.426, 7.979, 9.355, 8.891, 7.24, 8.315, 7.626, 7.897, 8.335, 7.208, 8.105, 7.25, 8.611, 7.194, 8.446, 7.594, 8.941, 8.281, 9.046, 8.327, 8.86, 7.903, 8.546, 7.671, 8.468, 8.268, 8.87, 8.437, 9.112, 8.558, 9.025, 8.46, 8.487, 8.698, 9.223, 8.238, 9.305, 8.596, 8.537, 8.765, 8.259, 9.148, 8.257, 9.064, 8.739, 8.77, 8.823, 8.904, 8.571, 9.221, 8.549, 8.719, 8.258, 8.418, 7.709, 8.511, 7.678, 8.118, 7.138, 8.973, 7.757, 7.586, 7.77, 7.567, 7.377, 7.724, 8.144, 8.146, 8.425, 8.605, 8.489, 8.725, 8.385, 8.94, 8.474, 9.174, 9.103, 9.084, 9.613, 9.277, 8.946, 9.735, 8.81, 8.412, 9.164, 8.452, 8.696, 8.916, 8.234, 8.186, 8.392, 8.108, 8.355, 7.532, 7.803, 7.911, 7.725, 8.455, 8.022, 8.553, 8.146, 8.852, 8.122, 8.946, 8.184, 8.933, 8.147, 8.989, 8.363, 9.136, 8.594, 9.303, 8.793, 8.679, 8.639, 8.876, 8.218, 9.172, 8.057, 8.706, 8.699, 8.316, 8.925, 8.499, 8.536, 7.986, 7.833, 8.136, 7.97, 8.029, 8.218, 7.849, 8.041, 8.321, 8.036, 8.623, 7.87, 8.282, 8.021, 8.038, 8.343, 8.237, 7.985, 8.581, 7.803, 7.941, 8.237, 7.539, 8.183, 7.61, 7.981, 7.782, 7.943, 7.957, 7.764, 8.185, 7.802, 7.723, 8.072, 7.347, 8.09, 7.39, 7.844, 7.233, 7.619, 6.942, 7.404, 7.225, 7.527, 7.403, 7.717, 7.786, 7.833, 7.687, 7.515, 7.637, 7.509, 7.606, 7.738, 7.565, 7.544, 7.234, 7.537, 7.242, 7.34, 7.618, 7.551, 7.72, 7.694, 7.715, 7.902, 7.803, 7.962, 7.945, 7.641, 7.787, 7.631, 7.588, 8.038, 7.433, 7.968, 7.735, 7.975, 7.714, 7.966, 7.682, 8.24, 7.675, 8.172, 7.587, 7.887, 7.784, 8.007, 7.979, 8.029, 7.815, 7.568, 7.446, 7.392, 7.279, 7.537, 7.687, 7.184, 7.686, 7.894, 8.009, 7.534, 7.227, 7.13, 7.124, 7.233, 7.083, 7.27, 7.583, 7.217, 7.327, 7.283, 7.377, 7.545, 7.16, 7.636, 7.045, 7.553, 7.165, 7.371, 7.405, 7.477, 7.56, 7.457, 7.542, 8.311, 8.188, 7.945, 7.722, 7.642, 7.874, 7.914, 7.609, 7.998, 7.533, 7.953, 7.393, 7.942, 7.199, 7.777, 7.346, 7.663, 7.561, 7.699, 7.812, 7.686, 7.721, 7.639, 7.623, 7.638, 7.597, 7.73, 7.668, 7.725, 7.429, 7.584, 7.257, 7.182, 7.473, 7.311, 7.477, 7.488, 7.499, 7.19, 7.563, 7.269, 7.639, 7.211, 7.753, 7.266, 7.797, 7.467, 7.698, 7.412, 7.925, 7.245, 8.054, 7.06, 7.654, 7.672, 7.85, 7.929, 7.478, 7.028, 7.559, 7.106, 7.256, 7.187, 7.732, 7.832, 7.729, 7.692, 7.652, 7.25, 8.043, 7.974, 8.152, 7.862, 8.206, 7.895, 8.2, 7.613, 8.098, 7.699, 8.295, 7.844, 8.007, 7.872, 7.818, 7.983, 7.69, 7.745, 7.734, 7.657, 7.773, 7.299, 7.375, 7.212, 7.408, 7.303]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.177, 0.043, 0.623, 0.061, 0.581, 0.101, 0.028, 0.119, 0.016, 0.702, 0.39, 1.899, 0.773, 0.596, 0.819, 0.889, 0.326, 0.468, 0.789, 1.423, 0.944, 1.161, 0.851, 0.566, 0.311, 0.317, 0.301, 0.376, 0.601, 0.781, 0.966, 1.487, 1.022, 2.534, 1.251, 1.367, 1.908, 1.544, 2.117, 1.193, 2.449, 1.765, 1.71, 2.643, 1.803, 4.065, 2.727, 3.602, 3.444, 3.395, 2.63, 3.711, 2.25, 2.385, 2.526, 2.428, 4.138, 3.459, 3.979, 4.242, 4.467, 3.992, 3.38, 3.397, 2.724, 4.187, 2.409, 3.933, 1.891, 4.32, 3.931, 4.112, 4.986, 3.476, 3.527, 3.627, 3.324, 3.656, 2.829, 3.799, 3.001, 4.009, 3.318, 3.438, 3.237, 2.18, 3.425, 2.132, 3.504, 2.649, 3.226, 3.108, 2.307, 3.236, 2.624, 4.24, 3.291, 4.615, 3.807, 3.688, 4.437, 4.041, 4.104, 3.943, 4.626, 5.023, 4.323, 5.192, 3.479, 5.598, 3.986, 5.082, 4.523, 5.961, 4.77, 5.95, 5.455, 3.379, 4.727, 4.541, 4.573, 5.333, 3.884, 4.438, 3.871, 4.647, 3.876, 4.667, 4.306, 5.672, 4.604, 4.955, 4.703, 4.679, 4.141, 4.731, 4.31, 4.704, 4.649, 4.985, 4.586, 4.91, 4.473, 4.895, 4.647, 4.931, 5.371, 5.793, 5.524, 5.657, 5.725, 5.29, 5.327, 5.132, 5.629, 5.004, 5.412, 5.195, 4.818, 4.963, 4.597, 4.464, 4.576, 4.873, 4.431, 5.119, 4.847, 4.15, 5.832, 3.913, 4.259, 3.724, 4.741, 4.504, 4.244, 2.598, 3.94, 3.445, 3.992, 4.376, 4.532, 4.543, 4.93, 4.712, 4.843, 4.877, 5.009, 4.934, 5.106, 4.95, 5.144, 5.044, 5.604, 4.754, 6.329, 4.859, 4.828, 5.911, 4.509, 6.179, 5.257, 5.707, 4.362, 5.626, 4.273, 5.085, 3.74, 4.086, 4.184, 3.763, 4.77, 4.024, 4.888, 4.211, 4.905, 4.268, 4.659, 4.492, 4.495, 4.536, 4.492, 4.734, 4.661, 5.237, 4.929, 5.448, 4.793, 5.28, 5.497, 4.919, 5.072, 5.129, 4.771, 5.743, 4.369, 5.796, 4.705, 4.667, 4.669, 4.326, 4.957, 4.288, 5.023, 4.386, 4.397, 4.41, 4.517, 4.238, 4.697, 4.134, 4.276, 4.468, 4.067, 4.833, 4.065, 4.623, 4.044, 4.343, 3.788, 4.093, 4.289, 3.943, 4.172, 4.395, 4.565, 4.297, 4.507, 4.037, 4.479, 4.324, 4.105, 4.094, 3.407, 3.701, 3.651, 3.247, 3.623, 3.093, 3.142, 3.068, 3.25, 3.206, 3.854, 3.278, 4.097, 3.628, 3.752, 3.765, 3.299, 4.172, 3.607, 3.943, 3.7, 3.569, 3.503, 3.333, 3.503, 3.416, 3.457, 3.433, 3.57, 3.5, 3.731, 3.4, 4.069, 3.456, 4.002, 3.576, 3.587, 4.073, 3.539, 4.476, 3.581, 4.357, 4.178, 4.084, 4.294, 4.082, 3.67, 4.309, 3.475, 4.087, 3.685, 3.711, 3.795, 3.763, 3.612, 3.845, 3.77, 3.511, 3.609, 3.294, 3.56, 3.755, 4.073, 3.493, 3.6, 5.24, 3.16, 4.793, 2.937, 2.436, 2.652, 2.312, 3.256, 3.608, 3.548, 3.615, 2.942, 3.673, 3.026, 3.663, 3.341, 3.218, 3.24, 2.913, 3.264, 3.472, 3.796, 3.602, 4.211, 3.428, 4.022, 4.401, 4.603, 4.932, 4.406, 4.461, 4.454, 4.816, 3.938, 4.437, 3.729, 4.176, 3.82, 3.904, 3.86, 3.656, 3.699, 3.505, 3.511, 3.665, 3.379, 4.011, 3.426, 4.116, 3.429, 3.957, 3.41, 3.801, 3.383, 3.396, 3.071, 3.28, 3.703, 2.739, 3.863, 2.792, 4.009, 2.833, 3.068, 3.243, 3.033, 3.478, 2.899, 3.44, 2.87, 3.553, 2.962, 3.244, 3.159, 3.26, 3.1, 3.699, 3.651, 2.938, 4.387, 3.551, 3.787, 4.175, 3.602, 2.859, 2.932, 3.902, 3.24, 4.278, 4.578, 5.163, 4.746, 4.364, 3.746, 4.343, 4.407, 4.8, 4.646, 4.195, 4.691, 3.987, 4.598, 3.992, 4.361, 4.148, 4.518, 3.939, 4.26, 3.939, 4.182, 3.944, 4.093, 3.721, 3.973, 3.478, 3.905, 3.091, 3.249, 3.779, 3.135, 3.486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 47.03085, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 47.03085 to 46.95691, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 46.95691 to 46.87885, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 46.87885 to 46.78842, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 46.78842 to 46.65661, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 46.65661 to 46.50198, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 46.50198 to 46.33785, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 46.33785 to 46.12953, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 46.12953 to 45.89502, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 45.89502 to 45.64066, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 45.64066 to 45.31700, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 45.31700 to 44.98278, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 44.98278 to 44.64707, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 44.64707 to 44.32265, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 44.32265 to 43.93941, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 43.93941 to 43.37330, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 43.37330 to 42.72701, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 42.72701 to 42.52595, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 42.52595 to 42.51128, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 42.51128 to 42.10062, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 42.10062 to 40.93138, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 40.93138 to 40.11173, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 40.11173 to 39.38647, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 39.38647 to 38.17815, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 38.17815\n",
      "\n",
      "Epoch 00026: val_mae improved from 38.17815 to 36.47902, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 36.47902 to 36.35845, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 36.35845 to 34.98351, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 34.98351 to 33.89871, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 33.89871 to 32.16017, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00031: val_mae improved from 32.16017 to 31.96988, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00032: val_mae improved from 31.96988 to 30.84175, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 30.84175 to 29.83122, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00034: val_mae improved from 29.83122 to 28.05168, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00035: val_mae improved from 28.05168 to 27.09890, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00036: val_mae improved from 27.09890 to 27.06491, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00037: val_mae improved from 27.06491 to 24.17354, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00038: val_mae improved from 24.17354 to 24.10107, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00039: val_mae improved from 24.10107 to 23.19942, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_mae improved from 23.19942 to 22.97078, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00041: val_mae improved from 22.97078 to 21.45744, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00042: val_mae improved from 21.45744 to 21.05380, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00043: val_mae improved from 21.05380 to 19.28555, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00044: val_mae improved from 19.28555 to 17.24005, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00045: val_mae improved from 17.24005 to 14.66390, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00046: val_mae improved from 14.66390 to 14.06705, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00047: val_mae improved from 14.06705 to 12.84875, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00048: val_mae improved from 12.84875 to 11.75580, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00049: val_mae improved from 11.75580 to 8.13009, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 8.13009\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 8.13009\n",
      "\n",
      "Epoch 00052: val_mae improved from 8.13009 to 7.89199, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00053: val_mae improved from 7.89199 to 7.81627, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 7.81627\n",
      "\n",
      "Epoch 00092: val_mae improved from 7.81627 to 7.21047, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 7.21047\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 7.21047\n",
      "\n",
      "Epoch 00095: val_mae improved from 7.21047 to 6.84432, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 6.84432\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 6.84432\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 6.84432\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 6.84432\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 6.84432\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 6.84432\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 6.84432\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 6.84432\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 6.84432\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 6.84432\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 6.84432\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 6.84432\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 6.84432\n",
      "\n",
      "Epoch 00109: val_mae improved from 6.84432 to 6.60536, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 6.60536\n",
      "\n",
      "Epoch 00134: val_mae improved from 6.60536 to 6.20865, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 6.20865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00150: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 6.20865\n",
      "\n",
      "Epoch 00292: val_mae improved from 6.20865 to 6.13267, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 6.13267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00308: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 6.13267\n",
      "\n",
      "Epoch 00313: val_mae improved from 6.13267 to 6.10578, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 6.10578\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 6.10578\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 6.10578\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 6.10578\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 6.10578\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 6.10578\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 6.10578\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 6.10578\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 6.10578\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 6.10578\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 6.10578\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 6.10578\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 6.10578\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 6.10578\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 6.10578\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 6.10578\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 6.10578\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 6.10578\n",
      "\n",
      "Epoch 00332: val_mae improved from 6.10578 to 6.02620, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_6/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 6.02620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00463: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 6.02620\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 6.02620\n",
      "\n",
      "Lambda: 1 , Time: 0:03:58\n",
      "Train Error(all epochs): 1.723756194114685 \n",
      " [49.219, 49.138, 49.071, 48.992, 48.904, 48.799, 48.675, 48.521, 48.344, 48.136, 47.891, 47.612, 47.287, 46.91, 46.475, 45.97, 45.387, 44.768, 44.084, 43.345, 42.572, 41.747, 40.916, 40.04, 39.171, 38.286, 37.312, 36.377, 35.415, 34.445, 33.475, 32.509, 31.512, 30.445, 29.398, 28.342, 27.332, 26.264, 25.249, 24.263, 23.175, 22.16, 21.078, 20.101, 19.097, 18.029, 17.108, 15.901, 14.871, 13.917, 12.88, 12.094, 11.185, 10.373, 9.726, 9.159, 8.394, 8.016, 7.538, 7.172, 6.723, 6.137, 6.059, 5.619, 5.461, 5.515, 5.124, 5.054, 5.098, 4.764, 5.032, 5.027, 5.055, 4.762, 4.681, 4.659, 4.416, 4.316, 4.076, 4.299, 4.038, 4.194, 4.147, 3.986, 3.937, 4.049, 3.926, 3.945, 4.04, 3.842, 3.953, 4.094, 4.26, 3.992, 3.972, 3.939, 3.7, 3.417, 3.481, 3.305, 3.278, 3.211, 3.396, 3.421, 3.722, 3.808, 4.097, 3.827, 4.136, 3.929, 3.707, 4.036, 3.897, 3.776, 3.504, 3.607, 3.352, 3.436, 3.105, 3.085, 2.936, 2.844, 3.011, 3.101, 3.338, 3.771, 3.805, 3.847, 3.814, 3.923, 4.007, 3.807, 3.633, 3.299, 3.373, 3.216, 3.252, 3.008, 3.084, 2.93, 2.994, 3.059, 3.287, 3.264, 3.555, 3.348, 3.24, 3.181, 3.108, 3.143, 3.123, 3.177, 3.297, 3.27, 3.21, 3.373, 3.193, 3.161, 2.982, 2.886, 2.899, 2.875, 3.092, 3.182, 3.218, 3.438, 3.591, 3.552, 3.511, 3.346, 3.209, 3.598, 3.42, 3.371, 3.253, 3.168, 3.12, 3.094, 3.07, 3.045, 2.728, 2.797, 2.872, 3.0, 2.933, 2.749, 2.778, 2.548, 2.624, 2.49, 2.719, 2.841, 3.037, 3.027, 3.199, 3.138, 3.295, 3.357, 3.16, 3.301, 3.167, 3.077, 2.874, 2.946, 3.04, 3.003, 3.066, 2.872, 2.65, 2.853, 2.766, 3.11, 2.993, 2.829, 2.845, 2.737, 2.614, 2.275, 2.209, 2.169, 2.151, 2.293, 2.509, 2.551, 3.227, 3.048, 3.183, 3.117, 2.981, 3.174, 3.242, 3.352, 3.087, 2.874, 2.688, 3.147, 2.976, 3.124, 2.819, 2.894, 2.846, 3.025, 2.81, 2.983, 2.744, 2.867, 2.579, 2.657, 2.914, 2.852, 2.707, 2.755, 2.467, 2.595, 2.608, 2.492, 2.48, 2.548, 2.595, 2.83, 2.549, 2.839, 2.727, 2.839, 2.668, 2.822, 2.869, 2.871, 2.775, 2.839, 2.786, 3.012, 2.828, 2.923, 2.793, 2.815, 2.732, 2.725, 2.765, 2.719, 2.608, 2.638, 2.476, 2.502, 2.813, 2.673, 2.691, 2.689, 2.818, 2.468, 2.56, 2.548, 2.485, 2.555, 2.532, 2.469, 2.754, 2.558, 2.77, 2.849, 2.617, 2.47, 2.476, 2.277, 2.27, 2.389, 2.729, 2.865, 2.857, 2.838, 2.757, 2.634, 2.623, 2.96, 2.656, 2.901, 2.737, 2.749, 2.558, 2.48, 2.418, 2.702, 2.666, 2.687, 2.678, 2.654, 2.462, 2.392, 2.219, 2.684, 2.543, 2.533, 2.55, 2.437, 2.542, 2.535, 2.434, 2.133, 2.026, 1.862, 1.796, 1.992, 2.068, 2.394, 2.348, 2.44, 2.857, 3.03, 3.169, 2.988, 2.859, 2.796, 2.989, 2.887, 2.837, 2.778, 2.766, 2.825, 2.482, 2.513, 2.243, 2.124, 1.983, 2.034, 1.955, 2.184, 2.384, 2.708, 2.841, 2.692, 2.855, 2.628, 2.684, 2.653, 2.994, 3.247, 3.238, 2.991, 2.914, 2.488, 2.502, 2.331, 2.266, 2.197, 2.169, 2.229, 2.1, 2.099, 1.845, 1.724, 1.799, 2.053, 2.184, 2.394, 2.401, 2.305, 2.313, 2.176, 2.109, 2.249, 2.226, 2.203, 2.169, 2.242, 2.444, 2.691, 2.835, 2.625, 2.499, 2.509, 2.574, 2.323, 2.943, 2.79, 3.023, 2.726, 2.584, 2.581, 2.647, 2.552, 2.852, 2.93, 2.55, 2.682, 2.595, 2.503, 2.44, 2.516, 2.44, 2.321, 2.328, 2.273, 2.086, 2.087, 1.927, 2.214, 2.388, 2.471, 2.439, 2.279, 2.093, 2.014, 1.873, 2.081, 1.96, 2.077, 2.375, 2.631, 2.655, 2.487, 2.364, 2.346, 2.12, 2.226, 2.238, 2.396, 2.811, 3.07, 2.778, 2.78, 2.664, 2.613, 2.855, 2.683, 2.44, 2.404, 2.471, 2.15, 2.166, 1.998, 1.951, 2.133, 2.175, 2.336, 2.366, 2.445, 2.883, 2.651, 2.467, 2.431, 2.57, 2.716, 2.771, 2.695, 2.397, 2.326, 2.413, 2.412, 2.437, 2.205, 2.243, 2.12, 1.972, 2.18, 2.081, 2.105, 2.166, 2.028, 2.115, 2.04]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.007, 0.006, 0.003, 0.021, 0.004, 0.025, 0.012, 0.041, 0.087, 0.051, 0.117, 0.133, 0.142, 0.207, 0.309, 0.269, 0.4, 0.471, 0.538, 0.615, 0.555, 0.672, 0.723, 0.796, 0.995, 0.998, 1.01, 1.239, 1.132, 1.358, 1.481, 1.466, 1.454, 1.39, 1.426, 1.433, 1.37, 1.311, 1.465, 1.36, 1.426, 1.462, 1.412, 1.308, 1.461, 1.394, 1.38, 1.47, 1.393, 1.428, 1.5, 1.566, 1.457, 1.401, 1.45, 1.335, 1.203, 1.24, 1.153, 1.198, 1.116, 1.251, 1.274, 1.413, 1.458, 1.701, 1.468, 1.562, 1.514, 1.395, 1.481, 1.531, 1.385, 1.336, 1.316, 1.151, 1.256, 1.109, 1.082, 1.033, 0.982, 1.066, 1.105, 1.227, 1.565, 1.562, 1.515, 1.431, 1.616, 1.503, 1.433, 1.388, 1.216, 1.207, 1.245, 1.258, 1.1, 1.164, 1.065, 1.063, 1.127, 1.278, 1.183, 1.443, 1.383, 1.194, 1.153, 1.176, 1.212, 1.172, 1.245, 1.256, 1.292, 1.219, 1.346, 1.23, 1.196, 1.076, 1.014, 1.079, 1.086, 1.116, 1.343, 1.134, 1.407, 1.449, 1.488, 1.355, 1.281, 1.167, 1.47, 1.32, 1.255, 1.279, 1.287, 1.159, 1.158, 1.183, 1.262, 1.002, 1.126, 1.088, 1.164, 1.131, 0.999, 1.084, 0.961, 1.002, 0.894, 1.094, 1.127, 1.147, 1.199, 1.274, 1.247, 1.26, 1.34, 1.312, 1.282, 1.32, 1.273, 1.012, 1.233, 1.202, 1.173, 1.208, 1.045, 0.994, 1.116, 1.108, 1.206, 1.276, 1.184, 1.015, 1.121, 1.001, 0.805, 0.798, 0.79, 0.76, 0.872, 1.039, 0.911, 1.432, 1.29, 1.297, 1.236, 1.163, 1.236, 1.306, 1.307, 1.347, 1.101, 1.097, 1.198, 1.197, 1.374, 1.013, 1.176, 1.169, 1.2, 1.148, 1.201, 1.133, 1.096, 0.947, 1.022, 1.189, 1.139, 1.044, 1.138, 0.9, 1.031, 1.08, 0.944, 1.052, 1.004, 0.987, 1.179, 0.932, 1.179, 1.133, 1.12, 1.075, 1.141, 1.159, 1.144, 1.164, 1.06, 1.096, 1.182, 1.185, 1.206, 1.034, 1.263, 1.111, 1.089, 1.125, 1.049, 1.111, 0.994, 1.053, 0.971, 1.141, 1.121, 1.052, 1.172, 1.129, 0.923, 1.015, 0.966, 1.03, 1.06, 1.021, 0.945, 1.183, 1.063, 1.123, 1.183, 1.049, 0.973, 0.97, 0.94, 0.826, 0.953, 1.11, 1.287, 1.268, 1.059, 1.183, 1.103, 0.928, 1.27, 1.044, 1.251, 1.197, 1.051, 1.046, 0.957, 0.961, 1.115, 1.059, 1.166, 1.117, 1.133, 0.991, 0.964, 0.81, 1.144, 1.098, 1.023, 1.07, 1.017, 1.018, 1.056, 0.92, 0.834, 0.798, 0.717, 0.667, 0.756, 0.814, 1.003, 0.867, 1.066, 1.223, 1.284, 1.291, 1.316, 1.065, 1.258, 1.316, 1.182, 1.199, 1.128, 1.115, 1.235, 0.99, 1.08, 0.817, 0.84, 0.781, 0.812, 0.77, 0.94, 0.993, 1.096, 1.208, 1.143, 1.215, 1.048, 1.158, 1.07, 1.359, 1.406, 1.301, 1.216, 1.25, 0.958, 1.053, 0.9, 0.948, 0.896, 0.867, 0.932, 0.829, 0.886, 0.672, 0.708, 0.715, 0.796, 0.983, 0.998, 1.042, 0.957, 0.908, 0.871, 0.792, 0.978, 0.936, 0.912, 0.927, 0.898, 1.099, 1.139, 1.154, 1.094, 1.014, 0.992, 1.098, 0.919, 1.374, 1.19, 1.327, 1.07, 1.105, 1.11, 1.099, 1.097, 1.187, 1.226, 1.031, 1.119, 1.06, 0.98, 1.078, 1.109, 1.062, 0.891, 1.027, 0.955, 0.849, 0.861, 0.727, 0.953, 1.029, 1.127, 1.029, 0.866, 0.848, 0.856, 0.685, 0.97, 0.759, 0.848, 1.055, 1.088, 1.161, 1.061, 0.864, 1.062, 0.838, 0.931, 0.936, 0.986, 1.314, 1.474, 1.148, 1.164, 1.15, 0.996, 1.336, 1.13, 0.962, 1.016, 1.086, 0.838, 0.974, 0.832, 0.775, 0.967, 0.886, 0.982, 1.043, 1.033, 1.274, 1.113, 1.02, 0.989, 1.148, 1.18, 1.121, 1.218, 0.98, 0.998, 1.122, 0.976, 1.051, 0.909, 0.974, 0.862, 0.816, 0.923, 0.867, 0.992, 0.873, 0.89, 0.879, 0.813]\n",
      "Val Error(all epochs): 6.026198387145996 \n",
      " [47.031, 46.957, 46.879, 46.788, 46.657, 46.502, 46.338, 46.13, 45.895, 45.641, 45.317, 44.983, 44.647, 44.323, 43.939, 43.373, 42.727, 42.526, 42.511, 42.101, 40.931, 40.112, 39.386, 38.178, 38.296, 36.479, 36.358, 34.984, 33.899, 32.16, 31.97, 30.842, 29.831, 28.052, 27.099, 27.065, 24.174, 24.101, 23.199, 22.971, 21.457, 21.054, 19.286, 17.24, 14.664, 14.067, 12.849, 11.756, 8.13, 8.905, 8.984, 7.892, 7.816, 8.023, 8.32, 8.306, 8.431, 10.823, 8.753, 11.648, 11.281, 10.269, 9.371, 9.019, 11.088, 10.606, 10.286, 10.854, 9.653, 10.158, 10.128, 10.413, 9.759, 9.402, 14.068, 10.996, 13.235, 11.785, 11.5, 10.758, 10.512, 10.337, 8.811, 9.95, 8.882, 9.358, 8.946, 9.265, 8.085, 7.997, 8.209, 7.21, 7.411, 7.66, 6.844, 7.551, 6.99, 7.215, 7.498, 7.549, 7.592, 7.476, 7.698, 7.705, 7.483, 7.536, 7.309, 7.386, 6.605, 7.189, 7.295, 7.032, 8.806, 8.289, 8.191, 7.556, 7.17, 7.42, 7.21, 7.003, 7.191, 7.027, 7.158, 7.684, 6.7, 7.068, 7.08, 7.493, 7.087, 7.046, 7.084, 6.855, 7.29, 6.209, 6.758, 6.995, 7.06, 6.861, 7.197, 6.837, 7.565, 7.032, 7.008, 7.123, 6.575, 6.838, 7.018, 6.797, 7.036, 6.507, 7.762, 7.973, 6.482, 8.838, 7.044, 6.797, 6.603, 6.423, 6.879, 6.434, 6.995, 6.856, 6.966, 7.284, 7.101, 6.826, 7.038, 6.771, 7.259, 6.529, 8.391, 8.645, 8.44, 9.1, 11.425, 10.553, 11.512, 9.244, 9.001, 7.661, 7.053, 7.071, 7.225, 6.981, 7.491, 7.402, 8.114, 8.197, 7.227, 7.671, 7.576, 7.205, 7.019, 6.839, 7.173, 6.924, 6.824, 7.603, 7.294, 6.776, 7.282, 6.882, 7.282, 7.685, 7.186, 7.637, 6.705, 7.186, 7.265, 6.524, 7.569, 6.497, 7.429, 6.966, 7.122, 6.61, 6.849, 6.992, 7.092, 7.2, 7.297, 6.838, 7.464, 6.726, 7.046, 7.106, 6.596, 6.983, 7.302, 8.092, 8.845, 8.041, 8.485, 7.055, 7.274, 6.686, 6.877, 7.092, 6.681, 7.277, 6.529, 7.849, 7.32, 8.168, 7.938, 7.291, 7.037, 6.501, 6.984, 6.427, 6.988, 6.915, 6.514, 7.306, 7.348, 7.386, 7.519, 7.787, 7.063, 7.173, 7.624, 6.787, 7.366, 7.014, 6.831, 7.722, 7.6, 7.443, 6.911, 6.612, 7.1, 6.785, 6.716, 7.22, 7.31, 7.335, 7.121, 6.982, 6.995, 7.019, 7.388, 6.867, 7.396, 6.497, 7.739, 7.968, 8.039, 7.024, 6.582, 6.587, 7.07, 6.133, 7.358, 6.243, 7.341, 6.945, 6.923, 6.776, 6.435, 7.217, 6.745, 7.003, 6.828, 6.563, 6.967, 6.784, 6.587, 7.541, 6.638, 6.66, 7.307, 6.977, 6.106, 6.939, 6.267, 7.581, 7.006, 7.299, 6.917, 6.917, 7.042, 7.238, 7.41, 6.328, 6.416, 6.663, 6.283, 6.588, 6.181, 6.653, 6.827, 6.026, 7.2, 6.435, 7.184, 6.771, 7.41, 7.021, 6.83, 7.092, 6.896, 7.064, 6.869, 7.034, 6.323, 6.967, 7.407, 7.001, 7.985, 7.616, 6.585, 7.643, 6.817, 7.287, 7.159, 6.848, 6.575, 7.166, 7.069, 7.197, 7.095, 7.263, 7.154, 7.54, 6.995, 7.366, 7.233, 7.562, 7.659, 7.108, 7.065, 7.408, 7.346, 6.566, 7.486, 6.709, 6.667, 6.137, 6.698, 6.571, 6.807, 7.466, 7.566, 7.382, 6.887, 7.704, 6.931, 7.501, 7.48, 7.617, 7.554, 7.743, 7.551, 7.333, 7.551, 7.375, 7.079, 7.315, 7.026, 7.51, 6.906, 7.532, 7.264, 7.638, 7.907, 7.856, 6.718, 6.844, 7.101, 6.691, 7.375, 7.501, 7.318, 7.263, 7.098, 6.838, 8.142, 6.137, 6.503, 6.789, 6.946, 6.375, 8.009, 7.314, 7.231, 7.032, 7.532, 7.973, 8.276, 7.492, 8.104, 8.765, 7.859, 7.671, 7.96, 7.584, 7.027, 7.775, 6.595, 6.911, 7.099, 7.422, 7.642, 7.26, 7.573, 7.404, 7.229, 7.392, 7.236, 7.65, 7.398, 7.138, 7.021, 6.944, 7.584, 7.113, 6.983, 7.605, 6.448, 7.432, 8.214, 7.784, 7.666, 7.745, 6.133, 6.967, 6.409, 6.937, 6.5, 6.53, 6.805, 6.976, 7.401, 7.597, 7.631, 8.134, 8.165, 6.343, 6.786, 6.749, 6.993, 7.328, 6.369, 8.12, 7.244, 7.469, 7.231, 7.188, 7.273, 6.81, 7.631, 6.658, 7.116, 6.898, 7.157, 7.642, 7.47, 7.457, 7.084, 6.841]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.017, 0.032, 0.07, 0.105, 0.222, 0.276, 0.342, 0.426, 1.161, 0.905, 0.93, 2.793, 2.48, 2.872, 2.571, 2.381, 2.703, 6.406, 2.973, 7.16, 6.528, 5.502, 4.2, 3.347, 6.366, 5.357, 4.991, 5.956, 4.452, 5.087, 6.211, 6.08, 5.23, 4.706, 10.044, 6.918, 9.081, 7.913, 7.647, 6.826, 6.788, 6.185, 4.635, 5.816, 4.653, 5.345, 5.011, 4.993, 3.484, 3.592, 2.613, 2.973, 2.328, 4.019, 2.101, 3.392, 3.246, 3.084, 3.115, 3.575, 2.576, 2.917, 2.289, 3.125, 2.696, 3.048, 3.388, 2.822, 2.83, 2.526, 2.972, 3.18, 5.536, 5.061, 4.75, 3.593, 3.562, 3.325, 3.148, 2.505, 2.134, 1.827, 1.551, 1.875, 2.238, 2.523, 2.265, 2.934, 3.232, 3.834, 2.829, 3.76, 2.168, 2.542, 2.73, 3.409, 2.844, 2.219, 2.108, 2.078, 1.513, 1.947, 1.959, 2.379, 2.604, 2.192, 3.473, 2.083, 2.566, 1.532, 1.165, 1.099, 2.005, 0.8, 1.036, 2.0, 2.257, 1.725, 2.391, 2.688, 2.45, 2.65, 2.134, 2.609, 3.096, 1.988, 2.72, 2.516, 2.467, 2.551, 1.037, 1.324, 1.238, 1.412, 1.105, 1.256, 1.089, 0.985, 1.06, 1.401, 1.675, 1.506, 1.402, 1.716, 1.206, 1.273, 1.069, 0.937, 1.168, 1.147, 1.125, 1.605, 2.376, 2.396, 1.838, 2.354, 3.785, 3.366, 4.038, 3.052, 3.794, 3.066, 3.668, 3.341, 3.502, 3.806, 3.124, 2.737, 3.234, 3.096, 3.195, 2.956, 2.425, 2.701, 1.938, 1.949, 1.81, 2.459, 1.782, 2.839, 2.105, 1.707, 1.612, 2.377, 1.93, 2.316, 2.702, 3.313, 3.74, 4.829, 6.464, 4.243, 6.122, 3.165, 3.474, 2.466, 3.586, 3.632, 3.899, 3.067, 2.735, 2.296, 3.615, 4.389, 5.314, 3.512, 3.414, 2.806, 2.767, 3.413, 2.027, 3.242, 2.658, 3.614, 3.835, 3.711, 4.146, 3.41, 3.634, 2.766, 3.814, 2.844, 3.078, 3.599, 2.626, 3.552, 4.507, 4.095, 2.578, 2.588, 3.475, 3.008, 3.159, 4.152, 4.365, 4.178, 3.588, 3.651, 3.025, 4.32, 3.693, 3.339, 2.728, 2.99, 3.36, 5.258, 4.267, 3.425, 2.02, 2.004, 1.647, 2.083, 1.578, 2.956, 2.359, 3.732, 3.2, 3.051, 2.667, 3.328, 3.049, 3.197, 3.071, 2.931, 1.816, 2.432, 2.444, 1.955, 2.365, 1.594, 1.093, 1.46, 3.308, 2.955, 2.872, 3.796, 3.576, 3.624, 3.977, 2.616, 3.597, 3.999, 3.148, 2.89, 1.945, 1.969, 2.441, 2.058, 2.889, 1.876, 2.409, 2.302, 1.781, 2.438, 2.343, 3.578, 3.835, 3.298, 2.639, 2.815, 2.781, 2.869, 3.161, 3.159, 2.231, 2.943, 3.659, 3.888, 3.699, 4.305, 1.829, 2.437, 3.069, 3.069, 4.607, 2.808, 3.739, 3.105, 4.596, 3.656, 4.321, 3.391, 4.182, 4.05, 3.386, 4.115, 2.926, 5.006, 4.15, 3.612, 4.177, 4.115, 3.742, 2.411, 3.367, 4.073, 2.042, 2.54, 3.559, 3.585, 3.443, 4.378, 4.679, 3.878, 3.365, 4.337, 3.213, 3.475, 3.963, 3.496, 3.555, 3.415, 3.605, 2.403, 4.417, 3.046, 3.453, 2.045, 3.557, 2.672, 3.305, 3.075, 3.484, 3.189, 4.88, 2.829, 2.896, 3.846, 2.738, 3.2, 3.502, 4.553, 3.392, 3.681, 2.788, 1.699, 1.572, 2.372, 2.143, 2.837, 1.854, 2.932, 4.459, 4.758, 3.83, 3.877, 4.397, 5.049, 5.328, 3.793, 5.3, 6.186, 4.381, 4.247, 4.666, 3.403, 3.856, 3.333, 2.927, 1.871, 3.264, 3.636, 3.562, 3.731, 3.238, 3.344, 3.115, 3.508, 3.109, 3.705, 3.835, 3.353, 2.562, 3.091, 3.307, 3.554, 3.359, 2.227, 2.387, 4.466, 4.831, 4.905, 4.313, 4.706, 2.407, 2.358, 2.219, 1.741, 2.432, 2.006, 2.193, 2.942, 2.711, 4.359, 2.788, 5.555, 3.893, 3.401, 2.339, 3.457, 3.795, 3.568, 2.538, 5.088, 3.952, 4.763, 2.499, 4.534, 3.05, 2.939, 4.427, 3.191, 3.139, 3.341, 3.404, 4.01, 4.041, 3.469, 2.7, 3.169]\n",
      "\n",
      "#Fold: 6 \n",
      "Trainig set size: 420 , Time: 0:11:55 , best_lambda: 1 , min_  , error: 6.026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test starts:  467 , ends:  518\n",
      "1/1 [==============================] - 0s 846us/step - loss: 92.0229 - mse: 47.1336 - mae: 5.6258 - fp_mae: 1.8482\n",
      "average_error:  5.626 , fp_average_error:  1.848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:00<00:00, 212.77it/s]\n",
      " 96%|█████████▌| 99/103 [00:00<00:00, 239.34it/s]]\n",
      "100%|██████████| 103/103 [00:00<00:00, 236.70it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 226.85it/s]\n",
      "100%|██████████| 107/107 [00:00<00:00, 204.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Fold: 7 , Training Size: 420 , Validation size: 47 , Test Size 52\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 48.26091, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 48.26091 to 48.06826, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 48.06826 to 47.83591, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 47.83591 to 47.56895, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 47.56895 to 47.28155, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 47.28155 to 47.00253, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 47.00253 to 46.71366, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 46.71366 to 46.35990, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 46.35990 to 45.95475, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 45.95475 to 45.40283, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 45.40283 to 44.90068, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 44.90068 to 44.02805, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 44.02805 to 43.14040, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 43.14040 to 42.46157, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 42.46157 to 41.71423, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 41.71423 to 40.78441, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 40.78441 to 39.70013, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 39.70013 to 39.20828, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 39.20828 to 38.66106, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 38.66106 to 38.24316, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 38.24316 to 37.14854, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 37.14854 to 35.33937, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 35.33937\n",
      "\n",
      "Epoch 00024: val_mae improved from 35.33937 to 35.18484, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 35.18484 to 33.29179, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 33.29179 to 32.29432, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 32.29432 to 30.72961, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 30.72961 to 30.00128, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 30.00128 to 28.62412, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 28.62412 to 28.31145, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00031: val_mae improved from 28.31145 to 27.80925, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00032: val_mae improved from 27.80925 to 26.87459, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 26.87459 to 26.10109, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00034: val_mae improved from 26.10109 to 25.27991, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00035: val_mae improved from 25.27991 to 24.01041, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 24.01041\n",
      "\n",
      "Epoch 00037: val_mae improved from 24.01041 to 23.51089, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00038: val_mae improved from 23.51089 to 22.31368, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00039: val_mae improved from 22.31368 to 20.80678, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 20.80678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00041: val_mae improved from 20.80678 to 15.92114, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 15.92114\n",
      "\n",
      "Epoch 00043: val_mae improved from 15.92114 to 13.18713, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00044: val_mae improved from 13.18713 to 12.14377, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00045: val_mae improved from 12.14377 to 10.84906, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 10.84906\n",
      "\n",
      "Epoch 00047: val_mae improved from 10.84906 to 9.77848, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 9.77848\n",
      "\n",
      "Epoch 00049: val_mae improved from 9.77848 to 9.21743, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 9.21743\n",
      "\n",
      "Epoch 00156: val_mae improved from 9.21743 to 8.73171, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 8.73171\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 8.73171\n",
      "\n",
      "Epoch 00159: val_mae improved from 8.73171 to 8.53625, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 8.53625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00176: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 8.53625\n",
      "\n",
      "Epoch 00202: val_mae improved from 8.53625 to 8.36445, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 8.36445\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 8.36445\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 8.36445\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 8.36445\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 8.36445\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 8.36445\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 8.36445\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 8.36445\n",
      "\n",
      "Epoch 00211: val_mae improved from 8.36445 to 8.21869, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 8.21869\n",
      "\n",
      "Epoch 00213: val_mae improved from 8.21869 to 8.19903, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 8.19903\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 8.19903\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 8.19903\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 8.19903\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 8.19903\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 8.19903\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 8.19903\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 8.19903\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 8.19903\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 8.19903\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 8.19903\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 8.19903\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 8.19903\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 8.19903\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 8.19903\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 8.19903\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 8.19903\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 8.19903\n",
      "\n",
      "Epoch 00232: val_mae improved from 8.19903 to 8.03986, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 8.03986\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 8.03986\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 8.03986\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 8.03986\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 8.03986\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 8.03986\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 8.03986\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 8.03986\n",
      "\n",
      "Epoch 00241: val_mae improved from 8.03986 to 8.02789, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 8.02789\n",
      "\n",
      "Epoch 00243: val_mae improved from 8.02789 to 7.91373, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 7.91373\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 7.91373\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 7.91373\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 7.91373\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 7.91373\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 7.91373\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 7.91373\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 7.91373\n",
      "\n",
      "Epoch 00252: val_mae improved from 7.91373 to 7.90929, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 7.90929\n",
      "\n",
      "Epoch 00254: val_mae improved from 7.90929 to 7.84107, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 7.84107\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 7.84107\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 7.84107\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 7.84107\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 7.84107\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 7.84107\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 7.84107\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 7.84107\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 7.84107\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 7.84107\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 7.84107\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 7.84107\n",
      "\n",
      "Epoch 00267: val_mae improved from 7.84107 to 7.72925, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 7.72925\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 7.72925\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 7.72925\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 7.72925\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 7.72925\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 7.72925\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 7.72925\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 7.72925\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 7.72925\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 7.72925\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 7.72925\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 7.72925\n",
      "\n",
      "Epoch 00280: val_mae improved from 7.72925 to 7.68085, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 7.68085\n",
      "\n",
      "Epoch 00282: val_mae improved from 7.68085 to 7.42277, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 7.42277\n",
      "\n",
      "Epoch 00284: val_mae improved from 7.42277 to 7.23895, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 7.23895\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 7.23895\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 7.23895\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 7.23895\n",
      "\n",
      "Epoch 00289: val_mae improved from 7.23895 to 7.21172, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 7.21172\n",
      "\n",
      "Epoch 00291: val_mae improved from 7.21172 to 7.04276, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00292: val_mae did not improve from 7.04276\n",
      "\n",
      "Epoch 00293: val_mae improved from 7.04276 to 6.92941, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 6.92941\n",
      "\n",
      "Epoch 00315: val_mae improved from 6.92941 to 6.92383, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00316: val_mae improved from 6.92383 to 6.69399, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 6.69399\n",
      "\n",
      "Epoch 00363: val_mae improved from 6.69399 to 6.20585, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 6.20585\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 6.20585\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 6.20585\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 6.20585\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 6.20585\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 6.20585\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 6.20585\n",
      "\n",
      "Epoch 00371: val_mae improved from 6.20585 to 6.09617, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 6.09617\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 6.09617\n",
      "\n",
      "Epoch 00374: val_mae improved from 6.09617 to 5.76478, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 5.76478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00434: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 5.76478\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 5.76478\n",
      "\n",
      "Lambda: 0.01 , Time: 0:03:59\n",
      "Train Error(all epochs): 0.9420835375785828 \n",
      " [49.335, 49.239, 49.164, 49.081, 48.988, 48.881, 48.758, 48.612, 48.441, 48.232, 47.995, 47.716, 47.405, 47.051, 46.651, 46.193, 45.696, 45.133, 44.533, 43.889, 43.207, 42.522, 41.805, 41.052, 40.235, 39.345, 38.518, 37.636, 36.751, 35.848, 34.909, 33.977, 33.022, 32.027, 31.033, 30.024, 29.077, 28.108, 27.069, 25.928, 24.933, 23.817, 22.724, 21.63, 20.603, 19.603, 18.599, 17.689, 16.753, 15.916, 15.023, 14.24, 13.541, 12.866, 12.251, 11.718, 11.161, 10.636, 10.213, 9.894, 9.617, 9.302, 9.187, 8.822, 8.617, 8.192, 8.085, 8.07, 7.946, 7.954, 7.872, 7.822, 7.902, 7.677, 7.715, 7.588, 7.428, 7.521, 7.745, 7.645, 7.533, 7.629, 7.416, 7.239, 7.039, 6.91, 6.735, 6.68, 6.638, 6.627, 6.578, 6.492, 6.401, 6.258, 6.18, 6.145, 6.066, 6.064, 6.043, 6.04, 6.137, 6.232, 6.329, 6.37, 6.354, 6.345, 6.384, 6.224, 6.216, 6.15, 6.05, 5.849, 5.907, 5.837, 5.905, 5.783, 5.609, 5.585, 5.655, 5.769, 5.674, 5.819, 5.876, 5.897, 5.855, 5.914, 6.193, 6.329, 6.302, 6.068, 5.95, 5.966, 5.757, 5.635, 5.608, 5.485, 5.384, 5.288, 5.366, 5.34, 5.338, 5.262, 5.374, 5.361, 5.347, 5.272, 5.191, 5.204, 5.278, 5.325, 5.365, 5.43, 5.299, 5.304, 5.154, 5.183, 5.181, 5.129, 4.918, 4.886, 4.85, 4.778, 4.729, 4.756, 4.802, 4.862, 5.003, 4.978, 4.942, 5.029, 5.072, 5.075, 5.1, 5.209, 5.011, 4.925, 4.808, 4.781, 4.737, 4.634, 4.568, 4.671, 4.623, 4.691, 4.821, 4.87, 4.868, 4.772, 4.749, 4.68, 4.649, 4.799, 4.859, 4.852, 4.839, 4.818, 4.723, 4.549, 4.448, 4.323, 4.29, 4.282, 4.236, 4.224, 4.208, 4.256, 4.276, 4.224, 4.258, 4.153, 4.136, 4.121, 4.148, 4.245, 4.214, 4.242, 4.325, 4.351, 4.218, 4.15, 4.193, 4.278, 4.213, 4.107, 4.042, 3.958, 3.965, 4.099, 4.29, 4.454, 4.408, 4.583, 4.629, 4.624, 4.52, 4.241, 4.17, 4.039, 3.984, 3.968, 3.936, 3.952, 3.91, 3.934, 3.88, 3.792, 3.684, 3.609, 3.532, 3.484, 3.437, 3.407, 3.42, 3.451, 3.44, 3.39, 3.405, 3.46, 3.528, 3.456, 3.509, 3.671, 3.824, 3.968, 4.054, 4.031, 3.984, 3.998, 3.901, 3.818, 3.849, 3.894, 3.838, 3.875, 3.775, 3.713, 3.692, 3.583, 3.571, 3.571, 3.472, 3.36, 3.336, 3.269, 3.252, 3.124, 3.103, 3.141, 3.151, 3.18, 3.154, 3.156, 3.13, 3.143, 3.188, 3.221, 3.205, 3.152, 3.127, 3.171, 3.217, 3.254, 3.254, 3.309, 3.301, 3.32, 3.259, 3.187, 3.109, 3.039, 3.01, 2.922, 2.845, 2.913, 3.045, 3.251, 3.271, 3.28, 3.235, 3.516, 3.566, 3.436, 3.237, 3.443, 3.311, 3.1, 3.239, 3.23, 3.156, 3.007, 4.042, 4.693, 5.351, 4.961, 4.626, 4.471, 3.922, 3.556, 3.362, 3.184, 3.034, 2.904, 2.675, 2.594, 2.486, 2.372, 2.26, 2.342, 2.35, 2.407, 2.501, 2.613, 2.736, 2.682, 2.439, 2.193, 2.188, 2.149, 1.982, 1.92, 1.867, 1.877, 1.999, 1.86, 1.764, 1.685, 1.733, 1.787, 1.96, 2.035, 2.118, 2.017, 2.028, 1.995, 1.863, 1.719, 1.674, 1.578, 1.462, 1.358, 1.306, 1.249, 1.265, 1.329, 1.371, 1.394, 1.436, 1.55, 1.54, 1.576, 1.635, 1.613, 1.468, 1.396, 1.333, 1.317, 1.242, 1.181, 1.171, 1.198, 1.282, 1.338, 1.423, 1.446, 1.491, 1.507, 1.539, 1.559, 1.56, 1.519, 1.503, 1.474, 1.486, 1.429, 1.455, 1.477, 1.509, 1.511, 1.615, 1.555, 1.55, 1.422, 1.392, 1.308, 1.267, 1.223, 1.182, 1.201, 1.161, 1.164, 1.134, 1.125, 1.15, 1.157, 1.19, 1.176, 1.19, 1.156, 1.191, 1.169, 1.164, 1.181, 1.184, 1.251, 1.211, 1.189, 1.137, 1.145, 1.217, 1.259, 1.243, 1.178, 1.153, 1.118, 1.113, 1.076, 1.077, 1.112, 1.111, 1.164, 1.152, 1.121, 1.072, 1.031, 0.997, 0.954, 0.959, 0.966, 0.967, 0.949, 0.942, 0.987, 1.03, 1.033, 1.066, 1.06, 1.02, 0.985, 0.962, 1.074, 1.162, 1.154, 1.298, 1.33, 1.255, 1.189, 1.053, 1.037, 1.016, 1.041, 1.066, 1.057, 1.068, 1.171, 1.209, 1.106, 1.102, 1.115, 1.054, 1.063]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011, 0.0, 0.007, 0.012, 0.013, 0.024, 0.044, 0.036, 0.044, 0.06, 0.091, 0.103, 0.151, 0.222, 0.225, 0.224, 0.281, 0.385, 0.423, 0.543, 0.546, 0.633, 0.665, 0.681, 0.771, 0.767, 0.978, 0.965, 1.011, 1.013, 1.165, 1.056, 1.107, 1.066, 1.078, 1.045, 1.27, 1.186, 1.053, 1.219, 1.216, 0.966, 1.059, 0.941, 0.9, 0.829, 0.846, 0.858, 0.863, 0.824, 0.787, 0.728, 0.704, 0.71, 0.734, 0.619, 0.783, 0.691, 0.771, 0.874, 0.834, 1.111, 0.922, 1.006, 1.031, 0.774, 0.996, 0.864, 0.798, 0.774, 0.653, 0.879, 0.787, 0.672, 0.65, 0.579, 0.742, 0.846, 0.673, 0.782, 0.8, 0.902, 0.77, 0.794, 1.12, 1.129, 1.022, 0.934, 0.928, 0.958, 0.82, 0.803, 0.721, 0.646, 0.696, 0.582, 0.665, 0.629, 0.649, 0.561, 0.702, 0.691, 0.782, 0.635, 0.552, 0.621, 0.716, 0.638, 0.812, 0.804, 0.602, 0.724, 0.71, 0.628, 0.694, 0.644, 0.512, 0.493, 0.523, 0.495, 0.433, 0.491, 0.531, 0.51, 0.673, 0.657, 0.51, 0.722, 0.709, 0.662, 0.777, 0.785, 0.681, 0.631, 0.573, 0.57, 0.524, 0.485, 0.491, 0.523, 0.494, 0.596, 0.676, 0.558, 0.703, 0.667, 0.615, 0.629, 0.58, 0.599, 0.74, 0.694, 0.759, 0.654, 0.649, 0.538, 0.464, 0.418, 0.443, 0.412, 0.377, 0.433, 0.41, 0.439, 0.475, 0.451, 0.444, 0.453, 0.403, 0.417, 0.4, 0.532, 0.449, 0.447, 0.6, 0.58, 0.499, 0.518, 0.466, 0.553, 0.542, 0.456, 0.417, 0.425, 0.459, 0.459, 0.561, 0.729, 0.74, 0.737, 0.813, 0.752, 0.809, 0.601, 0.527, 0.51, 0.433, 0.492, 0.42, 0.505, 0.408, 0.543, 0.397, 0.438, 0.369, 0.355, 0.261, 0.236, 0.273, 0.24, 0.276, 0.277, 0.298, 0.246, 0.288, 0.32, 0.367, 0.331, 0.352, 0.509, 0.521, 0.646, 0.703, 0.717, 0.591, 0.67, 0.624, 0.581, 0.64, 0.606, 0.677, 0.601, 0.452, 0.623, 0.568, 0.502, 0.433, 0.478, 0.449, 0.329, 0.39, 0.328, 0.365, 0.3, 0.258, 0.311, 0.3, 0.342, 0.314, 0.357, 0.301, 0.373, 0.361, 0.407, 0.411, 0.349, 0.395, 0.37, 0.467, 0.412, 0.502, 0.459, 0.529, 0.494, 0.509, 0.437, 0.39, 0.367, 0.368, 0.278, 0.246, 0.365, 0.429, 0.632, 0.524, 0.64, 0.519, 0.792, 0.818, 0.764, 0.663, 0.749, 0.764, 0.608, 0.854, 0.833, 0.877, 0.723, 1.064, 1.403, 1.969, 1.527, 1.648, 1.464, 1.213, 0.893, 0.962, 0.863, 0.782, 0.742, 0.55, 0.644, 0.708, 0.744, 0.693, 0.735, 0.713, 0.818, 0.825, 0.877, 0.959, 0.98, 0.766, 0.767, 0.627, 0.73, 0.523, 0.586, 0.487, 0.51, 0.614, 0.555, 0.501, 0.38, 0.513, 0.447, 0.673, 0.579, 0.776, 0.554, 0.701, 0.574, 0.6, 0.412, 0.48, 0.374, 0.366, 0.265, 0.289, 0.221, 0.286, 0.292, 0.319, 0.368, 0.291, 0.508, 0.374, 0.5, 0.407, 0.514, 0.321, 0.354, 0.266, 0.323, 0.269, 0.213, 0.264, 0.197, 0.349, 0.275, 0.438, 0.323, 0.485, 0.36, 0.517, 0.368, 0.526, 0.322, 0.489, 0.307, 0.488, 0.321, 0.442, 0.361, 0.492, 0.345, 0.525, 0.442, 0.435, 0.386, 0.345, 0.348, 0.269, 0.318, 0.23, 0.294, 0.257, 0.254, 0.271, 0.214, 0.319, 0.216, 0.304, 0.243, 0.296, 0.24, 0.273, 0.306, 0.224, 0.331, 0.234, 0.363, 0.273, 0.304, 0.243, 0.245, 0.382, 0.301, 0.356, 0.272, 0.287, 0.276, 0.253, 0.294, 0.208, 0.324, 0.209, 0.345, 0.227, 0.307, 0.207, 0.228, 0.202, 0.175, 0.182, 0.216, 0.204, 0.184, 0.199, 0.183, 0.271, 0.186, 0.279, 0.243, 0.23, 0.217, 0.193, 0.265, 0.352, 0.258, 0.434, 0.346, 0.375, 0.317, 0.26, 0.253, 0.224, 0.238, 0.283, 0.229, 0.314, 0.276, 0.39, 0.249, 0.294, 0.318, 0.248, 0.288]\n",
      "Val Error(all epochs): 5.764776229858398 \n",
      " [48.261, 48.068, 47.836, 47.569, 47.282, 47.003, 46.714, 46.36, 45.955, 45.403, 44.901, 44.028, 43.14, 42.462, 41.714, 40.784, 39.7, 39.208, 38.661, 38.243, 37.149, 35.339, 35.361, 35.185, 33.292, 32.294, 30.73, 30.001, 28.624, 28.311, 27.809, 26.875, 26.101, 25.28, 24.01, 24.25, 23.511, 22.314, 20.807, 21.838, 15.921, 16.667, 13.187, 12.144, 10.849, 12.083, 9.778, 10.503, 9.217, 9.749, 10.037, 9.974, 10.542, 9.943, 12.018, 11.679, 12.581, 13.456, 13.531, 14.68, 13.13, 16.532, 14.006, 17.727, 18.437, 18.817, 19.639, 19.728, 19.365, 19.232, 17.346, 19.461, 15.893, 16.227, 15.336, 15.553, 16.738, 14.378, 16.491, 15.595, 17.773, 18.07, 20.339, 17.536, 17.107, 17.305, 16.369, 16.191, 14.475, 14.683, 12.953, 12.763, 11.858, 12.388, 11.519, 11.819, 11.909, 11.197, 11.389, 11.012, 11.023, 10.741, 9.949, 11.504, 10.73, 10.876, 10.733, 9.943, 10.697, 9.815, 9.755, 10.277, 9.62, 10.738, 10.207, 10.657, 10.333, 10.235, 10.837, 10.247, 10.622, 9.857, 10.238, 10.476, 9.611, 10.386, 10.35, 11.069, 10.858, 13.331, 12.094, 12.56, 12.867, 12.32, 11.62, 12.221, 11.622, 11.108, 11.501, 10.938, 11.452, 10.956, 11.675, 10.193, 10.918, 9.808, 10.259, 9.733, 10.516, 9.384, 10.842, 9.222, 9.434, 9.336, 9.3, 8.732, 9.053, 8.879, 8.536, 8.857, 8.695, 8.635, 8.609, 8.643, 8.913, 8.694, 9.215, 8.79, 9.041, 9.082, 8.802, 8.665, 9.219, 8.621, 9.431, 8.97, 9.192, 9.179, 9.292, 8.738, 9.294, 8.721, 9.17, 8.786, 9.488, 8.733, 9.388, 9.413, 9.928, 9.161, 9.443, 9.242, 9.508, 9.831, 9.348, 9.311, 8.786, 9.124, 8.683, 8.642, 8.682, 8.364, 8.758, 8.496, 8.666, 8.68, 8.579, 8.591, 8.433, 8.845, 8.219, 8.589, 8.199, 8.38, 8.289, 8.721, 8.798, 8.713, 9.215, 8.952, 8.745, 8.74, 8.91, 8.342, 8.483, 8.725, 8.452, 8.366, 8.46, 8.614, 8.573, 8.04, 8.874, 8.161, 8.915, 8.379, 8.307, 8.408, 8.137, 8.386, 8.028, 8.493, 7.914, 8.364, 8.07, 8.542, 8.538, 8.428, 8.359, 8.257, 8.522, 7.909, 8.336, 7.841, 8.189, 7.98, 8.109, 8.068, 8.056, 8.221, 8.155, 8.052, 8.175, 9.027, 8.074, 8.07, 7.729, 8.127, 8.099, 8.088, 8.555, 8.276, 8.533, 8.357, 8.284, 8.172, 8.319, 7.746, 8.312, 7.681, 7.929, 7.423, 7.939, 7.239, 7.732, 7.348, 7.365, 7.656, 7.212, 7.755, 7.043, 7.645, 6.929, 7.629, 6.975, 7.339, 7.304, 7.341, 7.42, 7.411, 7.63, 7.256, 7.733, 7.245, 8.181, 7.272, 8.373, 8.071, 8.02, 7.976, 7.899, 7.495, 7.197, 7.0, 6.924, 6.694, 7.479, 7.35, 7.861, 9.183, 8.39, 8.519, 7.692, 7.517, 7.68, 7.769, 8.253, 7.947, 8.343, 8.2, 9.761, 8.76, 14.621, 9.949, 17.007, 17.063, 17.889, 19.061, 17.201, 16.802, 16.069, 16.12, 15.363, 14.384, 14.442, 13.951, 13.683, 11.756, 10.908, 13.462, 9.689, 14.243, 11.606, 11.769, 11.35, 11.133, 11.512, 9.183, 9.441, 7.808, 7.793, 6.713, 6.206, 6.752, 6.746, 6.501, 6.299, 6.702, 6.333, 6.689, 6.096, 6.847, 6.431, 5.765, 6.473, 5.971, 6.489, 6.141, 6.085, 6.068, 6.027, 6.042, 5.934, 6.017, 5.955, 6.101, 5.959, 6.198, 6.075, 6.306, 6.146, 6.356, 6.311, 6.354, 6.256, 6.32, 6.489, 6.278, 6.347, 6.197, 6.401, 6.228, 6.315, 6.368, 6.318, 6.52, 6.427, 6.617, 6.45, 6.639, 6.403, 6.589, 6.6, 6.549, 6.984, 6.48, 7.034, 6.365, 7.067, 6.723, 6.926, 6.9, 6.706, 6.976, 6.725, 7.095, 6.887, 6.95, 6.761, 6.71, 6.591, 6.7, 6.642, 6.587, 6.522, 6.403, 6.414, 6.34, 6.469, 6.546, 6.53, 6.822, 6.487, 6.684, 6.607, 6.474, 6.374, 6.384, 6.452, 6.164, 6.576, 6.156, 6.366, 6.152, 6.473, 6.329, 6.517, 6.469, 6.484, 6.337, 6.427, 6.202, 6.257, 6.229, 6.363, 6.276, 6.386, 6.437, 6.414, 6.521, 6.364, 6.503, 6.302, 6.386, 6.275, 6.489, 6.321, 6.598, 6.374, 6.511, 6.518, 6.561, 6.811, 6.844, 6.783, 6.927, 6.709, 6.949, 6.877, 6.647, 6.666, 6.504, 6.466, 6.486, 6.333, 6.577, 6.329, 6.838, 6.399, 6.851]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.152, 0.262, 0.546, 0.364, 1.315, 1.052, 2.626, 2.11, 3.557, 3.594, 4.769, 3.613, 6.28, 5.978, 7.359, 8.056, 8.242, 9.636, 7.882, 11.619, 9.074, 13.074, 13.847, 14.062, 15.107, 15.13, 14.792, 14.626, 12.757, 14.773, 11.247, 11.629, 10.709, 10.91, 12.149, 9.61, 11.952, 10.89, 13.3, 13.508, 15.884, 13.074, 12.564, 12.873, 11.807, 11.733, 9.771, 10.011, 8.11, 7.866, 6.986, 7.51, 6.616, 6.928, 7.046, 6.227, 6.368, 6.044, 5.887, 5.538, 4.53, 6.539, 5.381, 5.718, 5.389, 4.43, 5.282, 4.275, 4.038, 4.879, 3.896, 5.614, 4.752, 5.54, 5.029, 4.813, 5.754, 4.963, 5.227, 4.36, 4.808, 5.359, 3.968, 5.286, 4.99, 6.094, 5.755, 8.972, 7.333, 7.967, 8.151, 7.704, 6.909, 7.661, 6.886, 6.216, 6.713, 6.084, 6.747, 6.265, 7.088, 5.276, 6.155, 4.627, 5.156, 4.389, 5.756, 3.876, 6.024, 3.591, 4.07, 3.831, 3.837, 2.446, 3.425, 2.846, 2.694, 2.302, 2.887, 2.386, 2.647, 2.625, 3.266, 2.815, 3.964, 3.303, 3.532, 3.376, 3.421, 3.003, 3.711, 3.234, 4.217, 3.87, 4.297, 4.04, 4.349, 3.306, 4.469, 3.075, 4.197, 3.276, 4.676, 3.667, 4.601, 4.777, 5.385, 4.448, 4.857, 4.574, 4.895, 5.525, 4.916, 4.424, 4.244, 4.157, 3.92, 3.739, 3.771, 3.437, 3.875, 3.698, 3.797, 4.005, 3.778, 3.813, 3.581, 4.107, 3.267, 3.816, 3.245, 3.481, 3.574, 4.052, 4.304, 4.067, 4.942, 4.509, 4.172, 4.251, 4.445, 3.541, 3.89, 4.308, 3.721, 3.71, 3.484, 4.478, 3.919, 3.48, 4.668, 3.598, 4.624, 4.204, 4.124, 4.255, 3.794, 4.203, 3.66, 4.453, 3.42, 4.103, 3.826, 4.469, 4.611, 4.367, 4.424, 4.268, 4.66, 3.749, 4.314, 3.702, 4.121, 3.938, 4.012, 4.134, 3.961, 4.467, 4.072, 4.169, 4.371, 5.502, 4.069, 4.152, 3.377, 4.177, 4.571, 4.413, 5.064, 4.501, 4.988, 4.793, 4.955, 4.573, 4.877, 3.924, 4.982, 3.898, 4.206, 3.571, 4.169, 3.335, 4.012, 3.463, 3.575, 3.885, 3.37, 4.08, 3.155, 3.82, 3.202, 3.819, 3.323, 3.564, 3.682, 3.658, 3.96, 3.721, 4.419, 3.395, 4.548, 3.852, 5.028, 3.954, 5.368, 5.103, 4.938, 4.94, 4.87, 4.247, 3.862, 3.381, 3.526, 3.31, 4.324, 4.528, 4.965, 6.819, 6.005, 6.077, 5.049, 5.273, 4.479, 4.741, 4.677, 5.454, 6.287, 4.886, 3.221, 2.477, 0.936, 7.817, 15.731, 15.727, 16.65, 17.81, 16.014, 15.644, 14.905, 15.008, 14.569, 13.788, 13.793, 13.325, 13.046, 11.073, 10.111, 12.949, 8.897, 13.763, 10.887, 11.101, 10.643, 10.451, 10.933, 8.396, 8.71, 6.835, 6.667, 5.156, 4.383, 5.194, 4.967, 4.758, 3.877, 4.836, 3.926, 4.64, 4.331, 4.814, 4.66, 2.867, 4.104, 3.79, 4.371, 4.152, 3.61, 3.918, 3.505, 3.644, 3.235, 3.325, 2.958, 3.382, 2.892, 3.321, 3.62, 3.804, 3.727, 4.303, 4.122, 4.258, 4.124, 4.198, 4.5, 4.052, 4.205, 3.818, 4.345, 3.913, 4.228, 4.113, 4.329, 4.393, 4.461, 4.677, 4.322, 4.835, 4.223, 4.726, 4.439, 4.689, 5.156, 4.557, 5.226, 4.501, 5.078, 5.031, 5.086, 5.229, 5.014, 5.281, 5.12, 5.435, 5.343, 5.188, 5.085, 4.931, 4.794, 5.002, 4.799, 4.881, 4.441, 4.644, 4.157, 4.522, 4.325, 4.872, 4.584, 5.187, 4.735, 4.837, 4.908, 4.472, 4.342, 4.272, 4.34, 3.962, 4.321, 4.141, 3.966, 4.137, 4.171, 4.533, 4.518, 4.654, 4.579, 4.231, 4.52, 3.851, 4.199, 3.914, 4.446, 4.081, 4.48, 4.379, 4.601, 4.55, 4.59, 4.456, 4.319, 4.308, 4.097, 4.678, 4.281, 4.827, 4.563, 4.366, 4.651, 4.437, 5.129, 5.249, 5.072, 5.389, 5.144, 5.256, 5.44, 4.942, 5.104, 4.767, 4.634, 4.828, 4.244, 4.938, 4.278, 5.322, 4.525, 5.279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 48.28120, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 48.28120 to 48.19593, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 48.19593 to 48.08881, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 48.08881 to 47.95383, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 47.95383 to 47.82429, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 47.82429 to 47.66396, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 47.66396 to 47.52242, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 47.52242 to 47.34626, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 47.34626 to 47.11071, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 47.11071 to 46.80138, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 46.80138 to 46.47655, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 46.47655 to 46.15164, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 46.15164 to 45.76441, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 45.76441 to 45.11457, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 45.11457 to 44.63893, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 44.63893 to 44.13192, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 44.13192 to 43.64319, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 43.64319 to 42.50149, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 42.50149 to 41.55832, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 41.55832\n",
      "\n",
      "Epoch 00021: val_mae improved from 41.55832 to 40.79063, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 40.79063 to 39.55824, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 39.55824\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 39.55824\n",
      "\n",
      "Epoch 00025: val_mae improved from 39.55824 to 37.24113, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 37.24113\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 37.24113\n",
      "\n",
      "Epoch 00028: val_mae improved from 37.24113 to 36.55403, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 36.55403\n",
      "\n",
      "Epoch 00030: val_mae improved from 36.55403 to 35.73080, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00031: val_mae improved from 35.73080 to 35.49279, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00032: val_mae improved from 35.49279 to 35.37478, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 35.37478 to 34.20194, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 34.20194\n",
      "\n",
      "Epoch 00035: val_mae improved from 34.20194 to 32.17542, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00036: val_mae improved from 32.17542 to 31.03852, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00037: val_mae improved from 31.03852 to 28.28964, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00038: val_mae improved from 28.28964 to 27.03384, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00039: val_mae improved from 27.03384 to 25.14595, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00040: val_mae improved from 25.14595 to 21.54864, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 21.54864\n",
      "\n",
      "Epoch 00042: val_mae improved from 21.54864 to 20.94783, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00043: val_mae improved from 20.94783 to 20.16578, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 20.16578\n",
      "\n",
      "Epoch 00045: val_mae improved from 20.16578 to 19.59941, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00046: val_mae did not improve from 19.59941\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 19.59941\n",
      "\n",
      "Epoch 00048: val_mae improved from 19.59941 to 19.02907, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 19.02907\n",
      "\n",
      "Epoch 00050: val_mae improved from 19.02907 to 18.96539, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00051: val_mae improved from 18.96539 to 17.69543, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00052: val_mae improved from 17.69543 to 16.81194, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 16.81194\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 16.81194\n",
      "\n",
      "Epoch 00055: val_mae improved from 16.81194 to 16.76609, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 16.76609\n",
      "\n",
      "Epoch 00057: val_mae improved from 16.76609 to 15.99397, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00058: val_mae improved from 15.99397 to 15.33997, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 15.33997\n",
      "\n",
      "Epoch 00060: val_mae improved from 15.33997 to 14.01345, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 14.01345\n",
      "\n",
      "Epoch 00062: val_mae improved from 14.01345 to 13.31651, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00063: val_mae improved from 13.31651 to 13.06586, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00064: val_mae improved from 13.06586 to 11.92611, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 11.92611\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 11.92611\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 11.92611\n",
      "\n",
      "Epoch 00068: val_mae improved from 11.92611 to 10.64979, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 10.64979\n",
      "\n",
      "Epoch 00070: val_mae improved from 10.64979 to 9.84780, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 9.84780\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 9.84780\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 9.84780\n",
      "\n",
      "Epoch 00074: val_mae improved from 9.84780 to 9.56265, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 9.56265\n",
      "\n",
      "Epoch 00076: val_mae improved from 9.56265 to 8.82219, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 8.82219\n",
      "\n",
      "Epoch 00078: val_mae improved from 8.82219 to 8.02970, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00079: val_mae improved from 8.02970 to 8.00038, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00080: val_mae improved from 8.00038 to 7.98223, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00081: val_mae improved from 7.98223 to 7.30260, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 7.30260\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 7.30260\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 7.30260\n",
      "\n",
      "Epoch 00085: val_mae improved from 7.30260 to 7.26592, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 7.26592\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 7.26592\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 7.26592\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 7.26592\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 7.26592\n",
      "\n",
      "Epoch 00091: val_mae improved from 7.26592 to 7.25405, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 7.25405\n",
      "\n",
      "Epoch 00093: val_mae improved from 7.25405 to 6.69032, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 6.69032\n",
      "\n",
      "Epoch 00095: val_mae improved from 6.69032 to 5.83042, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 5.83042\n",
      "\n",
      "Epoch 00097: val_mae improved from 5.83042 to 5.60592, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 5.60592\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 5.60592\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 5.60592\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 5.60592\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 5.60592\n",
      "\n",
      "Epoch 00103: val_mae improved from 5.60592 to 5.42906, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 5.42906\n",
      "\n",
      "Epoch 00105: val_mae improved from 5.42906 to 5.21952, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00106: val_mae improved from 5.21952 to 5.10015, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 5.10015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00120: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 5.10015\n",
      "\n",
      "Epoch 00128: val_mae improved from 5.10015 to 4.82291, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 4.82291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00278: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 4.82291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00439: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 4.82291\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 4.82291\n",
      "\n",
      "Lambda: 0.1 , Time: 0:04:05\n",
      "Train Error(all epochs): 0.9780781269073486 \n",
      " [49.354, 49.261, 49.181, 49.092, 48.987, 48.86, 48.705, 48.518, 48.291, 48.026, 47.713, 47.34, 46.908, 46.411, 45.844, 45.183, 44.409, 43.57, 42.686, 41.722, 40.717, 39.644, 38.539, 37.376, 36.156, 34.922, 33.618, 32.29, 30.94, 29.586, 28.204, 26.805, 25.44, 24.064, 22.744, 21.432, 20.15, 18.93, 17.76, 16.699, 15.621, 14.699, 13.919, 13.112, 12.466, 11.848, 11.25, 10.704, 10.199, 9.89, 9.665, 9.176, 8.68, 8.258, 7.916, 7.599, 7.36, 6.99, 6.777, 6.359, 6.287, 5.965, 5.883, 5.729, 5.462, 5.13, 5.121, 4.879, 4.87, 4.862, 4.709, 4.457, 4.352, 4.301, 4.303, 4.208, 4.3, 4.266, 4.22, 4.205, 4.177, 4.006, 3.95, 3.985, 4.031, 4.001, 3.99, 3.922, 3.748, 3.84, 3.769, 3.929, 3.909, 3.878, 3.996, 3.853, 3.854, 3.734, 3.754, 3.628, 3.505, 3.33, 3.484, 3.536, 3.765, 3.83, 3.542, 3.413, 3.403, 3.321, 3.327, 3.269, 3.32, 3.336, 3.35, 3.384, 3.403, 3.413, 3.191, 3.118, 3.019, 2.931, 2.918, 2.922, 2.96, 2.864, 3.125, 3.036, 3.253, 3.248, 3.187, 3.06, 3.067, 3.218, 3.023, 3.246, 2.899, 2.907, 2.609, 2.489, 2.549, 2.622, 2.634, 2.82, 2.927, 3.024, 2.782, 2.655, 2.772, 2.618, 2.711, 2.794, 2.791, 2.62, 2.544, 2.593, 2.588, 2.672, 2.49, 2.567, 2.621, 2.427, 2.416, 2.284, 2.235, 2.155, 2.222, 2.229, 2.445, 2.458, 2.434, 2.445, 2.15, 2.434, 2.374, 2.364, 2.48, 2.547, 2.692, 2.507, 2.763, 2.708, 2.749, 2.841, 2.663, 2.613, 2.614, 2.477, 2.501, 2.318, 2.307, 2.204, 2.136, 2.272, 2.204, 2.209, 2.293, 2.135, 2.075, 2.078, 2.166, 2.26, 2.369, 2.34, 2.502, 2.235, 2.226, 2.168, 2.096, 1.972, 2.035, 2.039, 2.206, 2.083, 2.21, 1.904, 1.923, 1.76, 1.657, 1.627, 1.756, 1.955, 2.072, 2.025, 2.083, 1.857, 1.764, 1.772, 1.758, 1.893, 1.651, 1.644, 1.708, 1.793, 1.996, 1.89, 1.911, 1.888, 1.737, 1.681, 1.57, 1.527, 1.558, 1.632, 1.655, 1.702, 1.858, 2.029, 2.143, 2.125, 2.102, 1.991, 2.174, 1.937, 1.783, 1.573, 1.518, 1.494, 1.314, 1.266, 1.299, 1.341, 1.449, 1.541, 1.717, 1.715, 1.832, 1.68, 1.908, 1.858, 1.844, 1.611, 1.699, 1.739, 1.673, 1.691, 1.809, 1.849, 1.705, 1.734, 1.538, 1.471, 1.471, 1.565, 1.498, 1.521, 1.656, 1.477, 1.465, 1.519, 1.511, 1.414, 1.368, 1.47, 1.642, 1.513, 1.494, 1.451, 1.5, 1.615, 1.499, 1.619, 1.442, 1.488, 1.619, 1.661, 1.7, 1.733, 1.78, 1.803, 1.577, 1.45, 1.393, 1.337, 1.327, 1.271, 1.229, 1.273, 1.283, 1.236, 1.158, 1.275, 1.331, 1.466, 1.381, 1.379, 1.405, 1.311, 1.36, 1.513, 1.66, 1.82, 1.688, 1.704, 1.882, 2.041, 1.935, 2.038, 2.078, 2.156, 2.064, 1.879, 1.648, 1.438, 1.308, 1.292, 1.259, 1.211, 1.172, 1.204, 1.136, 1.099, 1.006, 1.009, 0.978, 1.016, 1.165, 1.476, 1.282, 1.387, 1.54, 1.711, 1.776, 1.854, 1.734, 1.685, 1.423, 1.272, 1.196, 1.153, 1.266, 1.39, 1.383, 1.401, 1.434, 1.537, 1.68, 1.507, 1.505, 1.579, 1.544, 1.456, 1.365, 1.209, 1.152, 1.023, 1.018, 1.065, 1.206, 1.358, 1.594, 1.705, 1.801, 1.782, 1.705, 1.649, 1.501, 1.527, 1.493, 1.434, 1.202, 1.185, 1.238, 1.275, 1.275, 1.356, 1.477, 1.48, 1.486, 1.628, 1.668, 1.588, 1.631, 1.654, 1.844, 1.712, 1.625, 1.492, 1.333, 1.291, 1.264, 1.188, 1.221, 1.153, 1.09, 1.087, 1.251, 1.406, 1.265, 1.263, 1.352, 1.346, 1.289, 1.19, 1.1, 1.171, 1.213, 1.252, 1.291, 1.274, 1.295, 1.357, 1.38, 1.366, 1.374, 1.591, 1.534, 1.601, 1.394, 1.311, 1.364, 1.452, 1.32, 1.314, 1.285, 1.406, 1.335, 1.286, 1.228, 1.098, 1.085, 1.043, 1.272, 1.492, 1.654, 1.789, 1.831, 1.375, 1.18, 1.038, 1.127, 1.274, 1.245, 1.19, 1.382, 1.576, 1.768, 1.697, 1.802, 1.687, 1.769, 1.699, 1.769, 1.622, 1.669, 1.509, 1.684, 1.505, 1.526, 1.498, 1.451, 1.422, 1.448, 1.455, 1.37, 1.345, 1.368, 1.256, 1.114, 1.068]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005, 0.01, 0.009, 0.041, 0.077, 0.076, 0.156, 0.225, 0.271, 0.373, 0.476, 0.528, 0.605, 0.654, 0.793, 0.933, 0.945, 0.924, 0.932, 0.956, 1.0, 1.079, 1.036, 1.128, 1.008, 1.178, 1.143, 1.213, 1.307, 1.237, 1.175, 1.274, 1.207, 1.3, 1.389, 1.38, 1.373, 1.331, 1.357, 1.445, 1.425, 1.521, 1.496, 1.562, 1.504, 1.615, 1.495, 1.496, 1.502, 1.617, 1.644, 1.541, 1.599, 1.535, 1.574, 1.458, 1.712, 1.601, 1.583, 1.766, 1.591, 1.638, 1.54, 1.621, 1.472, 1.471, 1.377, 1.39, 1.5, 1.542, 1.599, 1.529, 1.373, 1.474, 1.296, 1.45, 1.333, 1.32, 1.43, 1.372, 1.403, 1.441, 1.439, 1.35, 1.224, 1.219, 1.243, 1.139, 1.183, 1.262, 1.123, 1.354, 1.301, 1.391, 1.372, 1.394, 1.268, 1.247, 1.508, 1.14, 1.514, 1.161, 1.252, 1.099, 0.977, 1.051, 1.058, 1.121, 1.247, 1.16, 1.448, 1.207, 0.988, 1.397, 0.985, 1.197, 1.253, 1.187, 1.212, 1.047, 1.095, 1.028, 1.308, 0.981, 1.146, 1.172, 1.008, 1.005, 0.969, 0.928, 0.907, 1.015, 0.892, 1.087, 1.068, 1.058, 1.067, 0.901, 1.091, 1.04, 1.059, 1.119, 1.111, 1.317, 0.943, 1.351, 1.21, 1.158, 1.377, 1.2, 1.195, 1.204, 0.991, 1.241, 0.886, 1.124, 0.981, 0.854, 1.085, 0.92, 0.963, 1.046, 0.953, 0.932, 0.89, 1.016, 1.033, 1.039, 1.068, 1.184, 0.91, 1.137, 0.845, 0.964, 0.888, 0.852, 0.942, 1.068, 0.841, 1.136, 0.735, 0.935, 0.684, 0.725, 0.729, 0.735, 0.918, 0.962, 0.913, 0.966, 0.838, 0.753, 0.833, 0.741, 0.939, 0.616, 0.74, 0.878, 0.703, 1.03, 0.794, 0.877, 0.842, 0.774, 0.819, 0.621, 0.762, 0.657, 0.74, 0.756, 0.753, 0.864, 0.952, 1.0, 1.011, 1.042, 0.774, 1.092, 0.791, 0.865, 0.671, 0.677, 0.663, 0.553, 0.612, 0.508, 0.668, 0.621, 0.725, 0.875, 0.736, 0.912, 0.762, 0.937, 0.77, 0.966, 0.684, 0.787, 0.789, 0.787, 0.737, 0.847, 0.853, 0.765, 0.818, 0.716, 0.671, 0.674, 0.698, 0.735, 0.617, 0.878, 0.582, 0.723, 0.703, 0.685, 0.66, 0.616, 0.684, 0.802, 0.635, 0.762, 0.661, 0.626, 0.855, 0.604, 0.828, 0.629, 0.7, 0.771, 0.709, 0.874, 0.782, 0.813, 0.87, 0.722, 0.662, 0.6, 0.636, 0.603, 0.586, 0.582, 0.569, 0.6, 0.578, 0.505, 0.617, 0.578, 0.774, 0.579, 0.682, 0.679, 0.562, 0.654, 0.719, 0.779, 0.866, 0.784, 0.803, 0.934, 0.928, 0.97, 0.968, 0.95, 1.011, 0.939, 0.887, 0.764, 0.684, 0.579, 0.607, 0.539, 0.586, 0.468, 0.625, 0.438, 0.552, 0.448, 0.438, 0.457, 0.48, 0.488, 0.811, 0.512, 0.697, 0.76, 0.775, 0.823, 0.962, 0.729, 0.9, 0.604, 0.583, 0.518, 0.584, 0.513, 0.715, 0.594, 0.649, 0.713, 0.667, 0.824, 0.704, 0.711, 0.67, 0.79, 0.617, 0.673, 0.516, 0.558, 0.418, 0.474, 0.503, 0.526, 0.675, 0.756, 0.781, 0.898, 0.811, 0.821, 0.736, 0.772, 0.708, 0.677, 0.713, 0.581, 0.482, 0.632, 0.558, 0.586, 0.644, 0.679, 0.724, 0.681, 0.8, 0.761, 0.734, 0.818, 0.69, 0.917, 0.802, 0.728, 0.741, 0.55, 0.609, 0.615, 0.518, 0.649, 0.454, 0.561, 0.488, 0.565, 0.719, 0.562, 0.564, 0.683, 0.578, 0.61, 0.568, 0.463, 0.61, 0.517, 0.615, 0.557, 0.604, 0.645, 0.57, 0.712, 0.62, 0.603, 0.857, 0.634, 0.825, 0.642, 0.572, 0.615, 0.732, 0.602, 0.622, 0.569, 0.706, 0.56, 0.656, 0.523, 0.547, 0.451, 0.547, 0.547, 0.772, 0.781, 0.812, 0.951, 0.536, 0.584, 0.468, 0.463, 0.686, 0.525, 0.557, 0.74, 0.645, 0.933, 0.77, 0.873, 0.703, 0.955, 0.712, 0.923, 0.726, 0.851, 0.538, 0.95, 0.601, 0.785, 0.709, 0.663, 0.731, 0.595, 0.72, 0.648, 0.624, 0.65, 0.56, 0.575, 0.457]\n",
      "Val Error(all epochs): 4.822906970977783 \n",
      " [48.281, 48.196, 48.089, 47.954, 47.824, 47.664, 47.522, 47.346, 47.111, 46.801, 46.477, 46.152, 45.764, 45.115, 44.639, 44.132, 43.643, 42.501, 41.558, 41.626, 40.791, 39.558, 39.628, 39.804, 37.241, 38.853, 38.26, 36.554, 36.883, 35.731, 35.493, 35.375, 34.202, 34.206, 32.175, 31.039, 28.29, 27.034, 25.146, 21.549, 21.935, 20.948, 20.166, 21.314, 19.599, 20.185, 20.466, 19.029, 19.386, 18.965, 17.695, 16.812, 17.308, 16.945, 16.766, 17.223, 15.994, 15.34, 15.998, 14.013, 15.095, 13.317, 13.066, 11.926, 12.376, 11.966, 12.382, 10.65, 10.939, 9.848, 10.023, 9.86, 10.162, 9.563, 9.75, 8.822, 9.639, 8.03, 8.0, 7.982, 7.303, 7.718, 7.66, 7.41, 7.266, 7.796, 7.391, 7.497, 7.365, 7.796, 7.254, 7.622, 6.69, 6.936, 5.83, 5.936, 5.606, 5.93, 5.995, 5.734, 6.521, 5.929, 5.429, 5.977, 5.22, 5.1, 5.282, 5.887, 5.567, 6.152, 5.578, 5.722, 5.758, 6.733, 5.479, 6.299, 5.801, 5.627, 5.732, 5.565, 5.107, 5.626, 5.271, 5.716, 5.31, 5.427, 5.754, 4.823, 5.731, 5.272, 5.726, 5.212, 6.879, 5.624, 7.476, 6.566, 6.03, 5.915, 5.795, 5.271, 5.222, 5.399, 4.968, 5.565, 5.373, 5.421, 5.296, 6.173, 5.8, 5.16, 6.443, 5.364, 6.062, 5.336, 6.157, 5.439, 5.393, 5.702, 5.534, 6.291, 5.453, 5.852, 5.575, 5.975, 5.497, 6.148, 5.797, 5.346, 5.901, 5.284, 5.781, 5.271, 5.657, 5.443, 5.157, 5.861, 5.441, 6.061, 6.016, 6.971, 6.805, 5.977, 6.28, 6.439, 6.993, 8.465, 6.998, 9.516, 7.175, 8.21, 6.682, 6.196, 6.792, 6.404, 7.154, 7.07, 6.879, 7.568, 6.431, 7.341, 5.874, 6.317, 6.051, 5.932, 5.908, 6.545, 5.334, 6.728, 5.448, 5.57, 5.384, 5.537, 6.062, 7.039, 6.125, 6.268, 5.435, 6.151, 5.546, 5.689, 6.12, 5.646, 6.324, 5.633, 5.988, 5.612, 5.591, 5.949, 5.587, 6.119, 5.919, 6.3, 6.232, 5.738, 6.361, 6.143, 6.923, 5.83, 5.938, 5.694, 5.698, 5.623, 5.612, 5.842, 5.431, 6.104, 5.609, 5.64, 6.535, 5.616, 6.389, 5.894, 5.919, 5.668, 5.822, 5.343, 5.7, 5.295, 5.554, 5.679, 5.614, 5.663, 5.499, 6.016, 5.792, 6.418, 5.782, 6.341, 5.746, 5.942, 5.744, 5.611, 5.545, 5.455, 5.795, 5.505, 6.32, 5.887, 6.093, 5.948, 5.649, 5.66, 5.596, 5.604, 5.762, 5.956, 5.733, 5.952, 6.461, 5.839, 6.028, 5.576, 6.041, 5.786, 5.889, 6.226, 5.745, 5.558, 5.549, 5.423, 5.372, 5.648, 5.596, 6.11, 5.77, 5.895, 5.708, 5.63, 5.796, 6.18, 6.594, 5.765, 5.927, 5.813, 5.77, 5.568, 5.627, 5.756, 5.524, 5.339, 5.547, 5.523, 5.487, 5.627, 5.561, 5.744, 5.645, 5.763, 5.711, 5.695, 6.193, 6.12, 6.065, 6.067, 6.161, 6.071, 5.629, 5.968, 6.071, 5.842, 5.792, 5.969, 5.798, 5.765, 5.61, 5.356, 5.727, 5.681, 5.51, 5.652, 5.465, 5.469, 5.407, 5.509, 5.294, 5.725, 5.521, 5.484, 5.558, 5.692, 5.631, 5.981, 6.398, 5.725, 5.854, 5.61, 5.495, 5.674, 5.827, 5.509, 6.006, 5.457, 6.086, 6.338, 5.807, 6.575, 5.876, 5.43, 5.792, 5.768, 5.738, 5.553, 5.535, 5.463, 5.497, 5.349, 5.414, 5.563, 5.487, 5.647, 5.703, 5.958, 5.877, 6.452, 6.274, 5.945, 5.576, 6.341, 5.919, 5.878, 7.199, 6.208, 7.217, 6.602, 6.887, 6.863, 5.65, 5.892, 5.72, 6.054, 6.278, 6.587, 6.277, 6.513, 6.063, 5.923, 6.132, 5.772, 5.989, 5.973, 5.821, 5.758, 5.819, 5.483, 5.451, 5.867, 5.685, 5.864, 5.841, 6.322, 6.05, 5.952, 6.414, 5.693, 6.076, 5.464, 6.042, 5.517, 5.492, 5.674, 5.751, 5.493, 5.745, 5.392, 5.829, 6.251, 6.029, 6.525, 5.827, 6.81, 5.764, 6.822, 5.886, 5.692, 5.434, 5.6, 5.419, 5.683, 5.679, 5.559, 5.516, 5.827, 5.197, 5.68, 5.799, 5.47, 5.768, 5.248, 5.367, 5.442, 5.577, 5.433, 5.454, 5.861, 5.472, 6.632, 6.041, 6.188, 6.129, 5.911, 5.608, 5.88, 5.793, 5.547, 6.1, 5.953, 5.751, 5.463, 5.942, 6.119, 5.936, 5.871, 5.157, 5.79, 5.412, 5.656, 6.203, 6.253, 5.721, 5.667]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005, 0.034, 0.018, 0.012, 0.096, 0.011, 0.0, 0.095, 0.115, 0.011, 0.437, 0.395, 0.106, 0.192, 0.085, 0.049, 0.097, 0.11, 0.018, 0.184, 0.021, 0.028, 0.178, 0.481, 0.223, 0.316, 0.051, 0.146, 0.327, 0.137, 0.118, 0.229, 0.082, 0.177, 0.235, 0.168, 0.148, 0.334, 0.289, 0.417, 0.279, 0.353, 0.437, 0.554, 0.667, 0.871, 0.985, 0.793, 1.092, 0.67, 0.978, 1.308, 0.996, 0.924, 1.593, 1.033, 1.61, 1.343, 1.674, 1.137, 1.473, 1.067, 1.46, 1.085, 1.958, 2.164, 1.966, 1.88, 1.473, 1.561, 1.542, 1.355, 1.935, 1.599, 1.94, 2.161, 2.014, 1.606, 1.695, 2.183, 2.104, 2.328, 1.906, 2.756, 1.804, 2.016, 1.986, 2.07, 2.456, 3.003, 3.168, 2.671, 4.665, 2.862, 5.096, 4.216, 3.906, 3.868, 3.293, 2.795, 2.585, 2.516, 2.628, 2.387, 3.29, 2.845, 1.928, 3.711, 3.13, 2.744, 3.772, 3.122, 3.325, 2.896, 3.398, 2.758, 3.051, 3.086, 2.862, 3.933, 3.07, 3.157, 3.531, 3.241, 3.002, 3.441, 2.605, 2.622, 2.975, 2.859, 3.101, 3.045, 2.863, 2.653, 2.521, 3.039, 2.877, 4.085, 3.362, 4.688, 4.405, 3.917, 3.705, 4.697, 4.672, 6.272, 4.968, 6.578, 5.37, 5.61, 4.813, 3.998, 4.195, 4.613, 4.363, 5.239, 4.477, 4.65, 4.007, 4.696, 3.394, 4.058, 3.264, 3.86, 3.269, 4.07, 3.174, 4.418, 3.222, 3.164, 2.747, 3.651, 3.425, 4.831, 3.967, 3.887, 3.412, 3.728, 3.352, 3.432, 3.335, 3.557, 3.746, 3.712, 3.714, 3.22, 3.441, 3.109, 3.663, 3.505, 3.571, 3.893, 3.617, 3.367, 3.99, 3.886, 4.466, 3.768, 3.587, 3.323, 2.996, 3.132, 2.875, 3.084, 3.065, 3.24, 2.776, 3.173, 2.841, 3.22, 3.053, 3.403, 2.97, 3.169, 3.227, 2.846, 2.808, 3.025, 2.928, 3.347, 3.062, 3.208, 2.73, 3.86, 2.587, 4.196, 2.808, 4.179, 2.696, 4.084, 2.959, 3.415, 2.973, 3.334, 3.135, 3.186, 3.876, 3.579, 3.737, 3.622, 3.547, 3.295, 3.152, 3.477, 3.158, 3.821, 3.541, 3.793, 4.177, 3.717, 3.587, 3.57, 3.384, 3.866, 3.35, 3.862, 3.509, 3.013, 3.349, 2.64, 3.06, 3.326, 2.865, 3.802, 3.256, 3.548, 3.651, 3.378, 3.704, 3.919, 4.193, 3.734, 3.302, 3.888, 3.022, 3.443, 3.012, 3.282, 3.118, 2.653, 3.087, 2.84, 2.925, 2.854, 2.885, 2.948, 3.001, 2.629, 3.112, 3.123, 3.298, 3.807, 3.219, 3.126, 4.079, 3.298, 3.279, 3.658, 3.501, 3.699, 3.459, 3.274, 3.522, 3.166, 3.629, 2.937, 3.723, 2.701, 3.495, 2.77, 3.128, 3.03, 2.975, 2.918, 3.046, 2.765, 3.27, 2.827, 3.081, 3.202, 3.589, 3.118, 4.52, 3.331, 3.605, 3.144, 3.083, 3.006, 3.611, 2.743, 3.818, 2.92, 3.928, 4.04, 3.638, 4.071, 4.048, 3.039, 3.661, 3.833, 3.266, 3.678, 2.791, 3.226, 2.931, 2.793, 3.065, 3.031, 2.858, 3.477, 2.916, 3.884, 3.656, 4.193, 4.269, 3.6, 2.938, 2.515, 2.931, 2.176, 2.427, 2.318, 2.448, 2.364, 2.61, 2.491, 2.932, 2.68, 3.396, 4.111, 3.623, 5.017, 4.123, 4.55, 4.178, 3.764, 4.145, 3.267, 3.238, 3.236, 2.822, 3.552, 2.738, 3.324, 3.208, 3.461, 3.467, 3.807, 3.356, 4.276, 3.811, 3.702, 4.324, 3.166, 3.913, 2.942, 3.613, 3.209, 3.069, 3.622, 2.982, 3.347, 3.584, 2.542, 3.394, 2.522, 2.799, 3.014, 2.612, 2.963, 2.687, 2.623, 2.843, 2.643, 3.164, 3.006, 3.292, 3.303, 3.485, 3.26, 3.232, 3.281, 2.595, 3.609, 2.811, 3.718, 3.046, 2.926, 3.128, 2.799, 3.155, 2.996, 3.032, 3.532, 3.059, 4.192, 3.873, 4.105, 3.789, 3.991, 2.956, 3.942, 2.832, 3.762, 3.736, 3.764, 3.423, 3.11, 3.81, 3.966, 4.026, 3.697, 3.064, 3.437, 2.813, 2.981, 3.033, 2.793, 2.937, 2.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 48.23732, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 48.23732 to 48.14997, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 48.14997 to 48.05354, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 48.05354 to 47.95466, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 47.95466 to 47.84896, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 47.84896 to 47.76041, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 47.76041 to 47.66756, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 47.66756 to 47.56577, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 47.56577 to 47.42166, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 47.42166 to 47.20144, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 47.20144 to 47.09916, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 47.09916 to 46.76477, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 46.76477 to 46.37125, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 46.37125 to 46.10224, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 46.10224 to 45.36058, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 45.36058 to 44.79763, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 44.79763 to 44.50333, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 44.50333 to 43.71361, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 43.71361 to 43.06399, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 43.06399 to 42.86070, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 42.86070 to 41.54609, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 41.54609 to 40.63381, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 40.63381 to 40.00391, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 40.00391 to 39.15341, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 39.15341 to 38.15394, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 38.15394 to 36.92480, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 36.92480 to 33.70778, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 33.70778\n",
      "\n",
      "Epoch 00029: val_mae improved from 33.70778 to 32.73679, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 32.73679 to 30.66871, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 30.66871\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 30.66871\n",
      "\n",
      "Epoch 00033: val_mae improved from 30.66871 to 28.25743, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00034: val_mae improved from 28.25743 to 25.83584, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00035: val_mae improved from 25.83584 to 24.70700, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00036: val_mae improved from 24.70700 to 22.49815, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 22.49815\n",
      "\n",
      "Epoch 00038: val_mae improved from 22.49815 to 20.70169, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00039: val_mae improved from 20.70169 to 18.51087, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00040: val_mae improved from 18.51087 to 17.93495, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00041: val_mae improved from 17.93495 to 16.31278, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00042: val_mae improved from 16.31278 to 13.66574, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00043: val_mae improved from 13.66574 to 13.61932, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00044: val_mae improved from 13.61932 to 12.82350, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00045: val_mae improved from 12.82350 to 9.69771, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 9.69771\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 9.69771\n",
      "\n",
      "Epoch 00048: val_mae improved from 9.69771 to 9.11349, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00049: val_mae improved from 9.11349 to 8.97617, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00050: val_mae improved from 8.97617 to 8.89252, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 8.89252\n",
      "\n",
      "Epoch 00089: val_mae improved from 8.89252 to 8.71665, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 8.71665\n",
      "\n",
      "Epoch 00091: val_mae improved from 8.71665 to 8.34571, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 8.34571\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 8.34571\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 8.34571\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 8.34571\n",
      "\n",
      "Epoch 00096: val_mae improved from 8.34571 to 7.58020, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 7.58020\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 7.58020\n",
      "\n",
      "Epoch 00099: val_mae improved from 7.58020 to 6.97191, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 6.97191\n",
      "\n",
      "Epoch 00101: val_mae improved from 6.97191 to 6.44714, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 6.44714\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 6.44714\n",
      "\n",
      "Epoch 00104: val_mae improved from 6.44714 to 6.07282, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 6.07282\n",
      "\n",
      "Epoch 00128: val_mae improved from 6.07282 to 5.88068, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00129: val_mae improved from 5.88068 to 5.76676, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 5.76676\n",
      "\n",
      "Epoch 00131: val_mae improved from 5.76676 to 4.75850, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_7/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 4.75850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00152: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 4.75850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00313: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 4.75850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00474: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 4.75850\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 4.75850\n",
      "\n",
      "Lambda: 1 , Time: 0:04:03\n",
      "Train Error(all epochs): 1.60405433177948 \n",
      " [49.338, 49.234, 49.147, 49.055, 48.957, 48.848, 48.72, 48.572, 48.399, 48.205, 47.975, 47.703, 47.391, 47.039, 46.631, 46.162, 45.66, 45.091, 44.453, 43.711, 42.906, 42.052, 41.147, 40.189, 39.181, 38.152, 37.071, 36.026, 34.862, 33.681, 32.432, 31.233, 30.023, 28.8, 27.557, 26.307, 25.041, 23.841, 22.599, 21.441, 20.168, 19.073, 17.877, 16.85, 15.779, 14.676, 13.745, 12.632, 11.689, 10.826, 9.936, 9.355, 8.537, 8.076, 7.802, 7.089, 6.705, 6.019, 5.67, 5.399, 5.0, 4.843, 4.615, 4.657, 4.55, 4.573, 4.363, 4.529, 4.629, 4.29, 4.199, 4.245, 3.923, 3.951, 3.517, 3.677, 3.57, 3.43, 3.359, 3.515, 3.862, 3.848, 3.573, 3.545, 3.628, 3.643, 3.803, 3.668, 4.048, 3.844, 3.657, 3.634, 3.114, 2.979, 3.322, 3.293, 3.46, 3.691, 3.685, 3.591, 3.585, 3.475, 3.411, 3.135, 2.977, 3.167, 3.28, 3.281, 3.587, 3.333, 3.402, 3.162, 3.476, 3.749, 3.657, 3.524, 3.209, 2.951, 3.013, 3.1, 3.233, 3.072, 3.367, 3.321, 3.079, 3.042, 3.168, 3.421, 3.274, 3.365, 3.063, 2.906, 2.712, 2.791, 2.657, 2.941, 2.906, 2.933, 2.952, 2.665, 2.882, 3.078, 3.184, 3.185, 2.781, 2.91, 3.096, 3.355, 3.095, 2.925, 3.19, 3.037, 2.76, 2.659, 2.811, 2.487, 2.598, 2.765, 2.831, 3.083, 3.017, 2.952, 3.505, 3.022, 3.17, 2.867, 2.971, 2.923, 2.91, 3.085, 2.894, 2.79, 2.724, 2.688, 2.665, 2.827, 2.867, 2.585, 2.551, 2.539, 2.711, 2.949, 2.923, 3.142, 3.434, 3.304, 3.573, 3.198, 3.155, 3.094, 2.597, 2.544, 2.49, 2.655, 3.073, 2.988, 3.159, 2.957, 3.005, 2.81, 3.036, 2.911, 3.095, 2.849, 2.747, 2.785, 2.57, 2.566, 2.614, 3.028, 2.872, 3.354, 2.854, 2.815, 2.594, 2.741, 2.545, 2.276, 2.238, 2.751, 2.462, 2.649, 2.217, 2.417, 2.25, 2.31, 2.405, 2.567, 2.37, 2.642, 2.989, 2.609, 2.488, 2.481, 2.405, 2.766, 2.792, 2.909, 2.878, 2.718, 2.539, 2.549, 2.654, 2.616, 2.664, 2.854, 2.874, 2.523, 2.226, 2.381, 2.498, 2.683, 2.775, 2.757, 2.568, 2.126, 2.046, 2.037, 1.924, 1.846, 1.995, 2.267, 2.588, 2.469, 2.56, 2.652, 2.422, 2.365, 2.488, 2.645, 3.043, 2.992, 2.836, 3.107, 3.198, 3.402, 3.332, 3.021, 2.85, 2.477, 2.268, 2.402, 2.608, 2.45, 2.452, 2.317, 2.202, 2.406, 2.379, 2.369, 2.125, 2.155, 1.98, 1.942, 1.905, 2.024, 2.05, 1.943, 1.986, 2.062, 2.059, 2.144, 2.293, 2.336, 2.516, 2.625, 3.071, 3.116, 2.837, 3.181, 2.857, 2.613, 2.64, 2.574, 2.563, 2.232, 2.306, 2.782, 2.539, 2.514, 2.22, 2.075, 2.034, 2.138, 2.388, 2.205, 2.107, 2.211, 2.257, 2.268, 2.389, 2.53, 2.594, 2.525, 2.473, 2.443, 2.197, 2.01, 1.932, 2.147, 2.337, 2.4, 2.207, 2.337, 2.087, 1.972, 2.017, 2.106, 2.02, 2.13, 2.089, 2.073, 2.262, 2.493, 2.551, 2.677, 2.628, 2.986, 2.642, 2.799, 2.356, 2.683, 2.923, 2.601, 2.499, 2.676, 2.694, 2.829, 2.687, 2.758, 2.365, 2.264, 2.022, 2.057, 2.054, 2.146, 2.31, 2.623, 2.551, 2.415, 2.321, 2.263, 2.379, 2.409, 2.834, 2.53, 2.595, 2.488, 2.521, 2.503, 2.861, 2.698, 2.671, 2.355, 2.491, 2.221, 2.413, 2.499, 2.551, 2.293, 2.331, 2.083, 2.26, 2.241, 2.576, 2.597, 3.198, 2.783, 2.485, 2.178, 1.995, 1.908, 1.802, 1.971, 1.824, 1.773, 1.604, 1.677, 1.733, 1.946, 2.037, 2.286, 2.198, 2.481, 2.394, 2.643, 2.804, 2.56, 2.448, 2.088, 2.08, 2.142, 2.199, 2.49, 2.52, 2.602, 2.733, 2.659, 2.375, 2.297, 2.419, 2.366, 2.411, 2.107, 2.089, 2.026, 2.059, 1.966, 2.155, 1.902, 1.795, 1.607, 1.645, 1.877, 2.039, 2.242, 2.245, 2.003, 2.183, 2.114, 2.207, 2.346, 2.046, 2.104, 2.232, 2.48, 2.407, 2.566, 2.358, 2.323, 2.288, 2.284, 2.141, 2.291, 2.077, 2.2, 2.065, 1.961, 1.864, 1.808, 1.753, 1.815, 1.81, 1.785, 1.937, 2.012, 2.083, 1.948, 1.941, 2.101, 1.931, 2.088, 2.351, 2.638, 2.416, 2.392, 2.432, 2.2, 2.24, 2.328]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.021, 0.01, 0.015, 0.029, 0.031, 0.058, 0.062, 0.114, 0.095, 0.127, 0.145, 0.151, 0.152, 0.297, 0.225, 0.341, 0.599, 0.498, 0.631, 0.554, 0.629, 0.714, 0.752, 0.756, 0.912, 1.01, 1.117, 1.215, 1.231, 1.324, 1.475, 1.33, 1.346, 1.402, 1.365, 1.378, 1.243, 1.27, 1.267, 1.239, 1.154, 1.303, 1.442, 1.514, 1.286, 1.312, 1.409, 1.368, 1.482, 1.397, 1.565, 1.547, 1.318, 1.42, 1.146, 1.064, 1.257, 1.247, 1.304, 1.432, 1.441, 1.363, 1.441, 1.29, 1.332, 1.192, 1.121, 1.112, 1.308, 1.187, 1.414, 1.267, 1.311, 1.159, 1.348, 1.528, 1.46, 1.407, 1.274, 1.022, 1.18, 1.161, 1.262, 1.184, 1.279, 1.275, 1.209, 1.128, 1.185, 1.38, 1.343, 1.306, 1.111, 1.1, 0.968, 1.076, 0.967, 1.126, 1.141, 1.095, 1.185, 0.948, 1.094, 1.273, 1.209, 1.231, 1.081, 1.15, 1.166, 1.365, 1.205, 1.155, 1.179, 1.203, 1.055, 1.008, 1.105, 0.894, 0.942, 1.056, 1.159, 1.195, 1.266, 1.064, 1.531, 1.102, 1.28, 1.1, 1.138, 1.181, 1.065, 1.245, 1.134, 1.097, 1.082, 1.013, 1.045, 1.064, 1.169, 0.921, 1.033, 0.986, 0.999, 1.186, 1.128, 1.299, 1.377, 1.357, 1.377, 1.315, 1.211, 1.32, 0.964, 0.967, 0.992, 0.983, 1.266, 1.18, 1.271, 1.191, 1.268, 1.055, 1.223, 1.17, 1.216, 1.1, 1.119, 1.11, 0.941, 1.013, 1.01, 1.288, 1.175, 1.37, 1.141, 1.09, 1.002, 1.106, 0.999, 0.905, 0.824, 1.131, 0.962, 1.088, 0.804, 0.912, 0.862, 0.883, 0.913, 1.065, 0.905, 1.075, 1.208, 1.097, 0.964, 0.979, 0.948, 1.105, 1.143, 1.14, 1.2, 1.051, 0.97, 0.968, 1.134, 1.053, 1.08, 1.12, 1.164, 1.062, 0.784, 0.941, 1.001, 1.085, 1.133, 1.154, 1.055, 0.767, 0.76, 0.796, 0.694, 0.73, 0.755, 0.9, 1.073, 1.01, 1.035, 1.037, 0.959, 0.92, 1.023, 1.041, 1.227, 1.345, 1.145, 1.238, 1.348, 1.481, 1.388, 1.225, 1.191, 0.999, 0.894, 0.966, 1.083, 1.009, 0.95, 0.978, 0.834, 0.974, 1.01, 0.962, 0.773, 0.877, 0.761, 0.745, 0.743, 0.772, 0.84, 0.725, 0.773, 0.813, 0.792, 0.84, 0.967, 0.903, 1.051, 1.135, 1.216, 1.437, 1.146, 1.358, 1.167, 1.022, 1.032, 1.096, 1.079, 0.905, 0.906, 1.208, 1.023, 1.066, 0.86, 0.812, 0.824, 0.89, 0.951, 0.944, 0.781, 0.904, 0.887, 0.904, 1.056, 1.076, 1.117, 1.027, 0.948, 0.987, 0.921, 0.801, 0.776, 0.863, 0.98, 1.047, 0.831, 1.003, 0.832, 0.804, 0.744, 0.934, 0.767, 0.837, 0.835, 0.836, 0.958, 1.11, 1.062, 1.08, 1.16, 1.193, 1.167, 1.183, 0.997, 1.086, 1.313, 1.074, 0.975, 1.181, 1.103, 1.261, 1.138, 1.213, 0.904, 0.924, 0.785, 0.875, 0.868, 0.906, 0.94, 1.175, 1.076, 0.99, 0.989, 0.925, 1.05, 1.046, 1.273, 1.01, 1.048, 1.053, 1.131, 1.002, 1.291, 1.136, 1.183, 0.951, 1.121, 0.804, 1.176, 1.006, 1.109, 0.92, 1.006, 0.837, 0.989, 0.883, 1.201, 1.036, 1.53, 1.098, 1.052, 0.867, 0.8, 0.828, 0.74, 0.858, 0.658, 0.78, 0.592, 0.704, 0.712, 0.842, 0.783, 1.055, 0.89, 1.101, 1.044, 1.114, 1.203, 1.06, 1.07, 0.862, 0.844, 0.902, 0.906, 1.162, 1.061, 1.193, 1.215, 1.019, 1.063, 0.964, 1.088, 0.967, 1.018, 0.865, 0.925, 0.817, 0.92, 0.807, 0.929, 0.784, 0.724, 0.663, 0.624, 0.867, 0.847, 0.942, 0.905, 0.895, 0.93, 0.885, 0.978, 1.008, 0.797, 0.972, 0.901, 1.112, 1.031, 1.147, 0.994, 0.929, 0.971, 1.016, 0.876, 1.058, 0.861, 0.916, 0.921, 0.835, 0.779, 0.77, 0.736, 0.736, 0.784, 0.742, 0.784, 0.872, 0.911, 0.806, 0.822, 0.902, 0.864, 0.861, 1.023, 1.197, 1.066, 0.993, 1.081, 0.931, 0.911, 1.076]\n",
      "Val Error(all epochs): 4.75850248336792 \n",
      " [48.237, 48.15, 48.054, 47.955, 47.849, 47.76, 47.668, 47.566, 47.422, 47.201, 47.099, 46.765, 46.371, 46.102, 45.361, 44.798, 44.503, 43.714, 43.064, 42.861, 41.546, 40.634, 40.004, 39.153, 38.154, 36.925, 33.708, 35.173, 32.737, 30.669, 31.372, 30.969, 28.257, 25.836, 24.707, 22.498, 22.515, 20.702, 18.511, 17.935, 16.313, 13.666, 13.619, 12.824, 9.698, 10.096, 10.162, 9.113, 8.976, 8.893, 8.929, 9.57, 9.47, 9.614, 12.004, 11.287, 11.441, 11.227, 11.055, 11.28, 11.57, 10.451, 11.76, 11.097, 12.246, 10.618, 12.11, 13.364, 10.78, 15.205, 10.446, 14.08, 10.518, 11.176, 13.444, 11.689, 12.529, 11.77, 10.135, 12.198, 12.057, 11.904, 13.083, 9.01, 10.332, 11.01, 10.295, 12.322, 8.717, 9.99, 8.346, 8.941, 10.324, 9.886, 10.309, 7.58, 8.414, 7.996, 6.972, 7.793, 6.447, 7.38, 6.827, 6.073, 7.362, 7.215, 6.519, 9.062, 7.158, 7.211, 7.727, 6.925, 10.489, 6.907, 7.828, 8.651, 8.03, 9.019, 6.575, 8.153, 7.032, 8.413, 8.356, 7.149, 8.567, 8.233, 7.202, 5.881, 5.767, 6.387, 4.759, 7.352, 6.059, 6.741, 7.245, 6.525, 7.286, 5.779, 6.711, 6.868, 5.804, 5.841, 6.385, 5.676, 6.369, 5.769, 6.074, 6.098, 6.131, 6.636, 7.028, 7.355, 6.919, 6.889, 6.68, 6.784, 6.207, 5.734, 7.092, 5.732, 6.353, 5.718, 6.513, 6.401, 7.903, 6.767, 6.482, 6.505, 8.048, 7.39, 6.959, 6.447, 6.935, 7.24, 7.472, 6.783, 7.791, 7.581, 6.469, 6.494, 8.273, 6.167, 6.31, 6.431, 6.651, 7.019, 7.251, 7.293, 6.374, 6.122, 7.137, 6.238, 5.948, 6.261, 7.437, 5.607, 7.022, 5.738, 6.833, 5.803, 6.338, 6.067, 6.815, 6.35, 6.678, 6.756, 6.017, 7.843, 6.506, 6.462, 6.351, 6.231, 5.768, 7.045, 5.766, 6.894, 6.247, 6.832, 7.22, 6.587, 6.312, 6.754, 6.206, 6.448, 5.901, 6.069, 5.638, 5.896, 6.556, 5.67, 6.028, 5.609, 6.089, 6.179, 5.8, 5.772, 5.833, 5.162, 5.746, 5.41, 6.106, 5.391, 5.824, 5.869, 6.153, 6.312, 5.957, 6.632, 7.618, 6.058, 6.919, 6.128, 5.625, 5.776, 5.723, 6.33, 6.315, 5.882, 6.132, 6.557, 5.894, 6.11, 6.557, 5.929, 6.71, 5.591, 5.828, 6.51, 5.821, 5.985, 5.747, 6.626, 6.028, 5.596, 5.916, 5.276, 6.597, 5.741, 6.358, 6.375, 7.137, 7.791, 6.354, 6.614, 5.824, 5.795, 5.769, 6.079, 5.384, 6.835, 5.565, 6.721, 6.524, 6.158, 6.312, 6.023, 6.28, 6.12, 5.775, 5.923, 6.193, 5.915, 5.683, 5.626, 5.509, 5.839, 5.559, 5.748, 5.907, 5.849, 6.571, 7.706, 5.89, 7.458, 6.337, 6.361, 6.158, 6.296, 6.932, 6.446, 6.126, 6.656, 5.899, 5.981, 5.144, 5.924, 6.187, 6.176, 5.489, 6.165, 5.458, 5.816, 5.625, 5.83, 5.942, 5.774, 5.261, 5.848, 5.527, 5.889, 5.69, 5.651, 5.465, 5.757, 5.682, 5.615, 5.816, 5.458, 5.889, 6.212, 5.923, 5.903, 5.897, 6.21, 5.771, 6.288, 6.554, 6.3, 6.686, 6.69, 5.418, 6.218, 5.459, 6.341, 5.999, 6.741, 6.575, 6.081, 6.404, 5.697, 5.512, 6.057, 5.878, 5.86, 6.101, 6.145, 7.096, 6.17, 7.095, 6.681, 7.569, 6.98, 5.954, 6.199, 5.453, 6.098, 5.561, 5.86, 5.73, 5.792, 5.124, 5.595, 5.785, 5.895, 5.59, 5.461, 5.687, 5.737, 6.29, 5.792, 6.37, 6.238, 6.534, 6.235, 5.996, 5.759, 5.655, 6.69, 5.868, 6.439, 5.925, 6.117, 6.029, 6.15, 5.822, 6.135, 5.8, 6.574, 6.124, 6.386, 5.886, 6.352, 6.215, 5.887, 6.457, 6.204, 6.364, 6.229, 6.018, 6.084, 5.739, 5.708, 5.451, 5.583, 5.981, 6.792, 6.024, 6.148, 6.29, 5.948, 6.159, 5.983, 6.719, 6.162, 6.348, 5.509, 6.309, 5.657, 5.636, 5.505, 5.809, 5.417, 5.842, 5.408, 5.986, 5.623, 6.561, 6.21, 5.671, 6.107, 6.046, 6.888, 6.159, 6.238, 6.027, 6.235, 6.498, 6.248, 6.245, 6.658, 6.157, 5.962, 5.937, 5.541, 6.035, 5.754, 6.237, 6.268, 6.152, 6.011, 6.506, 5.786, 6.411, 6.164, 6.661, 6.095, 6.539, 6.78, 6.012, 6.488, 6.193, 6.19, 6.075, 5.857, 6.4, 6.056, 5.719, 5.48, 5.53, 5.675]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.011, 0.012, 0.056, 0.104, 0.122, 0.151, 0.23, 0.254, 0.328, 0.927, 0.962, 0.937, 3.421, 2.56, 3.141, 3.62, 4.7, 5.184, 2.76, 9.344, 6.839, 7.782, 7.745, 7.015, 7.59, 7.715, 5.774, 8.767, 6.681, 8.89, 6.059, 8.233, 9.331, 5.16, 11.211, 5.924, 10.091, 5.961, 6.661, 9.238, 7.308, 8.939, 7.686, 6.026, 8.348, 8.542, 8.603, 9.374, 4.948, 6.703, 7.662, 6.854, 10.219, 6.21, 7.595, 6.171, 6.535, 7.838, 6.861, 8.002, 4.485, 5.89, 5.749, 4.706, 5.367, 3.565, 5.082, 5.221, 2.747, 4.733, 4.861, 4.13, 7.238, 5.216, 5.158, 5.852, 3.988, 8.474, 3.634, 5.343, 6.09, 5.051, 6.233, 3.111, 6.23, 3.621, 5.432, 5.785, 4.743, 6.082, 5.709, 5.189, 4.012, 4.072, 3.905, 2.582, 4.835, 3.247, 4.45, 5.338, 3.078, 4.583, 2.763, 3.094, 4.026, 2.765, 3.48, 3.485, 2.731, 2.443, 1.766, 2.826, 2.182, 3.202, 4.223, 5.35, 4.495, 5.12, 4.736, 4.311, 4.431, 3.715, 2.044, 4.795, 2.776, 1.687, 1.493, 3.204, 3.824, 5.255, 4.967, 3.755, 3.973, 5.997, 4.437, 4.889, 4.637, 4.576, 5.292, 5.506, 4.737, 5.78, 5.266, 3.525, 3.95, 6.311, 3.213, 3.633, 3.788, 3.641, 5.529, 4.85, 5.515, 3.662, 4.316, 4.896, 4.043, 4.062, 4.397, 6.057, 2.306, 4.961, 2.739, 3.815, 3.04, 3.132, 2.346, 4.548, 3.016, 4.558, 4.93, 3.799, 5.653, 3.972, 3.116, 4.477, 3.538, 3.398, 4.341, 2.707, 4.322, 3.81, 3.801, 5.146, 3.388, 4.043, 3.213, 3.317, 4.078, 2.714, 2.41, 2.418, 2.484, 1.835, 3.005, 2.011, 2.521, 3.491, 1.69, 3.229, 1.743, 2.365, 2.21, 3.029, 2.234, 3.139, 1.898, 2.874, 3.159, 3.396, 4.609, 4.229, 4.679, 5.326, 3.498, 4.841, 3.998, 3.522, 1.807, 2.75, 3.924, 3.244, 3.482, 3.292, 3.19, 2.381, 3.069, 2.977, 2.52, 3.753, 1.852, 2.061, 4.117, 2.924, 3.867, 2.769, 1.896, 2.398, 2.737, 2.953, 2.722, 4.797, 2.296, 4.123, 4.13, 5.202, 6.611, 4.291, 4.432, 3.392, 3.631, 2.258, 3.607, 2.416, 4.18, 3.075, 3.91, 3.828, 2.929, 3.037, 3.571, 2.758, 2.986, 2.576, 2.26, 3.352, 2.357, 2.239, 2.854, 1.759, 1.738, 3.23, 2.744, 3.637, 3.464, 4.086, 5.954, 3.898, 5.618, 4.652, 4.436, 3.99, 3.891, 4.945, 4.161, 3.456, 4.448, 2.482, 2.621, 2.119, 2.896, 3.392, 2.396, 2.596, 3.507, 2.235, 2.514, 2.131, 1.856, 3.653, 1.772, 2.312, 3.083, 1.422, 2.605, 2.116, 2.405, 1.952, 2.364, 1.722, 2.071, 2.258, 2.103, 2.461, 3.498, 1.73, 2.802, 2.534, 1.401, 2.54, 3.06, 2.875, 4.008, 4.797, 3.436, 2.459, 4.124, 2.699, 3.424, 4.119, 3.392, 4.588, 3.162, 4.819, 3.748, 3.104, 3.558, 2.973, 3.367, 3.318, 3.673, 4.658, 3.119, 5.283, 3.925, 5.508, 4.728, 3.177, 3.837, 2.7, 3.566, 2.642, 2.147, 2.397, 3.372, 1.976, 1.948, 3.216, 2.367, 2.68, 2.92, 2.533, 3.264, 3.962, 2.137, 3.833, 3.076, 3.89, 3.51, 3.208, 2.535, 2.701, 4.387, 3.542, 4.114, 3.404, 3.102, 3.794, 3.144, 3.203, 2.901, 2.759, 3.847, 2.246, 3.308, 2.741, 2.527, 3.643, 2.72, 2.425, 3.266, 2.814, 3.671, 2.897, 3.256, 2.89, 2.163, 2.456, 1.556, 2.714, 1.013, 2.37, 3.094, 3.128, 2.267, 3.048, 2.08, 1.326, 1.005, 2.028, 2.397, 1.481, 2.469, 2.603, 2.198, 2.637, 1.934, 2.561, 1.806, 2.322, 1.814, 1.326, 1.361, 2.054, 2.714, 2.346, 4.204, 3.736, 3.132, 3.498, 2.949, 4.202, 3.465, 2.315, 4.293, 3.577, 2.571, 3.544, 2.268, 3.458, 3.107, 3.352, 3.64, 3.772, 3.491, 3.665, 2.84, 3.447, 3.281, 3.399, 2.969, 3.133, 3.744, 2.987, 3.294, 2.205, 3.291, 3.266, 2.501, 2.253, 2.36, 3.168, 2.323, 2.417, 2.527]\n",
      "\n",
      "#Fold: 7 \n",
      "Trainig set size: 420 , Time: 0:12:09 , best_lambda: 1 , min_  , error: 4.759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test starts:  467 , ends:  518\n",
      "1/1 [==============================] - 0s 639us/step - loss: 146.9553 - mse: 68.4939 - mae: 6.4748 - fp_mae: 3.4332\n",
      "average_error:  6.475 , fp_average_error:  3.433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:00<00:00, 255.83it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 213.59it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 226.89it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 230.03it/s]\n",
      "100%|██████████| 107/107 [00:00<00:00, 227.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Fold: 8 , Training Size: 420 , Validation size: 47 , Test Size 52\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 49.57434, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 49.57434 to 49.46825, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 49.46825 to 49.36541, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 49.36541 to 49.25353, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 49.25353 to 49.10878, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 49.10878 to 48.89400, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 48.89400 to 48.66972, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 48.66972 to 48.35040, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 48.35040 to 48.02641, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 48.02641 to 47.62838, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 47.62838 to 47.34400, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 47.34400 to 47.26276, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 47.26276 to 46.92244, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 46.92244 to 46.52394, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 46.52394 to 46.09940, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 46.09940 to 45.98755, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 45.98755 to 45.57693, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 45.57693 to 44.85143, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 44.85143 to 44.59271, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 44.59271 to 44.02182, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 44.02182 to 43.07391, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 43.07391 to 42.13226, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 42.13226 to 41.62039, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 41.62039 to 41.55639, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 41.55639 to 41.08068, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 41.08068 to 39.59949, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 39.59949 to 39.03677, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 39.03677 to 38.69025, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 38.69025 to 37.42651, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 37.42651 to 35.71491, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 35.71491\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 35.71491\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 35.71491\n",
      "\n",
      "Epoch 00034: val_mae improved from 35.71491 to 34.11725, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 34.11725\n",
      "\n",
      "Epoch 00036: val_mae improved from 34.11725 to 33.44492, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00037: val_mae improved from 33.44492 to 32.38195, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00038: val_mae improved from 32.38195 to 31.68622, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 31.68622\n",
      "\n",
      "Epoch 00040: val_mae improved from 31.68622 to 29.95424, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 29.95424\n",
      "\n",
      "Epoch 00042: val_mae improved from 29.95424 to 29.31881, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00043: val_mae improved from 29.31881 to 28.13790, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00044: val_mae improved from 28.13790 to 27.81575, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 27.81575\n",
      "\n",
      "Epoch 00046: val_mae improved from 27.81575 to 27.55569, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00047: val_mae improved from 27.55569 to 24.73605, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00048: val_mae improved from 24.73605 to 24.39483, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00049: val_mae improved from 24.39483 to 20.36049, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00050: val_mae improved from 20.36049 to 18.72417, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00051: val_mae improved from 18.72417 to 15.51929, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00052: val_mae improved from 15.51929 to 14.65056, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 14.65056\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 14.65056\n",
      "\n",
      "Epoch 00055: val_mae improved from 14.65056 to 13.94826, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 13.94826\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 13.94826\n",
      "\n",
      "Epoch 00058: val_mae improved from 13.94826 to 13.17841, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00059: val_mae improved from 13.17841 to 11.03549, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00060: val_mae improved from 11.03549 to 10.87096, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 10.87096\n",
      "\n",
      "Epoch 00062: val_mae improved from 10.87096 to 9.58269, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 9.58269\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 9.58269\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 9.58269\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 9.58269\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 9.58269\n",
      "\n",
      "Epoch 00068: val_mae improved from 9.58269 to 8.91373, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 8.91373\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 8.91373\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 8.91373\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 8.91373\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 8.91373\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 8.91373\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 8.91373\n",
      "\n",
      "Epoch 00076: val_mae improved from 8.91373 to 8.55668, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00077: val_mae improved from 8.55668 to 7.86125, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 7.86125\n",
      "\n",
      "Epoch 00079: val_mae improved from 7.86125 to 7.69825, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 7.69825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00150: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 7.69825\n",
      "\n",
      "Epoch 00243: val_mae improved from 7.69825 to 7.68462, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 7.68462\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 7.68462\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 7.68462\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 7.68462\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 7.68462\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 7.68462\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 7.68462\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 7.68462\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 7.68462\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 7.68462\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 7.68462\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 7.68462\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 7.68462\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 7.68462\n",
      "\n",
      "Epoch 00258: val_mae improved from 7.68462 to 7.56685, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 7.56685\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 7.56685\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 7.56685\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 7.56685\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 7.56685\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 7.56685\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 7.56685\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 7.56685\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 7.56685\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 7.56685\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 7.56685\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 7.56685\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 7.56685\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 7.56685\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 7.56685\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 7.56685\n",
      "\n",
      "Epoch 00275: val_mae improved from 7.56685 to 7.51197, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 7.51197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00302: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 7.51197\n",
      "\n",
      "Epoch 00380: val_mae improved from 7.51197 to 7.41750, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 7.41750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00460: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 7.41750\n",
      "\n",
      "Epoch 00464: val_mae improved from 7.41750 to 7.34849, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 7.34849\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 7.34849\n",
      "\n",
      "Lambda: 0.01 , Time: 0:04:00\n",
      "Train Error(all epochs): 0.4742152690887451 \n",
      " [48.99, 48.87, 48.783, 48.69, 48.58, 48.447, 48.29, 48.098, 47.872, 47.593, 47.266, 46.895, 46.468, 45.997, 45.46, 44.872, 44.214, 43.52, 42.768, 41.946, 41.048, 40.099, 39.109, 38.055, 36.945, 35.84, 34.668, 33.447, 32.153, 30.895, 29.491, 28.184, 26.76, 25.42, 24.023, 22.632, 21.257, 19.83, 18.373, 16.92, 15.622, 14.297, 12.949, 11.898, 10.843, 9.857, 9.035, 8.299, 7.619, 7.198, 6.616, 5.791, 5.365, 5.169, 4.669, 4.538, 4.442, 4.214, 4.305, 4.087, 3.717, 3.584, 3.465, 3.563, 3.535, 3.603, 3.543, 3.295, 3.145, 3.032, 3.004, 2.961, 3.123, 3.188, 3.235, 3.323, 3.598, 3.131, 2.877, 2.789, 2.631, 2.609, 2.628, 2.486, 2.414, 2.286, 2.347, 2.469, 2.311, 2.212, 2.133, 2.103, 2.148, 2.297, 2.36, 2.564, 2.479, 2.618, 2.871, 2.695, 2.542, 2.181, 2.015, 1.928, 1.904, 1.818, 1.958, 1.968, 2.115, 2.174, 2.252, 2.389, 2.328, 2.229, 2.11, 1.907, 1.862, 1.687, 1.658, 1.666, 1.707, 1.761, 1.946, 1.95, 1.934, 1.955, 1.871, 1.827, 1.881, 1.67, 1.639, 1.715, 1.893, 1.908, 2.082, 1.996, 1.924, 2.073, 1.969, 1.93, 1.946, 1.851, 1.688, 1.535, 1.415, 1.295, 1.397, 1.37, 1.382, 1.296, 1.233, 1.32, 1.488, 1.652, 1.639, 1.673, 1.529, 1.507, 1.52, 1.497, 1.289, 1.25, 1.137, 1.155, 1.339, 1.474, 1.53, 1.759, 1.641, 1.64, 1.697, 1.611, 1.663, 1.7, 1.662, 2.008, 1.865, 1.804, 1.779, 1.683, 1.672, 1.451, 1.284, 1.326, 1.336, 1.354, 1.27, 1.338, 1.523, 1.503, 1.576, 1.504, 1.349, 1.247, 1.133, 1.027, 0.927, 0.832, 0.827, 0.858, 0.934, 1.014, 1.068, 1.223, 1.239, 1.226, 1.026, 0.867, 0.759, 0.702, 0.693, 0.765, 0.716, 0.866, 1.049, 1.198, 1.399, 1.433, 1.538, 1.636, 1.702, 1.616, 1.496, 1.402, 1.243, 1.305, 1.368, 1.331, 1.257, 1.293, 1.182, 1.14, 1.127, 1.157, 1.244, 1.192, 1.092, 1.144, 1.064, 1.147, 1.074, 1.042, 1.09, 1.103, 1.167, 1.208, 1.218, 1.254, 1.189, 1.251, 1.3, 1.262, 1.28, 1.242, 1.186, 1.141, 1.046, 1.015, 1.202, 1.368, 1.4, 1.357, 1.379, 1.353, 1.256, 1.175, 1.191, 1.243, 1.219, 1.188, 1.125, 1.098, 1.044, 0.991, 1.016, 0.975, 1.045, 1.086, 1.154, 1.109, 1.173, 1.167, 1.028, 0.96, 0.941, 0.829, 0.848, 0.941, 0.947, 0.989, 0.992, 0.879, 0.781, 0.814, 0.819, 0.84, 0.876, 0.909, 0.913, 0.949, 1.036, 1.027, 1.039, 1.05, 1.155, 1.161, 1.154, 1.09, 0.993, 0.915, 0.941, 1.05, 1.075, 1.004, 1.047, 1.044, 1.011, 0.935, 0.844, 0.779, 0.796, 0.807, 0.753, 0.698, 0.701, 0.77, 0.925, 1.112, 1.184, 1.005, 0.993, 1.074, 0.916, 0.919, 0.976, 1.042, 1.048, 1.135, 1.067, 1.088, 1.132, 1.097, 1.129, 1.182, 1.257, 1.224, 1.189, 1.254, 1.33, 1.378, 1.348, 1.182, 1.144, 1.064, 1.146, 1.057, 1.031, 1.003, 0.977, 0.948, 0.979, 0.879, 0.861, 0.854, 0.811, 0.815, 0.872, 0.834, 0.821, 0.933, 1.069, 1.152, 1.16, 1.074, 1.12, 1.076, 1.061, 1.059, 1.022, 1.081, 1.197, 1.237, 1.241, 1.232, 1.194, 1.111, 1.238, 1.161, 1.098, 1.01, 0.971, 0.865, 0.747, 0.719, 0.758, 0.768, 0.74, 0.645, 0.576, 0.528, 0.474, 0.496, 0.543, 0.584, 0.634, 0.674, 0.716, 0.845, 0.972, 1.021, 1.037, 1.063, 1.186, 1.247, 1.232, 1.204, 1.171, 1.159, 1.169, 1.101, 1.054, 0.978, 0.935, 1.074, 1.041, 1.056, 1.01, 0.948, 0.936, 0.897, 0.933, 0.855, 0.836, 0.84, 0.856, 0.914, 0.912, 0.97, 0.972, 0.993, 1.006, 0.985, 0.974, 1.057, 1.102, 1.097, 1.091, 0.972, 0.848, 0.768, 0.764, 0.766, 0.747, 0.728, 0.67, 0.704, 0.834, 0.832, 0.821, 1.013, 1.128, 1.17, 1.079, 1.216, 1.425, 1.245, 1.178, 1.098, 0.981, 0.897, 0.862, 0.79, 0.691, 0.705, 0.737, 0.768, 0.757, 0.726, 0.728, 0.698, 0.672, 0.623, 0.561, 0.554, 0.648, 0.763, 0.801, 0.802, 0.793, 0.769, 0.772, 0.838, 0.82, 0.773, 0.762, 0.847, 0.882, 0.852, 0.841, 0.868]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.001, 0.004, 0.023, 0.04, 0.045, 0.046, 0.088, 0.13, 0.188, 0.239, 0.405, 0.403, 0.673, 0.668, 0.644, 0.734, 0.84, 0.867, 1.018, 1.089, 1.153, 1.372, 1.33, 1.215, 1.232, 1.229, 1.32, 1.358, 1.475, 1.437, 1.333, 1.273, 1.239, 1.213, 1.254, 1.341, 1.417, 1.407, 1.464, 1.667, 1.34, 1.226, 1.167, 1.181, 1.052, 1.211, 1.068, 1.111, 0.96, 1.029, 1.115, 1.003, 0.997, 0.868, 0.941, 0.902, 1.074, 1.062, 1.208, 1.09, 1.214, 1.407, 1.151, 1.217, 0.904, 0.88, 0.787, 0.897, 0.749, 0.889, 0.9, 0.869, 1.079, 0.954, 1.198, 1.025, 1.103, 0.958, 0.846, 0.855, 0.727, 0.751, 0.742, 0.754, 0.792, 0.875, 0.962, 0.841, 0.941, 0.812, 0.861, 0.845, 0.797, 0.716, 0.809, 0.897, 0.913, 0.98, 1.006, 0.802, 1.042, 0.92, 0.925, 0.87, 0.947, 0.669, 0.785, 0.653, 0.598, 0.612, 0.675, 0.598, 0.648, 0.543, 0.645, 0.696, 0.835, 0.759, 0.806, 0.74, 0.716, 0.672, 0.756, 0.541, 0.624, 0.504, 0.549, 0.627, 0.762, 0.68, 0.889, 0.802, 0.763, 0.83, 0.803, 0.774, 0.81, 0.841, 0.909, 0.954, 0.837, 0.84, 0.76, 0.863, 0.654, 0.591, 0.633, 0.633, 0.655, 0.617, 0.646, 0.71, 0.757, 0.737, 0.755, 0.65, 0.562, 0.558, 0.474, 0.435, 0.414, 0.354, 0.456, 0.401, 0.549, 0.478, 0.625, 0.569, 0.608, 0.464, 0.396, 0.371, 0.333, 0.327, 0.388, 0.332, 0.418, 0.54, 0.585, 0.686, 0.756, 0.723, 0.807, 0.847, 0.708, 0.768, 0.651, 0.608, 0.624, 0.672, 0.627, 0.6, 0.651, 0.533, 0.61, 0.508, 0.601, 0.562, 0.653, 0.469, 0.588, 0.488, 0.571, 0.516, 0.51, 0.509, 0.561, 0.571, 0.597, 0.589, 0.606, 0.597, 0.594, 0.635, 0.617, 0.646, 0.573, 0.629, 0.519, 0.505, 0.531, 0.555, 0.726, 0.646, 0.677, 0.66, 0.658, 0.599, 0.581, 0.59, 0.607, 0.569, 0.587, 0.517, 0.553, 0.523, 0.471, 0.506, 0.44, 0.592, 0.439, 0.649, 0.498, 0.596, 0.584, 0.478, 0.463, 0.461, 0.421, 0.39, 0.496, 0.456, 0.475, 0.507, 0.42, 0.359, 0.421, 0.366, 0.432, 0.418, 0.481, 0.431, 0.466, 0.518, 0.46, 0.587, 0.438, 0.638, 0.516, 0.586, 0.547, 0.432, 0.479, 0.459, 0.512, 0.533, 0.466, 0.535, 0.471, 0.532, 0.415, 0.44, 0.365, 0.4, 0.363, 0.389, 0.346, 0.349, 0.365, 0.494, 0.519, 0.613, 0.49, 0.466, 0.561, 0.445, 0.447, 0.477, 0.544, 0.477, 0.613, 0.484, 0.508, 0.601, 0.482, 0.584, 0.569, 0.632, 0.598, 0.543, 0.663, 0.576, 0.755, 0.586, 0.614, 0.519, 0.496, 0.59, 0.461, 0.563, 0.412, 0.551, 0.364, 0.55, 0.371, 0.451, 0.416, 0.407, 0.391, 0.421, 0.429, 0.355, 0.517, 0.444, 0.637, 0.518, 0.541, 0.567, 0.493, 0.543, 0.49, 0.538, 0.462, 0.67, 0.52, 0.675, 0.542, 0.639, 0.455, 0.656, 0.575, 0.503, 0.545, 0.46, 0.418, 0.364, 0.345, 0.362, 0.393, 0.351, 0.314, 0.303, 0.215, 0.271, 0.202, 0.302, 0.275, 0.325, 0.349, 0.334, 0.427, 0.477, 0.501, 0.547, 0.493, 0.592, 0.599, 0.607, 0.588, 0.578, 0.578, 0.544, 0.555, 0.493, 0.483, 0.448, 0.542, 0.51, 0.51, 0.494, 0.441, 0.458, 0.435, 0.495, 0.375, 0.483, 0.363, 0.455, 0.443, 0.413, 0.539, 0.41, 0.538, 0.458, 0.502, 0.457, 0.516, 0.541, 0.531, 0.525, 0.481, 0.399, 0.36, 0.398, 0.365, 0.378, 0.358, 0.327, 0.336, 0.451, 0.379, 0.404, 0.547, 0.5, 0.659, 0.453, 0.638, 0.681, 0.629, 0.546, 0.605, 0.43, 0.435, 0.42, 0.383, 0.322, 0.349, 0.352, 0.379, 0.351, 0.382, 0.315, 0.39, 0.292, 0.336, 0.265, 0.264, 0.335, 0.353, 0.435, 0.354, 0.419, 0.36, 0.382, 0.404, 0.427, 0.35, 0.406, 0.392, 0.438, 0.416, 0.407, 0.417]\n",
      "Val Error(all epochs): 7.3484930992126465 \n",
      " [49.574, 49.468, 49.365, 49.254, 49.109, 48.894, 48.67, 48.35, 48.026, 47.628, 47.344, 47.263, 46.922, 46.524, 46.099, 45.988, 45.577, 44.851, 44.593, 44.022, 43.074, 42.132, 41.62, 41.556, 41.081, 39.599, 39.037, 38.69, 37.427, 35.715, 35.911, 35.889, 36.24, 34.117, 34.689, 33.445, 32.382, 31.686, 32.676, 29.954, 30.422, 29.319, 28.138, 27.816, 28.097, 27.556, 24.736, 24.395, 20.36, 18.724, 15.519, 14.651, 17.293, 15.297, 13.948, 15.434, 14.088, 13.178, 11.035, 10.871, 10.963, 9.583, 11.984, 11.168, 10.738, 10.097, 9.914, 8.914, 9.573, 8.988, 10.197, 10.479, 8.942, 9.399, 8.944, 8.557, 7.861, 8.512, 7.698, 7.948, 8.54, 8.404, 8.245, 8.327, 8.13, 8.529, 8.586, 8.27, 8.83, 8.233, 8.414, 8.55, 8.615, 8.231, 8.295, 8.032, 8.008, 8.673, 7.848, 8.397, 8.115, 8.284, 8.165, 8.373, 8.944, 8.462, 8.605, 8.185, 8.208, 8.011, 8.547, 8.099, 8.736, 8.483, 8.283, 8.616, 8.053, 8.449, 8.384, 8.25, 8.488, 7.994, 8.929, 7.839, 8.923, 8.325, 8.351, 8.244, 8.72, 8.811, 8.954, 9.711, 9.707, 10.252, 9.306, 9.998, 9.473, 9.136, 9.119, 9.457, 8.996, 9.316, 8.905, 8.872, 9.051, 8.56, 8.829, 8.594, 8.765, 8.693, 8.619, 8.636, 8.432, 8.508, 9.157, 8.486, 9.132, 8.523, 8.67, 9.037, 8.382, 9.084, 8.021, 8.916, 8.251, 8.203, 9.211, 8.225, 8.789, 8.635, 8.06, 9.111, 8.092, 8.58, 8.504, 8.008, 8.536, 8.444, 8.02, 8.505, 8.023, 7.937, 8.391, 7.872, 8.6, 7.953, 8.198, 8.558, 7.851, 8.608, 7.868, 8.359, 8.103, 8.19, 8.458, 8.236, 8.326, 8.454, 8.152, 8.629, 8.272, 8.675, 8.556, 8.548, 8.569, 8.659, 8.589, 8.708, 8.586, 8.658, 8.455, 8.268, 8.808, 7.702, 8.764, 8.207, 8.036, 8.669, 8.104, 8.287, 8.506, 7.934, 8.531, 8.29, 7.96, 8.703, 7.78, 8.396, 8.258, 7.895, 8.554, 7.845, 8.459, 8.105, 8.326, 8.088, 8.271, 8.014, 8.262, 8.143, 8.003, 8.561, 7.685, 8.531, 8.046, 8.076, 8.58, 8.045, 8.218, 8.606, 7.689, 8.625, 7.875, 8.147, 8.434, 7.95, 8.825, 7.567, 8.843, 7.772, 8.367, 8.222, 8.211, 8.083, 8.038, 7.824, 8.244, 7.678, 8.41, 7.633, 8.224, 7.74, 8.068, 8.138, 7.512, 8.305, 7.577, 8.339, 7.861, 8.064, 8.039, 7.967, 8.092, 7.826, 8.002, 8.212, 7.644, 8.503, 7.658, 8.342, 7.854, 7.993, 8.001, 7.758, 8.143, 7.674, 8.136, 7.888, 7.906, 8.33, 7.71, 8.56, 7.69, 8.639, 7.908, 8.438, 8.113, 8.3, 8.277, 7.995, 8.214, 8.119, 7.976, 8.351, 7.846, 8.383, 7.881, 8.241, 8.068, 8.093, 8.239, 7.794, 8.162, 8.036, 7.855, 8.312, 7.638, 8.345, 7.693, 8.183, 7.763, 8.04, 8.101, 7.663, 8.518, 7.691, 8.181, 8.197, 7.871, 8.642, 7.898, 8.477, 8.076, 8.36, 8.173, 8.151, 8.24, 7.813, 8.212, 7.881, 8.395, 8.659, 7.993, 8.224, 8.109, 7.933, 8.033, 8.086, 7.876, 8.307, 7.974, 8.319, 7.965, 8.262, 7.743, 8.312, 7.859, 8.022, 7.948, 7.788, 7.938, 7.787, 7.96, 7.758, 7.956, 7.924, 7.759, 7.875, 8.366, 7.417, 8.831, 7.864, 7.967, 8.266, 8.197, 8.42, 8.558, 8.607, 8.102, 8.519, 8.049, 8.254, 8.294, 7.99, 8.362, 7.919, 8.289, 7.953, 8.109, 8.046, 7.898, 8.208, 7.761, 8.304, 7.787, 7.986, 8.083, 7.73, 8.079, 7.778, 7.906, 8.055, 7.599, 8.066, 7.76, 8.101, 7.734, 7.975, 7.856, 7.788, 8.249, 7.487, 8.219, 7.752, 7.811, 7.976, 7.71, 8.552, 7.709, 8.308, 7.757, 8.002, 7.657, 7.802, 7.687, 7.746, 7.723, 7.693, 8.035, 7.771, 7.983, 7.636, 7.683, 7.883, 7.535, 8.175, 7.968, 8.354, 8.277, 7.835, 8.16, 8.022, 8.112, 8.147, 8.191, 8.048, 8.125, 8.114, 7.852, 7.876, 7.861, 7.68, 8.534, 7.348, 7.997, 7.675, 7.968, 7.929, 8.048, 7.923, 7.902, 7.76, 7.711, 7.869, 7.597, 7.688, 7.669, 7.621, 7.745, 7.556, 7.701, 7.657, 7.598, 7.744, 7.523, 7.682, 7.587, 7.635, 7.699, 8.051, 8.53, 8.141, 8.704, 8.125, 7.948, 8.22, 7.959, 7.98, 8.552, 7.673]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.065, 0.068, 0.132, 0.218, 0.296, 0.31, 0.371, 0.489, 0.446, 0.517, 0.466, 0.786, 0.735, 0.836, 1.154, 0.714, 0.844, 1.039, 1.158, 1.199, 1.536, 1.457, 1.446, 1.334, 1.376, 1.763, 1.48, 1.949, 1.896, 3.119, 2.806, 2.853, 3.035, 2.019, 3.023, 2.108, 3.142, 2.632, 2.492, 2.24, 2.662, 2.3, 2.695, 3.151, 2.024, 2.965, 2.751, 4.182, 3.513, 4.642, 4.894, 4.699, 5.131, 5.327, 5.184, 5.573, 5.335, 6.507, 5.393, 6.101, 4.166, 5.506, 4.674, 5.666, 5.128, 6.466, 6.247, 5.581, 5.004, 4.195, 4.52, 4.213, 4.055, 4.169, 4.233, 5.042, 4.655, 6.152, 5.067, 5.155, 4.127, 5.225, 4.193, 3.81, 4.166, 3.434, 2.815, 3.615, 3.084, 4.878, 4.993, 5.171, 6.318, 4.798, 5.976, 5.707, 3.804, 4.622, 3.397, 4.302, 3.378, 4.165, 3.277, 4.186, 3.715, 4.335, 3.707, 4.755, 3.12, 4.581, 3.443, 4.025, 3.21, 3.68, 3.358, 3.141, 3.5, 3.356, 2.952, 5.037, 2.814, 4.495, 4.474, 4.61, 4.583, 3.723, 4.55, 5.048, 5.038, 5.024, 3.344, 4.23, 5.669, 4.007, 4.543, 4.265, 3.457, 4.722, 4.018, 3.923, 4.269, 3.591, 4.697, 4.819, 4.058, 4.11, 3.495, 3.68, 3.457, 3.818, 3.236, 3.925, 3.115, 4.035, 3.156, 4.339, 3.505, 4.551, 3.905, 3.698, 3.809, 3.181, 3.649, 3.28, 3.549, 3.561, 3.653, 4.284, 4.156, 4.058, 4.791, 3.561, 4.686, 4.475, 3.973, 5.388, 4.663, 4.91, 4.67, 4.431, 5.451, 4.337, 4.988, 5.564, 4.208, 5.606, 4.011, 5.089, 4.197, 4.885, 3.963, 4.643, 3.62, 3.872, 3.568, 3.708, 4.111, 3.829, 4.098, 3.989, 3.925, 3.915, 4.574, 4.236, 5.348, 4.73, 5.062, 4.875, 4.995, 5.305, 3.608, 5.657, 4.055, 5.218, 5.441, 4.83, 4.263, 4.184, 3.426, 4.322, 4.02, 4.605, 4.55, 4.623, 4.877, 4.055, 4.516, 3.995, 4.78, 4.146, 5.756, 4.328, 5.392, 4.826, 4.854, 4.69, 4.514, 4.463, 4.438, 4.461, 4.207, 4.087, 4.885, 4.536, 4.67, 4.855, 4.41, 4.962, 4.7, 4.83, 4.876, 4.62, 5.25, 4.945, 5.645, 4.555, 6.241, 4.83, 6.137, 5.368, 6.132, 5.327, 5.185, 5.225, 4.793, 4.765, 4.942, 4.06, 5.246, 4.349, 4.144, 4.061, 4.026, 3.636, 3.976, 3.649, 3.76, 3.883, 4.081, 3.78, 4.616, 4.624, 4.553, 4.468, 4.635, 4.91, 4.626, 5.759, 4.095, 5.467, 5.097, 5.076, 5.861, 4.768, 5.383, 5.2, 4.451, 4.733, 4.062, 4.577, 4.358, 5.312, 5.124, 5.719, 6.732, 4.947, 5.79, 4.155, 4.552, 4.041, 5.115, 4.043, 5.598, 4.068, 4.509, 4.891, 4.262, 4.309, 4.281, 4.096, 4.229, 4.493, 4.066, 4.669, 5.224, 4.145, 4.396, 4.367, 3.932, 3.365, 4.661, 2.795, 3.78, 3.314, 2.915, 4.087, 3.486, 2.704, 4.107, 2.184, 4.065, 3.013, 3.83, 4.011, 3.474, 3.989, 3.51, 4.135, 3.643, 3.811, 3.744, 3.641, 3.776, 3.423, 3.936, 3.273, 3.98, 3.551, 4.135, 4.102, 3.91, 4.193, 4.258, 4.001, 5.132, 4.732, 4.323, 5.569, 4.52, 4.962, 5.569, 4.328, 5.483, 5.434, 4.489, 5.792, 4.53, 4.626, 4.784, 4.531, 6.326, 4.649, 6.102, 4.483, 5.542, 4.596, 4.515, 4.882, 4.724, 4.598, 5.178, 5.289, 5.462, 4.682, 4.582, 4.691, 4.018, 4.649, 3.558, 3.466, 3.286, 3.203, 3.555, 3.237, 3.424, 3.184, 3.34, 3.103, 3.272, 3.457, 3.077, 3.738, 3.698, 3.417, 3.825, 3.403, 3.744, 4.494, 3.494, 5.261, 3.439, 3.816, 3.619, 3.638, 3.88, 4.107, 3.576, 4.575, 3.665, 4.294, 3.643, 4.267, 3.844, 4.291, 3.861, 3.777, 4.184, 3.896, 4.542, 4.519, 4.187, 4.409, 3.96, 2.997, 4.063, 2.958, 3.82, 3.779, 3.681, 3.581, 4.148, 2.958, 4.553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 49.59482, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 49.59482 to 49.42815, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 49.42815 to 49.25937, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 49.25937 to 49.07051, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 49.07051 to 48.82976, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 48.82976 to 48.49367, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 48.49367 to 48.06712, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 48.06712 to 47.51327, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 47.51327 to 46.83978, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 46.83978 to 46.15709, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 46.15709 to 45.50780, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 45.50780 to 44.69056, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 44.69056 to 43.74254, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 43.74254 to 43.10636, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 43.10636 to 42.73928, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 42.73928 to 41.55832, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 41.55832 to 39.52908, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 39.52908 to 37.73441, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 37.73441 to 37.45295, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 37.45295 to 36.64950, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 36.64950 to 34.18512, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 34.18512 to 33.01929, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 33.01929\n",
      "\n",
      "Epoch 00024: val_mae improved from 33.01929 to 32.47822, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 32.47822 to 30.00674, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 30.00674\n",
      "\n",
      "Epoch 00027: val_mae improved from 30.00674 to 26.97591, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 26.97591 to 25.14874, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 25.14874 to 23.23984, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 23.23984 to 22.30539, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00031: val_mae improved from 22.30539 to 16.26597, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00032: val_mae improved from 16.26597 to 15.04314, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 15.04314 to 12.38780, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00034: val_mae improved from 12.38780 to 12.02923, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 12.02923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00058: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 12.02923\n",
      "\n",
      "Epoch 00069: val_mae improved from 12.02923 to 10.97500, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 10.97500\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 10.97500\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 10.97500\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 10.97500\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 10.97500\n",
      "\n",
      "Epoch 00075: val_mae improved from 10.97500 to 10.67673, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 10.67673\n",
      "\n",
      "Epoch 00077: val_mae improved from 10.67673 to 10.09486, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00078: val_mae improved from 10.09486 to 9.74007, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 9.74007\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 9.74007\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 9.74007\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 9.74007\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 9.74007\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 9.74007\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 9.74007\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 9.74007\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 9.74007\n",
      "\n",
      "Epoch 00088: val_mae improved from 9.74007 to 8.53798, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 8.53798\n",
      "\n",
      "Epoch 00090: val_mae improved from 8.53798 to 7.53877, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 7.53877\n",
      "\n",
      "Epoch 00124: val_mae improved from 7.53877 to 7.36358, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 7.36358\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 7.36358\n",
      "\n",
      "Epoch 00127: val_mae improved from 7.36358 to 7.29731, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 7.29731\n",
      "\n",
      "Epoch 00129: val_mae improved from 7.29731 to 7.11391, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 7.11391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00190: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 7.11391\n",
      "\n",
      "Epoch 00215: val_mae improved from 7.11391 to 7.10769, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 7.10769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00348: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 7.10769\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 7.10769\n",
      "\n",
      "Lambda: 0.1 , Time: 0:04:31\n",
      "Train Error(all epochs): 0.9571323990821838 \n",
      " [48.987, 48.85, 48.754, 48.64, 48.516, 48.368, 48.192, 47.984, 47.731, 47.429, 47.064, 46.644, 46.17, 45.623, 45.0, 44.305, 43.548, 42.693, 41.755, 40.724, 39.646, 38.463, 37.22, 35.903, 34.527, 33.087, 31.623, 30.1, 28.558, 27.015, 25.473, 23.911, 22.45, 20.95, 19.568, 18.289, 17.1, 15.793, 14.792, 13.571, 12.868, 11.948, 11.103, 10.711, 9.907, 9.397, 8.889, 8.348, 7.794, 7.278, 7.003, 6.537, 6.147, 5.838, 5.706, 5.433, 5.377, 5.413, 5.037, 5.331, 5.08, 4.923, 4.871, 4.67, 4.732, 4.662, 4.598, 4.465, 4.171, 4.149, 4.066, 3.996, 3.963, 4.063, 4.165, 4.05, 4.031, 3.973, 4.006, 4.03, 3.904, 3.805, 3.668, 3.563, 3.705, 3.732, 3.766, 3.904, 3.854, 3.833, 3.778, 3.701, 3.535, 3.566, 3.352, 3.407, 3.442, 3.384, 3.316, 3.332, 3.347, 3.252, 3.176, 3.195, 3.119, 3.013, 3.123, 3.259, 3.284, 3.13, 3.22, 3.099, 3.059, 3.05, 2.965, 2.769, 2.742, 2.64, 2.493, 2.421, 2.419, 2.577, 2.771, 2.989, 2.953, 3.272, 3.122, 3.117, 2.852, 2.862, 2.923, 3.026, 3.113, 2.868, 2.758, 2.865, 2.693, 2.712, 2.639, 2.76, 2.689, 2.67, 2.725, 2.659, 2.507, 2.318, 2.174, 2.034, 1.962, 2.108, 2.041, 2.132, 2.075, 2.211, 2.174, 2.155, 2.134, 2.098, 2.18, 2.432, 2.378, 2.29, 2.234, 2.144, 2.281, 2.352, 2.252, 2.154, 2.134, 2.183, 2.071, 2.04, 1.953, 1.838, 1.839, 1.706, 1.786, 1.876, 2.14, 2.155, 2.194, 2.251, 2.315, 2.181, 2.173, 2.108, 2.125, 2.05, 1.882, 1.974, 1.836, 1.941, 1.864, 1.98, 2.068, 2.056, 2.147, 2.051, 1.99, 1.817, 1.731, 1.871, 1.837, 2.036, 1.949, 1.91, 1.71, 1.611, 1.553, 1.51, 1.519, 1.761, 2.041, 2.157, 2.214, 2.161, 1.878, 1.921, 1.852, 1.794, 1.773, 1.811, 1.997, 2.012, 1.84, 1.646, 1.437, 1.375, 1.272, 1.338, 1.425, 1.362, 1.398, 1.286, 1.298, 1.402, 1.581, 1.674, 1.584, 1.484, 1.527, 1.507, 1.424, 1.387, 1.496, 1.587, 1.691, 1.899, 1.907, 2.129, 2.075, 2.033, 2.02, 2.044, 1.8, 1.503, 1.41, 1.433, 1.313, 1.313, 1.345, 1.314, 1.415, 1.525, 1.625, 1.842, 1.831, 1.852, 1.787, 1.919, 1.685, 1.542, 1.579, 1.64, 1.682, 1.63, 1.724, 1.67, 1.449, 1.407, 1.346, 1.361, 1.372, 1.411, 1.459, 1.57, 1.51, 1.472, 1.353, 1.255, 1.417, 1.526, 1.555, 1.578, 1.55, 1.478, 1.473, 1.634, 1.529, 1.574, 1.732, 1.835, 1.73, 1.866, 1.926, 1.885, 1.809, 1.676, 1.891, 1.816, 1.602, 1.502, 1.359, 1.3, 1.279, 1.259, 1.297, 1.393, 1.531, 1.763, 1.788, 1.754, 1.676, 1.564, 1.461, 1.238, 1.118, 1.072, 1.115, 1.194, 1.242, 1.405, 1.59, 1.687, 1.581, 1.475, 1.417, 1.706, 1.873, 1.753, 1.915, 1.916, 1.828, 1.676, 1.497, 1.406, 1.315, 1.327, 1.515, 1.509, 1.403, 1.497, 1.606, 1.95, 2.059, 1.894, 1.645, 1.355, 1.313, 1.44, 1.471, 1.582, 1.564, 1.603, 1.724, 1.778, 1.743, 1.76, 1.738, 1.635, 1.465, 1.515, 1.355, 1.36, 1.342, 1.484, 1.526, 1.524, 1.498, 1.477, 1.584, 1.615, 1.588, 1.548, 1.485, 1.489, 1.488, 1.461, 1.494, 1.722, 1.866, 1.781, 1.481, 1.467, 1.409, 1.488, 1.586, 1.578, 1.394, 1.379, 1.399, 1.462, 1.561, 1.557, 1.647, 1.69, 1.663, 1.673, 1.818, 1.801, 1.896, 1.758, 1.83, 1.675, 1.491, 1.627, 1.564, 1.574, 1.444, 1.433, 1.273, 1.234, 1.244, 1.198, 1.179, 1.166, 1.133, 1.141, 1.197, 1.24, 1.277, 1.328, 1.3, 1.233, 1.148, 1.229, 1.38, 1.434, 1.479, 1.51, 1.414, 1.266, 1.253, 1.223, 1.258, 1.471, 1.463, 1.343, 1.396, 1.454, 1.149, 1.285, 1.673, 1.862, 1.959, 1.972, 2.101, 1.837, 1.739, 1.794, 1.584, 1.392, 1.44, 1.356, 1.342, 1.309, 1.208, 1.079, 1.064, 0.957, 0.979, 1.081, 1.036, 1.127, 1.326, 1.413, 1.397, 1.45, 1.32, 1.395, 1.551, 1.604, 1.576, 1.719, 1.543, 1.615, 1.508, 1.266, 1.271, 1.179, 1.228, 1.182, 1.207, 1.376, 1.36, 1.39, 1.549, 1.608, 1.506, 1.482]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.008, 0.0, 0.028, 0.021, 0.072, 0.12, 0.2, 0.149, 0.279, 0.263, 0.383, 0.527, 0.543, 0.763, 0.765, 0.919, 0.973, 1.012, 1.071, 1.105, 1.163, 1.183, 1.207, 1.181, 1.274, 1.337, 1.43, 1.472, 1.546, 1.761, 1.681, 1.668, 1.742, 1.69, 1.755, 1.78, 1.814, 1.776, 1.674, 1.682, 1.581, 1.644, 1.55, 1.644, 1.722, 1.643, 1.695, 1.673, 1.723, 1.672, 1.632, 1.587, 1.44, 1.431, 1.518, 1.515, 1.535, 1.633, 1.646, 1.497, 1.63, 1.505, 1.468, 1.476, 1.329, 1.425, 1.402, 1.381, 1.476, 1.327, 1.379, 1.441, 1.276, 1.323, 1.297, 1.24, 1.311, 1.323, 1.527, 1.226, 1.428, 1.325, 1.262, 1.332, 1.267, 1.117, 1.146, 1.073, 1.084, 0.944, 1.023, 1.117, 1.151, 1.345, 1.289, 1.421, 1.263, 1.507, 1.129, 1.319, 1.259, 1.258, 1.457, 1.243, 1.168, 1.294, 1.153, 1.237, 1.084, 1.227, 1.179, 1.169, 1.155, 1.214, 1.158, 0.959, 0.944, 0.859, 0.848, 0.917, 0.853, 0.978, 0.851, 0.997, 0.94, 0.939, 0.897, 0.946, 0.94, 1.107, 1.029, 1.166, 0.888, 0.983, 1.097, 0.997, 1.013, 0.965, 0.922, 1.043, 0.878, 0.815, 1.041, 0.692, 0.835, 0.74, 0.793, 0.833, 0.971, 1.012, 0.983, 1.012, 1.11, 0.9, 1.05, 0.979, 0.882, 1.005, 0.79, 0.923, 0.811, 0.859, 0.917, 0.872, 0.931, 0.967, 1.001, 0.927, 0.895, 0.872, 0.767, 0.902, 0.822, 0.959, 0.827, 0.99, 0.69, 0.708, 0.779, 0.627, 0.692, 0.885, 0.891, 1.026, 1.08, 0.965, 0.828, 0.944, 0.819, 0.847, 0.851, 0.827, 0.856, 1.007, 0.787, 0.812, 0.602, 0.629, 0.61, 0.503, 0.774, 0.556, 0.666, 0.6, 0.607, 0.629, 0.705, 0.887, 0.581, 0.779, 0.714, 0.689, 0.668, 0.701, 0.654, 0.722, 0.821, 0.928, 0.774, 1.193, 0.815, 0.972, 0.967, 1.003, 0.757, 0.763, 0.598, 0.7, 0.633, 0.527, 0.733, 0.6, 0.592, 0.805, 0.738, 0.888, 0.85, 0.924, 0.787, 0.906, 0.811, 0.679, 0.757, 0.827, 0.711, 0.86, 0.717, 0.828, 0.643, 0.651, 0.687, 0.57, 0.707, 0.64, 0.659, 0.782, 0.71, 0.633, 0.691, 0.564, 0.647, 0.72, 0.805, 0.638, 0.787, 0.795, 0.573, 0.886, 0.65, 0.741, 0.87, 0.883, 0.75, 0.976, 0.897, 0.805, 0.962, 0.703, 0.882, 0.976, 0.614, 0.802, 0.573, 0.622, 0.655, 0.488, 0.698, 0.636, 0.686, 0.902, 0.792, 0.805, 0.905, 0.589, 0.807, 0.469, 0.568, 0.526, 0.487, 0.627, 0.524, 0.701, 0.716, 0.856, 0.684, 0.778, 0.664, 0.78, 0.89, 0.889, 0.81, 0.953, 0.897, 0.75, 0.749, 0.615, 0.675, 0.627, 0.671, 0.796, 0.602, 0.683, 0.839, 0.834, 1.008, 0.917, 0.709, 0.711, 0.522, 0.751, 0.656, 0.73, 0.811, 0.726, 0.757, 0.959, 0.658, 0.897, 0.812, 0.727, 0.665, 0.819, 0.527, 0.67, 0.652, 0.636, 0.79, 0.716, 0.666, 0.828, 0.623, 0.812, 0.709, 0.77, 0.65, 0.749, 0.63, 0.782, 0.621, 0.847, 0.898, 0.81, 0.767, 0.634, 0.637, 0.805, 0.635, 0.773, 0.705, 0.514, 0.765, 0.711, 0.699, 0.804, 0.735, 0.818, 0.809, 0.793, 0.806, 0.89, 0.88, 0.925, 0.592, 1.042, 0.563, 0.78, 0.772, 0.702, 0.661, 0.719, 0.559, 0.606, 0.59, 0.551, 0.568, 0.569, 0.497, 0.608, 0.493, 0.65, 0.578, 0.579, 0.718, 0.476, 0.553, 0.673, 0.573, 0.676, 0.813, 0.573, 0.729, 0.659, 0.508, 0.648, 0.612, 0.596, 0.828, 0.531, 0.646, 0.777, 0.477, 0.605, 0.881, 0.837, 0.876, 1.086, 0.843, 0.906, 0.865, 0.719, 0.855, 0.623, 0.657, 0.615, 0.712, 0.539, 0.653, 0.436, 0.572, 0.414, 0.462, 0.564, 0.445, 0.552, 0.659, 0.657, 0.609, 0.819, 0.504, 0.696, 0.752, 0.814, 0.617, 0.937, 0.659, 0.752, 0.778, 0.53, 0.593, 0.63, 0.45, 0.657, 0.529, 0.659, 0.616, 0.759, 0.643, 0.798, 0.765, 0.603]\n",
      "Val Error(all epochs): 7.107685565948486 \n",
      " [49.595, 49.428, 49.259, 49.071, 48.83, 48.494, 48.067, 47.513, 46.84, 46.157, 45.508, 44.691, 43.743, 43.106, 42.739, 41.558, 39.529, 37.734, 37.453, 36.649, 34.185, 33.019, 33.963, 32.478, 30.007, 30.177, 26.976, 25.149, 23.24, 22.305, 16.266, 15.043, 12.388, 12.029, 12.229, 12.15, 15.252, 17.886, 15.205, 16.661, 17.0, 19.96, 17.002, 19.381, 21.138, 21.167, 19.352, 20.087, 20.877, 16.698, 19.404, 19.448, 16.989, 18.742, 16.866, 17.249, 15.294, 13.673, 14.627, 14.207, 17.745, 17.088, 20.477, 16.071, 19.608, 15.115, 12.738, 12.229, 10.975, 11.316, 11.298, 11.119, 11.427, 11.515, 10.677, 10.965, 10.095, 9.74, 10.577, 10.511, 10.707, 11.404, 10.645, 10.441, 9.839, 10.357, 10.234, 8.538, 8.789, 7.539, 8.538, 8.308, 8.251, 9.014, 9.848, 8.772, 9.414, 8.976, 8.158, 9.354, 8.112, 8.186, 7.874, 8.853, 7.992, 8.409, 8.847, 7.758, 7.702, 7.74, 7.54, 8.261, 7.895, 8.369, 8.339, 8.318, 8.27, 7.74, 8.273, 8.029, 7.896, 7.621, 8.469, 7.364, 7.398, 8.323, 7.297, 8.161, 7.114, 8.134, 9.306, 8.254, 8.864, 9.639, 8.884, 8.235, 8.113, 9.039, 8.658, 9.153, 8.381, 8.186, 9.565, 7.715, 8.695, 7.762, 8.012, 7.774, 7.513, 7.787, 7.264, 7.818, 7.909, 7.706, 7.811, 7.768, 8.22, 8.071, 7.468, 8.014, 7.519, 7.535, 7.817, 8.062, 7.661, 8.143, 7.746, 7.997, 7.888, 7.83, 8.252, 7.908, 8.059, 7.179, 7.794, 7.611, 7.678, 8.167, 7.329, 7.776, 8.037, 7.37, 8.173, 7.843, 7.474, 7.958, 8.015, 7.607, 8.052, 8.044, 7.505, 8.189, 7.687, 7.694, 8.027, 7.936, 8.056, 7.62, 8.539, 7.852, 8.477, 8.005, 7.468, 8.018, 8.387, 7.386, 8.279, 7.864, 7.585, 7.824, 8.214, 7.463, 7.718, 8.463, 7.108, 8.309, 8.224, 7.624, 8.758, 7.622, 7.86, 8.293, 7.635, 8.046, 7.683, 7.836, 7.802, 7.82, 7.947, 7.954, 8.105, 7.649, 8.412, 7.757, 8.074, 8.312, 8.072, 8.207, 8.152, 8.247, 7.893, 8.009, 8.562, 7.957, 8.106, 8.712, 7.672, 8.34, 8.185, 7.661, 8.653, 8.218, 7.283, 8.233, 7.792, 7.618, 8.017, 7.735, 7.743, 7.672, 8.228, 7.459, 8.139, 8.263, 7.413, 8.316, 7.935, 7.74, 8.157, 7.786, 8.376, 7.702, 8.073, 7.701, 7.866, 8.224, 7.623, 8.007, 7.714, 8.102, 7.614, 8.242, 7.697, 7.97, 8.083, 7.705, 7.987, 7.803, 8.349, 7.646, 8.382, 8.243, 7.691, 8.485, 7.92, 8.079, 8.196, 8.04, 7.833, 8.567, 7.818, 8.368, 8.465, 7.606, 8.538, 7.956, 7.901, 8.203, 7.583, 8.144, 7.664, 7.983, 8.234, 7.867, 8.403, 8.133, 8.347, 8.285, 8.229, 8.305, 7.82, 7.986, 8.257, 7.755, 8.48, 8.24, 8.248, 8.52, 8.185, 8.619, 8.098, 8.63, 8.271, 7.979, 8.53, 8.611, 7.802, 8.647, 8.043, 8.085, 8.276, 8.051, 7.938, 8.165, 7.911, 7.902, 8.29, 8.543, 7.824, 8.685, 8.091, 7.85, 8.68, 7.953, 8.074, 8.114, 8.021, 8.016, 8.288, 8.021, 7.557, 8.422, 7.936, 7.575, 8.77, 7.96, 8.513, 7.939, 7.636, 8.164, 7.823, 8.106, 7.821, 8.343, 8.025, 8.08, 8.448, 7.695, 8.217, 7.841, 8.122, 8.166, 7.817, 8.525, 7.735, 8.656, 7.728, 8.309, 8.563, 7.78, 8.657, 8.275, 8.371, 7.97, 8.71, 7.504, 8.379, 7.934, 7.943, 8.788, 7.799, 8.409, 8.256, 7.761, 8.835, 7.965, 8.074, 8.631, 7.764, 8.552, 7.892, 8.518, 7.502, 8.923, 7.733, 8.676, 8.217, 8.15, 8.165, 8.104, 8.473, 8.172, 8.225, 8.392, 8.177, 8.604, 8.33, 8.374, 8.77, 8.299, 8.844, 8.282, 8.52, 8.654, 8.725, 8.302, 8.648, 8.537, 8.083, 8.445, 8.338, 8.262, 8.359, 8.114, 8.504, 8.254, 8.603, 8.106, 8.297, 8.61, 8.085, 8.797, 8.65, 8.35, 8.482, 8.092, 7.815, 8.612, 8.572, 8.28, 8.14, 8.271, 8.185, 8.162, 8.453, 8.204, 8.24, 8.57, 8.263, 8.432, 8.448, 8.578, 8.069, 8.918, 8.171, 8.393, 8.69, 8.39, 8.098, 8.963, 8.324, 8.469, 8.607, 8.365, 8.47, 8.228, 8.406, 8.509, 8.629, 7.978, 8.535, 8.363, 7.669, 8.732, 8.096, 8.059, 8.392, 8.303, 8.005, 8.401]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.167, 0.269, 1.792, 3.807, 5.155, 4.351, 8.841, 12.03, 8.467, 10.19, 9.837, 13.323, 10.156, 12.573, 14.138, 14.281, 12.416, 12.846, 13.53, 9.099, 12.113, 12.08, 9.332, 11.337, 9.266, 9.76, 8.047, 6.81, 8.205, 7.348, 11.261, 10.476, 14.156, 9.791, 13.468, 9.548, 7.723, 6.824, 5.543, 5.542, 5.533, 5.036, 5.779, 5.369, 5.559, 5.51, 5.127, 4.369, 5.651, 5.455, 5.818, 6.484, 5.596, 5.039, 4.479, 5.786, 5.246, 4.591, 3.376, 3.323, 3.29, 4.474, 4.03, 5.289, 6.236, 5.044, 5.164, 5.808, 4.206, 5.656, 4.852, 4.072, 4.044, 4.025, 3.995, 4.364, 4.133, 4.499, 3.773, 3.599, 3.931, 3.64, 4.144, 4.038, 4.871, 4.596, 4.02, 3.82, 3.657, 3.44, 3.099, 2.515, 2.986, 2.672, 2.728, 3.246, 4.128, 3.479, 3.425, 4.237, 5.469, 5.506, 5.752, 6.696, 6.38, 5.799, 5.324, 6.365, 6.046, 6.217, 5.522, 5.885, 6.554, 5.476, 5.674, 4.916, 4.957, 4.346, 3.97, 3.736, 3.368, 3.508, 3.61, 3.539, 3.825, 4.006, 4.062, 4.013, 3.78, 3.562, 3.754, 3.105, 3.731, 3.949, 3.718, 4.118, 4.261, 4.131, 4.421, 3.942, 4.673, 4.08, 3.539, 2.932, 2.642, 2.474, 2.648, 2.648, 3.109, 2.737, 2.442, 3.148, 2.912, 3.075, 2.775, 3.08, 3.405, 3.301, 3.436, 3.228, 3.178, 3.37, 3.025, 3.533, 2.962, 3.735, 3.836, 3.803, 4.403, 4.42, 4.37, 3.637, 3.434, 4.399, 3.856, 4.182, 4.295, 4.063, 3.583, 3.373, 3.669, 3.003, 4.34, 4.246, 3.961, 4.493, 5.083, 4.499, 4.419, 4.514, 4.461, 3.783, 4.449, 3.69, 3.551, 3.492, 3.582, 3.237, 3.061, 3.16, 2.871, 2.682, 2.937, 2.949, 2.78, 3.409, 2.951, 3.611, 3.491, 3.296, 3.208, 3.273, 3.596, 3.053, 3.194, 3.666, 2.638, 3.535, 4.134, 3.509, 4.85, 4.659, 3.783, 3.298, 3.499, 3.04, 3.425, 2.944, 3.251, 3.172, 3.435, 2.689, 3.113, 3.152, 2.969, 2.969, 3.234, 3.434, 3.079, 3.752, 3.613, 3.639, 2.935, 2.9, 3.567, 2.822, 3.485, 2.901, 2.979, 2.964, 2.748, 3.419, 2.522, 2.949, 2.997, 3.214, 3.121, 3.38, 3.711, 3.285, 4.031, 3.463, 3.492, 3.625, 3.45, 3.245, 3.796, 3.759, 2.841, 4.164, 3.28, 3.808, 4.244, 3.594, 4.992, 3.87, 4.527, 3.968, 4.005, 3.598, 3.753, 3.769, 3.438, 3.858, 3.642, 4.157, 3.444, 3.858, 3.83, 3.294, 3.757, 2.689, 2.966, 2.589, 2.356, 2.38, 2.555, 2.068, 2.738, 2.594, 2.842, 3.051, 2.66, 3.093, 3.561, 3.553, 2.546, 3.905, 3.503, 3.84, 3.843, 3.83, 3.069, 3.36, 3.726, 3.89, 3.506, 4.086, 3.996, 3.944, 3.672, 3.456, 3.147, 3.597, 3.148, 3.413, 3.183, 2.61, 3.061, 2.751, 2.974, 3.854, 3.395, 3.758, 4.374, 3.751, 4.965, 3.524, 3.482, 3.212, 3.566, 3.195, 3.171, 3.435, 3.445, 4.07, 3.228, 3.29, 3.464, 2.873, 3.339, 3.246, 2.759, 2.925, 2.699, 3.305, 2.97, 3.72, 3.571, 3.836, 4.314, 4.146, 3.702, 3.411, 3.669, 2.688, 3.85, 3.534, 3.873, 4.219, 3.778, 3.438, 3.512, 3.551, 3.592, 3.17, 3.103, 2.793, 3.136, 3.348, 3.385, 3.439, 3.391, 2.981, 2.798, 4.384, 2.642, 3.574, 3.368, 3.319, 2.908, 2.75, 3.168, 2.832, 3.203, 2.718, 2.919, 2.576, 3.129, 2.579, 3.081, 3.246, 3.141, 3.028, 3.286, 2.681, 3.543, 3.365, 3.314, 3.559, 3.645, 3.402, 3.333, 3.643, 3.141, 3.631, 2.838, 3.097, 3.492, 3.495, 3.277, 4.101, 3.562, 4.194, 3.744, 3.284, 3.703, 2.307, 2.696, 3.096, 2.748, 2.446, 2.74, 3.129, 2.876, 3.249, 3.154, 3.448, 3.265, 3.368, 3.531, 3.371, 2.991, 3.871, 3.0, 3.654, 4.298, 4.015, 3.281, 4.283, 4.113, 3.91, 4.431, 4.238, 3.865, 4.242, 3.899, 4.059, 3.669, 3.334, 3.862, 3.436, 3.122, 3.399, 3.677, 3.026, 3.577, 4.075, 3.174, 3.87]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 49.59767, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 49.59767 to 49.56334, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 49.56334 to 49.51049, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 49.51049 to 49.41759, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 49.41759 to 49.29968, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 49.29968 to 49.12944, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 49.12944 to 48.91322, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 48.91322 to 48.67283, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 48.67283 to 48.41740, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 48.41740 to 48.15771, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 48.15771 to 47.83955, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 47.83955 to 47.54706, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 47.54706 to 47.14493, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 47.14493 to 46.72755, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 46.72755 to 46.20194, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 46.20194 to 45.89571, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 45.89571 to 45.25552, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 45.25552 to 44.95132, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 44.95132 to 44.47924, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 44.47924 to 43.85496, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 43.85496 to 43.21686, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 43.21686 to 42.59327, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 42.59327 to 41.91066, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 41.91066 to 41.51235, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 41.51235 to 41.10540, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 41.10540 to 40.15853, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 40.15853 to 39.55986, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 39.55986 to 39.10140, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 39.10140 to 38.68786, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 38.68786 to 37.95911, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00031: val_mae improved from 37.95911 to 37.82861, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00032: val_mae improved from 37.82861 to 36.66840, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 36.66840 to 35.88526, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00034: val_mae improved from 35.88526 to 35.62787, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00035: val_mae improved from 35.62787 to 34.12245, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00036: val_mae improved from 34.12245 to 33.82119, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00037: val_mae improved from 33.82119 to 32.60286, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00038: val_mae improved from 32.60286 to 32.41600, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: val_mae improved from 32.41600 to 31.61505, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00040: val_mae improved from 31.61505 to 29.03275, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 29.03275\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 29.03275\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 29.03275\n",
      "\n",
      "Epoch 00044: val_mae improved from 29.03275 to 28.65092, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00045: val_mae improved from 28.65092 to 26.50224, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00046: val_mae improved from 26.50224 to 26.39887, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00047: val_mae improved from 26.39887 to 25.66890, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00048: val_mae improved from 25.66890 to 22.32755, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 22.32755\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 22.32755\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 22.32755\n",
      "\n",
      "Epoch 00052: val_mae improved from 22.32755 to 22.24534, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 22.24534\n",
      "\n",
      "Epoch 00054: val_mae improved from 22.24534 to 20.31949, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00055: val_mae improved from 20.31949 to 17.51756, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 17.51756\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 17.51756\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 17.51756\n",
      "\n",
      "Epoch 00059: val_mae improved from 17.51756 to 17.42160, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00060: val_mae improved from 17.42160 to 14.15827, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00061: val_mae improved from 14.15827 to 13.90094, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 13.90094\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 13.90094\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 13.90094\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 13.90094\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 13.90094\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 13.90094\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 13.90094\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 13.90094\n",
      "\n",
      "Epoch 00070: val_mae improved from 13.90094 to 11.85808, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 11.85808\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 11.85808\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 11.85808\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 11.85808\n",
      "\n",
      "Epoch 00075: val_mae improved from 11.85808 to 10.94044, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 10.94044\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 10.94044\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 10.94044\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 10.94044\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 10.94044\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 10.94044\n",
      "\n",
      "Epoch 00082: val_mae improved from 10.94044 to 9.63074, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 9.63074\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 9.63074\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 9.63074\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 9.63074\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 9.63074\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 9.63074\n",
      "\n",
      "Epoch 00089: val_mae improved from 9.63074 to 8.83210, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 8.83210\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 8.83210\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 8.83210\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 8.83210\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 8.83210\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 8.83210\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 8.83210\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 8.83210\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 8.83210\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 8.83210\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 8.83210\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 8.83210\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 8.83210\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 8.83210\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 8.83210\n",
      "\n",
      "Epoch 00105: val_mae improved from 8.83210 to 8.82447, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 8.82447\n",
      "\n",
      "Epoch 00107: val_mae improved from 8.82447 to 8.13874, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 8.13874\n",
      "\n",
      "Epoch 00109: val_mae improved from 8.13874 to 7.32589, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 7.32589\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 7.32589\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 7.32589\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 7.32589\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 7.32589\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 7.32589\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 7.32589\n",
      "\n",
      "Epoch 00117: val_mae improved from 7.32589 to 7.16386, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 7.16386\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 7.16386\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 7.16386\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 7.16386\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 7.16386\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 7.16386\n",
      "\n",
      "Epoch 00124: val_mae improved from 7.16386 to 7.08106, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 7.08106\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 7.08106\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 7.08106\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 7.08106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00129: val_mae did not improve from 7.08106\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 7.08106\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 7.08106\n",
      "\n",
      "Epoch 00132: val_mae improved from 7.08106 to 6.75726, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 6.75726\n",
      "\n",
      "Epoch 00170: val_mae improved from 6.75726 to 6.50400, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_8/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 6.50400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00284: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 6.50400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00445: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 6.50400\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 6.50400\n",
      "\n",
      "Lambda: 1 , Time: 0:04:16\n",
      "Train Error(all epochs): 1.3868584632873535 \n",
      " [49.022, 48.926, 48.851, 48.762, 48.663, 48.54, 48.402, 48.236, 48.037, 47.797, 47.527, 47.219, 46.861, 46.449, 45.988, 45.467, 44.892, 44.254, 43.569, 42.831, 42.023, 41.184, 40.292, 39.377, 38.406, 37.432, 36.417, 35.406, 34.363, 33.277, 32.164, 31.061, 29.945, 28.826, 27.706, 26.595, 25.452, 24.336, 23.216, 22.091, 21.018, 19.871, 18.812, 17.689, 16.7, 15.734, 14.647, 13.766, 12.725, 11.954, 11.127, 10.338, 9.61, 8.937, 8.4, 7.815, 7.366, 7.218, 6.912, 6.574, 6.237, 5.936, 5.772, 5.212, 4.973, 4.8, 4.897, 4.949, 4.778, 4.669, 4.356, 4.368, 4.25, 4.38, 4.237, 4.255, 3.985, 4.155, 4.089, 3.762, 3.733, 3.723, 4.114, 4.225, 4.091, 4.158, 3.849, 4.135, 4.002, 3.755, 3.45, 3.417, 3.252, 3.311, 3.425, 3.451, 3.739, 3.401, 3.545, 3.37, 3.582, 3.467, 3.23, 3.206, 3.239, 3.606, 3.499, 3.82, 3.549, 3.779, 3.344, 3.106, 3.113, 3.078, 3.092, 3.183, 2.945, 2.952, 3.028, 3.104, 3.419, 3.326, 3.304, 3.359, 3.262, 3.513, 3.218, 3.217, 3.091, 2.863, 3.168, 3.086, 3.233, 3.242, 2.995, 3.029, 2.869, 2.819, 2.802, 2.913, 2.678, 2.621, 2.88, 3.095, 3.253, 3.453, 3.226, 3.373, 2.999, 3.478, 3.258, 3.329, 3.087, 3.244, 3.009, 2.762, 2.62, 2.482, 2.386, 2.357, 2.404, 2.516, 2.747, 2.669, 2.952, 2.881, 2.796, 2.861, 3.105, 3.12, 2.962, 2.774, 2.672, 2.725, 2.688, 2.755, 3.06, 3.164, 3.153, 3.035, 2.825, 2.973, 2.888, 2.611, 2.753, 2.786, 2.878, 2.678, 2.855, 2.517, 2.695, 2.571, 2.534, 2.883, 2.707, 2.957, 3.057, 2.891, 3.012, 3.03, 2.896, 2.674, 2.498, 2.616, 2.632, 3.061, 2.932, 3.184, 3.048, 2.881, 2.867, 2.782, 2.523, 2.556, 2.631, 2.676, 2.672, 2.987, 2.633, 2.77, 2.74, 2.916, 3.073, 2.949, 2.972, 2.679, 2.574, 2.304, 2.518, 2.473, 2.256, 2.29, 2.562, 2.5, 2.661, 2.716, 2.916, 2.562, 2.572, 2.651, 2.482, 2.648, 2.614, 2.631, 2.503, 2.436, 2.567, 2.593, 2.72, 2.758, 2.711, 2.59, 2.583, 2.413, 2.347, 2.137, 2.035, 1.92, 1.992, 2.006, 2.179, 2.324, 2.81, 2.796, 2.782, 2.842, 2.943, 3.061, 2.897, 3.111, 3.012, 2.763, 2.717, 2.553, 2.479, 2.474, 2.326, 2.427, 2.316, 2.402, 2.349, 2.529, 2.834, 2.631, 2.566, 2.935, 2.741, 2.71, 2.596, 2.561, 2.392, 2.664, 2.629, 2.526, 2.435, 2.222, 2.335, 2.595, 2.417, 2.735, 2.559, 2.547, 2.397, 2.361, 2.35, 2.423, 2.429, 2.543, 2.309, 2.165, 2.04, 2.104, 2.059, 1.887, 2.162, 2.282, 2.243, 2.108, 2.112, 2.175, 2.092, 2.433, 2.358, 2.355, 2.495, 2.703, 2.524, 2.863, 2.727, 2.776, 2.387, 2.521, 2.3, 2.126, 2.093, 2.181, 2.149, 2.167, 2.228, 2.366, 2.785, 2.767, 2.606, 2.296, 2.416, 2.611, 2.392, 2.334, 2.496, 2.404, 2.497, 2.616, 2.585, 2.433, 2.432, 2.331, 2.721, 2.651, 3.171, 3.034, 3.062, 2.542, 2.489, 2.407, 2.477, 2.359, 2.38, 2.346, 2.29, 2.052, 1.929, 1.655, 1.723, 1.932, 2.133, 2.464, 2.416, 2.423, 2.104, 2.082, 2.277, 2.262, 2.308, 2.234, 2.036, 1.882, 1.965, 1.991, 1.991, 2.158, 2.315, 2.483, 2.988, 2.526, 2.601, 2.503, 2.515, 2.309, 2.12, 1.943, 2.042, 2.047, 2.321, 2.43, 2.459, 2.158, 2.28, 1.963, 1.935, 1.952, 2.232, 2.917, 2.928, 2.982, 2.715, 2.645, 2.498, 2.337, 2.349, 2.607, 2.268, 2.189, 2.249, 2.284, 2.048, 1.952, 1.861, 1.719, 1.489, 1.387, 1.694, 2.011, 1.973, 2.091, 2.392, 2.499, 2.781, 2.529, 2.747, 2.571, 2.584, 2.344, 2.239, 2.083, 1.966, 1.864, 1.702, 1.893, 2.181, 2.28, 2.445, 2.505, 2.617, 2.532, 2.536, 2.484, 2.505, 2.365, 2.349, 2.09, 2.083, 2.004, 1.789, 1.695, 1.717, 1.669, 1.824, 1.897, 2.38, 2.366, 2.625, 2.673, 2.64, 2.38, 2.095, 2.405, 2.4, 2.674, 2.596, 2.434, 2.134, 2.041, 2.114, 2.012, 1.897, 1.813, 1.751, 1.854, 1.785, 1.786, 1.763, 1.887, 1.912, 1.877, 1.765, 1.729, 1.776, 1.937, 2.25, 2.627]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.004, 0.012, 0.011, 0.011, 0.008, 0.027, 0.018, 0.046, 0.056, 0.064, 0.096, 0.076, 0.14, 0.157, 0.164, 0.25, 0.249, 0.345, 0.38, 0.441, 0.698, 0.788, 0.856, 0.959, 0.999, 1.097, 0.981, 1.057, 1.106, 1.293, 1.386, 1.323, 1.498, 1.288, 1.321, 1.384, 1.374, 1.404, 1.459, 1.336, 1.506, 1.423, 1.349, 1.239, 1.309, 1.558, 1.484, 1.51, 1.55, 1.412, 1.474, 1.492, 1.396, 1.159, 1.202, 1.078, 1.154, 1.235, 1.244, 1.474, 1.201, 1.344, 1.205, 1.395, 1.335, 1.143, 1.144, 1.128, 1.528, 1.271, 1.515, 1.271, 1.44, 1.282, 1.057, 1.138, 1.13, 1.106, 1.196, 1.09, 1.043, 1.094, 1.159, 1.349, 1.305, 1.173, 1.287, 1.213, 1.374, 1.136, 1.288, 1.195, 1.013, 1.156, 1.284, 1.224, 1.176, 1.131, 1.176, 0.996, 1.074, 1.015, 1.109, 1.004, 0.954, 1.118, 1.248, 1.235, 1.567, 1.258, 1.224, 1.186, 1.313, 1.2, 1.398, 1.219, 1.202, 1.183, 1.001, 0.973, 0.9, 0.847, 0.922, 0.882, 0.894, 1.116, 1.001, 1.118, 1.168, 1.042, 1.103, 1.154, 1.31, 1.153, 1.099, 0.926, 1.052, 1.003, 1.103, 1.171, 1.31, 1.224, 1.213, 1.074, 1.12, 1.19, 0.938, 1.044, 1.087, 1.22, 0.959, 1.263, 0.894, 1.022, 0.981, 0.998, 1.195, 1.014, 1.211, 1.228, 1.16, 1.135, 1.211, 1.174, 1.03, 0.956, 1.018, 0.956, 1.348, 1.052, 1.254, 1.281, 1.22, 1.142, 1.07, 0.979, 0.978, 1.097, 1.088, 1.014, 1.259, 0.957, 1.163, 1.071, 1.159, 1.235, 1.197, 1.222, 0.976, 1.0, 0.879, 0.987, 1.069, 0.785, 0.852, 1.113, 0.914, 1.074, 1.094, 1.307, 0.975, 0.942, 1.089, 0.946, 1.096, 1.028, 1.065, 1.003, 0.973, 0.995, 1.03, 1.08, 1.075, 1.139, 0.995, 1.063, 0.93, 0.898, 0.788, 0.767, 0.726, 0.744, 0.725, 0.915, 0.867, 1.221, 1.201, 1.091, 1.098, 1.167, 1.284, 1.164, 1.247, 1.302, 1.083, 1.105, 1.008, 0.99, 1.005, 0.838, 1.047, 0.879, 0.943, 0.916, 1.07, 1.169, 1.097, 0.952, 1.296, 1.064, 1.197, 1.013, 0.99, 0.882, 1.086, 1.103, 1.031, 1.025, 0.837, 0.916, 1.12, 0.98, 1.175, 0.984, 1.062, 0.854, 0.957, 0.918, 0.959, 1.006, 1.153, 0.836, 0.817, 0.84, 0.805, 0.778, 0.759, 0.832, 0.975, 0.935, 0.729, 0.848, 0.885, 0.849, 1.037, 0.944, 0.92, 0.992, 1.151, 0.886, 1.329, 1.125, 1.118, 0.911, 1.042, 0.917, 0.858, 0.771, 0.907, 0.86, 0.893, 0.963, 0.866, 1.199, 1.158, 1.107, 0.907, 1.016, 1.107, 0.986, 0.933, 1.037, 0.998, 0.953, 1.135, 1.126, 0.892, 0.953, 0.925, 1.224, 1.075, 1.495, 1.12, 1.406, 1.024, 0.989, 0.92, 1.144, 0.869, 1.018, 0.908, 1.051, 0.732, 0.805, 0.605, 0.687, 0.822, 0.88, 1.095, 0.943, 0.956, 0.856, 0.779, 0.978, 0.923, 1.053, 0.892, 0.778, 0.738, 0.82, 0.805, 0.805, 0.828, 1.104, 0.945, 1.415, 0.895, 1.092, 1.013, 1.055, 0.987, 0.881, 0.758, 0.836, 0.786, 1.042, 1.007, 1.114, 0.79, 1.019, 0.726, 0.841, 0.763, 0.953, 1.296, 1.269, 1.284, 1.136, 1.116, 1.046, 0.935, 0.948, 1.25, 0.903, 0.93, 0.775, 1.122, 0.827, 0.775, 0.704, 0.711, 0.539, 0.52, 0.709, 0.869, 0.826, 0.876, 0.987, 1.028, 1.29, 0.932, 1.305, 1.032, 1.093, 0.911, 1.014, 0.906, 0.735, 0.856, 0.72, 0.731, 0.95, 0.987, 0.973, 1.061, 1.158, 1.065, 1.076, 1.065, 1.077, 0.927, 1.068, 0.808, 0.905, 0.81, 0.741, 0.689, 0.642, 0.718, 0.75, 0.735, 1.082, 1.033, 1.09, 1.197, 1.084, 0.982, 0.833, 1.032, 1.041, 1.215, 1.063, 1.03, 0.898, 0.835, 0.797, 0.968, 0.747, 0.73, 0.721, 0.862, 0.623, 0.741, 0.762, 0.799, 0.816, 0.821, 0.686, 0.711, 0.76, 0.788, 1.042, 1.128]\n",
      "Val Error(all epochs): 6.504001617431641 \n",
      " [49.598, 49.563, 49.51, 49.418, 49.3, 49.129, 48.913, 48.673, 48.417, 48.158, 47.84, 47.547, 47.145, 46.728, 46.202, 45.896, 45.256, 44.951, 44.479, 43.855, 43.217, 42.593, 41.911, 41.512, 41.105, 40.159, 39.56, 39.101, 38.688, 37.959, 37.829, 36.668, 35.885, 35.628, 34.122, 33.821, 32.603, 32.416, 31.615, 29.033, 30.092, 29.131, 29.066, 28.651, 26.502, 26.399, 25.669, 22.328, 23.754, 23.832, 23.228, 22.245, 22.817, 20.319, 17.518, 18.915, 17.762, 18.352, 17.422, 14.158, 13.901, 14.311, 16.328, 18.735, 18.404, 16.045, 15.749, 15.097, 14.368, 11.858, 13.806, 13.781, 12.269, 13.817, 10.94, 12.147, 13.149, 11.686, 11.034, 10.976, 11.872, 9.631, 10.917, 10.373, 10.153, 9.661, 9.846, 10.114, 8.832, 9.579, 9.569, 9.498, 9.944, 9.419, 9.042, 10.055, 9.61, 8.96, 9.907, 10.263, 9.183, 9.084, 9.929, 9.353, 8.824, 9.269, 8.139, 8.838, 7.326, 7.91, 7.488, 7.482, 8.029, 7.524, 8.119, 7.956, 7.164, 9.223, 7.845, 8.261, 7.694, 8.175, 7.589, 7.081, 7.278, 8.085, 7.5, 7.512, 7.647, 8.734, 7.394, 6.757, 9.303, 7.284, 7.708, 8.516, 8.528, 8.036, 8.945, 7.842, 9.202, 8.302, 8.9, 7.741, 7.713, 7.698, 7.318, 7.328, 7.609, 7.406, 7.15, 8.143, 8.632, 8.804, 8.335, 8.335, 8.67, 8.308, 9.399, 8.334, 10.177, 8.614, 8.738, 8.604, 8.662, 7.669, 7.371, 9.215, 7.588, 6.504, 7.65, 7.591, 8.199, 8.692, 8.159, 9.38, 8.207, 9.008, 8.372, 8.225, 9.663, 8.263, 7.428, 9.287, 7.801, 7.308, 8.467, 8.102, 8.442, 7.485, 8.569, 7.58, 7.596, 7.148, 8.562, 7.357, 7.27, 8.48, 8.065, 7.77, 7.923, 7.584, 8.236, 8.125, 7.859, 7.476, 8.091, 7.912, 6.795, 7.476, 7.763, 8.85, 8.885, 8.309, 9.367, 9.442, 8.653, 10.004, 8.721, 8.914, 7.685, 8.289, 8.857, 7.579, 8.513, 7.248, 8.105, 8.063, 8.002, 7.845, 8.056, 8.798, 7.96, 8.48, 8.919, 8.097, 7.636, 9.018, 8.47, 8.018, 7.548, 8.39, 7.109, 8.981, 7.415, 9.312, 7.575, 7.92, 9.513, 8.43, 8.761, 9.227, 7.712, 8.234, 7.841, 7.76, 8.388, 8.312, 8.299, 8.711, 8.923, 7.521, 9.043, 7.327, 7.954, 8.005, 9.419, 9.804, 9.499, 9.466, 8.661, 9.261, 7.838, 8.409, 8.308, 8.799, 9.186, 8.941, 8.355, 8.179, 9.565, 7.874, 9.537, 9.792, 8.335, 8.686, 8.449, 8.002, 7.851, 7.71, 7.963, 7.38, 7.623, 7.883, 7.609, 8.382, 7.591, 7.877, 8.085, 7.888, 7.737, 7.57, 7.792, 7.904, 8.269, 7.497, 8.008, 7.672, 8.404, 8.075, 8.678, 8.393, 8.174, 9.576, 7.439, 8.721, 8.757, 8.382, 8.734, 7.787, 9.133, 7.845, 8.861, 8.966, 7.584, 8.699, 8.16, 7.869, 7.671, 7.576, 7.865, 7.832, 7.174, 8.139, 8.026, 8.306, 7.996, 7.891, 8.834, 9.568, 7.905, 9.559, 8.323, 8.631, 9.185, 8.095, 8.562, 9.1, 8.115, 9.672, 8.404, 7.864, 8.405, 7.956, 8.594, 8.64, 8.543, 9.953, 8.867, 7.515, 7.834, 9.172, 8.81, 10.265, 8.775, 8.086, 9.996, 8.137, 9.149, 8.684, 9.173, 9.022, 9.195, 8.496, 9.486, 8.632, 8.698, 8.654, 8.798, 8.092, 7.948, 7.163, 8.034, 7.707, 8.381, 8.603, 7.917, 8.688, 8.57, 8.337, 9.233, 8.161, 9.625, 9.563, 10.009, 9.303, 9.431, 9.068, 9.765, 10.844, 9.502, 9.902, 9.41, 8.926, 8.229, 9.78, 8.331, 9.412, 8.327, 8.472, 9.319, 8.816, 9.273, 9.746, 12.62, 11.086, 10.56, 11.117, 10.503, 10.028, 9.549, 10.027, 9.496, 8.374, 8.661, 9.549, 8.387, 8.764, 9.33, 8.774, 8.944, 9.213, 8.547, 8.513, 9.045, 8.767, 7.814, 8.638, 8.39, 8.069, 8.158, 7.604, 7.791, 8.075, 7.36, 8.242, 8.118, 7.577, 8.392, 7.688, 8.38, 8.098, 7.989, 8.911, 8.199, 8.247, 7.891, 8.503, 7.615, 8.005, 7.44, 7.797, 8.612, 8.094, 8.425, 8.938, 8.733, 8.781, 8.564, 9.975, 7.954, 9.093, 7.872, 8.116, 8.479, 7.697, 8.222, 7.528, 8.518, 8.03, 7.729, 8.31, 7.758, 8.4, 8.187, 8.227, 7.973, 8.097, 8.557, 7.986, 8.434, 8.004, 7.982, 8.144, 8.581, 8.716, 8.144, 8.606, 7.736, 8.12]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.11, 0.069, 0.18, 0.156, 0.342, 0.459, 0.776, 0.545, 0.332, 0.283, 0.266, 0.365, 0.422, 0.687, 0.495, 0.794, 0.688, 0.451, 0.669, 0.863, 0.94, 0.875, 0.88, 0.895, 0.992, 1.365, 1.018, 0.977, 1.152, 1.5, 1.424, 1.912, 1.066, 1.345, 1.313, 1.198, 1.14, 1.339, 1.315, 1.428, 1.483, 1.421, 1.544, 1.703, 1.719, 1.486, 1.418, 1.48, 1.317, 1.11, 1.299, 1.761, 2.052, 2.241, 2.11, 2.777, 3.143, 1.865, 2.292, 2.228, 2.081, 2.869, 2.286, 2.559, 1.839, 2.103, 2.053, 1.561, 3.048, 2.385, 2.067, 2.6, 2.019, 3.345, 1.996, 2.715, 2.382, 3.193, 2.708, 2.116, 2.233, 1.694, 1.758, 1.573, 1.595, 1.402, 1.986, 1.4, 2.131, 2.299, 2.565, 2.565, 2.967, 3.582, 5.193, 2.61, 1.803, 1.354, 0.925, 1.752, 1.663, 1.594, 1.517, 1.662, 1.71, 1.367, 1.871, 1.881, 1.605, 2.133, 2.238, 2.48, 2.06, 1.99, 2.697, 2.607, 3.172, 2.146, 2.189, 1.751, 1.891, 1.881, 1.809, 1.987, 1.886, 2.087, 1.908, 1.59, 1.752, 1.791, 1.476, 2.391, 1.652, 1.683, 1.543, 1.708, 2.526, 1.864, 2.253, 2.311, 2.049, 2.218, 2.035, 2.287, 1.691, 1.919, 2.353, 2.207, 2.507, 2.047, 2.718, 1.918, 2.229, 2.526, 2.433, 3.544, 2.517, 1.82, 1.601, 1.55, 1.56, 0.89, 1.768, 1.41, 1.222, 2.536, 2.089, 2.753, 1.585, 1.605, 2.672, 2.709, 3.334, 2.394, 2.913, 2.59, 2.504, 2.426, 2.502, 1.848, 2.145, 1.618, 2.669, 2.035, 3.008, 2.505, 2.358, 2.732, 2.093, 2.722, 1.904, 2.221, 2.194, 2.044, 2.695, 1.917, 2.022, 2.34, 2.181, 2.354, 2.769, 2.647, 2.443, 2.47, 2.26, 2.329, 2.178, 2.67, 2.846, 2.919, 4.067, 3.061, 1.136, 1.129, 1.449, 1.424, 0.906, 0.856, 1.326, 1.555, 1.446, 1.765, 1.549, 1.939, 1.626, 2.377, 2.333, 2.556, 2.481, 2.047, 2.322, 2.937, 2.256, 2.641, 3.104, 2.351, 3.035, 2.711, 3.95, 3.024, 2.86, 3.07, 3.906, 3.116, 3.361, 3.216, 2.939, 3.251, 2.827, 3.045, 2.784, 2.836, 3.036, 2.628, 3.217, 2.535, 2.574, 2.56, 2.911, 3.014, 2.846, 3.285, 2.693, 2.947, 2.531, 2.428, 2.847, 1.984, 2.455, 2.609, 2.349, 2.863, 2.687, 2.447, 3.851, 3.343, 3.653, 4.029, 3.27, 3.399, 3.316, 2.589, 3.287, 2.896, 2.092, 1.93, 2.712, 2.003, 1.921, 2.885, 2.866, 2.915, 2.615, 2.615, 2.295, 2.911, 1.869, 3.149, 2.317, 2.377, 2.675, 1.823, 2.009, 1.46, 1.475, 1.791, 1.897, 1.265, 1.061, 1.679, 1.416, 1.799, 2.031, 1.739, 1.729, 1.754, 1.979, 1.951, 1.93, 1.869, 2.222, 2.244, 2.752, 2.439, 2.334, 2.293, 3.347, 2.912, 3.352, 2.749, 2.641, 2.862, 2.416, 2.952, 2.321, 2.987, 2.455, 2.901, 2.209, 1.434, 1.747, 2.499, 2.746, 1.488, 1.938, 1.137, 1.343, 1.434, 1.689, 1.599, 2.197, 1.646, 1.951, 1.65, 2.476, 2.067, 1.559, 1.666, 1.419, 1.373, 1.048, 0.858, 1.53, 0.974, 1.659, 2.179, 1.491, 1.726, 1.942, 2.345, 2.349, 2.066, 2.795, 2.277, 2.269, 2.085, 2.525, 2.249, 2.182, 2.883, 2.3, 3.621, 3.035, 2.758, 3.181, 3.245, 3.255, 3.457, 3.773, 2.917, 3.314, 2.879, 2.652, 2.09, 2.99, 3.265, 3.624, 3.482, 3.434, 3.342, 3.548, 3.021, 3.205, 3.65, 4.019, 3.714, 2.962, 3.247, 2.706, 2.802, 2.614, 2.952, 2.281, 2.867, 2.468, 1.872, 1.722, 2.454, 2.777, 2.999, 3.06, 3.505, 4.117, 3.835, 3.274, 3.268, 3.439, 3.4, 2.976, 3.301, 2.419, 2.945, 3.565, 2.974, 3.043, 2.816, 2.954, 2.553, 3.045, 2.542, 2.794, 2.835, 2.916, 2.893, 2.985, 2.678]\n",
      "\n",
      "#Fold: 8 \n",
      "Trainig set size: 420 , Time: 0:12:48 , best_lambda: 1 , min_  , error: 6.504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test starts:  467 , ends:  518\n",
      "1/1 [==============================] - 0s 656us/step - loss: 134.6430 - mse: 66.3756 - mae: 6.5692 - fp_mae: 3.1073\n",
      "average_error:  6.569 , fp_average_error:  3.107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:00<00:00, 311.51it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 251.37it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 258.80it/s]\n",
      "100%|██████████| 107/107 [00:00<00:00, 274.09it/s]\n",
      "100%|██████████| 103/103 [00:00<00:00, 240.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Fold: 9 , Training Size: 421 , Validation size: 47 , Test Size 51\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 49.91146, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 49.91146 to 49.83652, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 49.83652 to 49.76793, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 49.76793 to 49.72489, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 49.72489 to 49.67362, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 49.67362 to 49.62626, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 49.62626 to 49.56359, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 49.56359 to 49.49321, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 49.49321 to 49.34698, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 49.34698 to 49.25033, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 49.25033 to 49.23967, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 49.23967 to 49.11858, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 49.11858 to 48.90651, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 48.90651 to 48.85005, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 48.85005 to 48.52778, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 48.52778 to 48.23508, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 48.23508\n",
      "\n",
      "Epoch 00018: val_mae improved from 48.23508 to 47.56817, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 47.56817 to 47.07309, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 47.07309 to 46.75269, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 46.75269 to 46.01434, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 46.01434 to 45.15754, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 45.15754\n",
      "\n",
      "Epoch 00024: val_mae improved from 45.15754 to 44.55563, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 44.55563 to 44.11272, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 44.11272 to 43.33472, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 43.33472\n",
      "\n",
      "Epoch 00028: val_mae improved from 43.33472 to 42.56472, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 42.56472 to 42.03245, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 42.03245\n",
      "\n",
      "Epoch 00031: val_mae improved from 42.03245 to 40.98645, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00032: val_mae improved from 40.98645 to 39.27336, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 39.27336\n",
      "\n",
      "Epoch 00034: val_mae improved from 39.27336 to 37.86011, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00035: val_mae improved from 37.86011 to 35.92493, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 35.92493\n",
      "\n",
      "Epoch 00037: val_mae improved from 35.92493 to 34.62477, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00038: val_mae improved from 34.62477 to 34.08378, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 34.08378\n",
      "\n",
      "Epoch 00040: val_mae improved from 34.08378 to 33.92454, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00041: val_mae improved from 33.92454 to 32.55344, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00042: val_mae improved from 32.55344 to 32.29177, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00043: val_mae improved from 32.29177 to 30.75720, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00044: val_mae improved from 30.75720 to 30.00781, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00045: val_mae improved from 30.00781 to 28.74039, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00046: val_mae improved from 28.74039 to 26.04966, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 26.04966\n",
      "\n",
      "Epoch 00048: val_mae improved from 26.04966 to 23.85997, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00049: val_mae improved from 23.85997 to 20.87992, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 20.87992\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 20.87992\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 20.87992\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 20.87992\n",
      "\n",
      "Epoch 00054: val_mae improved from 20.87992 to 20.08799, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00055: val_mae improved from 20.08799 to 15.47630, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 15.47630\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 15.47630\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 15.47630\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 15.47630\n",
      "\n",
      "Epoch 00060: val_mae improved from 15.47630 to 14.42342, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 14.42342\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 14.42342\n",
      "\n",
      "Epoch 00063: val_mae improved from 14.42342 to 13.01460, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00064: val_mae improved from 13.01460 to 11.51423, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 11.51423\n",
      "\n",
      "Epoch 00066: val_mae improved from 11.51423 to 10.57188, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 10.57188\n",
      "\n",
      "Epoch 00068: val_mae improved from 10.57188 to 10.10247, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 10.10247\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 10.10247\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 10.10247\n",
      "\n",
      "Epoch 00072: val_mae improved from 10.10247 to 9.62846, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 9.62846\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 9.62846\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 9.62846\n",
      "\n",
      "Epoch 00076: val_mae improved from 9.62846 to 9.37911, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 9.37911\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 9.37911\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 9.37911\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 9.37911\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 9.37911\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 9.37911\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 9.37911\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 9.37911\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 9.37911\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 9.37911\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 9.37911\n",
      "\n",
      "Epoch 00088: val_mae improved from 9.37911 to 9.11930, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 9.11930\n",
      "\n",
      "Epoch 00110: val_mae improved from 9.11930 to 8.84269, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 8.84269\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 8.84269\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 8.84269\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 8.84269\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 8.84269\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 8.84269\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 8.84269\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 8.84269\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 8.84269\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 8.84269\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 8.84269\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 8.84269\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 8.84269\n",
      "\n",
      "Epoch 00124: val_mae improved from 8.84269 to 8.75918, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 8.75918\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 8.75918\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 8.75918\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 8.75918\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 8.75918\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 8.75918\n",
      "\n",
      "Epoch 00131: val_mae improved from 8.75918 to 8.68454, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 8.68454\n",
      "\n",
      "Epoch 00133: val_mae improved from 8.68454 to 8.20311, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 8.20311\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 8.20311\n",
      "\n",
      "Epoch 00136: val_mae improved from 8.20311 to 8.10816, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 8.10816\n",
      "\n",
      "Epoch 00138: val_mae improved from 8.10816 to 7.99619, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00139: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 7.99619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00300: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 7.99619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00461: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 7.99619\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 7.99619\n",
      "\n",
      "Lambda: 0.01 , Time: 0:04:13\n",
      "Train Error(all epochs): 0.5894392132759094 \n",
      " [49.051, 48.957, 48.884, 48.802, 48.713, 48.605, 48.476, 48.321, 48.132, 47.91, 47.654, 47.352, 47.009, 46.615, 46.161, 45.661, 45.081, 44.441, 43.744, 42.963, 42.134, 41.297, 40.379, 39.439, 38.439, 37.41, 36.336, 35.226, 34.11, 32.944, 31.746, 30.53, 29.25, 27.97, 26.701, 25.384, 24.109, 22.812, 21.515, 20.305, 19.008, 17.889, 16.655, 15.591, 14.499, 13.569, 12.631, 11.66, 10.87, 9.964, 9.177, 8.565, 7.858, 7.566, 7.174, 6.65, 6.261, 5.789, 5.385, 5.015, 4.845, 4.872, 4.807, 4.747, 5.099, 4.698, 4.726, 4.467, 4.211, 4.252, 3.909, 4.126, 3.892, 3.823, 3.792, 3.697, 3.782, 3.866, 3.701, 3.515, 3.49, 3.407, 3.353, 3.108, 3.334, 3.286, 3.235, 3.169, 3.037, 3.071, 3.365, 3.277, 3.237, 3.36, 3.075, 3.062, 2.801, 2.616, 2.741, 2.986, 2.798, 2.705, 2.721, 2.707, 2.675, 2.667, 2.586, 2.472, 2.699, 2.623, 2.504, 2.374, 2.276, 2.182, 2.139, 2.019, 1.898, 1.846, 1.909, 1.974, 2.097, 2.16, 2.071, 2.331, 2.46, 2.554, 2.587, 2.498, 2.538, 2.309, 2.188, 2.13, 1.887, 2.073, 2.175, 2.287, 2.406, 2.596, 2.237, 2.103, 1.994, 1.801, 1.807, 1.787, 2.056, 2.231, 2.341, 2.342, 2.245, 2.32, 2.406, 2.088, 1.965, 1.672, 1.561, 1.456, 1.446, 1.389, 1.413, 1.411, 1.375, 1.416, 1.356, 1.389, 1.551, 1.501, 1.509, 1.692, 1.545, 1.893, 2.179, 2.052, 2.124, 2.103, 1.948, 1.998, 1.998, 1.975, 1.96, 1.888, 1.972, 2.001, 1.821, 1.828, 1.599, 1.493, 1.527, 1.372, 1.573, 1.728, 1.671, 1.62, 1.641, 1.608, 1.691, 1.573, 1.486, 1.428, 1.526, 1.552, 1.445, 1.422, 1.423, 1.414, 1.387, 1.24, 1.202, 1.268, 1.219, 1.177, 1.199, 1.347, 1.373, 1.285, 1.325, 1.427, 1.325, 1.179, 1.1, 1.163, 1.196, 1.119, 1.19, 1.294, 1.32, 1.2, 1.065, 1.102, 1.163, 1.083, 1.047, 1.138, 1.244, 1.415, 1.383, 1.34, 1.343, 1.388, 1.502, 1.402, 1.413, 1.325, 1.361, 1.319, 1.236, 1.287, 1.398, 1.417, 1.292, 1.142, 1.082, 1.179, 1.237, 1.22, 1.223, 1.338, 1.394, 1.503, 1.561, 1.601, 1.54, 1.539, 1.652, 1.64, 1.524, 1.401, 1.391, 1.322, 1.466, 1.751, 1.902, 1.978, 1.926, 1.705, 1.639, 1.411, 1.215, 1.162, 1.139, 1.107, 0.968, 0.948, 0.939, 0.925, 0.868, 0.862, 0.919, 0.927, 0.814, 0.851, 0.927, 0.951, 0.962, 0.952, 0.985, 1.045, 1.151, 1.142, 1.133, 1.077, 1.104, 1.1, 1.247, 1.444, 1.531, 1.319, 1.256, 1.337, 1.427, 1.42, 1.268, 1.272, 1.292, 1.208, 1.164, 1.168, 1.213, 1.268, 1.251, 1.194, 1.199, 1.273, 1.354, 1.26, 1.159, 1.14, 1.171, 1.228, 1.122, 1.064, 1.18, 1.291, 1.371, 1.269, 1.1, 0.93, 0.832, 0.869, 0.93, 0.956, 0.992, 1.032, 1.044, 1.003, 0.902, 0.842, 0.831, 0.892, 0.997, 1.084, 1.128, 1.176, 1.206, 1.191, 0.999, 0.876, 0.855, 0.858, 0.951, 0.928, 0.884, 0.945, 0.997, 1.016, 1.107, 1.185, 1.145, 1.191, 1.334, 1.481, 1.445, 1.443, 1.401, 1.36, 1.286, 1.318, 1.273, 1.271, 1.389, 1.578, 1.576, 1.271, 1.233, 1.245, 1.264, 1.277, 1.305, 1.4, 1.454, 1.403, 1.377, 1.342, 1.221, 1.195, 1.217, 1.2, 1.255, 1.148, 1.154, 1.12, 1.164, 1.193, 1.259, 1.148, 1.031, 1.004, 0.964, 1.164, 1.251, 1.145, 1.116, 1.216, 1.159, 1.128, 1.061, 1.104, 1.101, 0.946, 0.787, 0.729, 0.651, 0.625, 0.681, 0.739, 0.741, 0.633, 0.589, 0.599, 0.603, 0.632, 0.668, 0.65, 0.662, 0.703, 0.739, 0.771, 0.894, 1.051, 1.009, 0.937, 0.918, 0.883, 0.862, 0.941, 1.015, 0.889, 0.956, 1.134, 1.183, 1.097, 0.998, 1.0, 1.03, 0.963, 0.904, 0.874, 0.816, 0.721, 0.7, 0.793, 0.804, 0.748, 0.799, 0.843, 0.803, 0.813, 0.843, 0.828, 0.782, 0.931, 1.14, 1.22, 1.136, 1.169, 1.341, 1.427, 1.406, 1.364, 1.312, 1.246, 1.092, 1.023, 0.98, 1.001, 1.005, 0.902, 0.853, 0.927, 0.922, 0.888, 0.786, 0.815, 0.897, 0.855, 0.802, 0.787, 0.79, 0.78, 0.875, 0.957]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.004, 0.0, 0.016, 0.018, 0.034, 0.031, 0.046, 0.07, 0.081, 0.112, 0.147, 0.204, 0.209, 0.271, 0.284, 0.296, 0.404, 0.429, 0.562, 0.76, 0.675, 0.753, 0.845, 0.893, 0.984, 1.079, 1.248, 1.37, 1.367, 1.709, 1.609, 1.569, 1.574, 1.52, 1.583, 1.45, 1.623, 1.491, 1.448, 1.454, 1.452, 1.499, 1.564, 1.532, 1.412, 1.394, 1.378, 1.316, 1.294, 1.319, 1.367, 1.333, 1.333, 1.227, 1.265, 1.506, 1.409, 1.348, 1.557, 1.249, 1.324, 1.203, 1.069, 1.208, 1.332, 1.21, 1.194, 1.171, 1.163, 1.217, 1.194, 1.107, 1.146, 1.203, 1.156, 1.161, 1.059, 0.951, 0.979, 0.967, 0.887, 0.833, 0.811, 0.861, 0.877, 0.934, 1.049, 0.961, 1.118, 1.184, 1.154, 1.242, 1.145, 1.121, 1.09, 1.005, 0.957, 0.769, 0.993, 1.02, 1.031, 1.146, 1.207, 1.057, 0.924, 0.91, 0.77, 0.87, 0.847, 0.978, 1.084, 1.052, 1.193, 0.939, 1.144, 1.034, 0.995, 0.88, 0.802, 0.646, 0.695, 0.65, 0.645, 0.634, 0.687, 0.612, 0.69, 0.602, 0.681, 0.723, 0.708, 0.729, 0.827, 0.736, 0.863, 1.132, 0.917, 1.1, 0.967, 0.902, 0.904, 0.947, 0.888, 0.95, 0.849, 0.923, 0.968, 0.841, 0.894, 0.731, 0.701, 0.722, 0.592, 0.786, 0.882, 0.747, 0.814, 0.734, 0.771, 0.795, 0.752, 0.708, 0.655, 0.696, 0.762, 0.634, 0.715, 0.672, 0.67, 0.682, 0.566, 0.558, 0.619, 0.559, 0.572, 0.587, 0.602, 0.711, 0.575, 0.647, 0.693, 0.616, 0.572, 0.497, 0.548, 0.588, 0.519, 0.563, 0.639, 0.617, 0.588, 0.488, 0.527, 0.566, 0.513, 0.489, 0.537, 0.643, 0.677, 0.653, 0.667, 0.649, 0.661, 0.76, 0.624, 0.694, 0.65, 0.593, 0.673, 0.577, 0.601, 0.734, 0.645, 0.649, 0.52, 0.542, 0.506, 0.667, 0.541, 0.612, 0.656, 0.649, 0.72, 0.791, 0.669, 0.799, 0.73, 0.779, 0.812, 0.724, 0.673, 0.657, 0.625, 0.729, 0.879, 0.864, 1.023, 0.845, 0.806, 0.797, 0.653, 0.587, 0.546, 0.54, 0.549, 0.419, 0.482, 0.442, 0.423, 0.432, 0.414, 0.422, 0.471, 0.373, 0.382, 0.499, 0.424, 0.482, 0.477, 0.439, 0.512, 0.612, 0.501, 0.596, 0.516, 0.503, 0.57, 0.586, 0.671, 0.839, 0.559, 0.596, 0.697, 0.624, 0.746, 0.561, 0.633, 0.634, 0.531, 0.641, 0.504, 0.59, 0.655, 0.568, 0.605, 0.562, 0.605, 0.694, 0.579, 0.599, 0.512, 0.568, 0.611, 0.533, 0.503, 0.606, 0.59, 0.707, 0.575, 0.552, 0.431, 0.38, 0.434, 0.444, 0.453, 0.505, 0.464, 0.562, 0.469, 0.419, 0.42, 0.382, 0.447, 0.505, 0.527, 0.554, 0.559, 0.582, 0.61, 0.44, 0.45, 0.425, 0.372, 0.507, 0.445, 0.389, 0.506, 0.479, 0.455, 0.598, 0.549, 0.536, 0.664, 0.56, 0.805, 0.724, 0.639, 0.717, 0.587, 0.663, 0.65, 0.56, 0.673, 0.674, 0.685, 0.851, 0.521, 0.581, 0.646, 0.572, 0.679, 0.6, 0.715, 0.707, 0.661, 0.654, 0.662, 0.571, 0.602, 0.612, 0.571, 0.59, 0.569, 0.556, 0.54, 0.569, 0.548, 0.669, 0.525, 0.501, 0.507, 0.456, 0.552, 0.684, 0.465, 0.595, 0.583, 0.546, 0.537, 0.557, 0.446, 0.641, 0.351, 0.45, 0.312, 0.316, 0.289, 0.358, 0.315, 0.409, 0.267, 0.284, 0.306, 0.262, 0.315, 0.347, 0.275, 0.356, 0.346, 0.336, 0.408, 0.454, 0.46, 0.569, 0.39, 0.438, 0.465, 0.391, 0.444, 0.55, 0.391, 0.48, 0.589, 0.501, 0.597, 0.454, 0.462, 0.528, 0.45, 0.409, 0.468, 0.335, 0.404, 0.331, 0.362, 0.434, 0.322, 0.388, 0.446, 0.349, 0.442, 0.407, 0.367, 0.441, 0.42, 0.554, 0.675, 0.462, 0.609, 0.697, 0.615, 0.765, 0.583, 0.653, 0.618, 0.498, 0.484, 0.518, 0.411, 0.569, 0.37, 0.445, 0.434, 0.442, 0.437, 0.4, 0.352, 0.486, 0.376, 0.385, 0.406, 0.344, 0.407, 0.447, 0.412]\n",
      "Val Error(all epochs): 7.996192455291748 \n",
      " [49.911, 49.837, 49.768, 49.725, 49.674, 49.626, 49.564, 49.493, 49.347, 49.25, 49.24, 49.119, 48.907, 48.85, 48.528, 48.235, 48.277, 47.568, 47.073, 46.753, 46.014, 45.158, 45.434, 44.556, 44.113, 43.335, 43.69, 42.565, 42.032, 42.444, 40.986, 39.273, 40.217, 37.86, 35.925, 36.099, 34.625, 34.084, 34.367, 33.925, 32.553, 32.292, 30.757, 30.008, 28.74, 26.05, 28.087, 23.86, 20.88, 21.455, 21.75, 22.682, 22.083, 20.088, 15.476, 19.218, 17.237, 17.078, 15.834, 14.423, 15.687, 14.587, 13.015, 11.514, 13.462, 10.572, 11.363, 10.102, 10.82, 10.198, 10.991, 9.628, 10.572, 10.492, 9.983, 9.379, 10.152, 11.064, 11.29, 12.479, 11.392, 10.984, 10.607, 11.478, 10.409, 10.652, 12.346, 9.119, 9.809, 11.234, 11.518, 11.425, 15.448, 15.468, 14.668, 14.326, 15.293, 12.893, 12.971, 13.02, 10.747, 12.306, 11.73, 11.054, 10.392, 9.569, 9.338, 10.986, 9.885, 8.843, 10.198, 9.298, 10.347, 10.561, 9.712, 9.515, 9.798, 9.808, 9.557, 10.811, 10.24, 9.327, 9.906, 8.759, 9.592, 9.02, 8.997, 9.155, 8.933, 8.76, 8.685, 9.002, 8.203, 8.637, 8.578, 8.108, 8.946, 7.996, 8.369, 8.002, 8.498, 8.081, 8.53, 8.263, 8.726, 8.694, 8.943, 8.424, 8.612, 8.6, 8.481, 9.713, 9.169, 8.905, 9.196, 9.081, 9.089, 9.681, 9.099, 9.501, 9.018, 9.54, 8.815, 9.381, 8.835, 8.975, 8.811, 8.93, 8.515, 9.405, 9.084, 8.648, 9.588, 8.447, 8.857, 9.158, 8.587, 8.905, 8.587, 8.415, 9.14, 8.348, 8.544, 8.565, 8.782, 8.706, 8.724, 8.38, 8.871, 8.111, 8.85, 8.354, 8.63, 8.9, 8.782, 9.469, 9.096, 9.107, 9.325, 8.684, 8.556, 8.705, 8.313, 8.784, 8.407, 8.749, 8.643, 8.778, 8.641, 8.82, 8.623, 8.967, 8.66, 8.746, 8.878, 8.587, 8.841, 8.806, 8.777, 8.914, 8.863, 8.847, 8.953, 8.645, 8.999, 8.777, 8.926, 8.942, 8.829, 9.091, 8.876, 8.849, 9.103, 8.481, 8.791, 8.328, 8.493, 9.361, 8.629, 9.676, 9.336, 9.313, 9.26, 9.258, 8.737, 9.558, 8.806, 8.973, 8.996, 9.136, 8.778, 9.424, 8.781, 9.087, 9.551, 8.932, 9.173, 9.096, 8.974, 8.666, 9.25, 8.21, 8.893, 8.503, 8.853, 8.308, 8.554, 8.251, 8.892, 8.499, 8.9, 9.104, 8.636, 9.372, 8.527, 8.217, 8.893, 8.8, 8.726, 8.577, 8.24, 8.709, 8.481, 8.611, 8.764, 8.587, 8.753, 8.715, 8.647, 8.802, 8.62, 8.836, 8.647, 8.712, 8.698, 9.005, 8.477, 8.843, 8.793, 8.832, 8.838, 9.327, 8.507, 9.162, 8.791, 8.553, 8.686, 8.669, 8.604, 8.68, 8.627, 9.151, 8.493, 8.751, 8.905, 8.674, 9.3, 9.001, 8.913, 9.487, 8.502, 9.4, 8.299, 8.506, 8.815, 8.491, 9.364, 8.96, 9.056, 9.823, 8.795, 10.115, 8.863, 9.453, 9.002, 8.685, 9.389, 8.661, 8.914, 9.096, 8.44, 9.046, 8.479, 8.735, 8.874, 8.606, 8.959, 8.51, 8.971, 8.596, 8.587, 8.753, 8.614, 9.01, 8.759, 9.095, 8.412, 8.883, 8.725, 8.644, 9.054, 8.786, 8.745, 9.183, 8.74, 8.691, 9.322, 8.596, 9.202, 8.853, 8.846, 9.127, 9.232, 8.682, 9.042, 8.436, 8.688, 8.456, 8.807, 8.669, 8.462, 8.352, 8.556, 8.594, 8.161, 8.388, 8.269, 8.515, 8.585, 8.9, 8.616, 8.857, 8.937, 8.761, 8.544, 8.595, 8.765, 8.369, 8.392, 8.545, 8.272, 8.825, 8.322, 8.799, 8.452, 8.872, 8.364, 9.024, 8.481, 8.721, 8.719, 8.931, 8.67, 9.114, 8.556, 9.05, 8.424, 8.727, 8.697, 8.699, 8.583, 8.768, 8.544, 8.744, 8.556, 8.727, 8.641, 8.641, 8.579, 8.67, 8.544, 8.756, 8.529, 8.785, 8.451, 8.724, 8.534, 8.542, 8.595, 8.272, 8.648, 8.375, 8.725, 8.354, 8.636, 8.395, 8.755, 8.392, 8.752, 8.578, 8.769, 8.614, 8.715, 8.841, 8.683, 8.745, 8.344, 8.647, 8.431, 8.621, 8.536, 8.52, 8.656, 8.386, 8.686, 8.517, 8.668, 8.525, 8.629, 8.637, 8.494, 8.553, 8.466, 9.08, 8.332, 8.802, 8.223, 8.279, 8.455, 8.081, 8.562, 8.682, 8.512, 8.916, 8.864, 8.671, 9.005, 8.62, 8.76, 8.677, 8.724, 8.645, 8.608, 8.702, 8.628, 8.678, 8.464, 8.74, 8.441, 8.789]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.039, 0.0, 0.0, 0.0, 0.006, 0.301, 0.063, 0.055, 0.141, 0.195, 0.377, 0.235, 0.287, 1.395, 2.527, 1.148, 0.982, 2.793, 3.333, 3.699, 1.767, 1.826, 1.474, 1.748, 2.943, 1.804, 1.518, 3.225, 4.254, 6.193, 6.489, 5.618, 5.038, 4.78, 5.713, 4.625, 5.483, 7.452, 3.586, 3.943, 6.121, 6.202, 6.484, 10.635, 10.525, 9.995, 9.953, 10.594, 8.168, 8.428, 8.109, 6.258, 7.515, 6.738, 6.336, 5.929, 4.586, 4.218, 6.486, 5.423, 4.193, 5.925, 4.328, 5.969, 5.684, 5.287, 4.113, 4.909, 4.625, 4.072, 5.665, 5.557, 4.036, 4.452, 4.468, 4.342, 5.021, 4.786, 4.991, 5.45, 4.277, 4.92, 4.115, 3.596, 3.842, 3.835, 3.59, 4.511, 4.447, 3.886, 3.98, 3.582, 3.642, 3.599, 3.291, 4.193, 3.993, 4.539, 4.49, 4.027, 4.511, 4.785, 6.476, 5.643, 5.311, 5.041, 4.878, 4.702, 5.285, 4.61, 4.989, 4.59, 4.962, 4.16, 4.924, 3.926, 4.338, 4.376, 3.803, 3.28, 4.948, 4.755, 4.335, 5.885, 4.192, 4.632, 5.183, 4.371, 4.125, 4.184, 3.794, 4.768, 2.736, 3.464, 3.56, 4.11, 3.693, 3.437, 3.484, 4.031, 3.085, 3.733, 3.29, 3.539, 4.413, 3.826, 5.149, 4.444, 4.693, 4.716, 3.587, 3.814, 3.929, 3.228, 3.832, 3.306, 3.407, 3.455, 3.256, 3.327, 3.465, 2.937, 3.651, 3.335, 3.22, 3.744, 3.351, 3.654, 3.416, 2.843, 3.194, 2.957, 2.76, 3.354, 2.89, 3.472, 3.054, 2.85, 3.262, 3.045, 3.452, 3.65, 3.403, 4.068, 3.358, 3.763, 4.034, 3.884, 5.171, 4.222, 5.471, 5.201, 5.022, 4.842, 4.725, 4.024, 5.233, 4.182, 4.703, 4.552, 4.904, 4.395, 5.035, 4.328, 4.531, 5.226, 4.368, 4.751, 4.906, 4.016, 4.525, 5.123, 3.921, 5.023, 4.833, 4.521, 3.331, 3.701, 3.788, 3.991, 3.857, 4.537, 5.448, 4.908, 6.226, 4.244, 4.257, 5.158, 4.712, 4.666, 3.906, 3.702, 3.923, 3.481, 3.523, 3.789, 3.323, 3.599, 3.504, 3.099, 3.67, 3.128, 3.223, 3.594, 3.59, 3.359, 4.367, 3.498, 4.135, 4.258, 3.86, 4.179, 4.964, 3.626, 4.812, 4.456, 4.015, 4.376, 3.612, 4.065, 4.471, 4.109, 4.494, 3.823, 4.251, 3.944, 3.774, 4.822, 4.177, 4.465, 5.25, 3.908, 4.85, 3.825, 3.819, 3.966, 3.467, 4.935, 4.429, 4.55, 5.589, 4.268, 5.829, 4.893, 5.402, 4.58, 4.175, 4.974, 4.017, 4.041, 4.558, 3.442, 4.296, 3.589, 3.723, 4.171, 3.925, 3.919, 3.776, 4.055, 3.755, 4.228, 4.088, 4.46, 4.545, 4.63, 4.641, 3.741, 4.076, 4.258, 3.673, 4.541, 4.135, 3.662, 4.689, 3.927, 3.444, 4.935, 3.722, 4.55, 4.37, 4.197, 4.976, 5.269, 4.484, 4.685, 3.911, 4.01, 3.886, 3.656, 4.145, 3.342, 3.62, 3.892, 3.372, 3.69, 3.284, 3.099, 4.303, 4.623, 4.631, 4.734, 5.12, 5.442, 4.477, 4.46, 4.649, 4.75, 3.688, 3.687, 4.28, 3.415, 4.278, 3.837, 3.987, 4.125, 4.253, 3.386, 4.661, 3.795, 4.26, 4.366, 4.577, 4.203, 5.166, 3.937, 4.792, 3.811, 4.136, 4.133, 3.994, 3.656, 3.989, 3.365, 3.813, 3.651, 3.64, 3.622, 3.583, 3.263, 3.622, 3.218, 3.476, 3.389, 3.267, 3.096, 3.583, 2.818, 3.47, 3.345, 3.145, 3.478, 3.658, 3.217, 3.439, 3.443, 3.159, 3.819, 3.095, 3.604, 4.018, 3.676, 3.603, 4.083, 3.748, 3.882, 3.448, 3.308, 3.592, 2.995, 3.384, 3.413, 3.107, 3.67, 3.222, 3.506, 3.441, 3.45, 3.177, 3.653, 3.056, 3.464, 3.56, 3.235, 4.629, 3.159, 4.048, 3.512, 3.666, 4.019, 3.766, 3.745, 4.556, 3.621, 4.573, 4.415, 4.223, 4.354, 4.155, 3.657, 4.108, 3.593, 3.857, 3.812, 3.611, 3.855, 3.689, 3.304, 3.784, 2.998, 3.757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 49.89140, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 49.89140 to 49.81335, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 49.81335 to 49.69915, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 49.69915 to 49.57973, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 49.57973 to 49.44012, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 49.44012 to 49.28733, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 49.28733 to 49.10886, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 49.10886 to 48.92942, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 48.92942 to 48.71952, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 48.71952 to 48.42645, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 48.42645 to 48.22115, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 48.22115 to 48.04231, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 48.04231 to 47.72842, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 47.72842 to 47.15336, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 47.15336 to 46.86356, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 46.86356 to 46.69649, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 46.69649 to 46.10732, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 46.10732 to 45.44033, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 45.44033 to 44.65135, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 44.65135 to 43.68529, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00021: val_mae improved from 43.68529 to 42.64607, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00022: val_mae improved from 42.64607 to 42.05619, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 42.05619 to 41.24828, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 41.24828 to 40.01276, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 40.01276 to 38.78807, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00026: val_mae improved from 38.78807 to 36.99876, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00027: val_mae improved from 36.99876 to 35.34751, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00028: val_mae improved from 35.34751 to 33.68552, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00029: val_mae improved from 33.68552 to 33.03200, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 33.03200 to 29.73963, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00031: val_mae improved from 29.73963 to 28.79359, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00032: val_mae improved from 28.79359 to 26.81016, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 26.81016 to 25.04523, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00034: val_mae improved from 25.04523 to 24.21693, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00035: val_mae improved from 24.21693 to 21.32646, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 21.32646\n",
      "\n",
      "Epoch 00037: val_mae improved from 21.32646 to 19.80577, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00038: val_mae improved from 19.80577 to 19.20081, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00039: val_mae improved from 19.20081 to 19.18314, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_mae improved from 19.18314 to 18.46161, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 18.46161\n",
      "\n",
      "Epoch 00042: val_mae improved from 18.46161 to 17.31263, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00043: val_mae improved from 17.31263 to 17.06865, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00044: val_mae improved from 17.06865 to 15.86180, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00045: val_mae improved from 15.86180 to 15.17466, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00046: val_mae improved from 15.17466 to 14.49292, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 14.49292\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 14.49292\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 14.49292\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 14.49292\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 14.49292\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 14.49292\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 14.49292\n",
      "\n",
      "Epoch 00054: val_mae improved from 14.49292 to 14.07261, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 14.07261\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 14.07261\n",
      "\n",
      "Epoch 00057: val_mae improved from 14.07261 to 13.71680, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 13.71680\n",
      "\n",
      "Epoch 00059: val_mae improved from 13.71680 to 13.21095, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 13.21095\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 13.21095\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 13.21095\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 13.21095\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 13.21095\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 13.21095\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 13.21095\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 13.21095\n",
      "\n",
      "Epoch 00068: val_mae improved from 13.21095 to 12.70946, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 12.70946\n",
      "\n",
      "Epoch 00070: val_mae improved from 12.70946 to 12.22001, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 12.22001\n",
      "\n",
      "Epoch 00072: val_mae improved from 12.22001 to 11.83533, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 11.83533\n",
      "\n",
      "Epoch 00074: val_mae improved from 11.83533 to 11.27705, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 11.27705\n",
      "\n",
      "Epoch 00076: val_mae improved from 11.27705 to 9.85275, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 9.85275\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 9.85275\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 9.85275\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 9.85275\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 9.85275\n",
      "\n",
      "Epoch 00082: val_mae improved from 9.85275 to 9.26261, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00083: val_mae improved from 9.26261 to 6.92543, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 6.92543\n",
      "\n",
      "Epoch 00085: val_mae improved from 6.92543 to 6.13252, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00102: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00111: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00132: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 6.13252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00146: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00198: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00290: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 6.13252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00307: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00327: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00378: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00445: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 6.13252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00468: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 6.13252\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 6.13252\n",
      "\n",
      "Lambda: 0.1 , Time: 0:04:03\n",
      "Train Error(all epochs): 0.9247928857803345 \n",
      " [49.048, 48.945, 48.864, 48.777, 48.676, 48.556, 48.418, 48.252, 48.058, 47.831, 47.568, 47.269, 46.921, 46.519, 46.071, 45.546, 44.974, 44.299, 43.586, 42.827, 41.998, 41.134, 40.212, 39.254, 38.248, 37.204, 36.118, 35.033, 33.853, 32.713, 31.446, 30.205, 28.923, 27.611, 26.336, 24.995, 23.676, 22.366, 21.061, 19.774, 18.519, 17.248, 16.09, 14.898, 13.794, 12.722, 11.725, 10.875, 9.941, 9.203, 8.636, 7.987, 7.48, 6.924, 6.479, 6.173, 5.899, 5.776, 5.488, 5.434, 5.382, 5.161, 5.176, 5.141, 5.084, 4.957, 4.83, 4.745, 4.748, 4.789, 4.729, 4.554, 4.582, 4.477, 4.442, 4.442, 4.452, 4.369, 4.329, 4.394, 4.468, 4.603, 4.661, 4.97, 4.827, 4.69, 4.514, 4.294, 4.108, 3.974, 3.849, 3.807, 3.68, 3.831, 3.772, 3.87, 3.898, 4.128, 3.939, 3.823, 3.913, 3.74, 3.712, 3.989, 4.4, 4.19, 3.92, 3.561, 3.477, 3.488, 3.358, 3.397, 3.377, 3.481, 3.52, 3.451, 3.397, 3.407, 3.536, 3.3, 3.266, 3.152, 3.036, 3.01, 3.024, 3.03, 3.482, 3.625, 3.346, 3.489, 3.413, 3.295, 3.096, 2.856, 2.809, 2.698, 2.732, 2.788, 2.85, 2.886, 2.954, 3.1, 2.88, 2.752, 2.397, 2.539, 2.593, 2.669, 2.986, 3.316, 2.827, 3.038, 2.88, 2.773, 2.931, 2.798, 2.821, 2.849, 2.8, 2.691, 2.431, 2.337, 2.288, 2.22, 2.198, 2.297, 2.211, 2.316, 2.345, 2.319, 2.283, 2.177, 2.154, 2.265, 2.136, 2.378, 2.697, 2.602, 2.651, 2.551, 2.369, 2.362, 2.086, 2.048, 1.947, 2.034, 2.056, 2.032, 2.021, 1.876, 1.993, 2.074, 2.035, 1.848, 1.718, 1.646, 1.762, 1.854, 1.95, 1.919, 1.891, 1.779, 1.808, 1.708, 1.813, 1.831, 1.842, 1.725, 1.735, 1.765, 1.803, 1.912, 1.882, 1.988, 2.008, 1.916, 1.95, 1.98, 1.864, 1.871, 1.909, 1.935, 1.683, 1.724, 1.652, 1.53, 1.434, 1.291, 1.298, 1.298, 1.295, 1.393, 1.469, 1.625, 1.753, 1.628, 1.637, 1.732, 1.812, 1.783, 1.993, 1.893, 1.969, 1.972, 2.267, 2.272, 2.144, 2.115, 2.134, 2.129, 2.143, 1.993, 1.965, 1.877, 1.799, 1.717, 1.508, 1.404, 1.336, 1.186, 1.229, 1.219, 1.319, 1.421, 1.556, 1.591, 1.632, 1.57, 1.541, 1.384, 1.318, 1.178, 1.126, 1.128, 1.145, 1.189, 1.361, 1.459, 1.518, 1.599, 1.701, 1.75, 1.724, 1.541, 1.474, 1.505, 1.502, 1.638, 1.679, 1.678, 1.576, 1.586, 1.478, 1.365, 1.29, 1.258, 1.332, 1.474, 1.63, 1.607, 1.554, 1.468, 1.459, 1.411, 1.585, 1.71, 1.753, 1.511, 1.463, 1.432, 1.495, 1.676, 1.666, 1.673, 1.478, 1.577, 1.746, 1.703, 1.551, 1.59, 1.751, 1.962, 1.873, 1.724, 1.555, 1.621, 1.592, 1.403, 1.255, 1.397, 1.41, 1.443, 1.406, 1.391, 1.452, 1.421, 1.467, 1.452, 1.353, 1.39, 1.522, 1.462, 1.598, 1.466, 1.446, 1.416, 1.427, 1.542, 1.7, 1.785, 1.727, 1.496, 1.48, 1.523, 1.649, 1.671, 1.724, 1.771, 1.581, 1.704, 1.698, 1.734, 1.585, 1.616, 1.711, 1.777, 1.743, 1.697, 1.666, 1.624, 1.618, 1.761, 1.879, 1.723, 1.843, 1.78, 1.673, 1.58, 1.472, 1.428, 1.233, 1.167, 1.249, 1.239, 1.239, 1.175, 1.198, 1.237, 1.26, 1.28, 1.267, 1.196, 1.164, 1.185, 1.279, 1.373, 1.36, 1.405, 1.486, 1.504, 1.615, 1.728, 1.658, 1.706, 1.581, 1.713, 1.903, 1.776, 1.655, 1.805, 1.726, 1.91, 1.819, 1.589, 1.621, 1.675, 1.739, 1.534, 1.511, 1.374, 1.18, 1.173, 1.153, 1.209, 1.266, 1.329, 1.363, 1.304, 1.273, 1.325, 1.376, 1.383, 1.27, 1.195, 1.176, 1.188, 1.194, 1.239, 1.177, 1.087, 1.089, 1.133, 1.255, 1.39, 1.401, 1.454, 1.359, 1.443, 1.457, 1.49, 1.583, 1.661, 1.77, 1.595, 1.541, 1.493, 1.623, 1.805, 1.563, 1.535, 1.634, 1.72, 1.816, 1.809, 1.938, 1.795, 1.663, 1.603, 1.524, 1.454, 1.354, 1.426, 1.427, 1.458, 1.495, 1.424, 1.405, 1.424, 1.442, 1.518, 1.523, 1.627, 1.688, 1.838, 1.968, 1.983, 1.749, 1.668, 1.755, 1.847, 2.032, 2.006, 1.913, 1.747, 1.714, 1.476, 1.257, 1.083, 1.014, 0.925]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003, 0.005, 0.01, 0.005, 0.017, 0.015, 0.029, 0.042, 0.055, 0.074, 0.101, 0.173, 0.18, 0.274, 0.343, 0.458, 0.517, 0.6, 0.663, 0.762, 0.887, 1.013, 1.088, 1.205, 1.343, 1.387, 1.445, 1.594, 1.635, 1.613, 1.686, 1.58, 1.763, 1.707, 1.792, 1.709, 1.712, 1.78, 1.709, 1.714, 1.748, 1.743, 1.697, 1.699, 1.847, 1.862, 1.91, 2.076, 2.056, 1.978, 1.936, 1.747, 1.658, 1.534, 1.422, 1.52, 1.344, 1.486, 1.445, 1.57, 1.485, 1.859, 1.609, 1.482, 1.566, 1.434, 1.533, 1.542, 1.97, 1.906, 1.565, 1.432, 1.309, 1.334, 1.338, 1.346, 1.317, 1.49, 1.469, 1.372, 1.385, 1.27, 1.57, 1.292, 1.314, 1.249, 1.159, 1.19, 1.205, 1.215, 1.38, 1.689, 1.278, 1.511, 1.324, 1.368, 1.224, 1.061, 1.242, 0.979, 1.063, 1.161, 1.173, 1.191, 1.235, 1.341, 1.118, 1.214, 0.832, 1.039, 1.109, 1.001, 1.366, 1.523, 1.04, 1.398, 1.21, 1.092, 1.326, 1.079, 1.244, 1.164, 1.22, 1.127, 0.939, 0.957, 0.919, 0.92, 0.887, 0.94, 0.893, 0.917, 1.12, 0.878, 0.985, 0.965, 0.779, 1.13, 0.905, 0.931, 1.372, 0.938, 1.325, 1.147, 0.836, 1.148, 0.765, 0.916, 0.798, 0.823, 0.967, 0.732, 1.017, 0.744, 0.87, 0.926, 0.861, 0.777, 0.692, 0.738, 0.705, 0.889, 0.823, 0.852, 0.911, 0.675, 0.921, 0.657, 0.819, 0.815, 0.723, 0.821, 0.729, 0.761, 0.752, 0.958, 0.702, 1.021, 0.791, 0.953, 0.862, 0.845, 0.881, 0.815, 0.805, 0.979, 0.716, 0.768, 0.734, 0.635, 0.673, 0.485, 0.669, 0.472, 0.652, 0.576, 0.712, 0.639, 0.903, 0.639, 0.834, 0.742, 0.831, 0.822, 1.029, 0.691, 1.15, 0.812, 1.013, 1.23, 0.795, 1.177, 0.875, 0.924, 1.073, 0.843, 0.927, 0.876, 0.826, 0.738, 0.684, 0.664, 0.544, 0.574, 0.487, 0.603, 0.56, 0.648, 0.778, 0.603, 0.939, 0.526, 0.898, 0.496, 0.679, 0.508, 0.507, 0.52, 0.538, 0.518, 0.691, 0.631, 0.817, 0.615, 0.93, 0.748, 0.867, 0.682, 0.679, 0.728, 0.641, 0.804, 0.719, 0.914, 0.609, 0.87, 0.569, 0.693, 0.565, 0.611, 0.585, 0.749, 0.677, 0.77, 0.738, 0.682, 0.702, 0.592, 0.811, 0.754, 0.831, 0.712, 0.67, 0.595, 0.82, 0.734, 0.76, 0.831, 0.625, 0.781, 0.833, 0.722, 0.776, 0.779, 0.662, 1.171, 0.654, 0.96, 0.783, 0.552, 0.958, 0.467, 0.632, 0.693, 0.545, 0.839, 0.465, 0.784, 0.55, 0.76, 0.572, 0.802, 0.571, 0.669, 0.701, 0.711, 0.727, 0.716, 0.686, 0.614, 0.777, 0.59, 0.904, 0.804, 0.783, 0.809, 0.515, 0.845, 0.665, 0.879, 0.676, 0.974, 0.644, 0.901, 0.725, 0.808, 0.811, 0.64, 0.911, 0.773, 0.79, 0.892, 0.621, 0.909, 0.667, 0.919, 0.776, 0.946, 0.73, 1.014, 0.573, 0.937, 0.572, 0.714, 0.551, 0.511, 0.668, 0.424, 0.725, 0.46, 0.561, 0.648, 0.469, 0.728, 0.5, 0.581, 0.508, 0.59, 0.532, 0.73, 0.592, 0.632, 0.772, 0.618, 0.817, 0.869, 0.664, 0.871, 0.748, 0.703, 1.03, 0.713, 0.84, 0.872, 0.804, 0.884, 0.856, 0.742, 0.843, 0.603, 1.028, 0.552, 0.765, 0.628, 0.525, 0.568, 0.534, 0.554, 0.633, 0.57, 0.676, 0.59, 0.609, 0.579, 0.741, 0.486, 0.798, 0.392, 0.617, 0.555, 0.559, 0.607, 0.502, 0.555, 0.465, 0.589, 0.531, 0.756, 0.564, 0.78, 0.603, 0.615, 0.833, 0.488, 0.99, 0.519, 1.119, 0.58, 0.812, 0.652, 0.726, 0.914, 0.665, 0.731, 0.846, 0.598, 1.093, 0.656, 0.981, 0.837, 0.794, 0.821, 0.588, 0.796, 0.6, 0.618, 0.799, 0.522, 0.861, 0.543, 0.707, 0.691, 0.603, 0.786, 0.638, 0.819, 0.784, 0.783, 1.082, 0.734, 1.033, 0.603, 0.96, 0.746, 1.131, 0.798, 1.058, 0.718, 0.833, 0.638, 0.648, 0.511, 0.421, 0.469]\n",
      "Val Error(all epochs): 6.1325201988220215 \n",
      " [49.891, 49.813, 49.699, 49.58, 49.44, 49.287, 49.109, 48.929, 48.72, 48.426, 48.221, 48.042, 47.728, 47.153, 46.864, 46.696, 46.107, 45.44, 44.651, 43.685, 42.646, 42.056, 41.248, 40.013, 38.788, 36.999, 35.348, 33.686, 33.032, 29.74, 28.794, 26.81, 25.045, 24.217, 21.326, 21.403, 19.806, 19.201, 19.183, 18.462, 18.539, 17.313, 17.069, 15.862, 15.175, 14.493, 14.667, 14.841, 14.991, 14.754, 15.131, 14.595, 14.673, 14.073, 14.287, 14.429, 13.717, 13.901, 13.211, 13.264, 13.277, 13.758, 13.895, 14.842, 13.935, 14.826, 13.959, 12.709, 13.209, 12.22, 12.973, 11.835, 12.862, 11.277, 11.572, 9.853, 10.373, 10.09, 10.698, 10.813, 11.319, 9.263, 6.925, 7.241, 6.133, 6.691, 6.248, 7.246, 7.546, 8.207, 8.364, 8.575, 9.004, 8.513, 8.586, 8.651, 7.867, 6.762, 9.294, 8.17, 7.706, 7.752, 7.74, 6.937, 7.375, 6.78, 7.787, 6.55, 7.824, 7.298, 7.4, 7.535, 7.382, 6.429, 8.024, 6.193, 7.474, 6.837, 6.665, 7.543, 6.442, 6.924, 8.17, 7.256, 7.031, 7.606, 7.951, 6.467, 7.759, 8.618, 6.951, 7.008, 8.869, 9.949, 9.152, 9.289, 8.838, 7.284, 9.073, 7.296, 8.351, 9.999, 7.616, 7.292, 8.429, 6.883, 7.412, 8.234, 6.999, 7.044, 8.941, 7.437, 10.529, 7.977, 7.683, 7.24, 7.117, 7.206, 8.987, 7.364, 9.35, 7.978, 8.322, 7.624, 7.04, 7.822, 7.489, 7.282, 7.193, 6.836, 7.677, 7.204, 8.196, 7.94, 6.722, 7.703, 6.497, 7.049, 7.43, 6.639, 8.223, 6.667, 7.923, 7.963, 7.782, 8.985, 7.26, 7.527, 6.243, 7.561, 6.629, 7.38, 6.545, 7.153, 7.013, 7.082, 7.295, 7.079, 7.031, 7.086, 6.873, 7.781, 7.05, 7.975, 6.733, 7.458, 6.709, 7.495, 6.903, 7.127, 6.951, 6.803, 7.253, 6.982, 7.203, 7.294, 7.015, 7.22, 7.262, 8.658, 7.356, 6.773, 7.28, 6.734, 7.423, 6.53, 7.387, 7.062, 6.942, 7.404, 7.056, 7.518, 8.141, 7.304, 7.882, 7.459, 7.706, 6.845, 7.898, 7.007, 7.721, 6.789, 7.226, 7.711, 7.325, 9.827, 8.928, 7.346, 8.094, 8.134, 6.711, 7.252, 7.052, 6.904, 8.701, 7.077, 8.341, 7.387, 8.461, 8.162, 8.158, 8.562, 7.422, 8.548, 7.392, 7.653, 7.297, 7.083, 7.877, 6.961, 7.972, 7.1, 7.351, 7.21, 7.267, 7.475, 7.402, 7.19, 7.345, 7.213, 7.829, 10.022, 8.375, 9.88, 7.991, 8.836, 7.666, 7.828, 6.833, 7.34, 7.32, 7.272, 7.576, 7.299, 8.177, 8.023, 7.469, 7.193, 7.398, 8.323, 7.228, 7.332, 7.545, 6.94, 7.949, 6.915, 7.87, 6.874, 7.268, 7.88, 8.306, 8.041, 8.782, 7.301, 7.013, 7.494, 8.416, 7.483, 7.671, 9.135, 8.616, 10.572, 8.542, 6.929, 7.639, 7.825, 7.421, 8.076, 8.268, 7.656, 7.594, 7.253, 7.426, 6.925, 8.044, 7.293, 7.875, 7.236, 7.51, 7.327, 7.438, 7.586, 7.418, 7.596, 7.802, 8.912, 7.797, 7.737, 8.323, 7.556, 7.142, 7.56, 7.491, 7.657, 6.85, 6.926, 7.171, 7.46, 7.204, 7.074, 7.896, 7.99, 6.661, 7.808, 7.089, 6.979, 9.003, 6.814, 7.673, 7.12, 8.026, 6.74, 7.414, 7.424, 7.539, 8.256, 7.528, 8.398, 7.14, 7.22, 7.159, 7.469, 7.268, 7.103, 7.314, 7.17, 7.357, 7.234, 7.732, 7.14, 8.369, 7.78, 7.551, 7.801, 7.885, 6.831, 8.98, 7.079, 8.229, 7.095, 7.127, 7.336, 6.906, 7.065, 7.202, 7.254, 7.133, 8.237, 10.235, 10.701, 8.384, 8.305, 8.013, 8.199, 8.298, 7.288, 7.561, 8.469, 7.215, 7.296, 7.432, 7.811, 7.733, 7.527, 7.214, 7.647, 7.565, 7.582, 8.425, 7.928, 7.796, 9.417, 7.316, 7.636, 7.39, 7.25, 7.857, 7.664, 8.602, 8.192, 7.97, 7.962, 7.495, 7.745, 7.288, 7.3, 7.65, 7.312, 7.641, 7.331, 7.095, 7.596, 7.311, 7.626, 7.105, 8.315, 7.679, 7.562, 8.438, 7.652, 8.084, 8.685, 8.629, 10.107, 7.363, 8.268, 7.786, 7.434, 7.786, 7.277, 7.659, 8.336, 7.333, 8.399, 8.509, 7.682, 8.066, 7.862, 8.602, 8.536, 6.823, 7.386, 6.85, 7.79, 6.776, 7.612, 6.784, 8.002, 9.043, 7.564, 8.855, 15.784, 8.57, 10.73, 7.96, 7.943, 8.069, 7.686, 7.454, 7.597]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.005, 0.071, 0.083, 0.183, 0.198, 0.247, 0.282, 0.308, 0.407, 0.578, 0.82, 1.113, 2.071, 2.823, 3.994, 4.488, 4.463, 4.843, 5.183, 5.109, 4.742, 4.603, 4.832, 3.627, 4.136, 3.814, 3.254, 4.402, 5.099, 4.795, 6.9, 5.431, 6.876, 5.957, 5.292, 6.395, 5.629, 6.801, 4.888, 6.177, 5.002, 4.418, 4.353, 3.792, 4.494, 4.448, 4.94, 5.838, 4.16, 3.37, 4.481, 2.574, 2.617, 2.418, 2.33, 2.151, 2.626, 2.079, 2.836, 1.689, 2.679, 2.562, 1.791, 1.597, 2.574, 0.992, 1.407, 1.709, 1.662, 1.949, 2.66, 2.042, 4.298, 2.025, 2.516, 1.489, 1.764, 1.998, 1.547, 2.889, 2.717, 1.794, 2.461, 2.2, 2.486, 2.698, 2.042, 2.541, 2.661, 1.402, 2.324, 2.424, 1.964, 4.745, 2.597, 1.834, 5.441, 3.326, 2.928, 1.441, 0.859, 0.988, 1.009, 1.058, 1.836, 1.087, 1.874, 1.615, 0.989, 1.822, 1.668, 1.469, 2.028, 1.848, 1.404, 3.878, 1.891, 1.098, 1.637, 0.897, 1.283, 1.768, 2.398, 3.583, 2.782, 6.083, 3.717, 5.754, 4.737, 5.209, 4.272, 3.099, 4.008, 3.257, 3.449, 3.357, 2.358, 4.047, 2.636, 5.083, 4.514, 2.801, 4.793, 2.836, 2.733, 3.861, 3.315, 5.587, 3.726, 4.187, 5.141, 3.952, 6.234, 3.639, 4.187, 2.781, 2.939, 4.458, 2.953, 3.707, 2.277, 2.472, 2.163, 2.608, 2.913, 3.123, 3.469, 2.182, 1.917, 2.218, 2.259, 2.908, 2.34, 2.905, 3.337, 2.646, 3.532, 3.017, 2.364, 2.929, 2.346, 2.703, 3.255, 2.332, 2.559, 2.881, 1.026, 3.029, 2.694, 2.345, 2.95, 2.487, 2.644, 1.807, 2.807, 1.963, 3.081, 2.746, 4.039, 3.631, 3.717, 1.894, 1.876, 2.703, 2.159, 3.673, 3.353, 1.795, 3.189, 2.656, 1.651, 2.157, 1.001, 0.945, 1.698, 1.295, 4.972, 2.51, 2.69, 2.286, 1.918, 5.552, 2.868, 5.053, 4.117, 5.353, 4.67, 4.718, 4.64, 3.407, 4.85, 3.237, 4.244, 2.799, 2.382, 4.434, 2.957, 4.428, 2.988, 2.923, 2.735, 2.73, 2.37, 3.181, 2.573, 2.813, 1.872, 1.851, 0.823, 1.287, 0.854, 1.501, 1.305, 1.741, 2.537, 2.652, 2.185, 3.513, 2.267, 2.746, 3.41, 3.785, 4.262, 3.038, 3.455, 3.511, 1.039, 2.847, 2.164, 2.648, 1.946, 2.104, 1.941, 2.207, 2.425, 2.262, 3.767, 5.38, 4.275, 6.284, 2.783, 2.758, 2.474, 1.259, 2.098, 1.833, 0.77, 1.099, 0.534, 1.074, 2.901, 1.96, 4.627, 4.152, 4.512, 4.904, 3.073, 3.763, 2.653, 2.596, 2.455, 1.733, 1.999, 1.783, 2.268, 2.475, 1.917, 2.62, 1.731, 3.669, 3.474, 4.147, 5.558, 4.202, 3.743, 5.468, 1.931, 3.773, 2.079, 2.126, 1.648, 2.928, 2.408, 3.613, 1.529, 2.954, 2.804, 4.871, 4.396, 3.037, 4.527, 3.604, 3.421, 6.317, 2.45, 3.504, 1.694, 1.974, 4.116, 2.195, 1.786, 2.357, 1.157, 1.907, 1.053, 2.53, 1.961, 3.041, 4.131, 2.029, 3.193, 2.076, 2.333, 2.665, 2.12, 1.685, 2.32, 1.29, 1.343, 1.986, 1.53, 1.716, 2.194, 0.921, 2.224, 1.264, 2.647, 3.307, 2.688, 3.381, 3.31, 2.041, 2.834, 1.742, 1.629, 0.356, 0.725, 0.907, 1.551, 1.04, 5.761, 1.061, 2.866, 1.68, 0.972, 2.15, 2.166, 1.71, 1.782, 1.482, 2.896, 1.921, 3.486, 1.51, 3.33, 1.063, 1.747, 1.73, 0.75, 2.092, 1.903, 2.419, 2.332, 1.635, 1.933, 1.018, 1.469, 1.475, 1.639, 2.446, 1.648, 3.164, 2.498, 1.7, 3.458, 1.672, 3.963, 2.198, 3.069, 1.745, 2.713, 3.046, 1.061, 4.053, 2.125, 1.126, 1.979, 1.652, 1.055, 1.137, 0.701, 3.251, 0.954, 4.036, 2.296, 3.845, 3.172, 1.736, 5.369, 2.872, 5.212, 5.337, 3.879, 4.661, 4.039, 5.331, 5.44, 2.665, 3.646, 2.537, 4.232, 3.066, 3.347, 3.51, 1.482, 0.97, 1.633, 1.345, 0.522, 1.829, 0.765, 1.925, 1.724, 1.713, 2.502, 2.241, 2.684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_mae improved from inf to 49.84035, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00002: val_mae improved from 49.84035 to 49.75853, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00003: val_mae improved from 49.75853 to 49.67958, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00004: val_mae improved from 49.67958 to 49.56898, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00005: val_mae improved from 49.56898 to 49.45268, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00006: val_mae improved from 49.45268 to 49.30288, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00007: val_mae improved from 49.30288 to 49.17366, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00008: val_mae improved from 49.17366 to 48.94967, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00009: val_mae improved from 48.94967 to 48.77551, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00010: val_mae improved from 48.77551 to 48.62864, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00011: val_mae improved from 48.62864 to 48.27556, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00012: val_mae improved from 48.27556 to 48.03526, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00013: val_mae improved from 48.03526 to 47.67543, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00014: val_mae improved from 47.67543 to 47.12713, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00015: val_mae improved from 47.12713 to 46.79959, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00016: val_mae improved from 46.79959 to 46.26612, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00017: val_mae improved from 46.26612 to 45.77939, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00018: val_mae improved from 45.77939 to 45.63549, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00019: val_mae improved from 45.63549 to 44.99573, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00020: val_mae improved from 44.99573 to 44.42029, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 44.42029\n",
      "\n",
      "Epoch 00022: val_mae improved from 44.42029 to 43.77295, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00023: val_mae improved from 43.77295 to 43.01117, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00024: val_mae improved from 43.01117 to 42.62875, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00025: val_mae improved from 42.62875 to 41.37419, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 41.37419\n",
      "\n",
      "Epoch 00027: val_mae improved from 41.37419 to 39.55077, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 39.55077\n",
      "\n",
      "Epoch 00029: val_mae improved from 39.55077 to 39.10276, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00030: val_mae improved from 39.10276 to 37.17622, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 37.17622\n",
      "\n",
      "Epoch 00032: val_mae improved from 37.17622 to 35.56140, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00033: val_mae improved from 35.56140 to 35.50385, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00034: val_mae improved from 35.50385 to 35.46780, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 35.46780\n",
      "\n",
      "Epoch 00036: val_mae improved from 35.46780 to 32.71587, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00037: val_mae improved from 32.71587 to 31.38438, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 31.38438\n",
      "\n",
      "Epoch 00039: val_mae improved from 31.38438 to 29.00182, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00040: val_mae improved from 29.00182 to 28.09782, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 28.09782\n",
      "\n",
      "Epoch 00042: val_mae improved from 28.09782 to 26.81778, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00043: val_mae improved from 26.81778 to 24.37525, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00044: val_mae improved from 24.37525 to 23.23348, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00045: val_mae improved from 23.23348 to 21.79245, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00046: val_mae improved from 21.79245 to 19.52263, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00047: val_mae improved from 19.52263 to 18.28354, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00048: val_mae improved from 18.28354 to 17.45543, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 17.45543\n",
      "\n",
      "Epoch 00050: val_mae improved from 17.45543 to 16.86404, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00051: val_mae improved from 16.86404 to 16.10200, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00052: val_mae improved from 16.10200 to 13.89006, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00053: val_mae improved from 13.89006 to 13.42501, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 13.42501\n",
      "\n",
      "Epoch 00055: val_mae improved from 13.42501 to 12.90683, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00056: val_mae improved from 12.90683 to 11.32388, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00057: val_mae improved from 11.32388 to 11.22553, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 11.22553\n",
      "\n",
      "Epoch 00059: val_mae improved from 11.22553 to 10.80729, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 10.80729\n",
      "\n",
      "Epoch 00081: val_mae improved from 10.80729 to 10.57950, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00082: val_mae improved from 10.57950 to 10.32519, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00083: val_mae improved from 10.32519 to 9.67613, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 9.67613\n",
      "\n",
      "Epoch 00085: val_mae improved from 9.67613 to 9.46727, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00086: val_mae improved from 9.46727 to 9.37667, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00087: val_mae improved from 9.37667 to 9.28586, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00088: val_mae improved from 9.28586 to 9.17741, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00089: val_mae improved from 9.17741 to 9.16769, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 9.16769\n",
      "\n",
      "Epoch 00091: val_mae improved from 9.16769 to 8.23105, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 8.23105\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 8.23105\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 8.23105\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 8.23105\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 8.23105\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 8.23105\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 8.23105\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 8.23105\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 8.23105\n",
      "\n",
      "Epoch 00101: val_mae did not improve from 8.23105\n",
      "\n",
      "Epoch 00102: val_mae improved from 8.23105 to 7.93819, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00103: val_mae did not improve from 7.93819\n",
      "\n",
      "Epoch 00104: val_mae did not improve from 7.93819\n",
      "\n",
      "Epoch 00105: val_mae did not improve from 7.93819\n",
      "\n",
      "Epoch 00106: val_mae did not improve from 7.93819\n",
      "\n",
      "Epoch 00107: val_mae did not improve from 7.93819\n",
      "\n",
      "Epoch 00108: val_mae did not improve from 7.93819\n",
      "\n",
      "Epoch 00109: val_mae did not improve from 7.93819\n",
      "\n",
      "Epoch 00110: val_mae did not improve from 7.93819\n",
      "\n",
      "Epoch 00111: val_mae improved from 7.93819 to 7.76425, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00112: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00113: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00114: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00115: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00116: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00117: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00118: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00119: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00120: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00121: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00122: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00123: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00124: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00125: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00126: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00127: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00128: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00129: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00130: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00131: val_mae did not improve from 7.76425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00132: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00133: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00134: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00135: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00136: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00137: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00138: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00139: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00140: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00141: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00142: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00143: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00144: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00145: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00146: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00147: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00148: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00149: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00150: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00151: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00152: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00153: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00154: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00155: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00156: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00157: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00158: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00159: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00160: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00161: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00162: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00163: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00164: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00165: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00166: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00167: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00168: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00169: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00170: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00171: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00172: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00173: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00174: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00175: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00176: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00177: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00178: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00179: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00180: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00181: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00182: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00183: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00184: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00185: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00186: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00187: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00188: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00189: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00190: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00191: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00192: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00193: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00194: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00195: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00196: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00197: val_mae did not improve from 7.76425\n",
      "\n",
      "Epoch 00198: val_mae improved from 7.76425 to 7.72710, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00199: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00200: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00201: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00202: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00203: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00204: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00205: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00206: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00207: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00208: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00209: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00210: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00211: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00212: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00213: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00214: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00215: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00216: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00217: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00218: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00219: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00220: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00221: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00222: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00223: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00224: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00225: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00226: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00227: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00228: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00229: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00230: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00231: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00232: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00233: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00234: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00235: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00236: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00237: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00238: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00239: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00240: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00241: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00242: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00243: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00244: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00245: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00246: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00247: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00248: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00249: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00250: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00251: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00252: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00253: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00254: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00255: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00256: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00257: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00258: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00259: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00260: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00261: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00262: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00263: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00264: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00265: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00266: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00267: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00268: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00269: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00270: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00271: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00272: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00273: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00274: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00275: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00276: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00277: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00278: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00279: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00280: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00281: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00282: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00283: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00284: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00285: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00286: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00287: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00288: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00289: val_mae did not improve from 7.72710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00290: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00291: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00292: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00293: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00294: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00295: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00296: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00297: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00298: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00299: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00300: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00301: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00302: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00303: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00304: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00305: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00306: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00307: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00308: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00309: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00310: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00311: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00312: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00313: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00314: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00315: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00316: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00317: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00318: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00319: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00320: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00321: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00322: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00323: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00324: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00325: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00326: val_mae did not improve from 7.72710\n",
      "\n",
      "Epoch 00327: val_mae improved from 7.72710 to 7.69351, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00328: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00329: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00330: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00331: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00332: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00333: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00334: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00335: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00336: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00337: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00338: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00339: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00340: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00341: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00342: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00343: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00344: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00345: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00346: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00347: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00348: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00349: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00350: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00351: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00352: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00353: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00354: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00355: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00356: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00357: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00358: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00359: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00360: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00361: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00362: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00363: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00364: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00365: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00366: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00367: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00368: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00369: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00370: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00371: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00372: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00373: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00374: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00375: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00376: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00377: val_mae did not improve from 7.69351\n",
      "\n",
      "Epoch 00378: val_mae improved from 7.69351 to 7.65292, saving model to ML/data/pictures_100_100/testbed/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/17sensors/models/519/fold_9/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00379: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00380: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00381: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00382: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00383: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00384: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00385: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00386: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00387: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00388: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00389: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00390: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00391: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00392: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00393: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00394: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00395: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00396: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00397: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00398: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00399: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00400: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00401: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00402: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00403: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00404: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00405: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00406: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00407: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00408: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00409: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00410: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00411: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00412: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00413: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00414: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00415: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00416: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00417: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00418: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00419: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00420: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00421: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00422: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00423: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00424: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00425: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00426: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00427: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00428: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00429: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00430: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00431: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00432: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00433: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00434: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00435: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00436: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00437: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00438: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00439: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00440: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00441: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00442: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00443: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00444: val_mae did not improve from 7.65292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00445: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00446: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00447: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00448: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00449: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00450: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00451: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00452: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00453: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00454: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00455: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00456: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00457: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00458: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00459: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00460: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00461: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00462: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00463: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00464: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00465: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00466: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00467: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00468: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00469: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00470: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00471: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00472: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00473: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00474: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00475: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00476: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00477: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00478: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00479: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00480: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00481: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00482: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00483: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00484: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00485: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00486: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00487: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00488: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00489: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00490: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00491: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00492: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00493: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00494: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00495: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00496: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00497: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00498: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00499: val_mae did not improve from 7.65292\n",
      "\n",
      "Epoch 00500: val_mae did not improve from 7.65292\n",
      "\n",
      "Lambda: 1 , Time: 0:04:01\n",
      "Train Error(all epochs): 1.4304307699203491 \n",
      " [49.07, 49.008, 48.951, 48.89, 48.817, 48.729, 48.628, 48.509, 48.363, 48.185, 47.97, 47.714, 47.415, 47.07, 46.67, 46.216, 45.69, 45.12, 44.485, 43.819, 43.122, 42.368, 41.597, 40.816, 40.008, 39.176, 38.352, 37.519, 36.611, 35.73, 34.831, 33.943, 33.02, 32.134, 31.177, 30.24, 29.272, 28.29, 27.291, 26.314, 25.336, 24.36, 23.374, 22.458, 21.488, 20.579, 19.66, 18.804, 17.888, 17.03, 16.289, 15.458, 14.658, 13.98, 13.256, 12.589, 12.042, 11.488, 10.749, 10.36, 9.886, 9.276, 8.953, 8.545, 8.171, 7.805, 7.432, 7.376, 7.227, 6.949, 6.741, 6.458, 6.113, 5.847, 5.589, 5.363, 5.168, 5.039, 5.034, 4.916, 4.61, 4.57, 4.874, 4.499, 4.278, 4.742, 4.746, 4.737, 4.347, 4.182, 4.319, 4.008, 4.075, 4.002, 3.821, 3.884, 3.57, 3.633, 3.736, 3.914, 4.025, 3.835, 3.883, 3.616, 3.541, 3.425, 3.336, 3.422, 3.665, 3.82, 3.679, 3.427, 3.579, 3.56, 3.293, 3.405, 3.438, 3.386, 3.433, 3.239, 3.231, 3.212, 3.218, 3.188, 2.957, 2.911, 3.046, 3.123, 2.916, 2.917, 3.031, 2.836, 2.939, 3.09, 3.288, 3.098, 3.077, 2.928, 2.921, 3.075, 3.111, 3.344, 3.111, 2.919, 3.164, 2.965, 2.837, 2.551, 2.495, 2.528, 2.536, 2.773, 2.642, 2.429, 2.25, 2.5, 2.572, 2.631, 2.831, 3.039, 2.856, 2.68, 2.831, 2.88, 2.891, 2.891, 3.172, 2.868, 2.99, 3.059, 3.19, 2.849, 2.579, 2.349, 2.135, 2.104, 2.021, 2.165, 2.158, 2.147, 2.244, 2.507, 2.275, 2.416, 2.333, 2.44, 2.752, 3.241, 3.407, 2.997, 3.093, 3.042, 2.769, 2.75, 2.675, 2.596, 2.579, 2.474, 2.628, 2.723, 2.752, 2.529, 2.203, 2.127, 2.168, 2.213, 2.055, 2.028, 1.957, 2.003, 2.295, 2.381, 2.506, 2.217, 2.356, 2.463, 2.553, 2.197, 2.252, 2.405, 2.179, 2.306, 2.394, 2.377, 2.417, 2.312, 2.282, 2.32, 2.259, 2.369, 2.536, 2.562, 2.282, 2.239, 2.213, 2.408, 2.535, 2.465, 2.394, 2.749, 2.733, 2.781, 2.918, 2.729, 2.845, 2.781, 2.552, 2.655, 2.387, 2.378, 2.16, 2.094, 1.946, 1.907, 2.094, 2.069, 2.061, 1.892, 2.143, 1.881, 1.943, 2.058, 2.088, 2.351, 2.382, 2.208, 2.508, 2.434, 2.399, 2.256, 2.289, 2.267, 1.98, 2.005, 2.021, 2.031, 2.296, 2.443, 2.305, 2.263, 2.327, 2.142, 2.048, 2.107, 2.174, 2.35, 2.295, 2.366, 2.515, 2.626, 2.434, 2.368, 2.403, 2.295, 2.345, 2.193, 2.354, 2.242, 1.924, 1.818, 1.753, 1.778, 1.85, 2.011, 2.132, 1.943, 2.108, 2.358, 2.392, 2.062, 2.039, 1.966, 1.981, 1.934, 1.815, 1.656, 1.805, 1.764, 1.886, 2.043, 2.293, 2.489, 2.363, 2.386, 2.359, 2.334, 2.353, 2.434, 2.301, 2.173, 2.349, 2.161, 2.534, 2.299, 2.443, 2.473, 2.384, 2.459, 2.456, 2.334, 2.617, 2.319, 2.067, 2.051, 1.896, 2.017, 2.081, 2.096, 1.817, 1.643, 1.736, 1.714, 1.694, 1.772, 1.739, 2.001, 2.051, 2.444, 2.625, 2.679, 2.396, 2.421, 2.454, 2.447, 2.357, 2.429, 2.535, 2.772, 2.302, 1.992, 1.996, 2.001, 1.973, 2.031, 2.062, 2.471, 2.394, 2.263, 2.262, 2.029, 1.788, 1.666, 1.735, 1.829, 1.935, 2.121, 2.202, 2.307, 2.68, 2.35, 1.955, 1.761, 1.653, 1.677, 1.713, 1.93, 1.88, 1.764, 2.088, 2.174, 2.165, 2.484, 2.346, 2.521, 2.285, 2.276, 2.136, 2.04, 1.919, 1.728, 1.878, 1.795, 1.87, 1.863, 1.984, 1.965, 1.83, 1.746, 1.616, 1.478, 1.636, 1.738, 1.946, 1.958, 2.21, 2.547, 2.23, 2.392, 2.462, 2.551, 2.725, 2.599, 2.641, 2.615, 2.749, 2.608, 2.479, 2.348, 2.403, 2.264, 2.311, 2.089, 2.001, 1.8, 1.7, 1.594, 1.538, 1.498, 1.499, 1.564, 1.598, 1.543, 1.592, 1.841, 1.829, 1.877, 1.713, 1.884, 1.735, 1.755, 1.882, 1.974, 1.951, 2.135, 2.143, 2.315, 2.427, 2.09, 2.188, 2.185, 2.338, 2.366, 2.212, 2.519, 2.306, 2.411, 2.399, 2.294, 2.012, 2.003, 1.929, 1.832, 1.825, 1.915, 1.862, 1.733, 1.625, 1.541, 1.568, 1.702, 1.804, 1.773, 1.773, 1.657, 1.584, 1.43, 1.494, 1.595, 1.691, 1.76]\n",
      "Train FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.012, 0.018, 0.028, 0.041, 0.052, 0.053, 0.038, 0.106, 0.097, 0.096, 0.149, 0.166, 0.193, 0.269, 0.303, 0.283, 0.338, 0.386, 0.358, 0.435, 0.478, 0.503, 0.542, 0.583, 0.708, 0.823, 0.907, 0.948, 0.985, 1.018, 0.97, 1.057, 1.024, 1.036, 1.085, 1.18, 1.132, 1.118, 1.115, 1.368, 1.201, 1.161, 1.481, 1.541, 1.455, 1.305, 1.347, 1.335, 1.283, 1.295, 1.282, 1.191, 1.358, 1.116, 1.196, 1.245, 1.395, 1.382, 1.318, 1.375, 1.27, 1.297, 1.123, 1.163, 1.157, 1.401, 1.461, 1.277, 1.226, 1.41, 1.298, 1.138, 1.306, 1.224, 1.204, 1.316, 1.249, 1.109, 1.198, 1.074, 1.162, 1.1, 1.066, 1.092, 1.167, 1.131, 1.015, 1.098, 1.061, 1.045, 1.129, 1.281, 1.136, 1.105, 1.162, 0.988, 1.19, 1.145, 1.288, 1.204, 1.052, 1.142, 1.117, 1.098, 0.867, 0.828, 0.934, 0.93, 1.037, 1.007, 0.893, 0.8, 0.902, 0.904, 0.971, 1.141, 1.095, 1.179, 0.925, 1.1, 1.119, 1.094, 1.017, 1.298, 1.078, 1.084, 1.235, 1.104, 1.045, 1.015, 0.762, 0.757, 0.682, 0.767, 0.768, 0.769, 0.814, 0.77, 0.993, 0.777, 0.966, 0.83, 0.856, 1.089, 1.293, 1.268, 1.223, 1.145, 1.299, 0.993, 1.043, 1.016, 1.036, 0.965, 0.948, 0.962, 1.116, 1.128, 0.933, 0.812, 0.789, 0.743, 0.828, 0.705, 0.724, 0.724, 0.686, 0.824, 0.933, 1.004, 0.844, 0.863, 0.96, 1.092, 0.717, 0.873, 0.939, 0.793, 0.824, 0.911, 0.967, 0.94, 0.854, 0.854, 0.946, 0.737, 0.963, 0.933, 1.044, 0.871, 0.663, 0.875, 1.017, 0.98, 0.951, 0.91, 1.151, 1.095, 1.075, 1.149, 1.069, 1.092, 1.122, 0.951, 0.998, 0.897, 0.832, 0.854, 0.774, 0.828, 0.709, 0.855, 0.757, 0.766, 0.77, 0.762, 0.725, 0.752, 0.73, 0.836, 0.907, 0.942, 0.816, 1.023, 0.971, 0.931, 0.866, 0.845, 0.942, 0.698, 0.756, 0.788, 0.83, 0.864, 0.984, 0.942, 0.855, 0.796, 0.887, 0.716, 0.786, 0.934, 0.898, 0.911, 0.94, 1.044, 1.002, 0.936, 0.872, 0.992, 0.916, 0.955, 0.873, 0.929, 0.909, 0.675, 0.723, 0.601, 0.646, 0.763, 0.719, 0.918, 0.72, 0.786, 0.982, 0.969, 0.782, 0.723, 0.806, 0.75, 0.713, 0.746, 0.543, 0.733, 0.625, 0.802, 0.723, 1.008, 0.761, 1.003, 1.014, 1.0, 0.999, 0.957, 0.96, 0.894, 0.902, 0.915, 0.883, 1.079, 0.913, 0.989, 0.999, 0.801, 1.042, 1.021, 0.93, 1.183, 0.907, 0.719, 0.844, 0.702, 0.86, 0.828, 0.819, 0.732, 0.59, 0.66, 0.647, 0.658, 0.746, 0.619, 0.82, 0.829, 1.043, 1.08, 1.117, 0.913, 0.795, 1.043, 1.072, 0.956, 1.067, 0.951, 1.184, 1.038, 0.692, 0.794, 0.849, 0.765, 0.829, 0.85, 1.139, 0.864, 0.9, 0.997, 0.759, 0.692, 0.591, 0.734, 0.738, 0.74, 0.89, 0.863, 0.925, 1.176, 0.994, 0.699, 0.633, 0.671, 0.704, 0.618, 0.824, 0.709, 0.708, 0.829, 0.94, 0.892, 1.047, 0.927, 1.129, 0.848, 0.91, 0.811, 0.913, 0.762, 0.66, 0.841, 0.66, 0.742, 0.73, 0.805, 0.804, 0.722, 0.688, 0.647, 0.5, 0.712, 0.628, 0.838, 0.787, 0.843, 1.196, 0.9, 0.936, 1.027, 0.99, 1.187, 1.108, 1.056, 1.061, 1.263, 1.03, 1.013, 1.054, 1.067, 0.928, 0.974, 0.81, 0.9, 0.686, 0.666, 0.616, 0.65, 0.596, 0.534, 0.685, 0.581, 0.612, 0.624, 0.77, 0.773, 0.708, 0.69, 0.842, 0.597, 0.723, 0.796, 0.864, 0.764, 0.889, 0.897, 0.989, 0.881, 0.954, 0.93, 0.808, 0.93, 1.087, 0.86, 1.09, 1.019, 0.983, 1.105, 0.951, 0.812, 0.795, 0.824, 0.758, 0.745, 0.811, 0.768, 0.673, 0.659, 0.602, 0.63, 0.724, 0.685, 0.791, 0.623, 0.74, 0.666, 0.538, 0.588, 0.699, 0.604, 0.819]\n",
      "Val Error(all epochs): 7.652918815612793 \n",
      " [49.84, 49.759, 49.68, 49.569, 49.453, 49.303, 49.174, 48.95, 48.776, 48.629, 48.276, 48.035, 47.675, 47.127, 46.8, 46.266, 45.779, 45.635, 44.996, 44.42, 44.436, 43.773, 43.011, 42.629, 41.374, 41.424, 39.551, 39.951, 39.103, 37.176, 38.064, 35.561, 35.504, 35.468, 37.778, 32.716, 31.384, 32.125, 29.002, 28.098, 28.552, 26.818, 24.375, 23.233, 21.792, 19.523, 18.284, 17.455, 18.909, 16.864, 16.102, 13.89, 13.425, 14.267, 12.907, 11.324, 11.226, 11.366, 10.807, 10.966, 11.066, 11.477, 11.367, 11.488, 11.925, 12.11, 11.975, 11.996, 13.765, 12.924, 13.703, 13.958, 14.004, 12.114, 11.703, 12.256, 12.518, 11.674, 11.82, 11.091, 10.579, 10.325, 9.676, 10.336, 9.467, 9.377, 9.286, 9.177, 9.168, 9.524, 8.231, 9.892, 8.592, 9.048, 9.12, 8.8, 8.346, 9.527, 9.12, 8.387, 10.6, 7.938, 9.652, 8.622, 9.096, 8.943, 8.984, 9.39, 8.865, 8.194, 7.764, 8.236, 8.418, 9.098, 9.289, 8.904, 9.203, 8.466, 7.969, 8.332, 8.689, 8.417, 8.808, 8.486, 8.369, 9.267, 8.467, 8.644, 9.179, 8.811, 9.438, 8.968, 9.476, 8.709, 8.418, 8.708, 8.504, 8.488, 9.109, 8.919, 8.921, 7.877, 8.048, 8.28, 7.91, 8.049, 7.862, 8.788, 8.385, 9.373, 8.099, 8.445, 8.418, 8.334, 8.605, 8.332, 8.857, 8.635, 8.216, 8.147, 8.783, 9.039, 8.844, 8.363, 8.866, 8.568, 7.969, 8.548, 8.109, 8.688, 8.949, 8.384, 8.846, 8.169, 8.722, 8.361, 8.638, 8.567, 8.864, 9.332, 8.182, 8.884, 8.096, 8.396, 8.92, 8.167, 7.777, 8.6, 7.791, 8.68, 8.149, 8.206, 7.984, 7.882, 8.429, 8.121, 7.795, 7.727, 8.5, 8.612, 8.022, 8.209, 8.322, 8.551, 8.126, 8.447, 8.434, 8.764, 8.54, 8.828, 9.128, 8.602, 8.406, 8.625, 8.446, 8.641, 8.522, 8.536, 8.361, 8.277, 8.336, 8.706, 8.603, 8.749, 8.13, 8.685, 7.837, 8.294, 8.669, 8.389, 8.756, 8.364, 8.518, 8.32, 8.49, 8.322, 8.475, 8.071, 8.562, 8.585, 8.255, 8.388, 7.952, 8.691, 8.318, 8.665, 8.544, 9.233, 8.447, 8.251, 8.212, 8.515, 8.276, 8.433, 8.409, 8.714, 8.54, 8.409, 9.051, 8.627, 9.002, 9.107, 8.783, 8.663, 8.587, 8.403, 8.865, 8.809, 8.727, 8.882, 9.318, 8.715, 8.799, 8.986, 8.584, 8.523, 9.198, 8.383, 8.582, 8.524, 8.427, 9.58, 9.091, 8.699, 8.634, 9.079, 8.293, 8.872, 8.594, 8.239, 8.175, 8.066, 8.253, 8.102, 8.728, 8.573, 8.589, 8.598, 8.426, 8.201, 8.77, 8.465, 8.352, 8.678, 8.381, 8.815, 8.234, 8.645, 8.273, 8.539, 8.571, 8.044, 8.403, 8.269, 8.529, 8.581, 7.956, 8.507, 8.343, 8.646, 8.157, 8.476, 8.349, 8.455, 8.465, 8.579, 7.694, 8.647, 8.056, 8.31, 8.742, 8.721, 8.995, 8.432, 8.348, 8.461, 8.733, 8.32, 8.473, 8.889, 8.4, 8.58, 8.529, 8.734, 8.677, 8.502, 9.176, 8.328, 9.183, 8.723, 8.767, 8.432, 8.602, 8.542, 8.836, 8.42, 8.626, 8.534, 8.678, 8.079, 8.525, 8.603, 8.334, 8.684, 8.35, 8.472, 8.915, 9.908, 8.942, 9.15, 8.547, 8.744, 8.928, 8.575, 8.549, 8.795, 9.069, 7.653, 8.335, 8.771, 8.712, 8.845, 8.839, 8.293, 9.083, 8.505, 8.909, 8.843, 9.492, 9.312, 10.398, 9.426, 9.825, 9.209, 9.162, 8.723, 8.888, 8.778, 9.086, 8.071, 8.422, 8.635, 8.877, 8.497, 8.925, 8.685, 8.657, 8.608, 8.76, 8.515, 8.865, 9.303, 9.16, 9.114, 8.431, 8.832, 8.399, 8.63, 8.618, 8.896, 8.989, 8.793, 9.023, 8.934, 8.552, 8.638, 9.117, 9.517, 8.848, 8.313, 9.376, 8.364, 9.042, 9.001, 9.291, 8.487, 9.141, 10.016, 9.245, 8.478, 7.695, 8.955, 8.111, 8.49, 8.322, 8.411, 8.087, 8.493, 8.243, 8.492, 8.709, 8.634, 9.192, 8.682, 8.89, 8.701, 8.751, 8.795, 9.172, 8.68, 8.892, 8.833, 9.007, 8.543, 8.699, 8.708, 8.466, 8.056, 8.715, 8.991, 8.338, 8.676, 8.835, 8.838, 8.634, 8.298, 9.591, 8.795, 8.042, 8.392, 8.211, 8.185, 8.356, 7.98, 8.375, 8.16, 7.904, 8.48, 8.515, 8.337, 8.47, 8.308, 8.118, 8.377, 8.436, 8.428, 8.385, 8.399, 8.617, 8.476]\n",
      "Val FP Error(all epochs): 0.0 \n",
      " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.029, 0.06, 0.007, 0.054, 0.113, 0.35, 0.459, 0.257, 0.571, 1.252, 1.607, 1.833, 2.269, 2.955, 3.291, 4.218, 3.419, 3.994, 3.603, 3.358, 3.539, 3.595, 5.95, 6.244, 6.459, 6.937, 7.161, 4.924, 3.949, 4.385, 4.611, 3.466, 3.445, 3.603, 2.531, 1.985, 2.299, 1.697, 2.001, 2.518, 2.281, 2.366, 2.863, 2.596, 2.975, 3.135, 2.281, 2.333, 2.381, 1.888, 2.727, 2.627, 1.849, 3.176, 5.796, 2.775, 1.836, 2.162, 2.285, 2.117, 2.556, 1.72, 2.617, 2.26, 3.33, 4.224, 4.058, 4.724, 5.046, 4.009, 3.298, 4.338, 2.454, 3.259, 2.298, 3.457, 2.285, 2.785, 3.205, 1.269, 3.203, 1.928, 1.942, 2.233, 1.553, 2.234, 1.946, 3.075, 2.268, 3.574, 1.85, 2.005, 2.242, 1.56, 1.912, 2.658, 1.804, 3.085, 2.07, 2.916, 3.143, 2.376, 2.664, 1.788, 2.563, 2.516, 2.472, 2.117, 2.513, 2.14, 2.515, 2.408, 2.872, 3.0, 2.51, 2.088, 2.847, 2.327, 3.64, 2.969, 2.582, 3.104, 1.938, 4.45, 3.264, 3.769, 3.461, 3.018, 3.095, 3.048, 2.459, 2.721, 1.929, 2.229, 2.719, 2.452, 2.804, 3.013, 2.734, 2.971, 2.913, 3.486, 2.608, 2.492, 2.386, 4.214, 3.262, 3.075, 3.163, 2.954, 3.225, 3.544, 2.552, 4.224, 2.851, 3.227, 3.269, 2.115, 3.325, 2.478, 3.034, 2.622, 2.042, 2.816, 1.94, 2.677, 2.594, 3.002, 2.548, 3.103, 2.722, 2.829, 3.017, 2.895, 2.79, 3.046, 2.324, 3.369, 3.286, 3.215, 3.233, 3.776, 2.884, 3.539, 2.384, 3.366, 3.545, 3.914, 3.405, 4.32, 2.926, 3.381, 3.523, 2.222, 3.552, 3.414, 2.616, 3.911, 3.356, 4.461, 3.171, 3.17, 4.015, 3.574, 4.478, 3.523, 3.155, 3.166, 2.579, 2.87, 2.794, 3.031, 2.719, 2.689, 2.764, 2.136, 3.045, 2.266, 2.823, 3.442, 2.453, 3.105, 2.702, 2.957, 2.66, 2.69, 3.332, 2.822, 2.944, 3.069, 2.315, 3.431, 2.533, 2.808, 2.72, 2.063, 2.247, 2.849, 2.093, 3.118, 2.596, 3.082, 2.805, 2.641, 3.481, 3.465, 3.136, 3.736, 2.701, 3.729, 3.098, 3.78, 3.416, 3.142, 2.929, 3.01, 3.371, 3.039, 3.046, 3.067, 2.865, 3.631, 2.881, 3.343, 3.354, 3.257, 3.26, 3.013, 3.297, 3.104, 3.172, 3.493, 2.84, 3.134, 3.239, 3.495, 3.288, 3.567, 3.214, 3.567, 3.574, 4.109, 3.821, 3.943, 2.892, 3.363, 2.409, 3.625, 3.344, 3.626, 4.047, 2.792, 3.086, 3.219, 2.972, 4.106, 3.311, 3.303, 3.735, 2.773, 4.255, 2.862, 2.887, 2.649, 2.899, 3.151, 3.096, 3.0, 2.889, 2.788, 3.596, 2.909, 4.782, 2.891, 3.413, 3.705, 3.632, 3.585, 3.388, 3.546, 3.55, 5.204, 4.257, 3.273, 3.217, 3.599, 3.159, 3.144, 3.967, 2.876, 4.463, 3.585, 3.449, 3.745, 3.146, 3.073, 4.018, 2.89, 3.69, 3.263, 3.089, 4.457, 2.59, 3.034, 2.005, 2.639, 2.506, 2.297, 3.082, 3.137, 3.118, 3.249, 2.614, 3.174, 3.258, 2.818, 4.127, 2.992, 4.0, 3.499, 4.655, 3.881, 3.841, 3.722, 2.94, 3.86, 2.546, 3.467, 3.413, 3.072, 3.293, 3.006, 2.703, 3.324, 2.226, 3.16, 2.655, 2.496, 4.017, 2.825, 3.93, 2.626, 3.06, 3.375, 2.392, 3.224, 3.019, 3.173, 4.373, 3.149, 3.526, 1.921, 3.37, 3.905, 3.379, 4.442, 4.485, 4.108, 4.011, 3.711, 3.662, 3.449, 3.832, 3.093, 3.436, 2.663, 2.953, 2.55, 2.853, 2.671, 2.873, 2.725, 2.915, 3.535, 2.775, 3.105, 2.866, 2.846, 3.381, 2.545, 3.595, 3.172, 3.16, 3.323, 3.714, 3.27, 4.326, 3.031, 3.144, 3.252, 3.681, 4.087, 3.593, 3.358, 3.572, 3.544, 4.164, 4.093, 3.535, 3.756, 3.29, 3.233, 3.257, 2.788, 3.151, 3.044, 3.08, 3.125, 2.791, 2.95, 3.098, 2.928, 2.785, 3.021]\n",
      "\n",
      "#Fold: 9 \n",
      "Trainig set size: 421 , Time: 0:12:18 , best_lambda: 0.1 , min_  , error: 6.133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test starts:  468 , ends:  518\n",
      "1/1 [==============================] - 0s 665us/step - loss: 94.0326 - mse: 76.6236 - mae: 6.9570 - fp_mae: 4.5511\n",
      "average_error:  6.957 , fp_average_error:  4.551\n",
      "Number Sample: 519 , Average Error: 6.7143999999999995 FP Average Error: 3.0946\n"
     ]
    }
   ],
   "source": [
    "#K-Fold cross-validation\n",
    "import shutil\n",
    "dtime = datetime.datetime.now().strftime('_%Y%m_%d%H_%M')\n",
    "color = \"color\" if number_image_channels > 1 else \"gray\"\n",
    "image_dir = 'ML/data/pictures_' + str(max_x) + '_' + str(max_y) + '/' + propagation_model + (\n",
    "    \"/noisy_std_\" + str(std) if noise else \"\") + '/pu_' + pu_shape + '_su_' + su_shape + '_' + (\n",
    "    \"\" if su_shape == 'point' else str(su_szie)) + \"/\" + style + \"/\" + color +'/' + (\n",
    "    \"\" if pu_shape == 'point' and su_shape == 'point' else (intensity_degradation + '_' + str(slope))) + (\n",
    "    \"/\" + str(sensors_num) + \"sensors\" if sensors else \"/pus\") + \"/images\"\n",
    "TEST, CONSERVE = True, False\n",
    "mini_batch = 16 if max(max_x, max_y) == 1000 else 64\n",
    "epochs = 35 if max(max_x, max_y) == 1000 else 500\n",
    "MAX_QUEUE_SIZE, WORKERS = 6, 1\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "hyper_metric, mode = \"val_mae\", 'min'  # the metric that hyper parameters are tuned with\n",
    "prev_sample = 0\n",
    "lambda_vec = [0.01, 0.1, 1]  #0.003, 0.01, 0.03, 0.1, 0.3, 1, 3\n",
    "average_diff_power, fp_mean_power = [],[] #[7.177, 8.088, 8.183], [3.438, 3.506, 2.662]\n",
    "best_lambda = []\n",
    "average_diff_power_conserve, fp_mean_power_conserve = [], []\n",
    "all_cnns = []\n",
    "all_diff_power, all_fp_diff_power, all_diff_power_conserve, all_fp_diff_conserve = [], [], [], []\n",
    "\n",
    "data_reg_bak = data_reg\n",
    "\n",
    "for number_sample in number_samples: \n",
    "    data_reg = data_reg_bak\n",
    "    np.random.shuffle(data_reg)\n",
    "    data_reg = data_reg[:number_sample,:]\n",
    "    num_sample_bucket = data_reg.shape[0]//K_SIZE\n",
    "    k_samples = [num_sample_bucket] * K_SIZE\n",
    "    for i in range(data_reg.shape[0]%K_SIZE):\n",
    "        k_samples[i] += 1\n",
    "    k_samples = [0] + list(np.cumsum(k_samples))\n",
    "    print('number_samples:', number_sample)\n",
    "    all_cnns_k, best_lambda_k = [], []\n",
    "    diff_power_k, fp_diff_power_k, power_conserve_k, fp_power_conserve_k = [], [], [], []\n",
    "    data_reg_bak_k = data_reg\n",
    "    for k in range(K_SIZE):\n",
    "        data_reg = data_reg_bak_k\n",
    "        k_low_idx, k_high_idx = k_samples[k], k_samples[k+1] #inclusive, exclusive\n",
    "        data_k_test = data_reg[k_low_idx:k_high_idx, :]\n",
    "        if k == 0:\n",
    "            data_k_train = data_reg[k_high_idx:, :]\n",
    "        elif k == K_SIZE - 1:\n",
    "            data_k_train = data_reg[:k_low_idx, :]\n",
    "        else:\n",
    "            data_k_train = np.concatenate((data_reg[:k_low_idx, :], data_reg[k_high_idx:]))\n",
    "        np.random.shuffle(data_k_train)\n",
    "        data_reg = np.concatenate((data_k_train, data_k_test))\n",
    "        \n",
    "        #prepare to create images\n",
    "        if os.path.exists(image_dir):\n",
    "            shutil.rmtree(image_dir)\n",
    "        os.makedirs(image_dir)\n",
    "\n",
    "        # creating images\n",
    "        job_creating_images(data=data_reg)\n",
    "        \n",
    "        #create folder for \n",
    "        MODEL_PATH = '/'.join(image_dir.split('/')[:-1]) + '/models/' + str(number_sample) + \"/fold_\" + str(k)\n",
    "        if not os.path.exists(MODEL_PATH):\n",
    "            os.makedirs(MODEL_PATH)\n",
    "        MODEL_PATH += \"/best_model_lambda_\"\n",
    "        \n",
    "        number_start = time.time()\n",
    "        \n",
    "        #creating models\n",
    "        cnns = [cnn_model(10, lamb, 0) for lamb in lambda_vec]\n",
    "        for cnn in cnns:\n",
    "#             cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae', fp_mean])\n",
    "            cnn.compile(loss=custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                        optimizer='adam', metrics=['mse', 'mae', fp_mae])\n",
    "        checkpointers = [ModelCheckpoint(filepath=MODEL_PATH + str(lamb_idx)+ '.h5',\n",
    "                                         verbose=1, save_best_only=True, \n",
    "                                         monitor=hyper_metric,\n",
    "                                         mode=mode)\n",
    "                         for lamb_idx in range(len(lambda_vec))]\n",
    "        \n",
    "        # training\n",
    "        test_k_size = data_k_test.shape[0]\n",
    "        val_k_size = math.ceil((data_reg.shape[0] - test_k_size) * validation_size)\n",
    "        train_k_size  = data_reg.shape[0] - val_k_size - test_k_size\n",
    "        train_generator = DataBatchGenerator(dataset=data_reg[:train_k_size], batch_size=mini_batch,\n",
    "                                             start_idx=0, number_image_channels=number_image_channels,\n",
    "                                             max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "    \n",
    "        val_generator = DataBatchGenerator(dataset=data_reg[train_k_size:train_k_size + val_k_size], \n",
    "                                           batch_size=mini_batch,\n",
    "                                           start_idx=train_k_size,\n",
    "                                           number_image_channels=number_image_channels,\n",
    "                                           max_x=max_x, max_y=max_y, \n",
    "                                           float_memory_used=float_memory_used)\n",
    "        \n",
    "        \n",
    "        print(\"#Fold:\", k, \", Training Size:\", train_k_size, \", Validation size:\", val_k_size,\n",
    "              \", Test Size\", test_k_size)\n",
    "        \n",
    "        for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "            lambda_start = time.time()\n",
    "            cnns[lamb_idx].fit(train_generator, epochs=epochs, verbose=0,\n",
    "                               validation_data=val_generator, \n",
    "                               shuffle=False, callbacks=[checkpointers[lamb_idx]], \n",
    "                               workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                               use_multiprocessing=False)\n",
    "\n",
    "            print(\"\\nLambda:\", lamb, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - lambda_start))))\n",
    "            print(\"Train Error(all epochs):\", min(cnns[lamb_idx].history.history['mae']), '\\n', \n",
    "                  [round(val, 3) for val in cnns[lamb_idx].history.history['mae']])\n",
    "            print(\"Train FP Error(all epochs):\", min(cnns[lamb_idx].history.history['fp_mae']), '\\n',\n",
    "                  [round(val,3) for val in cnns[lamb_idx].history.history['fp_mae']])\n",
    "            print(\"Val Error(all epochs):\", min(cnns[lamb_idx].history.history['val_mae']), '\\n', \n",
    "                  [round(val,3) for val in cnns[lamb_idx].history.history['val_mae']])\n",
    "            print(\"Val FP Error(all epochs):\", min(cnns[lamb_idx].history.history['val_fp_mae']), '\\n',\n",
    "                  [round(val,3) for val in cnns[lamb_idx].history.history['val_fp_mae']])\n",
    "        \n",
    "        # find the best lambda\n",
    "        models_min_mae = [min(cnns[lam_idx].history.history[hyper_metric]) for\n",
    "                          lam_idx,_ in enumerate(lambda_vec)]\n",
    "        best_lamb_idx = models_min_mae.index(min(models_min_mae))\n",
    "        best_lambda_k.append(lambda_vec[best_lamb_idx])\n",
    "        print(\"\\n#Fold:\", k, \"\\nTrainig set size:\", train_k_size, \n",
    "              \", Time:\", str(datetime.timedelta(seconds=int(time.time() - number_start))),\n",
    "              \", best_lambda:\", lambda_vec[best_lamb_idx], \", min_\" ,\n",
    "              (\"fp_\" if hyper_metric == \"val_fp_mae\" else \"\"),\n",
    "              \", error:\", round(min(models_min_mae), 3))\n",
    "        all_cnns_k.append(cnns)\n",
    "        del cnns, train_generator, val_generator, checkpointers\n",
    "        \n",
    "        #evaluating\n",
    "        if TEST:\n",
    "            # evaluating test images\n",
    "            best_model = None\n",
    "            best_model = models.load_model(MODEL_PATH + str(best_lamb_idx) + '.h5', \n",
    "                                           custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                           'fp_mae': fp_mae,\n",
    "                                                          'mae':'mae', 'mse':'mse'})\n",
    "            test_generator = DataBatchGenerator(dataset=data_reg[train_k_size + val_k_size:], \n",
    "                                                batch_size=mini_batch,\n",
    "                                                start_idx=train_k_size + val_k_size, \n",
    "                                                number_image_channels=number_image_channels,\n",
    "                                                max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "\n",
    "            print(\"Test starts: \", train_k_size + val_k_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "            time.sleep(1)\n",
    "            test_res = best_model.evaluate(test_generator, verbose=1, \n",
    "                                           workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)\n",
    "\n",
    "            test_mae_idx, test_fp_mae_idx = [best_model.metrics_names.index(mtrc) \n",
    "                                             for mtrc in ['mae','fp_mae']]\n",
    "            test_mae, test_fp_mae = test_res[test_mae_idx], test_res[test_fp_mae_idx]\n",
    "            diff_power_k.append(round(test_mae, 3))\n",
    "            fp_diff_power_k.append(round(test_fp_mae, 3))\n",
    "            print('average_error: ', diff_power_k[-1], ', fp_average_error: ', \n",
    "                  fp_diff_power_k[-1])\n",
    "            del best_model, test_generator\n",
    "    all_diff_power.append(diff_power_k)\n",
    "    all_fp_diff_power.append(fp_diff_power_k)\n",
    "    average_diff_power.append(sum(diff_power_k)/len(diff_power_k))\n",
    "    fp_mean_power.append(sum(fp_diff_power_k)/len(fp_diff_power_k))\n",
    "    print(\"Number Sample:\", number_sample, \", Average Error:\", average_diff_power[-1], \n",
    "          \"FP Average Error:\", fp_mean_power[-1])\n",
    "    if CONSERVE:\n",
    "        all_diff_power_conserve.append(power_conserve_k)\n",
    "        all_fp_diff_conserve.append(fp_power_conserve_k)\n",
    "        average_diff_power_conserve.append(sum(power_conserve_k)/len(power_conserve_k))\n",
    "        fp_mean_power_conserve.append(sum(power_conserve_k)/len(power_conserve_k))\n",
    "    \n",
    "    \n",
    "    var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/' + intensity_degradation + '_' + str(slope) + '_' + \n",
    "                     dtime + \".dat\", \"wb\") # file for saving results\n",
    "    pickle.dump([average_diff_power, fp_mean_power, number_samples, best_lambda, \n",
    "                 dataset_name, average_diff_power_conserve, fp_mean_power_conserve, all_diff_power, \n",
    "                 all_fp_diff_power, all_diff_power_conserve, all_fp_diff_conserve],\n",
    "                 file=var_f)\n",
    "    var_f.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6.693, 7.092, 6.64, 7.477, 6.032, 7.583, 5.626, 6.475, 6.569, 6.957]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_diff_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.383777777777777"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([11.953, 6.949, 8.356, 7.897, 10.431, 10.15, 8.96, 10.969, 8.789])/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 17)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49. 48. 53. 45. 48. 62. 49. 42. 37. 42. 44. 57. 49. 53. 37. 44. 55. 52.\n",
      " 37. 47. 45. 66. 53. 43. 52. 34. 61. 59. 27. 43.]\n",
      "[62. 49. 42. 37. 42. 44. 57. 49. 53. 37. 44. 55. 52. 37. 47. 45. 66. 53.\n",
      " 43. 52. 34. 61. 59. 27. 43.]\n",
      "[49. 48. 53. 45. 48.]\n",
      "[49. 48. 53. 45. 48. 44. 57. 49. 53. 37. 44. 55. 52. 37. 47. 45. 66. 53.\n",
      " 43. 52. 34. 61. 59. 27. 43.]\n",
      "[62. 49. 42. 37. 42.]\n",
      "[49. 48. 53. 45. 48. 62. 49. 42. 37. 42. 37. 44. 55. 52. 37. 47. 45. 66.\n",
      " 53. 43. 52. 34. 61. 59. 27. 43.]\n",
      "[44. 57. 49. 53.]\n",
      "[49. 48. 53. 45. 48. 62. 49. 42. 37. 42. 44. 57. 49. 53. 37. 47. 45. 66.\n",
      " 53. 43. 52. 34. 61. 59. 27. 43.]\n",
      "[37. 44. 55. 52.]\n",
      "[49. 48. 53. 45. 48. 62. 49. 42. 37. 42. 44. 57. 49. 53. 37. 44. 55. 52.\n",
      " 53. 43. 52. 34. 61. 59. 27. 43.]\n",
      "[37. 47. 45. 66.]\n",
      "[49. 48. 53. 45. 48. 62. 49. 42. 37. 42. 44. 57. 49. 53. 37. 44. 55. 52.\n",
      " 37. 47. 45. 66. 61. 59. 27. 43.]\n",
      "[53. 43. 52. 34.]\n",
      "[49. 48. 53. 45. 48. 62. 49. 42. 37. 42. 44. 57. 49. 53. 37. 44. 55. 52.\n",
      " 37. 47. 45. 66. 53. 43. 52. 34.]\n",
      "[61. 59. 27. 43.]\n"
     ]
    }
   ],
   "source": [
    "print(data_reg[:, -1])\n",
    "for k in range(K_SIZE):\n",
    "    k_low_idx, k_high_idx = k_samples[k], k_samples[k+1] #inclusive, exclusive\n",
    "    data_k_test = data_reg[k_low_idx:k_high_idx, :]\n",
    "    if k == 0:\n",
    "        data_k_train = data_reg[k_high_idx:, :]\n",
    "    elif k == K_SIZE - 1:\n",
    "        data_k_train = data_reg[:k_low_idx, :]\n",
    "    else:\n",
    "        data_k_train = np.concatenate((data_reg[:k_low_idx, :], data_reg[k_high_idx:]))\n",
    "    print(data_k_train[:, -1])\n",
    "    print(data_k_test[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    <ipython-input-17-7771708fbff9>:129 loss\n        return K.mean(K.square(res))\n\n    AttributeError: 'int' object has no attribute 'mean'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-cc4d01b362e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m cnns[0].fit(train_generator, epochs=epochs, verbose=1,\n\u001b[1;32m      2\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m             workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2772\u001b[0m           \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2773\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[0;32m-> 2774\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2704\u001b[0m         relaxed_arg_shapes)\n\u001b[1;32m   2705\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 2706\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   2707\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    <ipython-input-17-7771708fbff9>:129 loss\n        return K.mean(K.square(res))\n\n    AttributeError: 'int' object has no attribute 'mean'\n"
     ]
    }
   ],
   "source": [
    "cnns[0].fit(train_generator, epochs=epochs, verbose=1,\n",
    "            validation_data=val_generator, shuffle=False, callbacks=[checkpointers[0]], \n",
    "            workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = custom_loss(np.ones((1,4)), np.zeros((1,4)))\n",
    "sess = K.get_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-39.58, -48.4 , -47.78, -44.89, -48.13, -47.62, -44.29, -48.38,\n",
       "        -47.21, -42.48, -48.32, -46.18, -48.45, -44.5 , -47.79, -48.3 ,\n",
       "        -46.81, -48.37,   1.  ,   5.  ,   5.  ,  44.  ],\n",
       "       [-39.11, -44.31, -48.29, -44.63, -28.97, -43.45, -42.39, -48.45,\n",
       "        -41.51, -47.1 , -48.31, -41.77, -48.5 , -44.73, -37.22, -46.92,\n",
       "        -45.72, -48.27,   1.  ,   1.  ,   4.  ,  54.  ],\n",
       "       [-44.57, -48.3 , -47.41, -45.44, -47.84, -48.05, -40.1 , -48.4 ,\n",
       "        -47.74, -43.97, -48.36, -47.69, -48.45, -47.35, -47.87, -46.03,\n",
       "        -45.8 , -48.37,   1.  ,   8.  ,   5.  ,  28.  ],\n",
       "       [-46.97, -48.03, -48.29, -48.23, -42.8 , -46.21, -47.44, -48.42,\n",
       "        -45.44, -44.07, -48.46, -48.1 , -48.49, -40.56, -24.66, -47.23,\n",
       "        -44.88, -47.04,   1.  ,   0.  ,   8.  ,  61.  ],\n",
       "       [-44.91, -46.79, -47.54, -47.35, -41.77, -39.21, -48.3 , -48.35,\n",
       "        -48.05, -42.41, -48.34, -48.38, -48.4 , -46.04, -42.97, -48.33,\n",
       "        -48.27, -48.37,   1.  ,   1.  ,   2.  ,  41.  ],\n",
       "       [-45.37, -47.04, -48.  , -46.39, -43.83, -38.73, -47.79, -48.47,\n",
       "        -47.99, -38.71, -48.32, -43.51, -48.46, -47.21, -46.6 , -48.22,\n",
       "        -47.65, -48.35,   1.  ,   2.  ,   3.  ,  61.  ],\n",
       "       [-41.85, -48.44, -45.91, -47.42, -32.12, -42.01, -47.12, -48.42,\n",
       "        -46.08, -30.93, -48.34, -40.38, -48.5 , -44.57, -41.18, -48.02,\n",
       "        -42.76, -48.37,   1.  ,   1.  ,   3.  ,  53.  ],\n",
       "       [-44.36, -48.34, -47.88, -46.72, -48.23, -48.25, -45.26, -48.36,\n",
       "        -45.1 , -46.81, -48.02, -48.26, -38.2 , -46.4 , -47.82, -45.23,\n",
       "        -46.87, -42.2 ,   1.  ,   9.  ,   8.  ,  36.  ],\n",
       "       [-44.91, -48.26, -47.21, -48.42, -48.25, -48.29, -43.35, -48.44,\n",
       "        -46.9 , -46.76, -48.12, -48.36, -48.47, -47.86, -48.23, -47.6 ,\n",
       "        -48.26, -39.88,   1.  ,   9.  ,   8.  ,  48.  ],\n",
       "       [-44.58, -46.1 , -36.75, -47.19, -47.63, -48.28, -45.02, -48.41,\n",
       "        -47.94, -46.21, -48.24, -48.42, -48.39, -48.07, -48.42, -48.35,\n",
       "        -48.27, -48.32,   1.  ,   9.  ,   1.  ,  54.  ],\n",
       "       [-42.65, -46.68, -47.94, -48.03, -41.67, -47.67, -47.04, -48.37,\n",
       "        -48.29, -41.7 , -48.29, -44.14, -48.54, -47.43, -47.16, -48.3 ,\n",
       "        -48.05, -48.37,   1.  ,   0.  ,   0.  ,  42.  ],\n",
       "       [-48.14, -48.38, -48.34, -48.29, -47.54, -48.24, -48.31, -48.37,\n",
       "        -44.85, -47.46, -48.35, -46.05, -48.53, -46.89, -30.34, -47.99,\n",
       "        -45.22, -47.55,   1.  ,   1.  ,   8.  ,  36.  ],\n",
       "       [-39.54, -48.31, -47.72, -47.63, -47.81, -48.35, -43.95, -48.4 ,\n",
       "        -45.86, -43.74, -48.36, -48.23, -48.46, -47.01, -48.12, -48.29,\n",
       "        -48.48, -48.37,   1.  ,   7.  ,   4.  ,  56.  ],\n",
       "       [-39.05, -41.71, -29.62, -43.3 , -44.72, -47.79, -39.07, -48.38,\n",
       "        -45.15, -41.47, -47.31, -47.9 , -48.48, -46.05, -48.23, -48.2 ,\n",
       "        -47.17, -48.34,   1.  ,   9.  ,   1.  ,  47.  ],\n",
       "       [-47.65, -48.31, -48.29, -48.17, -48.09, -48.16, -46.08, -48.4 ,\n",
       "        -47.84, -48.1 , -48.35, -48.33, -48.45, -48.01, -48.34, -46.07,\n",
       "        -47.55, -48.37,   1.  ,   7.  ,   8.  ,  37.  ],\n",
       "       [-47.41, -48.02, -48.34, -48.33, -47.82, -48.24, -45.2 , -48.38,\n",
       "        -47.49, -46.8 , -48.44, -48.25, -48.47, -47.9 , -48.11, -48.37,\n",
       "        -48.  , -48.29,   1.  ,   6.  ,   5.  ,  48.  ],\n",
       "       [-46.18, -48.27, -47.71, -48.43, -48.06, -48.36, -45.08, -48.34,\n",
       "        -47.59, -47.36, -48.34, -48.38, -48.46, -48.1 , -48.25, -48.08,\n",
       "        -48.38, -42.23,   1.  ,   9.  ,   8.  ,  52.  ],\n",
       "       [-41.74, -35.84, -47.19, -47.86, -44.77, -47.87, -47.91, -48.44,\n",
       "        -47.91, -45.97, -48.34, -48.24, -48.57, -47.72, -48.32, -48.37,\n",
       "        -48.26, -48.2 ,   1.  ,   6.  ,   1.  ,  49.  ],\n",
       "       [-47.74, -48.35, -48.38, -48.22, -47.98, -48.3 , -46.25, -48.35,\n",
       "        -46.85, -46.  , -48.34, -48.25, -48.5 , -45.46, -48.36, -44.02,\n",
       "        -42.95, -48.37,   1.  ,   5.  ,   7.  ,  40.  ],\n",
       "       [-48.27, -48.44, -48.3 , -48.43, -48.29, -48.3 , -48.22, -48.45,\n",
       "        -48.2 , -48.29, -48.43, -48.5 , -48.52, -48.2 , -48.35, -48.34,\n",
       "        -48.34, -48.37,   1.  ,   7.  ,   8.  ,  34.  ],\n",
       "       [-46.99, -47.58, -47.92, -46.43, -47.99, -46.31, -47.9 , -48.41,\n",
       "        -44.19, -42.01, -48.33, -46.99, -48.4 , -35.58, -42.26, -43.45,\n",
       "        -41.38, -48.37,   1.  ,   2.  ,   9.  ,  31.  ],\n",
       "       [-44.42, -48.1 , -47.81, -47.95, -48.09, -48.28, -46.85, -48.41,\n",
       "        -47.91, -47.87, -48.29, -48.26, -48.51, -48.13, -48.2 , -48.3 ,\n",
       "        -48.11, -48.14,   1.  ,   7.  ,   4.  ,  52.  ],\n",
       "       [-40.28, -48.35, -47.42, -46.26, -40.02, -38.41, -47.8 , -48.42,\n",
       "        -47.58, -40.94, -48.34, -42.97, -48.47, -48.22, -47.72, -48.43,\n",
       "        -48.33, -48.37,   1.  ,   3.  ,   2.  ,  41.  ],\n",
       "       [-36.51, -41.11, -46.35, -34.4 , -47.07, -47.85, -37.44, -48.38,\n",
       "        -47.8 , -42.1 , -48.13, -48.19, -48.44, -46.29, -47.83, -48.42,\n",
       "        -46.32, -47.08,   1.  ,   8.  ,   3.  ,  49.  ],\n",
       "       [-42.65, -48.3 , -47.12, -46.04, -47.74, -48.04, -43.88, -48.4 ,\n",
       "        -40.53, -44.12, -47.02, -47.53, -35.  , -44.09, -46.11, -41.11,\n",
       "        -44.95, -37.64,   1.  ,   9.  ,   8.  ,  52.  ],\n",
       "       [-48.08, -48.32, -48.35, -48.41, -46.47, -48.15, -48.49, -48.48,\n",
       "        -48.28, -47.96, -48.33, -48.34, -48.51, -48.4 , -48.37, -48.34,\n",
       "        -48.37, -48.32,   1.  ,   9.  ,   2.  ,  46.  ],\n",
       "       [-45.45, -47.76, -46.92, -45.2 , -47.89, -48.24, -46.85, -48.4 ,\n",
       "        -47.86, -47.38, -48.33, -48.18, -48.47, -48.24, -48.23, -48.12,\n",
       "        -48.21, -48.37,   1.  ,   8.  ,   4.  ,  37.  ],\n",
       "       [-41.96, -47.1 , -43.91, -47.48, -47.32, -48.39, -37.62, -48.38,\n",
       "        -44.32, -44.09, -47.  , -47.22, -47.99, -40.16, -45.41, -40.87,\n",
       "        -44.44, -29.52,   1.  ,   9.  ,   8.  ,  59.  ],\n",
       "       [-31.58, -26.66, -44.38, -46.13, -46.15, -46.24, -47.35, -48.41,\n",
       "        -48.15, -45.33, -48.39, -48.26, -48.49, -47.81, -48.36, -48.12,\n",
       "        -48.32, -48.37,   1.  ,   6.  ,   0.  ,  51.  ],\n",
       "       [-48.3 , -48.42, -48.41, -48.4 , -48.29, -48.29, -48.52, -48.38,\n",
       "        -48.28, -48.23, -48.43, -48.45, -48.48, -48.36, -48.38, -48.37,\n",
       "        -48.36, -48.37,   1.  ,   5.  ,   7.  ,  43.  ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_samples: 30 , New samples: 30\n",
      "Validation size: 6 , starts: 30 , ends: 35\n",
      "(30,)\n",
      "(30, 7, 100, 100)\n",
      "loss\n",
      "Tensor(\"ExpandDims:0\", shape=(None, 1), dtype=float32)\n",
      "Tensor(\"sequential_1/dense_5/BiasAdd:0\", shape=(None, 1), dtype=float32)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    <ipython-input-17-13720a3cf3d3>:134 loss\n        return K.mean(K.square(res))\n\n    AttributeError: 'int' object has no attribute 'mean'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3a2cde5ee55c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m                            \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlamb_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                            \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mWORKERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_QUEUE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                            use_multiprocessing=False)\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nLambda:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\", Time:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlambda_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /home/shahrokh/miniconda3/envs/research/lib/python3.7/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    <ipython-input-17-13720a3cf3d3>:134 loss\n        return K.mean(K.square(res))\n\n    AttributeError: 'int' object has no attribute 'mean'\n"
     ]
    }
   ],
   "source": [
    "# CNN: support batching\n",
    "TEST, CONSERVE = True, False\n",
    "mini_batch = 16 if max(max_x, max_y) == 1000 else 64\n",
    "epochs = 35 if max(max_x, max_y) == 1000 else 1000\n",
    "MAX_QUEUE_SIZE, WORKERS = 6, 1\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "hyper_metric, mode = \"val_mae\", 'min'  # the metric that hyper parameters are tuned with\n",
    "prev_sample = 0\n",
    "lambda_vec = [0, 0.001, 0.01, 0.1, 1]  #0.003, 0.01, 0.03, 0.1, 0.3, 1, 3\n",
    "average_diff_power, fp_mean_power = [],[] #[7.177, 8.088, 8.183], [3.438, 3.506, 2.662]\n",
    "best_lambda = []\n",
    "average_diff_power_conserve, fp_mean_power_conserve = [], []\n",
    "all_cnns = []\n",
    "if CONSERVE: # for conservative\n",
    "    prev_number_samples = [0] + number_samples[:-1]\n",
    "\n",
    "for num_sample_idx, number_sample in enumerate(number_samples):\n",
    "#     if num_sample_idx < 3:\n",
    "#         continue\n",
    "#     if num_sample_idx == 0:\n",
    "    if CONSERVE:\n",
    "        data_reg[prev_number_samples[num_sample_idx]:number_sample, -1] = data_reg[\n",
    "            prev_number_samples[num_sample_idx]:number_sample, -1] - 1 # conserv value\n",
    "    MODEL_PATH = '/'.join(image_dir.split('/')[:-1]) + '/models/' + str(number_sample)\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "    MODEL_PATH += \"/best_model_lambda_\"\n",
    "    if True:\n",
    "        cnns = [cnn_model(10, lamb, 0) for lamb in lambda_vec]\n",
    "        for cnn in cnns:\n",
    "#             cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae', fp_mean])\n",
    "            cnn.compile(loss=custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                        optimizer='adam', \n",
    "                        metrics=['mse', 'mae', fp_mae])\n",
    "        checkpointers = [ModelCheckpoint(filepath=MODEL_PATH + str(lamb_idx)+ '.h5',\n",
    "                                         verbose=1, save_best_only=True, \n",
    "                                         monitor=hyper_metric,\n",
    "                                         mode=mode)\n",
    "                         for lamb_idx in range(len(lambda_vec))]\n",
    "    else:\n",
    "        cnns = []\n",
    "        cnns = [models.load_model(MODEL_PATH + str(lamb_idx) + '.h5', \n",
    "                                  custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                  'fp_mae': fp_mae }) \n",
    "                for lamb_idx in range(len(lambda_vec))]\n",
    "    number_start = time.time()\n",
    "    train_generator = DataBatchGenerator(dataset=data_reg[prev_sample:number_sample], batch_size=mini_batch,\n",
    "                                         start_idx=prev_sample, number_image_channels=number_image_channels,\n",
    "                                         max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "    \n",
    "\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "    val_generator = DataBatchGenerator(dataset=data_reg[number_sample:number_sample+val_size], \n",
    "                                       batch_size=mini_batch,\n",
    "                                       start_idx=number_sample,\n",
    "                                       number_image_channels=number_image_channels,\n",
    "                                       max_x=max_x, max_y=max_y, \n",
    "                                       float_memory_used=float_memory_used)\n",
    "  \n",
    "    print('number_samples:', number_sample, \", New samples:\", number_sample - prev_sample)\n",
    "    print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", number_sample + val_size - 1)\n",
    "    \n",
    "    for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "        lambda_start = time.time()\n",
    "        cnns[lamb_idx].fit(train_generator, epochs=epochs, verbose=0,\n",
    "                           validation_data=val_generator, \n",
    "                           shuffle=False, callbacks=[checkpointers[lamb_idx]], \n",
    "                           workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                           use_multiprocessing=False)\n",
    "        \n",
    "        print(\"\\nLambda:\", lamb, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - lambda_start))))\n",
    "        print(\"Train Error(all epochs):\", min(cnns[lamb_idx].history.history['mae']), '\\n', \n",
    "              [round(val, 3) for val in cnns[lamb_idx].history.history['mae']])\n",
    "        print(\"Train FP Error(all epochs):\", min(cnns[lamb_idx].history.history['fp_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['fp_mae']])\n",
    "        print(\"Val Error(all epochs):\", min(cnns[lamb_idx].history.history['val_mae']), '\\n', \n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_mae']])\n",
    "        print(\"Val FP Error(all epochs):\", min(cnns[lamb_idx].history.history['val_fp_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_fp_mae']])\n",
    "\n",
    "    models_min_mae = [min(cnns[lam_idx].history.history[hyper_metric]) for\n",
    "                      lam_idx,_ in enumerate(lambda_vec)]\n",
    "    best_lamb_idx = models_min_mae.index(min(models_min_mae))\n",
    "    best_lambda.append(lambda_vec[best_lamb_idx])\n",
    "    print(\"\\nTrainig set size:\", number_sample, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - \n",
    "                                                                                              number_start))),\n",
    "          \", best_lambda:\", lambda_vec[best_lamb_idx], \", min_\" , (\"fp_\" if hyper_metric == \"val_fp_mae\" else \"\"),\n",
    "          \"error:\", round(min(models_min_mae), 3))\n",
    "    all_cnns.append(cnns)\n",
    "    del cnns, train_generator, val_generator, checkpointers\n",
    "    \n",
    "    if TEST:\n",
    "        # evaluating test images\n",
    "        best_model = None\n",
    "        best_model = models.load_model(MODEL_PATH + str(best_lamb_idx) + '.h5', \n",
    "                                       custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                       'fp_mae': fp_mae,\n",
    "                                                      'mae':'mae', 'mse':'mse'})\n",
    "        test_generator = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                            batch_size=mini_batch,\n",
    "                                            start_idx=number_sample + val_size, \n",
    "                                            number_image_channels=number_image_channels,\n",
    "                                            max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "\n",
    "        print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "        time.sleep(1)\n",
    "        test_res = best_model.evaluate(test_generator, verbose=1, \n",
    "                                       workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)\n",
    "        \n",
    "        test_mae_idx, test_fp_mae_idx = [best_model.metrics_names.index(mtrc) \n",
    "                                         for mtrc in ['mae','fp_mae']]\n",
    "        test_mae, test_fp_mae = test_res[test_mae_idx], test_res[test_fp_mae_idx]\n",
    "        average_diff_power.append(round(test_mae, 3))\n",
    "        fp_mean_power.append(round(test_fp_mae, 3))\n",
    "        print('average_error: ', average_diff_power[-1], ', fp_average_error: ', \n",
    "              fp_mean_power[-1])\n",
    "        \n",
    "        if False:\n",
    "            test_generator_conserve = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                                         batch_size=mini_batch,\n",
    "                                                         start_idx=number_sample + val_size, \n",
    "                                                         number_image_channels=number_image_channels,\n",
    "                                                         max_x=max_x, max_y=max_y, \n",
    "                                                         float_memory_used=float_memory_used, \n",
    "                                                         conserve=1)\n",
    "            test_res_conserve = best_model.evaluate(test_generator_conserve, verbose=1, \n",
    "                                                    workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                                                    use_multiprocessing=False)\n",
    "            test_mae_cons, test_fp_mae_cons = test_res_conserve[test_mae_idx], test_res_conserve[test_fp_mae_idx]\n",
    "            average_diff_power_conserve.append(round(test_mae_cons, 3))\n",
    "            fp_mean_power_conserve.append(round(test_fp_mae_cons, 3))\n",
    "            print('Conserve, average_error: ', average_diff_power_conserve[-1], ', fp_average_error: ',\n",
    "                 fp_mean_power_conserve[-1])\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        \n",
    "        var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/' + intensity_degradation + '_' + str(slope) + '_' + \n",
    "                     dtime + \".dat\", \"wb\") # file for saving results\n",
    "        pickle.dump([average_diff_power, fp_mean_power, number_samples, best_lambda, \n",
    "                     dataset_name, average_diff_power_conserve, fp_mean_power_conserve],\n",
    "                    file=var_f)\n",
    "        var_f.close()\n",
    "        del best_model, test_generator\n",
    "#     prev_sample = number_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(all_cnns[0][0].history.history['val_mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test starts:  252 , ends:  298\n",
      "1/1 [==============================] - 0s 513us/step - loss: 140.5992 - mse: 140.3283 - mae: 9.9862 - fp_mae: 5.2910\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_model = models.load_model(MODEL_PATH + str(1) + '.h5', \n",
    "                               custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                               'fp_mae': fp_mae,\n",
    "                                               'mae':'mae', 'mse':'mse'})\n",
    "test_generator = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                    batch_size=mini_batch,\n",
    "                                    start_idx=number_sample + val_size, \n",
    "                                    number_image_channels=number_image_channels,\n",
    "                                    max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "\n",
    "print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "time.sleep(1)\n",
    "test_res = best_model.evaluate(test_generator, verbose=1, \n",
    "                               workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 60, 90, 120, 150, 180, 210]\n",
      "[11.531, 12.167, 11.537, 7.128, 8.482, 6.527, 5.388]\n",
      "[0.808, 1.68, 0.854, 3.847, 1.295, 1.695, 2.236]\n",
      "[]\n",
      "[]\n",
      "[1, 1, 0.1, 1, 1, 1, 0.001]\n"
     ]
    }
   ],
   "source": [
    "print(number_samples)\n",
    "print(average_diff_power)\n",
    "print(fp_mean_power)\n",
    "# print(best_lambda)\n",
    "print(average_diff_power_conserve)\n",
    "print(fp_mean_power_conserve)\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    checkpointers = ModelCheckpoint(filepath=MODEL_PATH + str(0)+ 'new.h5',\n",
    "                                         verbose=1, save_best_only=True, \n",
    "                                         monitor=hyper_metric,\n",
    "                                         mode=mode)\n",
    "    number_start = time.time()\n",
    "    train_generator = DataBatchGenerator(dataset=data_reg[prev_sample:number_sample], batch_size=mini_batch,\n",
    "                                         start_idx=prev_sample, number_image_channels=number_image_channels,\n",
    "                                         max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "    \n",
    "\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "    val_generator = DataBatchGenerator(dataset=data_reg[number_sample:number_sample+val_size], \n",
    "                                       batch_size=mini_batch,\n",
    "                                       start_idx=number_sample,\n",
    "                                       number_image_channels=number_image_channels,\n",
    "                                       max_x=max_x, max_y=max_y, \n",
    "                                       float_memory_used=float_memory_used)\n",
    "  \n",
    "    print('number_samples:', number_sample, \", New samples:\", number_sample - prev_sample)\n",
    "    print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", number_sample + val_size - 1)\n",
    "    best_model.fit(train_generator, epochs=80, verbose=0,\n",
    "                   validation_data=val_generator, shuffle=True, callbacks=[checkpointers], \n",
    "                   workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                   use_multiprocessing=False, initial_epoch=60)\n",
    "    print(\"Train Error(all epochs):\", min(best_model.history.history['mae']), '\\n',\n",
    "          [round(val, 3) for val in best_model.history.history['mae']])\n",
    "    print(\"Train FP Error(all epochs):\", min(best_model.history.history['fp_mae']), '\\n',\n",
    "          [round(val,3) for val in best_model.history.history['fp_mae']])\n",
    "    print(\"Val Error(all epochs):\", min(best_model.history.history['val_mae']), '\\n', \n",
    "          [round(val,3) for val in best_model.history.history['val_mae']])\n",
    "    print(\"Val FP Error(all epochs):\", min(best_model.history.history['val_fp_mae']), '\\n',\n",
    "          [round(val,3) for val in best_model.history.history['val_fp_mae']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_best_model = models.load_model(MODEL_PATH + str(0) + 'new.h5', \n",
    "                               custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                               'fp_mae': fp_mae,\n",
    "                                               'mae':'mae', 'mse':'mse'})\n",
    "test_generator = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                            batch_size=mini_batch,\n",
    "                                            start_idx=number_sample + val_size, \n",
    "                                            number_image_channels=number_image_channels,\n",
    "                                            max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "\n",
    "print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "time.sleep(1)\n",
    "test_res = best_best_model.evaluate(test_generator, verbose=1, \n",
    "                                    workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE,\n",
    "                                    use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(train_generator, epochs=15, verbose=0,\n",
    "               validation_data=val_generator, shuffle=True, callbacks=[checkpointers], \n",
    "               workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "               use_multiprocessing=False, initial_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('/'.join(image_dir.split('/')[:-1]) + '/log_5__202008_2412_53.dat', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "[average_diff_power, fp_mean_power, number_samples, best_lambda, \n",
    " dataset_name, average_diff_power_conserve, fp_mean_power_conserve, all_diff_power, \n",
    " all_fp_diff_power, all_diff_power_conserve, all_fp_diff_conserve] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6.495, 7.823, 5.561, 7.709, 8.139]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_diff_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[250, 299]\n",
      "[6.158666666666666]\n",
      "[2.815]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[[6.157, 6.337, 5.982]]\n"
     ]
    }
   ],
   "source": [
    "print(number_samples)\n",
    "print(average_diff_power)\n",
    "print(fp_mean_power)\n",
    "print(best_lambda)\n",
    "print(average_diff_power_conserve)\n",
    "print(fp_mean_power_conserve)\n",
    "print(all_diff_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 150, 200, 250, 299]\n",
      "[11.892199999999999]\n",
      "[4.2056]\n",
      "[]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'average_power_conserve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7b3162151c82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_mean_power\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_lambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_power_conserve\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_mean_power_conserve\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'average_power_conserve' is not defined"
     ]
    }
   ],
   "source": [
    "#150\n",
    "print(number_samples)\n",
    "print(average_diff_power)\n",
    "print(fp_mean_power)\n",
    "print(best_lambda)\n",
    "print(average_power_conserve)\n",
    "print(fp_mean_power_conserve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = 5, validation_size = 0.2\n",
    "average_error = [8.763, 11.892, 9.746, 7.96, 7.14, 6.59]\n",
    "fp_error = [1.56, 4.2, 2.72, 2.29, 3.04, 3.02]\n",
    "# 6.5964 FP Average Error: 3.0208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = 10, validation_size = 0.1\n",
    "average_error = [11.05, 11.21, 8.23, 6.75, 6.12, 5.8]\n",
    "fp_error = [4.34, 3.8643, 3.743, 2.04, 3.34, 2.837]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
