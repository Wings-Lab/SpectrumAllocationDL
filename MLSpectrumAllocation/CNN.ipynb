{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datetime, time\n",
    "import os, sys\n",
    "import tqdm\n",
    "import gc\n",
    "from multiprocessing import Process\n",
    "Point = namedtuple('Point', ('x', 'y'))\n",
    "Circle = namedtuple('Circle', ('r'))\n",
    "Square = namedtuple('Square', ('side'))\n",
    "Rectangle = namedtuple('Rectangle', ('length', 'width'))\n",
    "PointWithDistance = namedtuple('PointWithDistance', ('p', 'dist'))\n",
    "float_memory_used = 'float16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT\n",
    "# PART 1\n",
    "# number_samples = [5] + list(range(10, 101, 10)) + [120, 150, 200, 250, 300, 400, 500, 700] + list(range(1000, 10001, 1000))\n",
    "number_samples = [128, 256, 512, 1024, 2048, 4096] \n",
    "# number_samples = [4096, 4915, 5734, 6554, 7373, 8192]\n",
    "\n",
    "# cnn_type = \"classification\"  # {\"classification\", \"regression\"}\n",
    "validation_size, noise_floor = 0.2, -110.0\n",
    "su_power = 0 # this is not actually su power just a number to show there is an SU in its image\n",
    "max_x, max_y, number_image_channels, su_szie = 100, 100, 8, 50  # su_size:30 for 1000, 10 for 100\n",
    "cell_size = 10\n",
    "pu_shape, su_shape = 'circle', 'circle' # shape = {'circle', 'square', 'point'}\n",
    "style = \"raw_power_min_max_norm\"  # {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "intensity_degradation, slope = 'log', 5  # 'log', 'linear', slope 3 for 1000, 5 for 100\n",
    "max_pus_num, max_sus_num = 20, 5\n",
    "propagation_model = 'splat' # 'splat', 'log', 'testbed'\n",
    "noise, std = False, 1 # False for splat\n",
    "if su_shape == 'circle':\n",
    "    su_param = Circle(su_szie)\n",
    "elif su_shape == 'square':\n",
    "    su_param = Square(su_szie)\n",
    "else:\n",
    "    su_param = None\n",
    "    \n",
    "sensors = False\n",
    "if sensors:\n",
    "    sensors_num = 1600\n",
    "    sensors_file_path = f\"data/sensors/square{100}/{sensors_num}/sensors.txt\"\n",
    "# num_pus = (data_reg.shape[1] - 3)//3\n",
    "\n",
    "# PART 2\n",
    "number_of_proccessors = 10\n",
    "memory_size_allowed = 4 # in Gigabyte\n",
    "float_size = 0\n",
    "if float_memory_used == \"float16\":\n",
    "    float_size = 16\n",
    "elif float_memory_used == \"float\" or \"float32\":\n",
    "    float_size = 32\n",
    "elif float_memory_used == \"float8\":\n",
    "    float_size = 8\n",
    "\n",
    "\n",
    "batch_size = int(memory_size_allowed / (max_x * max_y * number_image_channels * float_size/(8 * 1024 ** 3)))\n",
    "\n",
    "\n",
    "dtime = datetime.datetime.now().strftime('_%Y%m_%d%H_%M')\n",
    "color = \"color\" if number_image_channels > 1 else \"gray\"\n",
    "image_dir = 'ML/data/pictures_' + str(max_x) + '_' + str(max_y) + '/' + propagation_model + (\n",
    "    \"/noisy_std_\" + str(std) if noise else \"\") + '/pu_' + pu_shape + '_su_' + su_shape + '_' + (\n",
    "    \"\" if su_shape == 'point' else str(su_szie)) + \"/\" + style + \"/\" + color +'/' + (\n",
    "    \"\" if pu_shape == 'point' and su_shape == 'point' else (intensity_degradation + '_' + str(slope))) + (\n",
    "    \"/\" + str(sensors_num) + \"sensors\" if sensors else f\"/pus_1_sus_{number_image_channels}_channels\") + \"/images\"\n",
    "image_dir = 'ML/data/pictures_' + str(max_x) + '_' + str(max_y) + '_transfer/' + propagation_model + (\n",
    "    \"/noisy_std_\" + str(std) if noise else \"\") + '/pu_' + pu_shape + '_su_' + su_shape + '_' + (\n",
    "    \"\" if su_shape == 'point' else str(su_szie)) + \"/\" + style + \"/\" + color +'/' + (\n",
    "    \"\" if pu_shape == 'point' and su_shape == 'point' else (f\"{intensity_degradation}_{slope}\")) + (\n",
    "    \"/\" + str(sensors_num) + \"sensors\" if sensors else f\"/{max_pus_num}pus\") + \\\n",
    "        f\"_{max_sus_num}sus_{number_image_channels}channels\" + \"/images\"\n",
    "\n",
    "if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/images'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "num_columns = (sensors_num if sensors else max_pus_num * 3 + 1) + max_sus_num * 3 + 2\n",
    "cols = [i for i in range(num_columns)]\n",
    "dataset_name = \"dynamic_pus_using_pus_70000_min10_max20PUs_min1_max5SUs_square100grid_splat_2021_10_19_17_54.txt\"\n",
    "max_dataset_name = \"dynamic_pus_max_power_70000_min10_max20PUs_min1_max5SUs_square100grid_splat_2021_10_19_17_54.txt\"\n",
    "with open('/'.join(image_dir.split('/')[:-1]) + '/datasets' + dtime + '.txt', 'w') as set_file:\n",
    "    set_file.write(dataset_name + \"\\n\")\n",
    "    set_file.write(max_dataset_name)\n",
    "\n",
    "dataframe = pd.read_csv('data/' \n",
    "                        + dataset_name, delimiter=',', header=None, names=cols)\n",
    "dataframe_max = pd.read_csv('data/' \n",
    "                            + max_dataset_name, delimiter=',', header=None)\n",
    "\n",
    "dataframe.reset_index(drop=True, inplace=True)\n",
    "dataframe_max.reset_index(drop=True, inplace=True)\n",
    "dataframe_max[dataframe_max.shape[1] - 1] = dataframe_max[dataframe_max.shape[1] - 1].astype(float)\n",
    "\n",
    "dataframe_tot = pd.concat([dataframe, dataframe_max.iloc[:, dataframe_max.columns.values[-1:]]], axis=1,\n",
    "                        ignore_index=True)\n",
    "\n",
    "idx = dataframe_tot[dataframe_tot[dataframe_tot.columns[-1]] == -float('inf')].index\n",
    "dataframe_tot.drop(idx, inplace=True)\n",
    "\n",
    "data_reg = dataframe_tot.values\n",
    "data_reg[data_reg < noise_floor] = noise_floor\n",
    "# data_reg = np.concatenate((dataframe_tot.values[:, 0:dataframe_tot.shape[1]-3], \n",
    "#                            dataframe_tot.values[:, dataframe_tot.shape[1]-1:dataframe_tot.shape[1]]), axis=1)\n",
    "# data_class = dataframe_tot.values[:, 0:dataframe_tot.shape[1]-1]\n",
    "# y_class_power = dataframe_tot.values[:, -1]\n",
    "\n",
    "if sensors:\n",
    "    sensors_location = []\n",
    "    with open(sensors_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.split(',')\n",
    "            sensors_location.append(Point(int(float(line[0])), int(float(line[1]))))\n",
    "del dataframe, dataframe_tot, dataframe_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44931, 79)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 19126 and the array at index 1 has size 4000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2032855/1409826527.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m data_reg = np.concatenate((data_reg[:,:2500], np.ones((4000, 1)), data_reg[:, 2500:2504],\n\u001b[0m\u001b[1;32m      2\u001b[0m                data_reg[:, 2505:]), axis=1)\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 19126 and the array at index 1 has size 4000"
     ]
    }
   ],
   "source": [
    "data_reg = np.concatenate((data_reg[:,:2500], np.ones((4000, 1)), data_reg[:, 2500:2504],\n",
    "               data_reg[:, 2505:]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg[0, sensors_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg = data_reg[:30000][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg[512:1024, :] = data_reg[:512, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg[4096:8192, sensors_num:] = data_reg[:4096, sensors_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_reg[10, :])\n",
    "print(data_reg[266, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(p1: Point, p2: Point):\n",
    "    return ((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2) ** 0.5 * cell_size\n",
    "\n",
    "def calculate_mu_sigma(data, num_pus):\n",
    "    sum_non_noise = 0\n",
    "    for pu_n in range(num_pus): # calculate mu\n",
    "        sum_non_noise += data[pu_n*3+2]\n",
    "    mu = ((max_x * max_y - num_pus) * noise_floor + sum_non_noise)/(max_x * max_y)\n",
    "    sum_square = 0\n",
    "    for pu_n in range(num_pus): # calculate sigma\n",
    "        sum_square += (data[pu_n*3+2]-mu)**2\n",
    "    sum_square += (max_x * max_y - num_pus) * (noise_floor - mu)**2\n",
    "    sigma = math.sqrt(sum_square/(max_x * max_y))\n",
    "    return mu, sigma\n",
    "\n",
    "def get_pu_param(pu_shape: str, intensity_degradation: str, pu_p: float, noise_floor: float, slope: float):\n",
    "    pu_param = None\n",
    "    if pu_shape == 'circle':\n",
    "        if intensity_degradation == \"linear\":\n",
    "            pu_param = Circle(int((pu_p - noise_floor) / slope)) # linear\n",
    "        elif intensity_degradation == \"log\":\n",
    "            pu_param = Circle(int(10 ** ((pu_p - noise_floor) / (10 *slope)))) # log_based\n",
    "    elif pu_shape == 'square':\n",
    "        if intensity_degradation == \"linear\":\n",
    "            pu_param = Square(int(2 ** 0.5 * (pu_p - noise_floor) / slope)) # linear\n",
    "        elif intensity_degradation == \"log\":\n",
    "            pu_param = Square(int(2 ** 0.5 * 10 ** ((pu_p - noise_floor) / (10 *slope)))) # log_based\n",
    "    elif pu_shape == 'point':\n",
    "        pu_param = None\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported PU shape(create_image)! \", pu_shape)\n",
    "    return pu_param\n",
    "\n",
    "def create_image(data, slope, sensors_num, style=\"raw_power_z_score\", noise_floor=-90, pu_shape= 'circle', pu_param=None, \n",
    "                 su_shape='circle', su_param=None, intensity_degradation=\"log\", max_pu_power: float=0):  \n",
    "    # style = {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "    # intensity_degradation= {\"log\", \"linear\"}\n",
    "    # if param is None, it's automatically calculated. Highest brightness(or power value) (255 or 1.) would\n",
    "    # assigned to the center(PU location) and radius(side) would be calculated based on its power, slope, and noise floor.\n",
    "    # If it is given, intensity(power) of pixel beside center would be calculated in the same fashin with an exception that \n",
    "    # intensity below zero(noise_floor) would be replaced by zero(noise_floor)\n",
    "    if style == \"raw_power_min_max_norm\":\n",
    "        # In this way, PUs' location are replaced with their power(dBm) and the power would fade with \n",
    "        # slope till gets noise_floor(in circle shape)\n",
    "        \n",
    "        # creating pu matrix\n",
    "        image = np.zeros((1,number_image_channels,max_x, max_y), dtype=float_memory_used)\n",
    "        if not sensors:\n",
    "            pus_num = int(data[0])\n",
    "#             print(pus_num)\n",
    "            for pu_i in range(pus_num):\n",
    "                pu_x = max(0, min(max_x-1, int(data[pu_i * 3 + 1]))) \n",
    "                pu_y = max(0, min(max_x-1, int(data[pu_i * 3 + 2])))\n",
    "                pu_p = data[pu_i * 3 + 3]\n",
    "#                 print(pu_x, pu_y, pu_p)\n",
    "                if pu_param is None:\n",
    "                    pu_param_p = get_pu_param(pu_shape, intensity_degradation, pu_p, noise_floor, slope)\n",
    "                else:\n",
    "                    pu_param_p = pu_param\n",
    "                points = points_inside_shape(center=Point(pu_x, pu_y), shape=pu_shape, param=pu_param_p)\n",
    "                for point in points:\n",
    "                    if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                        if intensity_degradation == \"linear\":\n",
    "                            image[0][int(abs(pu_p))//10][point.p.x][point.p.y] += (pu_p - slope * point.dist - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                        elif intensity_degradation == \"log\":\n",
    "                            if point.dist < 1:\n",
    "                                image[0][int(abs(pu_p))//5][point.p.x][point.p.y] += (pu_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                            else:\n",
    "                                image[0][int(abs(pu_p))//5][point.p.x][point.p.y] += (pu_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                    max_pu_power - noise_floor)\n",
    "        else:\n",
    "            ss_param, ss_shape = pu_param, pu_shape\n",
    "            for ss_i in range(sensors_num):\n",
    "                ss_x, ss_y, ss_p = max(0, min(max_x-1, int(sensors_location[ss_i].x))), max(0, min(max_x-1, int(\n",
    "                    sensors_location[ss_i].y))), max(noise_floor, data[ss_i])\n",
    "                ss_channel = 0 \n",
    "#                 if -62.5 <= ss_p < -50.0:\n",
    "#                     ss_channel = 1\n",
    "#                 elif -75.0 <= ss_p < -62.6:\n",
    "#                     ss_channel = 2\n",
    "#                 elif -87.5 <= ss_p < -75.0:\n",
    "#                     ss_channel = 3\n",
    "#                 elif -100.0 <= ss_p < -87.5:\n",
    "#                     ss_channel = 4\n",
    "# #                 elif -70.0 <= ss_p < -65.0:\n",
    "# #                     ss_channel = 5\n",
    "#                 elif ss_p < -100.0:\n",
    "#                     ss_channel = 5\n",
    "#                 if -75 <= ss_p < -65.0:\n",
    "#                     ss_channel = 1\n",
    "#                 elif -90.0 <= ss_p < -75:\n",
    "#                     ss_channel = 2\n",
    "#                 elif -90.0 <= ss_p < -80.0:\n",
    "#                     ss_channel = 3\n",
    "#                 elif -100.0 <= ss_p < -90.0:\n",
    "#                     ss_channel = 4\n",
    "#                 if ss_p < -70.0:\n",
    "#                     ss_channel = 1\n",
    "                if ss_param is None:\n",
    "                    ss_param_p = get_pu_param(ss_shape, intensity_degradation, ss_p, noise_floor, slope)\n",
    "                else:\n",
    "                    ss_param_p = ss_param\n",
    "                points = points_inside_shape(center=Point(ss_x, ss_y), shape=ss_shape, param=ss_param_p)\n",
    "                for point in points:\n",
    "                    if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                        if intensity_degradation == \"linear\":\n",
    "                            image[0][ss_channel][point.p.x][point.p.y] += (ss_p - slope * point.dist - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                        elif intensity_degradation == \"log\":\n",
    "                            if point.dist < 1:\n",
    "                                image[0][ss_channel][point.p.x][point.p.y] += (ss_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                            else:\n",
    "                                image[0][ss_channel][point.p.x][point.p.y] += (ss_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                    max_pu_power - noise_floor)\n",
    "        del points\n",
    "        # creating su matrix\n",
    "        su_num_idx = sensors_num if sensors else (pus_num * 3 + 1)\n",
    "        su_num = int(data[su_num_idx])\n",
    "#         print(su_num)\n",
    "#         su_num = (len(data) - pus_num * (3 if not sensors else 1)) // 2\n",
    "#         if not (len(data) - pus_num * (3 if not sensors else 1)) % 2:\n",
    "#             raise ValueError(\"Data provided is not correct; can't get SUs' information(create_image)\")\n",
    "        if su_param is None:\n",
    "            # if su_param is unavailable, a circle(square) with radius(side) 1 is created\n",
    "            if su_shape == 'circle':\n",
    "                su_param = Circle(1)\n",
    "            elif su_shape == 'square':\n",
    "                su_param = Square(1)\n",
    "            elif su_shape == 'point':\n",
    "                su_param = None\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported SU shape(create_image)! \", su_shape)\n",
    "        \n",
    "        for su_i in range(su_num - 1):\n",
    "            su_x = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 1])))\n",
    "            su_y = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 2])))\n",
    "            su_p = data[su_num_idx + su_i * 3 + 3]\n",
    "#             su_p = su_intensity\n",
    "            su_param_p = get_pu_param(su_shape, intensity_degradation, su_p, noise_floor, slope)\n",
    "            points = points_inside_shape(center=Point(su_x, su_y), param=su_param_p, shape=su_shape)\n",
    "            su_channel = 0 if number_image_channels == 1 else -2\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                            su_val = (su_p - slope * point.dist - noise_floor)/(max_pu_power - noise_floor)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "                            su_val = (su_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                        else:\n",
    "                            su_val = (su_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                    image[0][su_channel][point.p.x][point.p.y] += su_val\n",
    "            del points\n",
    "        # the last and  target SU\n",
    "        su_intensity = 1.\n",
    "        su_x = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 1])))\n",
    "        su_y = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 2])))\n",
    "#         print(su_x, su_y)\n",
    "        points = points_inside_shape(center=Point(su_x, su_y), param=su_param, shape=su_shape)\n",
    "        su_channel = 0 if number_image_channels == 1 else -1\n",
    "        for point in points:\n",
    "            if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                image[0][su_channel][point.p.x][point.p.y] += su_intensity\n",
    "        del points\n",
    "        return image\n",
    "        \n",
    "#         pu_image = [[(noise_floor - mu)/sigma] * max_y for _ in range(max_x)]\n",
    "    elif style == \"image_intensity\":\n",
    "        # creating PU image\n",
    "        image = np.zeros((1,number_image_channels,max_x, max_y), dtype=float_memory_used)\n",
    "        pus_num = int(data[0])\n",
    "#             print(pus_num)\n",
    "        \n",
    "#         for pu_i in range(pus_num):\n",
    "#             pu_x, pu_y, pu_p = max(0, min(max_x-1, int(data[pu_i*3]))), max(0, min(max_x-1, int(data[pu_i*3+1]))), data[pu_i*3+2]\n",
    "#             if pu_param is None:\n",
    "#                 pu_param_p = get_pu_param(pu_shape, intensity_degradation, pu_p, noise_floor, slope)\n",
    "#             else:\n",
    "#                 pu_param_p = pu_param\n",
    "#             points = points_inside_shape(center=Point(pu_x, pu_y), shape=pu_shape, param=pu_param_p)\n",
    "#             for point in points:\n",
    "#                 if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "#                     if intensity_degradation == \"linear\":\n",
    "#                         image[0][0][point.p.x][point.p.y] += max((pu_p - slope * point.dist + abs(noise_floor))\n",
    "#                                                               /(pu_p + abs(noise_floor)), 0)\n",
    "#                     elif intensity_degradation == \"log\":\n",
    "#                         if point.dist < 1:\n",
    "#                             image[0][0][point.p.x][point.p.y] = 1\n",
    "#                         else:\n",
    "#                             image[0][0][point.p.x][point.p.y] += max((pu_p - slope * 10*math.log10(point.dist) + abs(noise_floor))\n",
    "#                                                                  /(pu_p + abs(noise_floor)), 0)\n",
    "#                     image[0][0][point.p.x][point.p.y] = min(image[0][0][point.p.x][point.p.y], 1.0)\n",
    "        for pu_i in range(pus_num):\n",
    "            pu_x = max(0, min(max_x-1, int(data[pu_i * 3 + 1]))) \n",
    "            pu_y = max(0, min(max_x-1, int(data[pu_i * 3 + 2])))\n",
    "            pu_p = data[pu_i * 3 + 3]\n",
    "            \n",
    "            if pu_param is None:\n",
    "                pu_param_p = get_pu_param(pu_shape, intensity_degradation, pu_p, noise_floor, slope)\n",
    "            else:\n",
    "                pu_param_p = pu_param\n",
    "            \n",
    "            points = points_inside_shape(center=Point(pu_x, pu_y), shape=pu_shape, param=pu_param_p)\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                        image[0][0][point.p.x][point.p.y] += (pu_p - slope * point.dist - noise_floor)/(\n",
    "                            max_pu_power - noise_floor)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "#                             image[0][0][point.p.x][point.p.y] += (pu_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                            image[0][0][point.p.x][point.p.y] += 0.1\n",
    "                        else:\n",
    "#                             image[0][0][point.p.x][point.p.y] += (pu_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "#                                 max_pu_power - noise_floor)\n",
    "                            image[0][0][point.p.x][point.p.y] += 0.1\n",
    "                        \n",
    "        # creating SU image\n",
    "        del points\n",
    "        # creating su matrix\n",
    "        su_num_idx = sensors_num if sensors else (pus_num * 3 + 1)\n",
    "        su_num = int(data[su_num_idx])\n",
    "#         print(su_num)\n",
    "#         su_num = (len(data) - pus_num * (3 if not sensors else 1)) // 2\n",
    "#         if not (len(data) - pus_num * (3 if not sensors else 1)) % 2:\n",
    "#             raise ValueError(\"Data provided is not correct; can't get SUs' information(create_image)\")\n",
    "        if su_param is None:\n",
    "            # if su_param is unavailable, a circle(square) with radius(side) 1 is created\n",
    "            if su_shape == 'circle':\n",
    "                su_param = Circle(1)\n",
    "            elif su_shape == 'square':\n",
    "                su_param = Square(1)\n",
    "            elif su_shape == 'point':\n",
    "                su_param = None\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported SU shape(create_image)! \", su_shape)\n",
    "        \n",
    "        for su_i in range(su_num - 1):\n",
    "            su_x = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 1])))\n",
    "            su_y = max(0, min(max_x-1, int(data[su_num_idx + su_i * 3 + 2])))\n",
    "            su_p = data[su_num_idx + su_i * 3 + 3]\n",
    "            \n",
    "#             su_p = su_intensity\n",
    "            points = points_inside_shape(center=Point(su_x, su_y), param=su_param, shape=su_shape)\n",
    "            su_channel = 0 if number_image_channels == 1 else -1\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                            su_val = (su_p - slope * point.dist - noise_floor)/(max_pu_power - noise_floor)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "                            su_val = (su_p - noise_floor) / (max_pu_power - noise_floor)\n",
    "                        else:\n",
    "                            su_val = (su_p - slope * 10*math.log10(point.dist) - noise_floor)/(\n",
    "                                max_pu_power - noise_floor)\n",
    "                    image[0][su_channel][point.p.x][point.p.y] += su_val\n",
    "            del points\n",
    "        # the last and  target SU\n",
    "        su_intensity = 1.\n",
    "        su_x = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 1])))\n",
    "        su_y = max(0, min(max_x-1, int(data[su_num_idx + (su_num - 1) * 3 + 2])))\n",
    "#         print(su_x, su_y)\n",
    "        points = points_inside_shape(center=Point(su_x, su_y), param=su_param, shape=su_shape)\n",
    "        su_channel = 0 if number_image_channels == 1 else -1\n",
    "        for point in points:\n",
    "            if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                image[0][su_channel][point.p.x][point.p.y] += su_intensity\n",
    "        del points\n",
    "        return image\n",
    "#         su_num = (len(data) - pus_num * 3) // 2\n",
    "#         if not (len(data) - pus_num * 3) % 2:\n",
    "#             raise ValueError(\"Data provided is not correct; can't get SUs' information(create_image)\")\n",
    "# #         su_image = np.zeros((max_x, max_y), dtype=float_memory_used)\n",
    "#         if su_param is None:\n",
    "#             # if su_param is unavailable, a circle(square) with radius(side) 1 is created\n",
    "#             if su_shape == 'circle':\n",
    "#                 su_param = Circle(1)\n",
    "#             elif su_shape == 'square':\n",
    "#                 su_param = Square(1)\n",
    "#             elif su_shape == 'point':\n",
    "#                 su_param = None\n",
    "#             else:\n",
    "#                 raise ValueError(\"Unsupported SU shape(create_image)! \", su_shape)\n",
    "#         su_intensity = 1.\n",
    "#         for su_i in range(su_num):\n",
    "#             su_x, su_y, su_p = max(0, min(max_x-1, int(data[pus_num * (3 if not sensors else 1) +su_i*2]))\n",
    "#                                   ), max(0, min(max_x-1, int(data[pus_num * (3 if not sensors else 1) + su_i*2+1]))), su_intensity\n",
    "#             points = points_inside_shape(center=Point(su_x, su_y), param=su_param, shape=su_shape)\n",
    "#             for point in points:\n",
    "#                 if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "#                     if number_image_channels > 1:\n",
    "#                         image[0][1][point.p.x][point.p.y] = su_intensity\n",
    "#                     elif number_image_channels == 1:\n",
    "#                         image[0][0][point.p.x][point.p.y] = su_intensity\n",
    "# #         return np.array([pu_image, su_image, [[0.] * max_y for _ in range(max_x)]], dtype='float32') # return like this to be able to display as an RGB image with pyplot.imshow(imsave)\n",
    "# #         return np.append(pu_image, su_image, axis=0)\n",
    "#         return image\n",
    "        \n",
    "            \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported style(create_image)! \", style)\n",
    "        \n",
    "def points_inside_shape(center: Point, shape: str, param)-> list:\n",
    "    # This function returns points+distance around center with defined shape\n",
    "    if shape == 'circle':\n",
    "        # First creates points inside a square(around orgigin) with 2*r side and then remove those with distance > r.\n",
    "        # Shift all remaining around center. O(4r^2)\n",
    "        r, origin = param.r, Point(0, 0)\n",
    "        square_points = set((Point(x, y) for x in range(max(-int(r/cell_size), -max_x), \n",
    "                             min(int(r/cell_size), max_x) + 1) \n",
    "                             for y in range(max(-r, -max_y), min(r, max_y) + 1)))\n",
    "        points = []\n",
    "        while square_points:\n",
    "            p = square_points.pop()\n",
    "            dist = euclidian_distance(p, origin)\n",
    "            if dist <= r:\n",
    "                points.append(PointWithDistance(Point(p.x + center.x, p.y + center.y), dist))\n",
    "                if p.x != 0:\n",
    "                    points.append(PointWithDistance(Point(-p.x + center.x, p.y + center.y), dist))\n",
    "                    square_points.remove(Point(-p.x, p.y))\n",
    "                if p.y != 0:\n",
    "                    points.append(PointWithDistance(Point(p.x + center.x, -p.y + center.y), dist))\n",
    "                    square_points.remove(Point(p.x, -p.y))\n",
    "                if p.x != 0 and p.y != 0:\n",
    "                    points.append(PointWithDistance(Point(-p.x + center.x, -p.y + center.y), dist))\n",
    "                    square_points.remove(Point(-p.x, -p.y))\n",
    "        del square_points\n",
    "        return points\n",
    "    elif shape == 'square':\n",
    "        half_side = param.side // 2\n",
    "        return [PointWithDistance(Point(x, y), euclidian_distance(Point(x, y), center)) for x in range(-half_side + center.x,\n",
    "                                                                                               half_side + center.x+1) \n",
    "                         for y in range(-half_side + center.y, half_side + center.y + 1)]\n",
    "    elif shape == 'point':\n",
    "        return [PointWithDistance(center, 0)]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported shape(points_inside_shape)! \", shape)\n",
    "        \n",
    "def read_image(image_num):\n",
    "    if False and style == \"image_intensity\":\n",
    "        image = plt.imread(image_dir + '/image' + str(image_num)+'.png')\n",
    "        image = np.swapaxes(image, 0, 2)\n",
    "        image = np.array(image[:number_image_channels], dtype=float_memory_used).reshape(1, number_image_channels, max_x, max_y)\n",
    "    elif  style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\" or style == \"image_intensity\":\n",
    "        suffix = 'npz'  # npy, npz\n",
    "        image = np.load(image_dir + '/image' + str(image_num) + '.' + suffix)  \n",
    "        if type(image) == np.lib.npyio.NpzFile:\n",
    "            image = image['a']\n",
    "    \n",
    "    return image\n",
    "    \n",
    "# TODO: Consider using min_max normalization becasue difference between values using\n",
    "# z-score is huge since most of the pixels have the same value, noise floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(data_reg[:,0:1:sensors_num], bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving images once to save time\n",
    "# run this cell just for creating images\n",
    "def creating_image(start, end):\n",
    "    # for image_num in range(115, data_reg.shape[0]):\n",
    "    # for image_num in range(1625, 5000):\n",
    "    for image_num in tqdm.tqdm(range(start, end+1)):  #4463, data_reg.shape[0]\n",
    "        image = create_image(data=data_reg[image_num], slope=slope, style=style, \n",
    "                             noise_floor=noise_floor,\n",
    "                             pu_shape=pu_shape, su_shape=su_shape, su_param=su_param, \n",
    "                             sensors_num=(sensors_num if sensors else 0), \n",
    "                             intensity_degradation=intensity_degradation, \n",
    "                             max_pu_power=0.0 if not sensors else -50)\n",
    "        if False and style == \"image_intensity\":\n",
    "            if number_image_channels != 3:\n",
    "                image = np.append(np.array(image[0]), np.zeros((3-number_image_channels,max_x, max_y), \n",
    "                                                               dtype=float_memory_used), axis=0)\n",
    "            image_save = np.swapaxes(image, 0, 2)\n",
    "            plt.imsave(image_dir + '/image' + str(image_num)+'.png', image_save)\n",
    "        elif style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\" or style == \"image_intensity\":\n",
    "    #         np.save(image_dir + '/image' + str(image_num), image)\n",
    "            np.savez_compressed(image_dir + '/image' + str(image_num), a=image)\n",
    "        del image\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3000/3000 [05:20<00:00,  9.35it/s]\n",
      "100%|███████████████████████████████████████| 3000/3000 [05:21<00:00,  9.32it/s]\n",
      "100%|███████████████████████████████████████| 3000/3000 [05:21<00:00,  9.32it/s]\n",
      "100%|███████████████████████████████████████| 3000/3000 [05:22<00:00,  9.31it/s]\n",
      "100%|███████████████████████████████████████| 3000/3000 [05:23<00:00,  9.28it/s]\n",
      "100%|███████████████████████████████████████| 3000/3000 [05:24<00:00,  9.26it/s]\n",
      "100%|███████████████████████████████████████| 3000/3000 [05:24<00:00,  9.26it/s]\n",
      "100%|███████████████████████████████████████| 3000/3000 [05:24<00:00,  9.24it/s]\n",
      "100%|███████████████████████████████████████| 3000/3000 [05:25<00:00,  9.23it/s]\n",
      "100%|███████████████████████████████████████| 3000/3000 [05:25<00:00,  9.21it/s]\n"
     ]
    }
   ],
   "source": [
    "jobs = []\n",
    "proc_sizes = [data_reg.shape[0]//number_of_proccessors] * (number_of_proccessors)\n",
    "proc_sizes[-1] += data_reg.shape[0]%number_of_proccessors\n",
    "proc_idx = [(sum(proc_sizes[:i]), sum(proc_sizes[:i+1])-1) for i in range(number_of_proccessors)]\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    p = Process(target=creating_image, args=(proc_idx[i][0], proc_idx[i][1]))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].join()\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].terminate()\n",
    "    jobs[i].close()\n",
    "del jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, point in enumerate(sensors_location):\n",
    "    print(idx+1, point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, point in enumerate(sensors_location):\n",
    "    print(idx+1, point,\"close\") if math.sqrt((point.x-917)**2+(point.y-415)**2)<=1.5 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = [0, 0, 0, 0]\n",
    "idxx = [[],[],[],[]]\n",
    "for i in range(data_reg.shape[0]):\n",
    "    pus_c = int(data_reg[i][0]) * 3 + 1\n",
    "    idx = int(data_reg[i][pus_c]) - 1\n",
    "    count[idx] += 1\n",
    "    idxx[idx].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(count)\n",
    "print(idxx[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm = read_image(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm[300].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg[:,-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.   ,   3.   ,  67.   , -25.176,  61.   ,  78.   , -21.186,\n",
       "        63.   ,  49.   , -28.75 ,  78.   ,  46.   ,  -7.82 ,  11.   ,\n",
       "        91.   ,  -6.503,  75.   ,  84.   , -27.874,  26.   ,  12.   ,\n",
       "        -6.862,  93.   ,  75.   , -24.521,  14.   ,  39.   , -21.42 ,\n",
       "        41.   ,  98.   , -13.011,   5.   ,  34.   ,  56.   ,   9.935,\n",
       "        46.   ,  92.   , -33.46 ,  80.   ,  32.   , -41.9  ,   1.   ,\n",
       "        92.   , -14.34 ,   5.   ,  25.   ,  52.067,   0.   ,     nan,\n",
       "           nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "           nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "           nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "           nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
       "           nan, -27.35 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 4320x4320 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(60,60))         # Sample figsize in inches\n",
    "sns.heatmap(imm[0][7], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(num_filters, kernel_lam, bias_lam):\n",
    "#     num_filters, lam = 5, 5\n",
    "    data_format = 'channels_first'\n",
    "    convolution_init, dense_init = \"lecun_normal\", \"RandomNormal\"  # lecun_normal\n",
    "    convolution_filter, dense_filter = 'selu', 'linear' #softsign, sigmoid; relu, linear\n",
    "    filter_shape, pool_size = (3, 3), (2,2)\n",
    "    cnn = models.Sequential()\n",
    "    cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(number_image_channels, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "#                           input_shape=(number_image_channels, max_x, max_y), data_format=data_format,\n",
    "#                           kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                           kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size=pool_size, data_format=data_format))\n",
    "    cnn.add(BatchNormalization(axis=1))\n",
    "    cnn.add(layers.Dropout(0.15))\n",
    "    \n",
    "    cnn.add(layers.Conv2D(1*num_filters, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Conv2D(3*num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "#                           input_shape=(number_image_channels, max_x, max_y), data_format=data_format,\n",
    "#                           kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                           kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size=pool_size, data_format=data_format))\n",
    "    cnn.add(BatchNormalization(axis=1))\n",
    "    cnn.add(layers.Dropout(0.15))\n",
    "    \n",
    "    cnn.add(layers.Conv2D(2*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Conv2D(2*num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "#                           input_shape=(number_image_channels, max_x, max_y), data_format=data_format,\n",
    "#                           kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                           kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "    cnn.add(BatchNormalization(axis=1))\n",
    "    cnn.add(layers.Dropout(0.15))\n",
    "    \n",
    "    cnn.add(layers.Conv2D(3*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "#                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                          kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "    cnn.add(BatchNormalization(axis=1))\n",
    "    cnn.add(layers.Dropout(0.15))\n",
    "# from here for 1000\n",
    "    if max(max_x, max_y) == 1000:\n",
    "        cnn.add(layers.Conv2D(1*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                             kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                             kernel_initializer=convolution_init))\n",
    "    #     cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "    #                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "    #                          kernel_initializer='lecun_normal'))\n",
    "        cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "        cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Dropout(0.25))\n",
    "\n",
    "        cnn.add(layers.Conv2D(2*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                             kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam), \n",
    "                             kernel_initializer=convolution_init))\n",
    "    #     cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Conv2D(2*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "    #                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "    #                          kernel_initializer='lecun_normal'))\n",
    "        cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "        cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Dropout(0.25))\n",
    "\n",
    "        cnn.add(layers.Conv2D(3*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                             kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam), \n",
    "                             kernel_initializer=convolution_init))\n",
    "    #     cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Conv2D(3*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "    #                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "    #                          kernel_initializer='lecun_normal'))\n",
    "        cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "        cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Dropout(0.25))\n",
    "#     cnn = layers.GlobalAveragePooling2D()(cnn)\n",
    "    cnn.add(layers.Flatten())\n",
    "    cnn.add(layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=convolution_init))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=convolution_init))\n",
    "    cnn.add(BatchNormalization())\n",
    "#     cnn.add(layers.Dropout(0.25))\n",
    "    cnn.add(layers.Dense(1, activation=dense_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=dense_init))\n",
    "    return cnn\n",
    "\n",
    "\n",
    "class DataBatchGenerator(Sequence):\n",
    "    def __init__(self, dataset:np.ndarray, batch_size:int, start_idx:int,\n",
    "                 number_image_channels:int,\n",
    "                 max_x, max_y, float_memory_used, conserve=0):\n",
    "#         print(dataset.shape[0])\n",
    "        self.dataset, self.batch_size, self.start_idx = dataset, batch_size, start_idx\n",
    "        self.number_image_channels, self.max_x, self.max_y = number_image_channels, max_x, max_y\n",
    "        self.float_memory_used = float_memory_used\n",
    "        self.conserve = conserve\n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.ceil(self.dataset.shape[0] / self.batch_size).astype(np.int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        size = min(self.dataset.shape[0] - idx * self.batch_size, self.batch_size)\n",
    "        batch_x = np.empty((size, self.number_image_channels, self.max_x, self.max_y), dtype=self.float_memory_used)\n",
    "        batch_y = np.empty((size), dtype=self.float_memory_used)\n",
    "        for i in range(size):\n",
    "            batch_x[i] = read_image(self.start_idx + idx * self.batch_size + i)\n",
    "            batch_y[i] = self.dataset[idx * self.batch_size + i][- 1 - self.conserve]\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "def custom_loss(fp_penalty_coef, fn_penalty_coef):\n",
    "    # custom loss function that penalize false positive and negative differently\n",
    "    def loss(y_true, y_pred):\n",
    "        res = y_pred - y_true\n",
    "        res = tf.where(res > 0, res * fp_penalty_coef, res * fn_penalty_coef)\n",
    "        return K.mean(K.square(res))\n",
    "    return loss\n",
    "\n",
    "def fp_mae(y_true, y_pred):\n",
    "    # custom metric that replace false negative with zero and return the mean of new vector\n",
    "    res = y_pred - y_true\n",
    "    res = tf.nn.relu(res)\n",
    "#     res = tf.where(res <= 0, 0, res)\n",
    "    return K.mean(res)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:26:00.297216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 15:26:00.301192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 15:26:00.301666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 15:26:00.302471: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-29 15:26:00.303064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 15:26:00.303392: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 15:26:00.303682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 15:26:00.615674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 15:26:00.616032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 15:26:00.616311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 15:26:00.616572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9982 MB memory:  -> device: 0, name: GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "cnn = cnn_model(10, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 10, 100, 100)      730       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 10, 50, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 10, 50, 50)        40        \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10, 50, 50)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 10, 50, 50)        910       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 25, 25)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 10, 25, 25)        40        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 25, 25)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 25, 25)        1820      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 20, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 20, 12, 12)        80        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 30, 12, 12)        5430      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 30, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 30, 6, 6)          120       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 30, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1080)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                21620     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 31,391\n",
      "Trainable params: 31,171\n",
      "Non-trainable params: 220\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_2759206/1594334578.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-29 15:26:07.309153: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 15:26:07.309440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 15:26:07.309621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 15:26:07.309800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 15:26:07.309947: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-29 15:26:07.310096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 9982 MB memory:  -> device: 0, name: GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "tf.test.is_gpu_available()\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2048, 4096]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_samples  # [128, 256, 512, 1024, 2048, 4096, 8192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_samples: 2048 , New samples: 2048\n",
      "Validation size: 410 , starts: 2048 , ends: 2457\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2759206/4170749990.py:114: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.ceil(self.dataset.shape[0] / self.batch_size).astype(np.int)\n",
      "2021-10-29 15:26:45.863657: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-10-29 15:26:47.354578: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8101\n",
      "2021-10-29 15:26:48.287483: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 - 4s - loss: 842.1705 - mse: 842.1705 - mae: 25.8287 - fp_mae: 24.2336 - val_loss: 881.5425 - val_mse: 881.5425 - val_mae: 26.0413 - val_fp_mae: 24.7395\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 26.04132, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_0.h5\n",
      "Epoch 2/100\n",
      "64/64 - 1s - loss: 786.7859 - mse: 786.7859 - mae: 24.7264 - fp_mae: 23.5341 - val_loss: 685.3392 - val_mse: 685.3392 - val_mae: 22.8524 - val_fp_mae: 21.2095\n",
      "\n",
      "Epoch 00002: val_mae improved from 26.04132 to 22.85244, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_0.h5\n",
      "Epoch 3/100\n",
      "64/64 - 1s - loss: 707.9209 - mse: 707.9209 - mae: 23.2817 - fp_mae: 22.5081 - val_loss: 565.3997 - val_mse: 565.3997 - val_mae: 20.9766 - val_fp_mae: 17.8785\n",
      "\n",
      "Epoch 00003: val_mae improved from 22.85244 to 20.97659, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_0.h5\n",
      "Epoch 4/100\n",
      "64/64 - 1s - loss: 620.3770 - mse: 620.3770 - mae: 21.7354 - fp_mae: 21.2805 - val_loss: 562.6343 - val_mse: 562.6343 - val_mae: 20.1895 - val_fp_mae: 20.0268\n",
      "\n",
      "Epoch 00004: val_mae improved from 20.97659 to 20.18946, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_0.h5\n",
      "Epoch 5/100\n",
      "64/64 - 1s - loss: 537.9593 - mse: 537.9593 - mae: 20.1979 - fp_mae: 19.9202 - val_loss: 385.0831 - val_mse: 385.0831 - val_mae: 16.2247 - val_fp_mae: 14.7389\n",
      "\n",
      "Epoch 00005: val_mae improved from 20.18946 to 16.22466, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_0.h5\n",
      "Epoch 6/100\n",
      "64/64 - 1s - loss: 461.0939 - mse: 461.0939 - mae: 18.6464 - fp_mae: 18.4610 - val_loss: 296.8427 - val_mse: 296.8427 - val_mae: 13.9202 - val_fp_mae: 11.9045\n",
      "\n",
      "Epoch 00006: val_mae improved from 16.22466 to 13.92021, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_0.h5\n",
      "Epoch 7/100\n",
      "64/64 - 1s - loss: 395.8819 - mse: 395.8819 - mae: 17.1404 - fp_mae: 16.9663 - val_loss: 258.1182 - val_mse: 258.1182 - val_mae: 12.7816 - val_fp_mae: 10.8349\n",
      "\n",
      "Epoch 00007: val_mae improved from 13.92021 to 12.78165, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_0.h5\n",
      "Epoch 8/100\n",
      "64/64 - 1s - loss: 336.2159 - mse: 336.2159 - mae: 15.6698 - fp_mae: 15.4462 - val_loss: 242.2237 - val_mse: 242.2237 - val_mae: 12.2923 - val_fp_mae: 11.6787\n",
      "\n",
      "Epoch 00008: val_mae improved from 12.78165 to 12.29230, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_0.h5\n",
      "Epoch 9/100\n",
      "64/64 - 1s - loss: 281.3033 - mse: 281.3033 - mae: 14.1826 - fp_mae: 13.8804 - val_loss: 242.1583 - val_mse: 242.1583 - val_mae: 12.4273 - val_fp_mae: 11.7538\n",
      "\n",
      "Epoch 00009: val_mae did not improve from 12.29230\n",
      "Epoch 10/100\n",
      "64/64 - 1s - loss: 232.9863 - mse: 232.9863 - mae: 12.7430 - fp_mae: 12.3148 - val_loss: 352.2132 - val_mse: 352.2132 - val_mae: 15.9482 - val_fp_mae: 15.7096\n",
      "\n",
      "Epoch 00010: val_mae did not improve from 12.29230\n",
      "Epoch 11/100\n",
      "64/64 - 1s - loss: 191.1481 - mse: 191.1481 - mae: 11.3747 - fp_mae: 10.7623 - val_loss: 244.8663 - val_mse: 244.8663 - val_mae: 12.6446 - val_fp_mae: 12.2028\n",
      "\n",
      "Epoch 00011: val_mae did not improve from 12.29230\n",
      "Epoch 12/100\n",
      "64/64 - 1s - loss: 154.9952 - mse: 154.9952 - mae: 10.0619 - fp_mae: 9.2746 - val_loss: 193.0642 - val_mse: 193.0642 - val_mae: 11.0579 - val_fp_mae: 10.0682\n",
      "\n",
      "Epoch 00012: val_mae improved from 12.29230 to 11.05788, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_0.h5\n",
      "Epoch 13/100\n",
      "64/64 - 1s - loss: 123.6744 - mse: 123.6744 - mae: 8.9148 - fp_mae: 7.8700 - val_loss: 215.0702 - val_mse: 215.0702 - val_mae: 11.6574 - val_fp_mae: 10.4664\n",
      "\n",
      "Epoch 00013: val_mae did not improve from 11.05788\n",
      "Epoch 14/100\n",
      "64/64 - 1s - loss: 105.8881 - mse: 105.8881 - mae: 8.2345 - fp_mae: 6.7894 - val_loss: 189.8604 - val_mse: 189.8604 - val_mae: 10.9738 - val_fp_mae: 9.4014\n",
      "\n",
      "Epoch 00014: val_mae improved from 11.05788 to 10.97384, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_0.h5\n",
      "Epoch 15/100\n",
      "64/64 - 1s - loss: 85.9280 - mse: 85.9280 - mae: 7.3503 - fp_mae: 5.7308 - val_loss: 151.7283 - val_mse: 151.7283 - val_mae: 9.6094 - val_fp_mae: 7.9985\n",
      "\n",
      "Epoch 00015: val_mae improved from 10.97384 to 9.60939, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_0.h5\n",
      "Epoch 16/100\n",
      "64/64 - 1s - loss: 75.2137 - mse: 75.2137 - mae: 6.8321 - fp_mae: 4.9440 - val_loss: 138.3108 - val_mse: 138.3108 - val_mae: 9.1639 - val_fp_mae: 7.0637\n",
      "\n",
      "Epoch 00016: val_mae improved from 9.60939 to 9.16390, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_0.h5\n",
      "Epoch 17/100\n",
      "64/64 - 1s - loss: 70.1190 - mse: 70.1190 - mae: 6.6413 - fp_mae: 4.4166 - val_loss: 109.5530 - val_mse: 109.5530 - val_mae: 8.1396 - val_fp_mae: 5.0110\n",
      "\n",
      "Epoch 00017: val_mae improved from 9.16390 to 8.13962, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_0.h5\n",
      "Epoch 18/100\n",
      "64/64 - 1s - loss: 63.1663 - mse: 63.1663 - mae: 6.2888 - fp_mae: 3.9119 - val_loss: 142.8785 - val_mse: 142.8785 - val_mae: 9.3177 - val_fp_mae: 7.1753\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 8.13962\n",
      "Epoch 19/100\n",
      "64/64 - 1s - loss: 59.4583 - mse: 59.4583 - mae: 6.1252 - fp_mae: 3.6063 - val_loss: 118.8208 - val_mse: 118.8208 - val_mae: 8.4710 - val_fp_mae: 4.5432\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 8.13962\n",
      "Epoch 20/100\n",
      "64/64 - 1s - loss: 57.7155 - mse: 57.7155 - mae: 6.0493 - fp_mae: 3.3540 - val_loss: 126.8748 - val_mse: 126.8748 - val_mae: 8.7544 - val_fp_mae: 6.4408\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 8.13962\n",
      "Epoch 21/100\n",
      "64/64 - 1s - loss: 54.3096 - mse: 54.3096 - mae: 5.8758 - fp_mae: 3.1888 - val_loss: 113.7998 - val_mse: 113.7998 - val_mae: 8.4473 - val_fp_mae: 4.6466\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 8.13962\n",
      "Epoch 22/100\n",
      "64/64 - 1s - loss: 53.3861 - mse: 53.3861 - mae: 5.8161 - fp_mae: 3.0344 - val_loss: 116.6885 - val_mse: 116.6885 - val_mae: 8.4130 - val_fp_mae: 4.3941\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 8.13962\n",
      "Epoch 23/100\n",
      "64/64 - 1s - loss: 50.9863 - mse: 50.9863 - mae: 5.6526 - fp_mae: 2.9203 - val_loss: 118.9269 - val_mse: 118.9269 - val_mae: 8.5579 - val_fp_mae: 5.0355\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 8.13962\n",
      "Epoch 24/100\n",
      "64/64 - 1s - loss: 50.6213 - mse: 50.6213 - mae: 5.6329 - fp_mae: 2.8978 - val_loss: 125.3950 - val_mse: 125.3950 - val_mae: 8.8329 - val_fp_mae: 3.7199\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 8.13962\n",
      "Epoch 25/100\n",
      "64/64 - 1s - loss: 49.3469 - mse: 49.3469 - mae: 5.5511 - fp_mae: 2.7941 - val_loss: 122.7988 - val_mse: 122.7988 - val_mae: 8.5106 - val_fp_mae: 5.1013\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 8.13962\n",
      "Epoch 26/100\n",
      "64/64 - 1s - loss: 47.5583 - mse: 47.5583 - mae: 5.4999 - fp_mae: 2.7459 - val_loss: 135.9561 - val_mse: 135.9561 - val_mae: 8.9547 - val_fp_mae: 6.0708\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 8.13962\n",
      "Epoch 27/100\n",
      "64/64 - 1s - loss: 44.9580 - mse: 44.9580 - mae: 5.2368 - fp_mae: 2.5549 - val_loss: 117.6656 - val_mse: 117.6656 - val_mae: 8.3750 - val_fp_mae: 4.1212\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 8.13962\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 - 1s - loss: 45.0307 - mse: 45.0307 - mae: 5.3162 - fp_mae: 2.5775 - val_loss: 114.4483 - val_mse: 114.4483 - val_mae: 8.2942 - val_fp_mae: 4.6063\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 8.13962\n",
      "Epoch 29/100\n",
      "64/64 - 1s - loss: 41.6132 - mse: 41.6132 - mae: 5.1143 - fp_mae: 2.5412 - val_loss: 121.1524 - val_mse: 121.1524 - val_mae: 8.4252 - val_fp_mae: 5.2791\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 8.13962\n",
      "Epoch 30/100\n",
      "64/64 - 1s - loss: 42.6125 - mse: 42.6125 - mae: 5.1671 - fp_mae: 2.6255 - val_loss: 117.5753 - val_mse: 117.5753 - val_mae: 8.4246 - val_fp_mae: 5.1465\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 8.13962\n",
      "Epoch 31/100\n",
      "64/64 - 1s - loss: 41.0810 - mse: 41.0810 - mae: 5.0522 - fp_mae: 2.4687 - val_loss: 124.0816 - val_mse: 124.0816 - val_mae: 8.6585 - val_fp_mae: 5.1553\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 8.13962\n",
      "Epoch 32/100\n",
      "64/64 - 1s - loss: 40.7350 - mse: 40.7350 - mae: 5.0171 - fp_mae: 2.4957 - val_loss: 124.9510 - val_mse: 124.9510 - val_mae: 8.6531 - val_fp_mae: 5.0635\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 8.13962\n",
      "Epoch 33/100\n",
      "64/64 - 1s - loss: 39.7276 - mse: 39.7276 - mae: 4.9554 - fp_mae: 2.4025 - val_loss: 129.4548 - val_mse: 129.4548 - val_mae: 8.6950 - val_fp_mae: 5.9449\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 8.13962\n",
      "Epoch 34/100\n",
      "64/64 - 1s - loss: 37.9276 - mse: 37.9276 - mae: 4.8583 - fp_mae: 2.4334 - val_loss: 133.7525 - val_mse: 133.7525 - val_mae: 8.8912 - val_fp_mae: 6.5375\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 8.13962\n",
      "Epoch 35/100\n",
      "64/64 - 1s - loss: 38.2168 - mse: 38.2168 - mae: 4.8435 - fp_mae: 2.4270 - val_loss: 125.1703 - val_mse: 125.1703 - val_mae: 8.5574 - val_fp_mae: 5.8198\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 8.13962\n",
      "Epoch 36/100\n",
      "64/64 - 1s - loss: 37.7983 - mse: 37.7983 - mae: 4.8369 - fp_mae: 2.3854 - val_loss: 134.1978 - val_mse: 134.1978 - val_mae: 8.8756 - val_fp_mae: 6.4285\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 8.13962\n",
      "Epoch 37/100\n",
      "64/64 - 1s - loss: 37.5406 - mse: 37.5406 - mae: 4.8086 - fp_mae: 2.3760 - val_loss: 124.7334 - val_mse: 124.7334 - val_mae: 8.6252 - val_fp_mae: 5.6117\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 8.13962\n",
      "Epoch 38/100\n",
      "64/64 - 1s - loss: 36.6526 - mse: 36.6526 - mae: 4.7714 - fp_mae: 2.3697 - val_loss: 123.6121 - val_mse: 123.6121 - val_mae: 8.5625 - val_fp_mae: 5.7124\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 8.13962\n",
      "Epoch 39/100\n",
      "64/64 - 1s - loss: 36.5190 - mse: 36.5190 - mae: 4.7509 - fp_mae: 2.3173 - val_loss: 129.2285 - val_mse: 129.2285 - val_mae: 8.7399 - val_fp_mae: 5.3229\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 8.13962\n",
      "Epoch 40/100\n",
      "64/64 - 1s - loss: 36.5799 - mse: 36.5799 - mae: 4.7718 - fp_mae: 2.3640 - val_loss: 127.5046 - val_mse: 127.5046 - val_mae: 8.7148 - val_fp_mae: 5.4920\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 8.13962\n",
      "Epoch 41/100\n",
      "64/64 - 1s - loss: 35.8252 - mse: 35.8252 - mae: 4.7516 - fp_mae: 2.3201 - val_loss: 129.4513 - val_mse: 129.4513 - val_mae: 8.7702 - val_fp_mae: 5.6670\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 8.13962\n",
      "Epoch 42/100\n",
      "64/64 - 1s - loss: 35.7589 - mse: 35.7589 - mae: 4.7228 - fp_mae: 2.3344 - val_loss: 125.8956 - val_mse: 125.8956 - val_mae: 8.6644 - val_fp_mae: 5.7774\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 8.13962\n",
      "Epoch 43/100\n",
      "64/64 - 1s - loss: 33.7641 - mse: 33.7641 - mae: 4.5977 - fp_mae: 2.3368 - val_loss: 125.4398 - val_mse: 125.4398 - val_mae: 8.6534 - val_fp_mae: 5.3540\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 8.13962\n",
      "Epoch 44/100\n",
      "64/64 - 1s - loss: 34.4179 - mse: 34.4179 - mae: 4.5835 - fp_mae: 2.2582 - val_loss: 130.8110 - val_mse: 130.8110 - val_mae: 8.8146 - val_fp_mae: 5.3687\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 8.13962\n",
      "Epoch 45/100\n",
      "64/64 - 1s - loss: 34.1646 - mse: 34.1646 - mae: 4.5942 - fp_mae: 2.2994 - val_loss: 129.5028 - val_mse: 129.5028 - val_mae: 8.8538 - val_fp_mae: 4.8387\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 8.13962\n",
      "Epoch 46/100\n",
      "64/64 - 1s - loss: 33.8999 - mse: 33.8999 - mae: 4.5889 - fp_mae: 2.2878 - val_loss: 126.3829 - val_mse: 126.3829 - val_mae: 8.7425 - val_fp_mae: 5.0914\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 8.13962\n",
      "Epoch 47/100\n",
      "64/64 - 1s - loss: 34.0887 - mse: 34.0887 - mae: 4.5856 - fp_mae: 2.2375 - val_loss: 128.2112 - val_mse: 128.2112 - val_mae: 8.8666 - val_fp_mae: 4.5537\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 8.13962\n",
      "Epoch 48/100\n",
      "64/64 - 1s - loss: 33.3610 - mse: 33.3610 - mae: 4.5447 - fp_mae: 2.2917 - val_loss: 130.8490 - val_mse: 130.8490 - val_mae: 8.7940 - val_fp_mae: 5.3287\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 8.13962\n",
      "Epoch 49/100\n",
      "64/64 - 1s - loss: 32.2738 - mse: 32.2738 - mae: 4.5272 - fp_mae: 2.2471 - val_loss: 123.9630 - val_mse: 123.9630 - val_mae: 8.6273 - val_fp_mae: 4.8358\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 8.13962\n",
      "Epoch 50/100\n",
      "64/64 - 2s - loss: 32.7031 - mse: 32.7031 - mae: 4.4954 - fp_mae: 2.2357 - val_loss: 129.0370 - val_mse: 129.0370 - val_mae: 8.8080 - val_fp_mae: 5.3685\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 8.13962\n",
      "Epoch 51/100\n",
      "64/64 - 1s - loss: 33.0939 - mse: 33.0939 - mae: 4.5442 - fp_mae: 2.2723 - val_loss: 125.8365 - val_mse: 125.8365 - val_mae: 8.6927 - val_fp_mae: 5.1005\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 8.13962\n",
      "Epoch 52/100\n",
      "64/64 - 1s - loss: 32.0419 - mse: 32.0419 - mae: 4.4560 - fp_mae: 2.1468 - val_loss: 120.3583 - val_mse: 120.3583 - val_mae: 8.5597 - val_fp_mae: 5.2651\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 8.13962\n",
      "Epoch 53/100\n",
      "64/64 - 1s - loss: 31.6885 - mse: 31.6885 - mae: 4.4444 - fp_mae: 2.2008 - val_loss: 125.9020 - val_mse: 125.9020 - val_mae: 8.6694 - val_fp_mae: 5.4375\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 8.13962\n",
      "Epoch 54/100\n",
      "64/64 - 1s - loss: 32.7747 - mse: 32.7747 - mae: 4.5257 - fp_mae: 2.2807 - val_loss: 129.6445 - val_mse: 129.6445 - val_mae: 8.9225 - val_fp_mae: 5.2228\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 8.13962\n",
      "Epoch 55/100\n",
      "64/64 - 1s - loss: 31.3125 - mse: 31.3125 - mae: 4.3811 - fp_mae: 2.2201 - val_loss: 133.4845 - val_mse: 133.4845 - val_mae: 8.9433 - val_fp_mae: 5.3047\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 8.13962\n",
      "Epoch 56/100\n",
      "64/64 - 1s - loss: 33.4342 - mse: 33.4342 - mae: 4.5050 - fp_mae: 2.1899 - val_loss: 127.5719 - val_mse: 127.5719 - val_mae: 8.7246 - val_fp_mae: 5.2190\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 8.13962\n",
      "Epoch 57/100\n",
      "64/64 - 1s - loss: 31.3916 - mse: 31.3916 - mae: 4.4041 - fp_mae: 2.2165 - val_loss: 143.9414 - val_mse: 143.9414 - val_mae: 9.2396 - val_fp_mae: 6.4081\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 8.13962\n",
      "Epoch 58/100\n",
      "64/64 - 1s - loss: 30.9263 - mse: 30.9263 - mae: 4.3891 - fp_mae: 2.1857 - val_loss: 124.2896 - val_mse: 124.2896 - val_mae: 8.6078 - val_fp_mae: 5.3902\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 8.13962\n",
      "Epoch 59/100\n",
      "64/64 - 1s - loss: 31.5653 - mse: 31.5653 - mae: 4.4046 - fp_mae: 2.1778 - val_loss: 121.1448 - val_mse: 121.1448 - val_mae: 8.6334 - val_fp_mae: 4.5430\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 8.13962\n",
      "Epoch 60/100\n",
      "64/64 - 1s - loss: 30.6198 - mse: 30.6198 - mae: 4.3299 - fp_mae: 2.1361 - val_loss: 121.8988 - val_mse: 121.8988 - val_mae: 8.4954 - val_fp_mae: 4.8738\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 8.13962\n",
      "Epoch 61/100\n",
      "64/64 - 1s - loss: 30.2940 - mse: 30.2940 - mae: 4.2940 - fp_mae: 2.1582 - val_loss: 129.3682 - val_mse: 129.3682 - val_mae: 8.8304 - val_fp_mae: 5.3089\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 8.13962\n",
      "Epoch 62/100\n",
      "64/64 - 1s - loss: 30.6747 - mse: 30.6747 - mae: 4.3450 - fp_mae: 2.1312 - val_loss: 133.7545 - val_mse: 133.7545 - val_mae: 8.9486 - val_fp_mae: 5.8067\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 8.13962\n",
      "Epoch 63/100\n",
      "64/64 - 1s - loss: 30.4402 - mse: 30.4402 - mae: 4.3192 - fp_mae: 2.1657 - val_loss: 121.7263 - val_mse: 121.7263 - val_mae: 8.5190 - val_fp_mae: 5.2339\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 8.13962\n",
      "Epoch 64/100\n",
      "64/64 - 1s - loss: 30.4686 - mse: 30.4686 - mae: 4.3357 - fp_mae: 2.1910 - val_loss: 127.6660 - val_mse: 127.6660 - val_mae: 8.7044 - val_fp_mae: 5.9578\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 8.13962\n",
      "Epoch 65/100\n",
      "64/64 - 1s - loss: 29.4839 - mse: 29.4839 - mae: 4.2598 - fp_mae: 2.1220 - val_loss: 128.0076 - val_mse: 128.0076 - val_mae: 8.7135 - val_fp_mae: 5.6909\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 8.13962\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 - 1s - loss: 29.4016 - mse: 29.4016 - mae: 4.2398 - fp_mae: 2.0623 - val_loss: 126.7001 - val_mse: 126.7001 - val_mae: 8.6148 - val_fp_mae: 5.4361\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 8.13962\n",
      "Epoch 67/100\n",
      "64/64 - 1s - loss: 29.7646 - mse: 29.7646 - mae: 4.3016 - fp_mae: 2.1817 - val_loss: 130.7030 - val_mse: 130.7030 - val_mae: 8.7331 - val_fp_mae: 6.0170\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 8.13962\n",
      "Epoch 68/100\n",
      "64/64 - 1s - loss: 30.0536 - mse: 30.0536 - mae: 4.3089 - fp_mae: 2.2232 - val_loss: 130.7557 - val_mse: 130.7557 - val_mae: 8.8524 - val_fp_mae: 4.2438\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 8.13962\n",
      "Epoch 69/100\n",
      "64/64 - 1s - loss: 28.6588 - mse: 28.6588 - mae: 4.2519 - fp_mae: 2.0990 - val_loss: 122.9196 - val_mse: 122.9196 - val_mae: 8.4419 - val_fp_mae: 4.8720\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 8.13962\n",
      "Epoch 70/100\n",
      "64/64 - 1s - loss: 28.0235 - mse: 28.0235 - mae: 4.1767 - fp_mae: 2.0843 - val_loss: 126.1890 - val_mse: 126.1890 - val_mae: 8.6369 - val_fp_mae: 5.1965\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 8.13962\n",
      "Epoch 71/100\n",
      "64/64 - 1s - loss: 28.7778 - mse: 28.7778 - mae: 4.2248 - fp_mae: 2.1027 - val_loss: 125.7009 - val_mse: 125.7009 - val_mae: 8.7282 - val_fp_mae: 5.0193\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 8.13962\n",
      "Epoch 72/100\n",
      "64/64 - 1s - loss: 27.1231 - mse: 27.1231 - mae: 4.1227 - fp_mae: 2.0192 - val_loss: 125.2937 - val_mse: 125.2937 - val_mae: 8.6465 - val_fp_mae: 4.8764\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 8.13962\n",
      "Epoch 73/100\n",
      "64/64 - 1s - loss: 27.8476 - mse: 27.8476 - mae: 4.1354 - fp_mae: 2.0421 - val_loss: 136.0361 - val_mse: 136.0361 - val_mae: 8.9613 - val_fp_mae: 5.0747\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 8.13962\n",
      "Epoch 74/100\n",
      "64/64 - 1s - loss: 27.1843 - mse: 27.1843 - mae: 4.0717 - fp_mae: 2.0225 - val_loss: 122.0391 - val_mse: 122.0391 - val_mae: 8.5678 - val_fp_mae: 5.0567\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 8.13962\n",
      "Epoch 75/100\n",
      "64/64 - 1s - loss: 28.4787 - mse: 28.4787 - mae: 4.1912 - fp_mae: 2.0602 - val_loss: 125.6480 - val_mse: 125.6480 - val_mae: 8.6810 - val_fp_mae: 5.5466\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 8.13962\n",
      "Epoch 76/100\n",
      "64/64 - 1s - loss: 27.7837 - mse: 27.7837 - mae: 4.1694 - fp_mae: 2.1076 - val_loss: 125.7179 - val_mse: 125.7179 - val_mae: 8.6666 - val_fp_mae: 4.7909\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 8.13962\n",
      "Epoch 77/100\n",
      "64/64 - 1s - loss: 27.9613 - mse: 27.9613 - mae: 4.1443 - fp_mae: 2.0629 - val_loss: 130.9461 - val_mse: 130.9461 - val_mae: 8.8452 - val_fp_mae: 5.4089\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 8.13962\n",
      "Epoch 78/100\n",
      "64/64 - 1s - loss: 26.9599 - mse: 26.9599 - mae: 4.0984 - fp_mae: 2.0643 - val_loss: 122.2793 - val_mse: 122.2793 - val_mae: 8.5442 - val_fp_mae: 5.0796\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 8.13962\n",
      "Epoch 79/100\n",
      "64/64 - 1s - loss: 28.0351 - mse: 28.0351 - mae: 4.1426 - fp_mae: 2.0361 - val_loss: 129.2730 - val_mse: 129.2730 - val_mae: 8.7891 - val_fp_mae: 5.6328\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 8.13962\n",
      "Epoch 80/100\n",
      "64/64 - 1s - loss: 27.6390 - mse: 27.6390 - mae: 4.1683 - fp_mae: 2.1013 - val_loss: 122.3713 - val_mse: 122.3713 - val_mae: 8.6681 - val_fp_mae: 4.3970\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 8.13962\n",
      "Epoch 81/100\n",
      "64/64 - 1s - loss: 27.1416 - mse: 27.1416 - mae: 4.1126 - fp_mae: 2.0431 - val_loss: 125.1536 - val_mse: 125.1536 - val_mae: 8.6122 - val_fp_mae: 5.0028\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 8.13962\n",
      "Epoch 82/100\n",
      "64/64 - 1s - loss: 27.5097 - mse: 27.5097 - mae: 4.1193 - fp_mae: 2.0194 - val_loss: 128.8077 - val_mse: 128.8077 - val_mae: 8.7718 - val_fp_mae: 5.6252\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 8.13962\n",
      "Epoch 83/100\n",
      "64/64 - 1s - loss: 27.7159 - mse: 27.7159 - mae: 4.0753 - fp_mae: 2.0952 - val_loss: 122.1908 - val_mse: 122.1908 - val_mae: 8.4886 - val_fp_mae: 4.7394\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 8.13962\n",
      "Epoch 84/100\n",
      "64/64 - 1s - loss: 26.9622 - mse: 26.9622 - mae: 4.0639 - fp_mae: 2.0541 - val_loss: 121.5654 - val_mse: 121.5654 - val_mae: 8.5912 - val_fp_mae: 5.0073\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 8.13962\n",
      "Epoch 85/100\n",
      "64/64 - 1s - loss: 27.1414 - mse: 27.1414 - mae: 4.1214 - fp_mae: 2.0355 - val_loss: 122.2556 - val_mse: 122.2556 - val_mae: 8.5612 - val_fp_mae: 4.8495\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 8.13962\n",
      "Epoch 86/100\n",
      "64/64 - 1s - loss: 27.7467 - mse: 27.7467 - mae: 4.1509 - fp_mae: 1.9883 - val_loss: 127.8266 - val_mse: 127.8266 - val_mae: 8.6942 - val_fp_mae: 5.3925\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 8.13962\n",
      "Epoch 87/100\n",
      "64/64 - 1s - loss: 27.0866 - mse: 27.0866 - mae: 4.1382 - fp_mae: 2.0889 - val_loss: 121.9745 - val_mse: 121.9745 - val_mae: 8.5448 - val_fp_mae: 4.9735\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 8.13962\n",
      "Epoch 88/100\n",
      "64/64 - 1s - loss: 26.7738 - mse: 26.7738 - mae: 4.0821 - fp_mae: 2.0829 - val_loss: 121.4936 - val_mse: 121.4936 - val_mae: 8.4416 - val_fp_mae: 5.0715\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 8.13962\n",
      "Epoch 89/100\n",
      "64/64 - 1s - loss: 26.3965 - mse: 26.3965 - mae: 4.0690 - fp_mae: 2.0366 - val_loss: 122.2248 - val_mse: 122.2248 - val_mae: 8.5572 - val_fp_mae: 5.1068\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 8.13962\n",
      "Epoch 90/100\n",
      "64/64 - 1s - loss: 26.6974 - mse: 26.6974 - mae: 4.0707 - fp_mae: 2.0152 - val_loss: 123.1071 - val_mse: 123.1071 - val_mae: 8.5560 - val_fp_mae: 4.7437\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 8.13962\n",
      "Epoch 91/100\n",
      "64/64 - 1s - loss: 26.8608 - mse: 26.8608 - mae: 4.0628 - fp_mae: 1.9919 - val_loss: 120.0186 - val_mse: 120.0186 - val_mae: 8.5097 - val_fp_mae: 4.3528\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 8.13962\n",
      "Epoch 92/100\n",
      "64/64 - 1s - loss: 26.1503 - mse: 26.1503 - mae: 4.0437 - fp_mae: 1.9735 - val_loss: 129.8238 - val_mse: 129.8238 - val_mae: 8.7488 - val_fp_mae: 5.6109\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 8.13962\n",
      "Epoch 93/100\n",
      "64/64 - 1s - loss: 25.6817 - mse: 25.6817 - mae: 3.9953 - fp_mae: 2.0566 - val_loss: 121.9447 - val_mse: 121.9447 - val_mae: 8.4950 - val_fp_mae: 4.9003\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 8.13962\n",
      "Epoch 94/100\n",
      "64/64 - 1s - loss: 26.4597 - mse: 26.4597 - mae: 4.0548 - fp_mae: 1.9817 - val_loss: 123.7531 - val_mse: 123.7531 - val_mae: 8.4991 - val_fp_mae: 4.8806\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 8.13962\n",
      "Epoch 95/100\n",
      "64/64 - 1s - loss: 25.4342 - mse: 25.4342 - mae: 3.9644 - fp_mae: 2.0153 - val_loss: 120.6716 - val_mse: 120.6716 - val_mae: 8.4423 - val_fp_mae: 5.4028\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 8.13962\n",
      "Epoch 96/100\n",
      "64/64 - 1s - loss: 25.7663 - mse: 25.7663 - mae: 3.9509 - fp_mae: 1.9952 - val_loss: 131.2861 - val_mse: 131.2861 - val_mae: 8.7846 - val_fp_mae: 4.8400\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 8.13962\n",
      "Epoch 97/100\n",
      "64/64 - 1s - loss: 26.1455 - mse: 26.1455 - mae: 4.0554 - fp_mae: 1.9705 - val_loss: 122.5924 - val_mse: 122.5924 - val_mae: 8.4679 - val_fp_mae: 4.8853\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 8.13962\n",
      "Epoch 98/100\n",
      "64/64 - 1s - loss: 25.1563 - mse: 25.1563 - mae: 3.9330 - fp_mae: 1.9140 - val_loss: 123.4395 - val_mse: 123.4395 - val_mae: 8.5687 - val_fp_mae: 4.5989\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 8.13962\n",
      "Epoch 99/100\n",
      "64/64 - 1s - loss: 25.5492 - mse: 25.5492 - mae: 3.9935 - fp_mae: 2.0387 - val_loss: 125.2043 - val_mse: 125.2043 - val_mae: 8.7107 - val_fp_mae: 4.7082\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 8.13962\n",
      "Epoch 100/100\n",
      "64/64 - 1s - loss: 25.6954 - mse: 25.6954 - mae: 4.0013 - fp_mae: 1.9796 - val_loss: 125.3145 - val_mse: 125.3145 - val_mae: 8.6226 - val_fp_mae: 5.6027\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 8.13962\n",
      "\n",
      "Lambda: 0 , Time: 0:02:23\n",
      "Train Error(all epochs): 3.933011054992676 \n",
      " [25.829, 24.726, 23.282, 21.735, 20.198, 18.646, 17.14, 15.67, 14.183, 12.743, 11.375, 10.062, 8.915, 8.234, 7.35, 6.832, 6.641, 6.289, 6.125, 6.049, 5.876, 5.816, 5.653, 5.633, 5.551, 5.5, 5.237, 5.316, 5.114, 5.167, 5.052, 5.017, 4.955, 4.858, 4.844, 4.837, 4.809, 4.771, 4.751, 4.772, 4.752, 4.723, 4.598, 4.583, 4.594, 4.589, 4.586, 4.545, 4.527, 4.495, 4.544, 4.456, 4.444, 4.526, 4.381, 4.505, 4.404, 4.389, 4.405, 4.33, 4.294, 4.345, 4.319, 4.336, 4.26, 4.24, 4.302, 4.309, 4.252, 4.177, 4.225, 4.123, 4.135, 4.072, 4.191, 4.169, 4.144, 4.098, 4.143, 4.168, 4.113, 4.119, 4.075, 4.064, 4.121, 4.151, 4.138, 4.082, 4.069, 4.071, 4.063, 4.044, 3.995, 4.055, 3.964, 3.951, 4.055, 3.933, 3.993, 4.001]\n",
      "Train FP Error(all epochs): 1.9140499830245972 \n",
      " [24.234, 23.534, 22.508, 21.281, 19.92, 18.461, 16.966, 15.446, 13.88, 12.315, 10.762, 9.275, 7.87, 6.789, 5.731, 4.944, 4.417, 3.912, 3.606, 3.354, 3.189, 3.034, 2.92, 2.898, 2.794, 2.746, 2.555, 2.578, 2.541, 2.626, 2.469, 2.496, 2.403, 2.433, 2.427, 2.385, 2.376, 2.37, 2.317, 2.364, 2.32, 2.334, 2.337, 2.258, 2.299, 2.288, 2.238, 2.292, 2.247, 2.236, 2.272, 2.147, 2.201, 2.281, 2.22, 2.19, 2.216, 2.186, 2.178, 2.136, 2.158, 2.131, 2.166, 2.191, 2.122, 2.062, 2.182, 2.223, 2.099, 2.084, 2.103, 2.019, 2.042, 2.022, 2.06, 2.108, 2.063, 2.064, 2.036, 2.101, 2.043, 2.019, 2.095, 2.054, 2.036, 1.988, 2.089, 2.083, 2.037, 2.015, 1.992, 1.973, 2.057, 1.982, 2.015, 1.995, 1.971, 1.914, 2.039, 1.98]\n",
      "Val Error(all epochs): 8.13962173461914 \n",
      " [26.041, 22.852, 20.977, 20.189, 16.225, 13.92, 12.782, 12.292, 12.427, 15.948, 12.645, 11.058, 11.657, 10.974, 9.609, 9.164, 8.14, 9.318, 8.471, 8.754, 8.447, 8.413, 8.558, 8.833, 8.511, 8.955, 8.375, 8.294, 8.425, 8.425, 8.659, 8.653, 8.695, 8.891, 8.557, 8.876, 8.625, 8.562, 8.74, 8.715, 8.77, 8.664, 8.653, 8.815, 8.854, 8.742, 8.867, 8.794, 8.627, 8.808, 8.693, 8.56, 8.669, 8.922, 8.943, 8.725, 9.24, 8.608, 8.633, 8.495, 8.83, 8.949, 8.519, 8.704, 8.713, 8.615, 8.733, 8.852, 8.442, 8.637, 8.728, 8.647, 8.961, 8.568, 8.681, 8.667, 8.845, 8.544, 8.789, 8.668, 8.612, 8.772, 8.489, 8.591, 8.561, 8.694, 8.545, 8.442, 8.557, 8.556, 8.51, 8.749, 8.495, 8.499, 8.442, 8.785, 8.468, 8.569, 8.711, 8.623]\n",
      "Val FP Error(all epochs): 3.7198846340179443 \n",
      " [24.739, 21.21, 17.879, 20.027, 14.739, 11.904, 10.835, 11.679, 11.754, 15.71, 12.203, 10.068, 10.466, 9.401, 7.999, 7.064, 5.011, 7.175, 4.543, 6.441, 4.647, 4.394, 5.035, 3.72, 5.101, 6.071, 4.121, 4.606, 5.279, 5.146, 5.155, 5.063, 5.945, 6.537, 5.82, 6.429, 5.612, 5.712, 5.323, 5.492, 5.667, 5.777, 5.354, 5.369, 4.839, 5.091, 4.554, 5.329, 4.836, 5.369, 5.101, 5.265, 5.438, 5.223, 5.305, 5.219, 6.408, 5.39, 4.543, 4.874, 5.309, 5.807, 5.234, 5.958, 5.691, 5.436, 6.017, 4.244, 4.872, 5.196, 5.019, 4.876, 5.075, 5.057, 5.547, 4.791, 5.409, 5.08, 5.633, 4.397, 5.003, 5.625, 4.739, 5.007, 4.849, 5.393, 4.973, 5.072, 5.107, 4.744, 4.353, 5.611, 4.9, 4.881, 5.403, 4.84, 4.885, 4.599, 4.708, 5.603]\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 - 2s - loss: 852.5021 - mse: 841.7341 - mae: 25.8310 - fp_mae: 24.2328 - val_loss: 886.2859 - val_mse: 875.7556 - val_mae: 25.8692 - val_fp_mae: 24.6708\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 25.86921, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_1.h5\n",
      "Epoch 2/100\n",
      "64/64 - 1s - loss: 791.2607 - mse: 780.9636 - mae: 24.6074 - fp_mae: 23.4676 - val_loss: 710.1658 - val_mse: 700.0570 - val_mae: 23.0537 - val_fp_mae: 21.5448\n",
      "\n",
      "Epoch 00002: val_mae improved from 25.86921 to 23.05370, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_1.h5\n",
      "Epoch 3/100\n",
      "64/64 - 1s - loss: 710.5831 - mse: 700.5363 - mae: 23.0954 - fp_mae: 22.3986 - val_loss: 571.4041 - val_mse: 561.3815 - val_mae: 20.6801 - val_fp_mae: 18.2256\n",
      "\n",
      "Epoch 00003: val_mae improved from 23.05370 to 20.68011, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_1.h5\n",
      "Epoch 4/100\n",
      "64/64 - 1s - loss: 626.6774 - mse: 616.6840 - mae: 21.6655 - fp_mae: 21.2219 - val_loss: 499.8746 - val_mse: 489.9084 - val_mae: 19.1438 - val_fp_mae: 16.4282\n",
      "\n",
      "Epoch 00004: val_mae improved from 20.68011 to 19.14385, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_1.h5\n",
      "Epoch 5/100\n",
      "64/64 - 1s - loss: 545.9858 - mse: 536.0770 - mae: 20.1190 - fp_mae: 19.8605 - val_loss: 407.9699 - val_mse: 398.1101 - val_mae: 16.2726 - val_fp_mae: 15.9335\n",
      "\n",
      "Epoch 00005: val_mae improved from 19.14385 to 16.27258, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_1.h5\n",
      "Epoch 6/100\n",
      "64/64 - 1s - loss: 474.7987 - mse: 464.9425 - mae: 18.6192 - fp_mae: 18.4376 - val_loss: 333.0165 - val_mse: 323.1082 - val_mae: 14.5027 - val_fp_mae: 14.2621\n",
      "\n",
      "Epoch 00006: val_mae improved from 16.27258 to 14.50270, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_1.h5\n",
      "Epoch 7/100\n",
      "64/64 - 1s - loss: 411.8235 - mse: 401.8735 - mae: 17.0685 - fp_mae: 16.9212 - val_loss: 280.5365 - val_mse: 270.5210 - val_mae: 13.2050 - val_fp_mae: 10.2796\n",
      "\n",
      "Epoch 00007: val_mae improved from 14.50270 to 13.20499, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_1.h5\n",
      "Epoch 8/100\n",
      "64/64 - 1s - loss: 357.1735 - mse: 347.0094 - mae: 15.6850 - fp_mae: 15.4553 - val_loss: 426.4270 - val_mse: 416.0683 - val_mae: 17.0170 - val_fp_mae: 16.7365\n",
      "\n",
      "Epoch 00008: val_mae did not improve from 13.20499\n",
      "Epoch 9/100\n",
      "64/64 - 1s - loss: 305.7006 - mse: 295.1070 - mae: 14.3450 - fp_mae: 13.9766 - val_loss: 438.8024 - val_mse: 427.9910 - val_mae: 17.2523 - val_fp_mae: 16.9158\n",
      "\n",
      "Epoch 00009: val_mae did not improve from 13.20499\n",
      "Epoch 10/100\n",
      "64/64 - 1s - loss: 254.8052 - mse: 243.8367 - mae: 12.9291 - fp_mae: 12.4400 - val_loss: 234.6292 - val_mse: 223.4629 - val_mae: 11.7512 - val_fp_mae: 11.0154\n",
      "\n",
      "Epoch 00010: val_mae improved from 13.20499 to 11.75116, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_1.h5\n",
      "Epoch 11/100\n",
      "64/64 - 1s - loss: 208.5882 - mse: 197.2396 - mae: 11.4914 - fp_mae: 10.8683 - val_loss: 159.3877 - val_mse: 147.8772 - val_mae: 9.5964 - val_fp_mae: 6.8313\n",
      "\n",
      "Epoch 00011: val_mae improved from 11.75116 to 9.59637, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_1.h5\n",
      "Epoch 12/100\n",
      "64/64 - 1s - loss: 172.0493 - mse: 160.3670 - mae: 10.3225 - fp_mae: 9.3950 - val_loss: 269.2276 - val_mse: 257.3724 - val_mae: 12.8521 - val_fp_mae: 11.7800\n",
      "\n",
      "Epoch 00012: val_mae did not improve from 9.59637\n",
      "Epoch 13/100\n",
      "64/64 - 1s - loss: 141.8528 - mse: 129.8516 - mae: 9.1122 - fp_mae: 7.9863 - val_loss: 153.4169 - val_mse: 141.2536 - val_mae: 9.3387 - val_fp_mae: 7.7595\n",
      "\n",
      "Epoch 00013: val_mae improved from 9.59637 to 9.33866, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_1.h5\n",
      "Epoch 14/100\n",
      "64/64 - 1s - loss: 119.5387 - mse: 107.2279 - mae: 8.2518 - fp_mae: 6.7853 - val_loss: 204.9495 - val_mse: 192.4932 - val_mae: 11.0754 - val_fp_mae: 9.1917\n",
      "\n",
      "Epoch 00014: val_mae did not improve from 9.33866\n",
      "Epoch 15/100\n",
      "64/64 - 1s - loss: 104.5424 - mse: 91.9380 - mae: 7.6262 - fp_mae: 5.8438 - val_loss: 156.1733 - val_mse: 143.4313 - val_mae: 9.5326 - val_fp_mae: 6.0677\n",
      "\n",
      "Epoch 00015: val_mae did not improve from 9.33866\n",
      "Epoch 16/100\n",
      "64/64 - 1s - loss: 93.8762 - mse: 81.0537 - mae: 7.1316 - fp_mae: 5.0966 - val_loss: 167.4013 - val_mse: 154.4724 - val_mae: 9.8561 - val_fp_mae: 7.0649\n",
      "\n",
      "Epoch 00016: val_mae did not improve from 9.33866\n",
      "Epoch 17/100\n",
      "64/64 - 1s - loss: 87.1719 - mse: 74.1395 - mae: 6.8350 - fp_mae: 4.5174 - val_loss: 178.3090 - val_mse: 165.1755 - val_mae: 10.3542 - val_fp_mae: 2.1809\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 9.33866\n",
      "Epoch 18/100\n",
      "64/64 - 1s - loss: 80.5327 - mse: 67.3275 - mae: 6.5170 - fp_mae: 4.0149 - val_loss: 134.9160 - val_mse: 121.6490 - val_mae: 8.9729 - val_fp_mae: 2.9521\n",
      "\n",
      "Epoch 00018: val_mae improved from 9.33866 to 8.97287, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_1.h5\n",
      "Epoch 19/100\n",
      "64/64 - 1s - loss: 78.2556 - mse: 64.9107 - mae: 6.3593 - fp_mae: 3.6952 - val_loss: 131.0065 - val_mse: 117.5534 - val_mae: 8.6008 - val_fp_mae: 5.1306\n",
      "\n",
      "Epoch 00019: val_mae improved from 8.97287 to 8.60078, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_1.h5\n",
      "Epoch 20/100\n",
      "64/64 - 1s - loss: 75.0523 - mse: 61.5340 - mae: 6.2234 - fp_mae: 3.4538 - val_loss: 128.7413 - val_mse: 115.1704 - val_mae: 8.6350 - val_fp_mae: 4.0047\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 8.60078\n",
      "Epoch 21/100\n",
      "64/64 - 1s - loss: 71.3966 - mse: 57.7724 - mae: 6.0688 - fp_mae: 3.2873 - val_loss: 134.7934 - val_mse: 121.1209 - val_mae: 8.9111 - val_fp_mae: 3.2768\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 8.60078\n",
      "Epoch 22/100\n",
      "64/64 - 1s - loss: 71.9202 - mse: 58.1810 - mae: 6.0546 - fp_mae: 3.1628 - val_loss: 145.2388 - val_mse: 131.4248 - val_mae: 9.0612 - val_fp_mae: 5.5685\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 8.60078\n",
      "Epoch 23/100\n",
      "64/64 - 1s - loss: 69.3780 - mse: 55.5275 - mae: 5.9407 - fp_mae: 3.1120 - val_loss: 136.4674 - val_mse: 122.5649 - val_mae: 8.6989 - val_fp_mae: 4.8797\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 8.60078\n",
      "Epoch 24/100\n",
      "64/64 - 2s - loss: 66.5210 - mse: 52.5775 - mae: 5.8014 - fp_mae: 2.9342 - val_loss: 141.2034 - val_mse: 127.2194 - val_mae: 8.9168 - val_fp_mae: 4.7901\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 8.60078\n",
      "Epoch 25/100\n",
      "64/64 - 1s - loss: 65.4699 - mse: 51.4309 - mae: 5.6520 - fp_mae: 2.8664 - val_loss: 143.0121 - val_mse: 128.9304 - val_mae: 9.1500 - val_fp_mae: 3.2927\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 8.60078\n",
      "Epoch 26/100\n",
      "64/64 - 1s - loss: 62.5212 - mse: 48.4233 - mae: 5.5691 - fp_mae: 2.7899 - val_loss: 129.8975 - val_mse: 115.7765 - val_mae: 8.5143 - val_fp_mae: 4.5048\n",
      "\n",
      "Epoch 00026: val_mae improved from 8.60078 to 8.51432, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_1.h5\n",
      "Epoch 27/100\n",
      "64/64 - 1s - loss: 62.2808 - mse: 48.1341 - mae: 5.4793 - fp_mae: 2.7438 - val_loss: 142.9288 - val_mse: 128.7488 - val_mae: 9.1768 - val_fp_mae: 3.3988\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 8.51432\n",
      "Epoch 28/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 - 1s - loss: 62.1467 - mse: 47.9356 - mae: 5.5267 - fp_mae: 2.7590 - val_loss: 142.5261 - val_mse: 128.2885 - val_mae: 9.0037 - val_fp_mae: 3.8303\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 8.51432\n",
      "Epoch 29/100\n",
      "64/64 - 1s - loss: 58.7965 - mse: 44.5466 - mae: 5.2976 - fp_mae: 2.6418 - val_loss: 143.1472 - val_mse: 128.8831 - val_mae: 9.1454 - val_fp_mae: 2.9839\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 8.51432\n",
      "Epoch 30/100\n",
      "64/64 - 1s - loss: 58.8847 - mse: 44.6105 - mae: 5.3274 - fp_mae: 2.6590 - val_loss: 130.0723 - val_mse: 115.7898 - val_mae: 8.6144 - val_fp_mae: 3.8916\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 8.51432\n",
      "Epoch 31/100\n",
      "64/64 - 1s - loss: 59.0155 - mse: 44.7157 - mae: 5.3258 - fp_mae: 2.7201 - val_loss: 185.7184 - val_mse: 171.3912 - val_mae: 10.3937 - val_fp_mae: 2.7376\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 8.51432\n",
      "Epoch 32/100\n",
      "64/64 - 2s - loss: 58.5717 - mse: 44.2291 - mae: 5.2764 - fp_mae: 2.6372 - val_loss: 133.6105 - val_mse: 119.2408 - val_mae: 8.6901 - val_fp_mae: 3.7336\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 8.51432\n",
      "Epoch 33/100\n",
      "64/64 - 1s - loss: 56.4803 - mse: 42.1008 - mae: 5.0960 - fp_mae: 2.4847 - val_loss: 161.3918 - val_mse: 146.9934 - val_mae: 9.4866 - val_fp_mae: 6.1907\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 8.51432\n",
      "Epoch 34/100\n",
      "64/64 - 1s - loss: 56.6210 - mse: 42.2099 - mae: 5.2063 - fp_mae: 2.6659 - val_loss: 133.1146 - val_mse: 118.6943 - val_mae: 8.6999 - val_fp_mae: 4.1306\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 8.51432\n",
      "Epoch 35/100\n",
      "64/64 - 1s - loss: 54.6590 - mse: 40.2479 - mae: 4.9850 - fp_mae: 2.5540 - val_loss: 139.8499 - val_mse: 125.4347 - val_mae: 8.8862 - val_fp_mae: 3.6119\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 8.51432\n",
      "Epoch 36/100\n",
      "64/64 - 1s - loss: 53.0858 - mse: 38.6659 - mae: 4.9340 - fp_mae: 2.3964 - val_loss: 139.2158 - val_mse: 124.8159 - val_mae: 8.9419 - val_fp_mae: 3.1630\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 8.51432\n",
      "Epoch 37/100\n",
      "64/64 - 1s - loss: 54.9153 - mse: 40.5089 - mae: 5.0053 - fp_mae: 2.4740 - val_loss: 137.6629 - val_mse: 123.2474 - val_mae: 8.8428 - val_fp_mae: 4.6051\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 8.51432\n",
      "Epoch 38/100\n",
      "64/64 - 1s - loss: 52.7674 - mse: 38.3450 - mae: 4.8633 - fp_mae: 2.4605 - val_loss: 136.4360 - val_mse: 121.9937 - val_mae: 8.6827 - val_fp_mae: 3.8636\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 8.51432\n",
      "Epoch 39/100\n",
      "64/64 - 1s - loss: 52.6042 - mse: 38.1668 - mae: 4.8729 - fp_mae: 2.4009 - val_loss: 136.4683 - val_mse: 122.0328 - val_mae: 8.8623 - val_fp_mae: 3.8246\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 8.51432\n",
      "Epoch 40/100\n",
      "64/64 - 1s - loss: 53.5749 - mse: 39.1392 - mae: 4.9281 - fp_mae: 2.4802 - val_loss: 139.6051 - val_mse: 125.1643 - val_mae: 8.8859 - val_fp_mae: 5.3235\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 8.51432\n",
      "Epoch 41/100\n",
      "64/64 - 1s - loss: 53.3104 - mse: 38.8558 - mae: 4.9722 - fp_mae: 2.4570 - val_loss: 138.6836 - val_mse: 124.2189 - val_mae: 8.8579 - val_fp_mae: 4.0088\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 8.51432\n",
      "Epoch 42/100\n",
      "64/64 - 1s - loss: 51.8232 - mse: 37.3613 - mae: 4.8262 - fp_mae: 2.4049 - val_loss: 138.4570 - val_mse: 123.9881 - val_mae: 8.8819 - val_fp_mae: 4.3324\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 8.51432\n",
      "Epoch 43/100\n",
      "64/64 - 1s - loss: 51.7643 - mse: 37.2968 - mae: 4.8393 - fp_mae: 2.4141 - val_loss: 141.9775 - val_mse: 127.5199 - val_mae: 9.0643 - val_fp_mae: 4.5632\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 8.51432\n",
      "Epoch 44/100\n",
      "64/64 - 1s - loss: 50.2848 - mse: 35.8371 - mae: 4.7496 - fp_mae: 2.4124 - val_loss: 138.5572 - val_mse: 124.1098 - val_mae: 8.8720 - val_fp_mae: 4.4290\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 8.51432\n",
      "Epoch 45/100\n",
      "64/64 - 1s - loss: 50.8618 - mse: 36.4058 - mae: 4.7112 - fp_mae: 2.3254 - val_loss: 142.0546 - val_mse: 127.5986 - val_mae: 9.0731 - val_fp_mae: 4.8294\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 8.51432\n",
      "Epoch 46/100\n",
      "64/64 - 1s - loss: 51.6213 - mse: 37.1619 - mae: 4.7802 - fp_mae: 2.3914 - val_loss: 154.4118 - val_mse: 139.9462 - val_mae: 9.3192 - val_fp_mae: 5.6928\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 8.51432\n",
      "Epoch 47/100\n",
      "64/64 - 1s - loss: 48.9863 - mse: 34.5302 - mae: 4.6610 - fp_mae: 2.3467 - val_loss: 141.3963 - val_mse: 126.9309 - val_mae: 8.9156 - val_fp_mae: 5.1010\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 8.51432\n",
      "Epoch 48/100\n",
      "64/64 - 1s - loss: 48.3131 - mse: 33.8755 - mae: 4.5781 - fp_mae: 2.3296 - val_loss: 142.0936 - val_mse: 127.6737 - val_mae: 8.9050 - val_fp_mae: 4.9914\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 8.51432\n",
      "Epoch 49/100\n",
      "64/64 - 1s - loss: 48.4838 - mse: 34.0827 - mae: 4.6046 - fp_mae: 2.2880 - val_loss: 136.3638 - val_mse: 121.9625 - val_mae: 8.7805 - val_fp_mae: 4.8811\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 8.51432\n",
      "Epoch 50/100\n",
      "64/64 - 1s - loss: 48.5497 - mse: 34.1572 - mae: 4.6297 - fp_mae: 2.3053 - val_loss: 140.3128 - val_mse: 125.9212 - val_mae: 8.9089 - val_fp_mae: 4.1700\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 8.51432\n",
      "Epoch 51/100\n",
      "64/64 - 1s - loss: 46.8622 - mse: 32.4893 - mae: 4.4755 - fp_mae: 2.2702 - val_loss: 132.9708 - val_mse: 118.6187 - val_mae: 8.5880 - val_fp_mae: 4.5895\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 8.51432\n",
      "Epoch 52/100\n",
      "64/64 - 1s - loss: 46.5425 - mse: 32.2112 - mae: 4.4496 - fp_mae: 2.1721 - val_loss: 146.7353 - val_mse: 132.4311 - val_mae: 9.0792 - val_fp_mae: 5.3879\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 8.51432\n",
      "Epoch 53/100\n",
      "64/64 - 1s - loss: 48.0510 - mse: 33.7771 - mae: 4.5665 - fp_mae: 2.3353 - val_loss: 154.3683 - val_mse: 140.0924 - val_mae: 9.4938 - val_fp_mae: 3.3579\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 8.51432\n",
      "Epoch 54/100\n",
      "64/64 - 1s - loss: 48.1345 - mse: 33.8304 - mae: 4.5866 - fp_mae: 2.3165 - val_loss: 139.0276 - val_mse: 124.7161 - val_mae: 8.8477 - val_fp_mae: 4.3982\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 8.51432\n",
      "Epoch 55/100\n",
      "64/64 - 1s - loss: 46.7694 - mse: 32.4656 - mae: 4.4725 - fp_mae: 2.1552 - val_loss: 153.0273 - val_mse: 138.7453 - val_mae: 9.2324 - val_fp_mae: 4.7998\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 8.51432\n",
      "Epoch 56/100\n",
      "64/64 - 1s - loss: 46.9718 - mse: 32.6884 - mae: 4.5044 - fp_mae: 2.2576 - val_loss: 150.0041 - val_mse: 135.7247 - val_mae: 9.2330 - val_fp_mae: 3.9735\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 8.51432\n",
      "Epoch 57/100\n",
      "64/64 - 1s - loss: 47.3806 - mse: 33.1135 - mae: 4.4906 - fp_mae: 2.2684 - val_loss: 153.6925 - val_mse: 139.4413 - val_mae: 9.3473 - val_fp_mae: 5.0545\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 8.51432\n",
      "Epoch 58/100\n",
      "64/64 - 1s - loss: 46.2739 - mse: 32.0283 - mae: 4.4595 - fp_mae: 2.1785 - val_loss: 140.8951 - val_mse: 126.6797 - val_mae: 8.9134 - val_fp_mae: 5.0606\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 8.51432\n",
      "Epoch 59/100\n",
      "64/64 - 2s - loss: 45.7280 - mse: 31.5095 - mae: 4.4665 - fp_mae: 2.1999 - val_loss: 148.0910 - val_mse: 133.8990 - val_mae: 9.0869 - val_fp_mae: 5.2813\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 8.51432\n",
      "Epoch 60/100\n",
      "64/64 - 1s - loss: 45.8325 - mse: 31.6632 - mae: 4.4826 - fp_mae: 2.3018 - val_loss: 139.1128 - val_mse: 124.9426 - val_mae: 8.9095 - val_fp_mae: 4.2139\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 8.51432\n",
      "Epoch 61/100\n",
      "64/64 - 1s - loss: 47.0435 - mse: 32.8907 - mae: 4.5367 - fp_mae: 2.3167 - val_loss: 139.4803 - val_mse: 125.3379 - val_mae: 8.8548 - val_fp_mae: 4.5513\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 8.51432\n",
      "Epoch 62/100\n",
      "64/64 - 1s - loss: 45.4532 - mse: 31.3234 - mae: 4.4123 - fp_mae: 2.2282 - val_loss: 143.9687 - val_mse: 129.8649 - val_mae: 9.1222 - val_fp_mae: 4.6585\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 8.51432\n",
      "Epoch 63/100\n",
      "64/64 - 1s - loss: 45.4446 - mse: 31.3628 - mae: 4.4281 - fp_mae: 2.2820 - val_loss: 138.1460 - val_mse: 124.0775 - val_mae: 8.9106 - val_fp_mae: 3.9355\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 8.51432\n",
      "Epoch 64/100\n",
      "64/64 - 1s - loss: 45.0840 - mse: 31.0368 - mae: 4.4144 - fp_mae: 2.2531 - val_loss: 148.6837 - val_mse: 134.6408 - val_mae: 9.1894 - val_fp_mae: 4.9664\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 8.51432\n",
      "Epoch 65/100\n",
      "64/64 - 1s - loss: 44.9635 - mse: 30.9279 - mae: 4.4191 - fp_mae: 2.1566 - val_loss: 138.1112 - val_mse: 124.1000 - val_mae: 8.8645 - val_fp_mae: 3.9624\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 8.51432\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 - 1s - loss: 44.3550 - mse: 30.3679 - mae: 4.3475 - fp_mae: 2.2054 - val_loss: 136.6508 - val_mse: 122.6766 - val_mae: 8.8163 - val_fp_mae: 3.8172\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 8.51432\n",
      "Epoch 67/100\n",
      "64/64 - 1s - loss: 43.9206 - mse: 29.9640 - mae: 4.3368 - fp_mae: 2.1709 - val_loss: 133.4436 - val_mse: 119.4925 - val_mae: 8.6738 - val_fp_mae: 4.4875\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 8.51432\n",
      "Epoch 68/100\n",
      "64/64 - 1s - loss: 44.1794 - mse: 30.2700 - mae: 4.3282 - fp_mae: 2.1901 - val_loss: 139.1039 - val_mse: 125.2161 - val_mae: 8.8822 - val_fp_mae: 5.4497\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 8.51432\n",
      "Epoch 69/100\n",
      "64/64 - 1s - loss: 44.1472 - mse: 30.2578 - mae: 4.2951 - fp_mae: 2.0965 - val_loss: 140.2414 - val_mse: 126.3719 - val_mae: 8.9433 - val_fp_mae: 4.7140\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 8.51432\n",
      "Epoch 70/100\n",
      "64/64 - 1s - loss: 44.1819 - mse: 30.3489 - mae: 4.3058 - fp_mae: 2.2039 - val_loss: 135.3736 - val_mse: 121.5583 - val_mae: 8.7392 - val_fp_mae: 4.3901\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 8.51432\n",
      "Epoch 71/100\n",
      "64/64 - 1s - loss: 43.3714 - mse: 29.5835 - mae: 4.2981 - fp_mae: 2.1989 - val_loss: 150.5085 - val_mse: 136.7419 - val_mae: 9.2329 - val_fp_mae: 5.5372\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 8.51432\n",
      "Epoch 72/100\n",
      "64/64 - 1s - loss: 42.5444 - mse: 28.7927 - mae: 4.2242 - fp_mae: 2.0876 - val_loss: 146.5164 - val_mse: 132.7900 - val_mae: 9.0640 - val_fp_mae: 5.9787\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 8.51432\n",
      "Epoch 73/100\n",
      "64/64 - 1s - loss: 43.5571 - mse: 29.8482 - mae: 4.3304 - fp_mae: 2.1515 - val_loss: 142.0908 - val_mse: 128.3884 - val_mae: 8.9677 - val_fp_mae: 4.1521\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 8.51432\n",
      "Epoch 74/100\n",
      "64/64 - 1s - loss: 45.1680 - mse: 31.4788 - mae: 4.3491 - fp_mae: 2.1819 - val_loss: 149.8852 - val_mse: 136.1870 - val_mae: 9.2401 - val_fp_mae: 5.6415\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 8.51432\n",
      "Epoch 75/100\n",
      "64/64 - 1s - loss: 43.7594 - mse: 30.0448 - mae: 4.2929 - fp_mae: 2.0979 - val_loss: 137.4894 - val_mse: 123.7818 - val_mae: 8.9473 - val_fp_mae: 3.7654\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 8.51432\n",
      "Epoch 76/100\n",
      "64/64 - 1s - loss: 42.4231 - mse: 28.7586 - mae: 4.1923 - fp_mae: 2.1027 - val_loss: 146.4478 - val_mse: 132.8150 - val_mae: 9.2630 - val_fp_mae: 3.3331\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 8.51432\n",
      "Epoch 77/100\n",
      "64/64 - 1s - loss: 43.3134 - mse: 29.6966 - mae: 4.2851 - fp_mae: 2.2365 - val_loss: 140.4333 - val_mse: 126.8149 - val_mae: 9.0406 - val_fp_mae: 4.2958\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 8.51432\n",
      "Epoch 78/100\n",
      "64/64 - 1s - loss: 42.8515 - mse: 29.2290 - mae: 4.2470 - fp_mae: 2.0588 - val_loss: 207.7305 - val_mse: 194.1312 - val_mae: 11.2198 - val_fp_mae: 2.2381\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 8.51432\n",
      "Epoch 79/100\n",
      "64/64 - 1s - loss: 43.6323 - mse: 30.0372 - mae: 4.3541 - fp_mae: 2.1536 - val_loss: 149.6612 - val_mse: 136.0775 - val_mae: 9.2758 - val_fp_mae: 3.4240\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 8.51432\n",
      "Epoch 80/100\n",
      "64/64 - 1s - loss: 42.2654 - mse: 28.7012 - mae: 4.1845 - fp_mae: 2.1233 - val_loss: 142.9325 - val_mse: 129.3955 - val_mae: 9.0670 - val_fp_mae: 4.1238\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 8.51432\n",
      "Epoch 81/100\n",
      "64/64 - 1s - loss: 42.3295 - mse: 28.8063 - mae: 4.2279 - fp_mae: 2.1246 - val_loss: 142.9086 - val_mse: 129.4120 - val_mae: 9.1407 - val_fp_mae: 5.0740\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 8.51432\n",
      "Epoch 82/100\n",
      "64/64 - 1s - loss: 41.9224 - mse: 28.4424 - mae: 4.2052 - fp_mae: 2.1474 - val_loss: 138.9113 - val_mse: 125.4540 - val_mae: 8.8451 - val_fp_mae: 4.8954\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 8.51432\n",
      "Epoch 83/100\n",
      "64/64 - 1s - loss: 41.8818 - mse: 28.4477 - mae: 4.2359 - fp_mae: 2.1789 - val_loss: 142.1343 - val_mse: 128.7249 - val_mae: 8.9965 - val_fp_mae: 3.6820\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 8.51432\n",
      "Epoch 84/100\n",
      "64/64 - 1s - loss: 41.4625 - mse: 28.0729 - mae: 4.1927 - fp_mae: 2.1580 - val_loss: 144.9894 - val_mse: 131.6096 - val_mae: 9.0634 - val_fp_mae: 4.4540\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 8.51432\n",
      "Epoch 85/100\n",
      "64/64 - 1s - loss: 43.1712 - mse: 29.8053 - mae: 4.2491 - fp_mae: 2.0841 - val_loss: 159.6924 - val_mse: 146.3384 - val_mae: 9.4391 - val_fp_mae: 6.2001\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 8.51432\n",
      "Epoch 86/100\n",
      "64/64 - 1s - loss: 41.9869 - mse: 28.6417 - mae: 4.1429 - fp_mae: 2.0704 - val_loss: 163.9319 - val_mse: 150.5733 - val_mae: 9.6000 - val_fp_mae: 6.2617\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 8.51432\n",
      "Epoch 87/100\n",
      "64/64 - 1s - loss: 42.8276 - mse: 29.4799 - mae: 4.2886 - fp_mae: 2.1401 - val_loss: 148.9988 - val_mse: 135.6463 - val_mae: 9.1057 - val_fp_mae: 5.7062\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 8.51432\n",
      "Epoch 88/100\n",
      "64/64 - 1s - loss: 42.1565 - mse: 28.8391 - mae: 4.2392 - fp_mae: 2.1837 - val_loss: 145.9756 - val_mse: 132.6767 - val_mae: 9.0563 - val_fp_mae: 5.5762\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 8.51432\n",
      "Epoch 89/100\n",
      "64/64 - 1s - loss: 42.4253 - mse: 29.1347 - mae: 4.2476 - fp_mae: 2.1103 - val_loss: 148.5098 - val_mse: 135.2442 - val_mae: 9.0376 - val_fp_mae: 5.6196\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 8.51432\n",
      "Epoch 90/100\n",
      "64/64 - 1s - loss: 41.5870 - mse: 28.3408 - mae: 4.2076 - fp_mae: 2.1020 - val_loss: 140.5429 - val_mse: 127.3238 - val_mae: 8.9630 - val_fp_mae: 4.1443\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 8.51432\n",
      "Epoch 91/100\n",
      "64/64 - 1s - loss: 42.5020 - mse: 29.2856 - mae: 4.2699 - fp_mae: 2.2059 - val_loss: 138.8559 - val_mse: 125.6284 - val_mae: 8.8983 - val_fp_mae: 4.2030\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 8.51432\n",
      "Epoch 92/100\n",
      "64/64 - 1s - loss: 42.2605 - mse: 29.0340 - mae: 4.2473 - fp_mae: 2.1074 - val_loss: 139.3924 - val_mse: 126.1884 - val_mae: 8.8100 - val_fp_mae: 4.3028\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 8.51432\n",
      "Epoch 93/100\n",
      "64/64 - 1s - loss: 41.2621 - mse: 28.0587 - mae: 4.1586 - fp_mae: 2.0896 - val_loss: 144.7785 - val_mse: 131.5937 - val_mae: 9.0316 - val_fp_mae: 4.1535\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 8.51432\n",
      "Epoch 94/100\n",
      "64/64 - 1s - loss: 41.2783 - mse: 28.1144 - mae: 4.1757 - fp_mae: 2.0191 - val_loss: 145.1331 - val_mse: 132.0007 - val_mae: 9.0393 - val_fp_mae: 4.6809\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 8.51432\n",
      "Epoch 95/100\n",
      "64/64 - 1s - loss: 41.3301 - mse: 28.2060 - mae: 4.1259 - fp_mae: 2.0784 - val_loss: 141.1422 - val_mse: 128.0300 - val_mae: 8.9232 - val_fp_mae: 4.7006\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 8.51432\n",
      "Epoch 96/100\n",
      "64/64 - 1s - loss: 42.0222 - mse: 28.9172 - mae: 4.2324 - fp_mae: 2.1798 - val_loss: 143.9431 - val_mse: 130.8417 - val_mae: 8.9193 - val_fp_mae: 5.4306\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 8.51432\n",
      "Epoch 97/100\n",
      "64/64 - 1s - loss: 39.9422 - mse: 26.8674 - mae: 4.0782 - fp_mae: 2.0876 - val_loss: 137.0408 - val_mse: 123.9793 - val_mae: 8.8573 - val_fp_mae: 4.5791\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 8.51432\n",
      "Epoch 98/100\n",
      "64/64 - 1s - loss: 41.4869 - mse: 28.4332 - mae: 4.1932 - fp_mae: 2.0312 - val_loss: 134.6636 - val_mse: 121.6383 - val_mae: 8.7507 - val_fp_mae: 4.5408\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 8.51432\n",
      "Epoch 99/100\n",
      "64/64 - 1s - loss: 40.9531 - mse: 27.9594 - mae: 4.1214 - fp_mae: 2.0972 - val_loss: 137.8491 - val_mse: 124.8759 - val_mae: 8.8562 - val_fp_mae: 4.4567\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 8.51432\n",
      "Epoch 100/100\n",
      "64/64 - 1s - loss: 40.9094 - mse: 27.9530 - mae: 4.1863 - fp_mae: 2.0723 - val_loss: 140.8426 - val_mse: 127.9102 - val_mae: 8.8932 - val_fp_mae: 5.5573\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 8.51432\n",
      "\n",
      "Lambda: 0.1 , Time: 0:02:22\n",
      "Train Error(all epochs): 4.078167915344238 \n",
      " [25.831, 24.607, 23.095, 21.666, 20.119, 18.619, 17.068, 15.685, 14.345, 12.929, 11.491, 10.322, 9.112, 8.252, 7.626, 7.132, 6.835, 6.517, 6.359, 6.223, 6.069, 6.055, 5.941, 5.801, 5.652, 5.569, 5.479, 5.527, 5.298, 5.327, 5.326, 5.276, 5.096, 5.206, 4.985, 4.934, 5.005, 4.863, 4.873, 4.928, 4.972, 4.826, 4.839, 4.75, 4.711, 4.78, 4.661, 4.578, 4.605, 4.63, 4.475, 4.45, 4.567, 4.587, 4.472, 4.504, 4.491, 4.46, 4.467, 4.483, 4.537, 4.412, 4.428, 4.414, 4.419, 4.348, 4.337, 4.328, 4.295, 4.306, 4.298, 4.224, 4.33, 4.349, 4.293, 4.192, 4.285, 4.247, 4.354, 4.185, 4.228, 4.205, 4.236, 4.193, 4.249, 4.143, 4.289, 4.239, 4.248, 4.208, 4.27, 4.247, 4.159, 4.176, 4.126, 4.232, 4.078, 4.193, 4.121, 4.186]\n",
      "Train FP Error(all epochs): 2.01906156539917 \n",
      " [24.233, 23.468, 22.399, 21.222, 19.86, 18.438, 16.921, 15.455, 13.977, 12.44, 10.868, 9.395, 7.986, 6.785, 5.844, 5.097, 4.517, 4.015, 3.695, 3.454, 3.287, 3.163, 3.112, 2.934, 2.866, 2.79, 2.744, 2.759, 2.642, 2.659, 2.72, 2.637, 2.485, 2.666, 2.554, 2.396, 2.474, 2.461, 2.401, 2.48, 2.457, 2.405, 2.414, 2.412, 2.325, 2.391, 2.347, 2.33, 2.288, 2.305, 2.27, 2.172, 2.335, 2.317, 2.155, 2.258, 2.268, 2.179, 2.2, 2.302, 2.317, 2.228, 2.282, 2.253, 2.157, 2.205, 2.171, 2.19, 2.097, 2.204, 2.199, 2.088, 2.152, 2.182, 2.098, 2.103, 2.237, 2.059, 2.154, 2.123, 2.125, 2.147, 2.179, 2.158, 2.084, 2.07, 2.14, 2.184, 2.11, 2.102, 2.206, 2.107, 2.09, 2.019, 2.078, 2.18, 2.088, 2.031, 2.097, 2.072]\n",
      "Val Error(all epochs): 8.514323234558105 \n",
      " [25.869, 23.054, 20.68, 19.144, 16.273, 14.503, 13.205, 17.017, 17.252, 11.751, 9.596, 12.852, 9.339, 11.075, 9.533, 9.856, 10.354, 8.973, 8.601, 8.635, 8.911, 9.061, 8.699, 8.917, 9.15, 8.514, 9.177, 9.004, 9.145, 8.614, 10.394, 8.69, 9.487, 8.7, 8.886, 8.942, 8.843, 8.683, 8.862, 8.886, 8.858, 8.882, 9.064, 8.872, 9.073, 9.319, 8.916, 8.905, 8.781, 8.909, 8.588, 9.079, 9.494, 8.848, 9.232, 9.233, 9.347, 8.913, 9.087, 8.909, 8.855, 9.122, 8.911, 9.189, 8.864, 8.816, 8.674, 8.882, 8.943, 8.739, 9.233, 9.064, 8.968, 9.24, 8.947, 9.263, 9.041, 11.22, 9.276, 9.067, 9.141, 8.845, 8.997, 9.063, 9.439, 9.6, 9.106, 9.056, 9.038, 8.963, 8.898, 8.81, 9.032, 9.039, 8.923, 8.919, 8.857, 8.751, 8.856, 8.893]\n",
      "Val FP Error(all epochs): 2.180906057357788 \n",
      " [24.671, 21.545, 18.226, 16.428, 15.934, 14.262, 10.28, 16.736, 16.916, 11.015, 6.831, 11.78, 7.76, 9.192, 6.068, 7.065, 2.181, 2.952, 5.131, 4.005, 3.277, 5.569, 4.88, 4.79, 3.293, 4.505, 3.399, 3.83, 2.984, 3.892, 2.738, 3.734, 6.191, 4.131, 3.612, 3.163, 4.605, 3.864, 3.825, 5.324, 4.009, 4.332, 4.563, 4.429, 4.829, 5.693, 5.101, 4.991, 4.881, 4.17, 4.59, 5.388, 3.358, 4.398, 4.8, 3.974, 5.055, 5.061, 5.281, 4.214, 4.551, 4.658, 3.935, 4.966, 3.962, 3.817, 4.487, 5.45, 4.714, 4.39, 5.537, 5.979, 4.152, 5.641, 3.765, 3.333, 4.296, 2.238, 3.424, 4.124, 5.074, 4.895, 3.682, 4.454, 6.2, 6.262, 5.706, 5.576, 5.62, 4.144, 4.203, 4.303, 4.153, 4.681, 4.701, 5.431, 4.579, 4.541, 4.457, 5.557]\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 - 2s - loss: 927.2966 - mse: 843.2924 - mae: 25.8646 - fp_mae: 24.2511 - val_loss: 1095.3969 - val_mse: 1031.9615 - val_mae: 28.2783 - val_fp_mae: 27.1903\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 28.27831, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_2.h5\n",
      "Epoch 2/100\n",
      "64/64 - 1s - loss: 836.2665 - mse: 785.2027 - mae: 24.6821 - fp_mae: 23.5038 - val_loss: 1038.3699 - val_mse: 996.4734 - val_mae: 27.6552 - val_fp_mae: 26.7726\n",
      "\n",
      "Epoch 00002: val_mae improved from 28.27831 to 27.65522, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_2.h5\n",
      "Epoch 3/100\n",
      "64/64 - 2s - loss: 740.2145 - mse: 702.7939 - mae: 23.1787 - fp_mae: 22.4355 - val_loss: 724.5031 - val_mse: 691.0116 - val_mae: 22.7842 - val_fp_mae: 21.4124\n",
      "\n",
      "Epoch 00003: val_mae improved from 27.65522 to 22.78423, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_2.h5\n",
      "Epoch 4/100\n",
      "64/64 - 1s - loss: 648.5294 - mse: 617.4377 - mae: 21.6820 - fp_mae: 21.2187 - val_loss: 644.2344 - val_mse: 615.1058 - val_mae: 21.0382 - val_fp_mae: 20.3660\n",
      "\n",
      "Epoch 00004: val_mae improved from 22.78423 to 21.03819, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_2.h5\n",
      "Epoch 5/100\n",
      "64/64 - 1s - loss: 564.9681 - mse: 537.3942 - mae: 20.1386 - fp_mae: 19.8514 - val_loss: 493.7663 - val_mse: 467.3066 - val_mae: 17.9897 - val_fp_mae: 17.0311\n",
      "\n",
      "Epoch 00005: val_mae improved from 21.03819 to 17.98971, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_2.h5\n",
      "Epoch 6/100\n",
      "64/64 - 1s - loss: 494.0316 - mse: 467.6877 - mae: 18.6275 - fp_mae: 18.4149 - val_loss: 385.7883 - val_mse: 359.3659 - val_mae: 15.3988 - val_fp_mae: 14.0841\n",
      "\n",
      "Epoch 00006: val_mae improved from 17.98971 to 15.39879, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_2.h5\n",
      "Epoch 7/100\n",
      "64/64 - 1s - loss: 432.6378 - mse: 406.2635 - mae: 17.1175 - fp_mae: 16.9372 - val_loss: 367.6584 - val_mse: 341.7137 - val_mae: 14.9158 - val_fp_mae: 14.6586\n",
      "\n",
      "Epoch 00007: val_mae improved from 15.39879 to 14.91576, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_2.h5\n",
      "Epoch 8/100\n",
      "64/64 - 1s - loss: 378.6230 - mse: 352.4660 - mae: 15.7909 - fp_mae: 15.5128 - val_loss: 540.3056 - val_mse: 513.2350 - val_mae: 19.2240 - val_fp_mae: 18.9905\n",
      "\n",
      "Epoch 00008: val_mae did not improve from 14.91576\n",
      "Epoch 9/100\n",
      "64/64 - 1s - loss: 332.1071 - mse: 304.2995 - mae: 14.4059 - fp_mae: 14.0343 - val_loss: 411.5606 - val_mse: 383.0436 - val_mae: 16.3283 - val_fp_mae: 16.1214\n",
      "\n",
      "Epoch 00009: val_mae did not improve from 14.91576\n",
      "Epoch 10/100\n",
      "64/64 - 1s - loss: 286.2501 - mse: 256.8092 - mae: 13.0812 - fp_mae: 12.5377 - val_loss: 744.9316 - val_mse: 714.2429 - val_mae: 22.0599 - val_fp_mae: 21.8155\n",
      "\n",
      "Epoch 00010: val_mae did not improve from 14.91576\n",
      "Epoch 11/100\n",
      "64/64 - 1s - loss: 244.6293 - mse: 213.0100 - mae: 11.8451 - fp_mae: 11.0573 - val_loss: 542.0287 - val_mse: 509.6705 - val_mae: 18.3916 - val_fp_mae: 17.7890\n",
      "\n",
      "Epoch 00011: val_mae did not improve from 14.91576\n",
      "Epoch 12/100\n",
      "64/64 - 1s - loss: 209.8552 - mse: 177.0093 - mae: 10.5903 - fp_mae: 9.6296 - val_loss: 235.8340 - val_mse: 202.1929 - val_mae: 11.1316 - val_fp_mae: 10.1724\n",
      "\n",
      "Epoch 00012: val_mae improved from 14.91576 to 11.13164, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_2.h5\n",
      "Epoch 13/100\n",
      "64/64 - 1s - loss: 182.1367 - mse: 147.9323 - mae: 9.6582 - fp_mae: 8.3695 - val_loss: 303.3151 - val_mse: 268.4907 - val_mae: 12.9925 - val_fp_mae: 11.7601\n",
      "\n",
      "Epoch 00013: val_mae did not improve from 11.13164\n",
      "Epoch 14/100\n",
      "64/64 - 1s - loss: 160.3816 - mse: 124.9913 - mae: 8.8232 - fp_mae: 7.2102 - val_loss: 220.5343 - val_mse: 184.5539 - val_mae: 10.5982 - val_fp_mae: 9.5158\n",
      "\n",
      "Epoch 00014: val_mae improved from 11.13164 to 10.59820, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_2.h5\n",
      "Epoch 15/100\n",
      "64/64 - 1s - loss: 145.9067 - mse: 109.5607 - mae: 8.2104 - fp_mae: 6.2852 - val_loss: 166.5448 - val_mse: 129.7455 - val_mae: 8.7026 - val_fp_mae: 6.7764\n",
      "\n",
      "Epoch 00015: val_mae improved from 10.59820 to 8.70258, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_2.h5\n",
      "Epoch 16/100\n",
      "64/64 - 1s - loss: 135.0577 - mse: 98.0341 - mae: 7.8162 - fp_mae: 5.5803 - val_loss: 164.9849 - val_mse: 127.4112 - val_mae: 8.7056 - val_fp_mae: 6.3736\n",
      "\n",
      "Epoch 00016: val_mae did not improve from 8.70258\n",
      "Epoch 17/100\n",
      "64/64 - 1s - loss: 128.1994 - mse: 90.3292 - mae: 7.5356 - fp_mae: 5.0080 - val_loss: 175.3570 - val_mse: 137.1817 - val_mae: 9.0820 - val_fp_mae: 7.1096\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 8.70258\n",
      "Epoch 18/100\n",
      "64/64 - 1s - loss: 124.1053 - mse: 85.6621 - mae: 7.2937 - fp_mae: 4.5775 - val_loss: 153.1852 - val_mse: 114.1774 - val_mae: 8.1911 - val_fp_mae: 5.0828\n",
      "\n",
      "Epoch 00018: val_mae improved from 8.70258 to 8.19112, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_2.h5\n",
      "Epoch 19/100\n",
      "64/64 - 1s - loss: 123.2423 - mse: 83.8319 - mae: 7.1733 - fp_mae: 4.2907 - val_loss: 168.2835 - val_mse: 128.4578 - val_mae: 8.9908 - val_fp_mae: 4.7477\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 8.19112\n",
      "Epoch 20/100\n",
      "64/64 - 1s - loss: 119.1082 - mse: 79.0059 - mae: 7.0320 - fp_mae: 4.0342 - val_loss: 260.2113 - val_mse: 219.8473 - val_mae: 11.6019 - val_fp_mae: 10.3719\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 8.19112\n",
      "Epoch 21/100\n",
      "64/64 - 1s - loss: 115.5495 - mse: 75.2004 - mae: 6.8457 - fp_mae: 3.8831 - val_loss: 190.3297 - val_mse: 149.8595 - val_mae: 9.4474 - val_fp_mae: 7.5181\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 8.19112\n",
      "Epoch 22/100\n",
      "64/64 - 1s - loss: 112.3233 - mse: 71.7406 - mae: 6.7296 - fp_mae: 3.7313 - val_loss: 169.4988 - val_mse: 128.8073 - val_mae: 8.6840 - val_fp_mae: 6.0107\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 8.19112\n",
      "Epoch 23/100\n",
      "64/64 - 1s - loss: 110.0576 - mse: 69.3185 - mae: 6.5969 - fp_mae: 3.5652 - val_loss: 181.8922 - val_mse: 141.1936 - val_mae: 9.3491 - val_fp_mae: 3.9675\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 8.19112\n",
      "Epoch 24/100\n",
      "64/64 - 1s - loss: 109.2304 - mse: 68.5545 - mae: 6.5778 - fp_mae: 3.5247 - val_loss: 169.5253 - val_mse: 128.8050 - val_mae: 8.9566 - val_fp_mae: 3.5085\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 8.19112\n",
      "Epoch 25/100\n",
      "64/64 - 1s - loss: 107.7119 - mse: 66.9566 - mae: 6.4974 - fp_mae: 3.4704 - val_loss: 180.5630 - val_mse: 139.6396 - val_mae: 9.0934 - val_fp_mae: 5.9793\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 8.19112\n",
      "Epoch 26/100\n",
      "64/64 - 1s - loss: 107.6961 - mse: 66.7429 - mae: 6.4201 - fp_mae: 3.3959 - val_loss: 561.0764 - val_mse: 519.9589 - val_mae: 18.3380 - val_fp_mae: 16.9913\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 8.19112\n",
      "Epoch 27/100\n",
      "64/64 - 1s - loss: 107.5619 - mse: 66.1007 - mae: 6.4314 - fp_mae: 3.4208 - val_loss: 286.6430 - val_mse: 245.0074 - val_mae: 12.2752 - val_fp_mae: 10.6450\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 8.19112\n",
      "Epoch 28/100\n",
      "64/64 - 1s - loss: 104.1428 - mse: 62.5241 - mae: 6.2572 - fp_mae: 3.3125 - val_loss: 303.1487 - val_mse: 261.5006 - val_mae: 12.5351 - val_fp_mae: 9.9953\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 8.19112\n",
      "Epoch 29/100\n",
      "64/64 - 1s - loss: 104.1172 - mse: 62.4282 - mae: 6.2394 - fp_mae: 3.2895 - val_loss: 174.7566 - val_mse: 133.1491 - val_mae: 8.9325 - val_fp_mae: 5.8477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00029: val_mae did not improve from 8.19112\n",
      "Epoch 30/100\n",
      "64/64 - 1s - loss: 102.0862 - mse: 60.6219 - mae: 6.1751 - fp_mae: 3.3001 - val_loss: 211.7991 - val_mse: 170.4330 - val_mae: 10.1486 - val_fp_mae: 4.5041\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 8.19112\n",
      "Epoch 31/100\n",
      "64/64 - 1s - loss: 99.3577 - mse: 58.1753 - mae: 5.9743 - fp_mae: 3.1979 - val_loss: 167.0926 - val_mse: 125.8756 - val_mae: 8.6373 - val_fp_mae: 4.8023\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 8.19112\n",
      "Epoch 32/100\n",
      "64/64 - 1s - loss: 98.4066 - mse: 57.2305 - mae: 5.9821 - fp_mae: 3.1821 - val_loss: 180.8713 - val_mse: 139.6178 - val_mae: 9.1553 - val_fp_mae: 4.1710\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 8.19112\n",
      "Epoch 33/100\n",
      "64/64 - 1s - loss: 96.9165 - mse: 55.6785 - mae: 5.9117 - fp_mae: 3.1082 - val_loss: 210.2741 - val_mse: 169.1038 - val_mae: 9.8164 - val_fp_mae: 6.4436\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 8.19112\n",
      "Epoch 34/100\n",
      "64/64 - 1s - loss: 98.1976 - mse: 57.0719 - mae: 5.9673 - fp_mae: 3.1701 - val_loss: 170.8127 - val_mse: 129.7290 - val_mae: 8.7461 - val_fp_mae: 5.1051\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 8.19112\n",
      "Epoch 35/100\n",
      "64/64 - 1s - loss: 92.9509 - mse: 52.1293 - mae: 5.7049 - fp_mae: 3.0600 - val_loss: 236.9148 - val_mse: 196.1935 - val_mae: 10.8023 - val_fp_mae: 8.5795\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 8.19112\n",
      "Epoch 36/100\n",
      "64/64 - 1s - loss: 94.1048 - mse: 53.4922 - mae: 5.7643 - fp_mae: 3.0504 - val_loss: 209.5999 - val_mse: 169.1041 - val_mae: 9.9631 - val_fp_mae: 7.9297\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 8.19112\n",
      "Epoch 37/100\n",
      "64/64 - 1s - loss: 93.9974 - mse: 53.5255 - mae: 5.8157 - fp_mae: 3.0519 - val_loss: 183.2410 - val_mse: 142.7849 - val_mae: 9.1794 - val_fp_mae: 5.3403\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 8.19112\n",
      "Epoch 38/100\n",
      "64/64 - 1s - loss: 92.2406 - mse: 51.9672 - mae: 5.7197 - fp_mae: 3.0418 - val_loss: 167.1890 - val_mse: 126.9003 - val_mae: 8.6131 - val_fp_mae: 5.3122\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 8.19112\n",
      "Epoch 39/100\n",
      "64/64 - 1s - loss: 94.9333 - mse: 54.6869 - mae: 5.8174 - fp_mae: 3.0830 - val_loss: 217.7778 - val_mse: 177.3288 - val_mae: 10.3661 - val_fp_mae: 7.8174\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 8.19112\n",
      "Epoch 40/100\n",
      "64/64 - 1s - loss: 91.4902 - mse: 51.0206 - mae: 5.6643 - fp_mae: 2.9707 - val_loss: 200.5899 - val_mse: 160.2554 - val_mae: 9.7697 - val_fp_mae: 6.4736\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 8.19112\n",
      "Epoch 41/100\n",
      "64/64 - 1s - loss: 91.9521 - mse: 51.7289 - mae: 5.6386 - fp_mae: 3.0321 - val_loss: 173.8137 - val_mse: 133.7742 - val_mae: 8.8251 - val_fp_mae: 5.5926\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 8.19112\n",
      "Epoch 42/100\n",
      "64/64 - 1s - loss: 91.3798 - mse: 51.2432 - mae: 5.5679 - fp_mae: 2.8244 - val_loss: 239.4056 - val_mse: 199.3821 - val_mae: 11.0361 - val_fp_mae: 7.4208\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 8.19112\n",
      "Epoch 43/100\n",
      "64/64 - 1s - loss: 90.9123 - mse: 51.1066 - mae: 5.6894 - fp_mae: 3.0710 - val_loss: 238.5759 - val_mse: 198.8284 - val_mae: 10.9322 - val_fp_mae: 9.3762\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 8.19112\n",
      "Epoch 44/100\n",
      "64/64 - 1s - loss: 90.4557 - mse: 50.6320 - mae: 5.5827 - fp_mae: 2.9618 - val_loss: 177.6799 - val_mse: 137.8159 - val_mae: 9.1179 - val_fp_mae: 5.4149\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 8.19112\n",
      "Epoch 45/100\n",
      "64/64 - 1s - loss: 87.1666 - mse: 47.5310 - mae: 5.4646 - fp_mae: 2.8750 - val_loss: 182.6512 - val_mse: 143.2570 - val_mae: 9.1472 - val_fp_mae: 6.6398\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 8.19112\n",
      "Epoch 46/100\n",
      "64/64 - 1s - loss: 86.3409 - mse: 47.1662 - mae: 5.4072 - fp_mae: 2.8760 - val_loss: 181.6925 - val_mse: 142.5330 - val_mae: 9.1259 - val_fp_mae: 5.9415\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 8.19112\n",
      "Epoch 47/100\n",
      "64/64 - 1s - loss: 84.8591 - mse: 45.8100 - mae: 5.3501 - fp_mae: 2.8228 - val_loss: 195.1666 - val_mse: 156.1998 - val_mae: 9.5734 - val_fp_mae: 5.8142\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 8.19112\n",
      "Epoch 48/100\n",
      "64/64 - 1s - loss: 85.2797 - mse: 46.3626 - mae: 5.3667 - fp_mae: 2.8045 - val_loss: 248.7010 - val_mse: 209.9394 - val_mae: 11.4490 - val_fp_mae: 8.6437\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 8.19112\n",
      "Epoch 49/100\n",
      "64/64 - 1s - loss: 86.2413 - mse: 47.6648 - mae: 5.4602 - fp_mae: 2.9090 - val_loss: 187.4741 - val_mse: 149.1440 - val_mae: 9.3766 - val_fp_mae: 6.7655\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 8.19112\n",
      "Epoch 50/100\n",
      "64/64 - 1s - loss: 85.6188 - mse: 47.2830 - mae: 5.3952 - fp_mae: 2.8780 - val_loss: 197.2624 - val_mse: 158.8987 - val_mae: 9.6931 - val_fp_mae: 7.2525\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 8.19112\n",
      "Epoch 51/100\n",
      "64/64 - 1s - loss: 84.7942 - mse: 46.5724 - mae: 5.4269 - fp_mae: 2.9130 - val_loss: 200.0874 - val_mse: 161.7350 - val_mae: 9.8585 - val_fp_mae: 6.8064\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 8.19112\n",
      "Epoch 52/100\n",
      "64/64 - 1s - loss: 84.6976 - mse: 46.4507 - mae: 5.3766 - fp_mae: 2.8773 - val_loss: 200.9737 - val_mse: 162.7012 - val_mae: 9.9776 - val_fp_mae: 5.7311\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 8.19112\n",
      "Epoch 53/100\n",
      "64/64 - 1s - loss: 82.2890 - mse: 44.1373 - mae: 5.2387 - fp_mae: 2.7729 - val_loss: 205.7540 - val_mse: 167.6509 - val_mae: 10.0053 - val_fp_mae: 7.1267\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 8.19112\n",
      "Epoch 54/100\n",
      "64/64 - 1s - loss: 85.3886 - mse: 47.3249 - mae: 5.4646 - fp_mae: 2.8238 - val_loss: 179.1851 - val_mse: 141.1757 - val_mae: 9.1596 - val_fp_mae: 5.5393\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 8.19112\n",
      "Epoch 55/100\n",
      "64/64 - 1s - loss: 83.5165 - mse: 45.5085 - mae: 5.3057 - fp_mae: 2.7897 - val_loss: 197.2835 - val_mse: 159.3057 - val_mae: 9.6805 - val_fp_mae: 6.5600\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 8.19112\n",
      "Epoch 56/100\n",
      "64/64 - 1s - loss: 80.7006 - mse: 42.8774 - mae: 5.1980 - fp_mae: 2.7171 - val_loss: 177.6877 - val_mse: 140.1359 - val_mae: 8.9378 - val_fp_mae: 5.3475\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 8.19112\n",
      "Epoch 57/100\n",
      "64/64 - 1s - loss: 81.9658 - mse: 44.5148 - mae: 5.2747 - fp_mae: 2.8211 - val_loss: 194.1099 - val_mse: 156.5836 - val_mae: 9.6264 - val_fp_mae: 6.7198\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 8.19112\n",
      "Epoch 58/100\n",
      "64/64 - 1s - loss: 81.7835 - mse: 44.4381 - mae: 5.2893 - fp_mae: 2.8382 - val_loss: 178.5630 - val_mse: 141.3644 - val_mae: 9.3146 - val_fp_mae: 5.0217\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 8.19112\n",
      "Epoch 59/100\n",
      "64/64 - 1s - loss: 79.5077 - mse: 42.4393 - mae: 5.1312 - fp_mae: 2.7083 - val_loss: 168.8033 - val_mse: 131.8553 - val_mae: 8.8784 - val_fp_mae: 4.3533\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 8.19112\n",
      "Epoch 60/100\n",
      "64/64 - 1s - loss: 81.5562 - mse: 44.7193 - mae: 5.3072 - fp_mae: 2.8469 - val_loss: 179.2960 - val_mse: 142.3635 - val_mae: 8.9851 - val_fp_mae: 5.9621\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 8.19112\n",
      "Epoch 61/100\n",
      "64/64 - 1s - loss: 81.2903 - mse: 44.2520 - mae: 5.2736 - fp_mae: 2.6897 - val_loss: 197.6390 - val_mse: 160.8160 - val_mae: 9.5725 - val_fp_mae: 6.9258\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 8.19112\n",
      "Epoch 62/100\n",
      "64/64 - 1s - loss: 79.6492 - mse: 42.9421 - mae: 5.2418 - fp_mae: 2.7372 - val_loss: 208.3420 - val_mse: 171.8123 - val_mae: 10.0249 - val_fp_mae: 7.6444\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 8.19112\n",
      "Epoch 63/100\n",
      "64/64 - 1s - loss: 81.0093 - mse: 44.6438 - mae: 5.2867 - fp_mae: 2.8349 - val_loss: 200.3532 - val_mse: 164.0764 - val_mae: 9.8745 - val_fp_mae: 7.0598\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 8.19112\n",
      "Epoch 64/100\n",
      "64/64 - 1s - loss: 81.5050 - mse: 45.0518 - mae: 5.2229 - fp_mae: 2.7539 - val_loss: 181.1931 - val_mse: 144.7709 - val_mae: 9.3161 - val_fp_mae: 5.7082\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 8.19112\n",
      "Epoch 65/100\n",
      "64/64 - 1s - loss: 81.3832 - mse: 45.0252 - mae: 5.2417 - fp_mae: 2.7762 - val_loss: 174.4697 - val_mse: 138.2440 - val_mae: 9.0837 - val_fp_mae: 5.6258\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 8.19112\n",
      "Epoch 66/100\n",
      "64/64 - 1s - loss: 78.4692 - mse: 42.4236 - mae: 5.1297 - fp_mae: 2.7419 - val_loss: 196.6888 - val_mse: 160.6370 - val_mae: 9.7496 - val_fp_mae: 7.2345\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 8.19112\n",
      "Epoch 67/100\n",
      "64/64 - 1s - loss: 76.7346 - mse: 40.7771 - mae: 5.0656 - fp_mae: 2.7117 - val_loss: 168.7338 - val_mse: 132.7825 - val_mae: 8.8854 - val_fp_mae: 4.3911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00067: val_mae did not improve from 8.19112\n",
      "Epoch 68/100\n",
      "64/64 - 1s - loss: 77.0794 - mse: 41.2867 - mae: 5.1092 - fp_mae: 2.6713 - val_loss: 171.3794 - val_mse: 135.7800 - val_mae: 9.0680 - val_fp_mae: 5.1162\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 8.19112\n",
      "Epoch 69/100\n",
      "64/64 - 1s - loss: 78.4331 - mse: 42.9252 - mae: 5.1728 - fp_mae: 2.6848 - val_loss: 176.2038 - val_mse: 140.8073 - val_mae: 9.1961 - val_fp_mae: 4.9860\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 8.19112\n",
      "Epoch 70/100\n",
      "64/64 - 1s - loss: 77.9965 - mse: 42.5888 - mae: 5.1569 - fp_mae: 2.7143 - val_loss: 180.5098 - val_mse: 145.2501 - val_mae: 9.2310 - val_fp_mae: 6.4179\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 8.19112\n",
      "Epoch 71/100\n",
      "64/64 - 1s - loss: 77.7119 - mse: 42.3702 - mae: 5.1415 - fp_mae: 2.7368 - val_loss: 181.3009 - val_mse: 146.0421 - val_mae: 9.2865 - val_fp_mae: 6.4641\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 8.19112\n",
      "Epoch 72/100\n",
      "64/64 - 1s - loss: 76.3224 - mse: 41.1374 - mae: 5.0581 - fp_mae: 2.7008 - val_loss: 226.2959 - val_mse: 191.1072 - val_mae: 10.6719 - val_fp_mae: 8.1851\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 8.19112\n",
      "Epoch 73/100\n",
      "64/64 - 1s - loss: 76.9856 - mse: 42.0590 - mae: 5.1337 - fp_mae: 2.7112 - val_loss: 215.4127 - val_mse: 180.4964 - val_mae: 10.3722 - val_fp_mae: 7.6838\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 8.19112\n",
      "Epoch 74/100\n",
      "64/64 - 1s - loss: 76.3550 - mse: 41.5227 - mae: 5.0497 - fp_mae: 2.6486 - val_loss: 188.2461 - val_mse: 153.4167 - val_mae: 9.4984 - val_fp_mae: 6.7623\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 8.19112\n",
      "Epoch 75/100\n",
      "64/64 - 1s - loss: 75.6043 - mse: 40.9564 - mae: 5.0557 - fp_mae: 2.6688 - val_loss: 178.6019 - val_mse: 143.9768 - val_mae: 9.2079 - val_fp_mae: 5.6293\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 8.19112\n",
      "Epoch 76/100\n",
      "64/64 - 1s - loss: 75.5665 - mse: 41.1075 - mae: 5.0282 - fp_mae: 2.6825 - val_loss: 175.6232 - val_mse: 141.1586 - val_mae: 9.2663 - val_fp_mae: 4.9666\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 8.19112\n",
      "Epoch 77/100\n",
      "64/64 - 1s - loss: 77.4212 - mse: 42.8563 - mae: 5.0881 - fp_mae: 2.6628 - val_loss: 189.2128 - val_mse: 154.4970 - val_mae: 9.4592 - val_fp_mae: 6.7344\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 8.19112\n",
      "Epoch 78/100\n",
      "64/64 - 1s - loss: 74.7495 - mse: 40.0491 - mae: 4.9516 - fp_mae: 2.4733 - val_loss: 201.8751 - val_mse: 167.4073 - val_mae: 9.8664 - val_fp_mae: 7.1693\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 8.19112\n",
      "Epoch 79/100\n",
      "64/64 - 1s - loss: 76.4101 - mse: 42.1636 - mae: 5.0791 - fp_mae: 2.7405 - val_loss: 180.3226 - val_mse: 146.0939 - val_mae: 9.2498 - val_fp_mae: 5.7356\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 8.19112\n",
      "Epoch 80/100\n",
      "64/64 - 1s - loss: 76.4379 - mse: 41.9707 - mae: 5.1428 - fp_mae: 2.7011 - val_loss: 177.6242 - val_mse: 143.1204 - val_mae: 9.0279 - val_fp_mae: 5.7239\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 8.19112\n",
      "Epoch 81/100\n",
      "64/64 - 1s - loss: 76.3553 - mse: 41.8600 - mae: 5.0945 - fp_mae: 2.6792 - val_loss: 185.5739 - val_mse: 151.2486 - val_mae: 9.3792 - val_fp_mae: 6.3939\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 8.19112\n",
      "Epoch 82/100\n",
      "64/64 - 1s - loss: 75.0248 - mse: 40.7987 - mae: 5.0724 - fp_mae: 2.7030 - val_loss: 176.3749 - val_mse: 142.1892 - val_mae: 9.1484 - val_fp_mae: 5.9004\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 8.19112\n",
      "Epoch 83/100\n",
      "64/64 - 1s - loss: 73.2797 - mse: 39.2118 - mae: 4.9321 - fp_mae: 2.5836 - val_loss: 190.3783 - val_mse: 156.3855 - val_mae: 9.6326 - val_fp_mae: 6.3032\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 8.19112\n",
      "Epoch 84/100\n",
      "64/64 - 1s - loss: 72.1659 - mse: 38.3529 - mae: 4.8356 - fp_mae: 2.5578 - val_loss: 181.0712 - val_mse: 147.3464 - val_mae: 9.4723 - val_fp_mae: 4.3335\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 8.19112\n",
      "Epoch 85/100\n",
      "64/64 - 1s - loss: 75.5032 - mse: 41.8674 - mae: 5.0719 - fp_mae: 2.7028 - val_loss: 204.4078 - val_mse: 170.6757 - val_mae: 9.8832 - val_fp_mae: 6.9923\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 8.19112\n",
      "Epoch 86/100\n",
      "64/64 - 1s - loss: 77.7692 - mse: 43.8534 - mae: 5.1745 - fp_mae: 2.6447 - val_loss: 190.5149 - val_mse: 156.6085 - val_mae: 9.3700 - val_fp_mae: 6.3501\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 8.19112\n",
      "Epoch 87/100\n",
      "64/64 - 1s - loss: 75.4553 - mse: 41.5351 - mae: 5.1193 - fp_mae: 2.7107 - val_loss: 197.1727 - val_mse: 163.3262 - val_mae: 9.8074 - val_fp_mae: 6.7320\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 8.19112\n",
      "Epoch 88/100\n",
      "64/64 - 1s - loss: 73.1191 - mse: 39.4223 - mae: 4.9571 - fp_mae: 2.6794 - val_loss: 271.7383 - val_mse: 237.9998 - val_mae: 11.9686 - val_fp_mae: 9.4213\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 8.19112\n",
      "Epoch 89/100\n",
      "64/64 - 1s - loss: 72.1479 - mse: 38.5407 - mae: 4.8664 - fp_mae: 2.5529 - val_loss: 204.4760 - val_mse: 171.0188 - val_mae: 9.8042 - val_fp_mae: 7.2707\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 8.19112\n",
      "Epoch 90/100\n",
      "64/64 - 1s - loss: 73.1064 - mse: 39.6863 - mae: 4.9784 - fp_mae: 2.5497 - val_loss: 181.6510 - val_mse: 148.3624 - val_mae: 9.1615 - val_fp_mae: 6.2537\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 8.19112\n",
      "Epoch 91/100\n",
      "64/64 - 1s - loss: 73.6843 - mse: 40.5283 - mae: 4.9832 - fp_mae: 2.6656 - val_loss: 165.8177 - val_mse: 132.4786 - val_mae: 8.8859 - val_fp_mae: 4.9028\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 8.19112\n",
      "Epoch 92/100\n",
      "64/64 - 1s - loss: 74.9867 - mse: 41.5281 - mae: 5.0702 - fp_mae: 2.6534 - val_loss: 169.8126 - val_mse: 136.4042 - val_mae: 8.9544 - val_fp_mae: 5.9089\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 8.19112\n",
      "Epoch 93/100\n",
      "64/64 - 2s - loss: 72.1875 - mse: 38.8192 - mae: 4.8751 - fp_mae: 2.4954 - val_loss: 168.3948 - val_mse: 135.1493 - val_mae: 9.0700 - val_fp_mae: 4.5768\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 8.19112\n",
      "Epoch 94/100\n",
      "64/64 - 1s - loss: 73.5829 - mse: 40.4719 - mae: 4.9984 - fp_mae: 2.6466 - val_loss: 187.3278 - val_mse: 154.2988 - val_mae: 9.5270 - val_fp_mae: 6.8389\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 8.19112\n",
      "Epoch 95/100\n",
      "64/64 - 1s - loss: 72.4298 - mse: 39.4929 - mae: 4.9444 - fp_mae: 2.6387 - val_loss: 167.2739 - val_mse: 134.2774 - val_mae: 9.0404 - val_fp_mae: 4.8924\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 8.19112\n",
      "Epoch 96/100\n",
      "64/64 - 1s - loss: 72.6030 - mse: 39.6035 - mae: 4.9810 - fp_mae: 2.6116 - val_loss: 217.4010 - val_mse: 184.4472 - val_mae: 10.3105 - val_fp_mae: 7.7877\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 8.19112\n",
      "Epoch 97/100\n",
      "64/64 - 1s - loss: 70.2925 - mse: 37.6298 - mae: 4.8436 - fp_mae: 2.5773 - val_loss: 226.3654 - val_mse: 193.8513 - val_mae: 10.7923 - val_fp_mae: 8.6260\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 8.19112\n",
      "Epoch 98/100\n",
      "64/64 - 1s - loss: 72.2041 - mse: 39.5587 - mae: 4.9397 - fp_mae: 2.5749 - val_loss: 178.8484 - val_mse: 146.1474 - val_mae: 9.2511 - val_fp_mae: 6.6372\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 8.19112\n",
      "Epoch 99/100\n",
      "64/64 - 1s - loss: 73.0162 - mse: 40.2143 - mae: 4.9912 - fp_mae: 2.5670 - val_loss: 181.8113 - val_mse: 149.1943 - val_mae: 9.4494 - val_fp_mae: 6.2567\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 8.19112\n",
      "Epoch 100/100\n",
      "64/64 - 1s - loss: 71.8548 - mse: 39.2539 - mae: 4.9194 - fp_mae: 2.5555 - val_loss: 170.9469 - val_mse: 138.5010 - val_mae: 9.0842 - val_fp_mae: 5.7952\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 8.19112\n",
      "\n",
      "Lambda: 1 , Time: 0:02:22\n",
      "Train Error(all epochs): 4.835556507110596 \n",
      " [25.865, 24.682, 23.179, 21.682, 20.139, 18.627, 17.118, 15.791, 14.406, 13.081, 11.845, 10.59, 9.658, 8.823, 8.21, 7.816, 7.536, 7.294, 7.173, 7.032, 6.846, 6.73, 6.597, 6.578, 6.497, 6.42, 6.431, 6.257, 6.239, 6.175, 5.974, 5.982, 5.912, 5.967, 5.705, 5.764, 5.816, 5.72, 5.817, 5.664, 5.639, 5.568, 5.689, 5.583, 5.465, 5.407, 5.35, 5.367, 5.46, 5.395, 5.427, 5.377, 5.239, 5.465, 5.306, 5.198, 5.275, 5.289, 5.131, 5.307, 5.274, 5.242, 5.287, 5.223, 5.242, 5.13, 5.066, 5.109, 5.173, 5.157, 5.142, 5.058, 5.134, 5.05, 5.056, 5.028, 5.088, 4.952, 5.079, 5.143, 5.095, 5.072, 4.932, 4.836, 5.072, 5.175, 5.119, 4.957, 4.866, 4.978, 4.983, 5.07, 4.875, 4.998, 4.944, 4.981, 4.844, 4.94, 4.991, 4.919]\n",
      "Train FP Error(all epochs): 2.4733078479766846 \n",
      " [24.251, 23.504, 22.435, 21.219, 19.851, 18.415, 16.937, 15.513, 14.034, 12.538, 11.057, 9.63, 8.369, 7.21, 6.285, 5.58, 5.008, 4.578, 4.291, 4.034, 3.883, 3.731, 3.565, 3.525, 3.47, 3.396, 3.421, 3.312, 3.289, 3.3, 3.198, 3.182, 3.108, 3.17, 3.06, 3.05, 3.052, 3.042, 3.083, 2.971, 3.032, 2.824, 3.071, 2.962, 2.875, 2.876, 2.823, 2.805, 2.909, 2.878, 2.913, 2.877, 2.773, 2.824, 2.79, 2.717, 2.821, 2.838, 2.708, 2.847, 2.69, 2.737, 2.835, 2.754, 2.776, 2.742, 2.712, 2.671, 2.685, 2.714, 2.737, 2.701, 2.711, 2.649, 2.669, 2.682, 2.663, 2.473, 2.741, 2.701, 2.679, 2.703, 2.584, 2.558, 2.703, 2.645, 2.711, 2.679, 2.553, 2.55, 2.666, 2.653, 2.495, 2.647, 2.639, 2.612, 2.577, 2.575, 2.567, 2.556]\n",
      "Val Error(all epochs): 8.191118240356445 \n",
      " [28.278, 27.655, 22.784, 21.038, 17.99, 15.399, 14.916, 19.224, 16.328, 22.06, 18.392, 11.132, 12.993, 10.598, 8.703, 8.706, 9.082, 8.191, 8.991, 11.602, 9.447, 8.684, 9.349, 8.957, 9.093, 18.338, 12.275, 12.535, 8.932, 10.149, 8.637, 9.155, 9.816, 8.746, 10.802, 9.963, 9.179, 8.613, 10.366, 9.77, 8.825, 11.036, 10.932, 9.118, 9.147, 9.126, 9.573, 11.449, 9.377, 9.693, 9.859, 9.978, 10.005, 9.16, 9.681, 8.938, 9.626, 9.315, 8.878, 8.985, 9.573, 10.025, 9.875, 9.316, 9.084, 9.75, 8.885, 9.068, 9.196, 9.231, 9.287, 10.672, 10.372, 9.498, 9.208, 9.266, 9.459, 9.866, 9.25, 9.028, 9.379, 9.148, 9.633, 9.472, 9.883, 9.37, 9.807, 11.969, 9.804, 9.162, 8.886, 8.954, 9.07, 9.527, 9.04, 10.311, 10.792, 9.251, 9.449, 9.084]\n",
      "Val FP Error(all epochs): 3.5084633827209473 \n",
      " [27.19, 26.773, 21.412, 20.366, 17.031, 14.084, 14.659, 18.99, 16.121, 21.816, 17.789, 10.172, 11.76, 9.516, 6.776, 6.374, 7.11, 5.083, 4.748, 10.372, 7.518, 6.011, 3.968, 3.508, 5.979, 16.991, 10.645, 9.995, 5.848, 4.504, 4.802, 4.171, 6.444, 5.105, 8.579, 7.93, 5.34, 5.312, 7.817, 6.474, 5.593, 7.421, 9.376, 5.415, 6.64, 5.942, 5.814, 8.644, 6.765, 7.253, 6.806, 5.731, 7.127, 5.539, 6.56, 5.347, 6.72, 5.022, 4.353, 5.962, 6.926, 7.644, 7.06, 5.708, 5.626, 7.234, 4.391, 5.116, 4.986, 6.418, 6.464, 8.185, 7.684, 6.762, 5.629, 4.967, 6.734, 7.169, 5.736, 5.724, 6.394, 5.9, 6.303, 4.334, 6.992, 6.35, 6.732, 9.421, 7.271, 6.254, 4.903, 5.909, 4.577, 6.839, 4.892, 7.788, 8.626, 6.637, 6.257, 5.795]\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 - 2s - loss: 1492.8613 - mse: 844.5601 - mae: 25.8718 - fp_mae: 24.2563 - val_loss: 1289.4030 - val_mse: 927.7659 - val_mae: 26.8738 - val_fp_mae: 25.4197\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 26.87376, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_3.h5\n",
      "Epoch 2/100\n",
      "64/64 - 1s - loss: 1050.9506 - mse: 799.1761 - mae: 24.9316 - fp_mae: 23.6433 - val_loss: 1105.0962 - val_mse: 930.7974 - val_mae: 26.9024 - val_fp_mae: 25.4855\n",
      "\n",
      "Epoch 00002: val_mae did not improve from 26.87376\n",
      "Epoch 3/100\n",
      "64/64 - 1s - loss: 862.9659 - mse: 726.3430 - mae: 23.5317 - fp_mae: 22.6540 - val_loss: 996.6366 - val_mse: 887.0074 - val_mae: 26.2167 - val_fp_mae: 24.7793\n",
      "\n",
      "Epoch 00003: val_mae improved from 26.87376 to 26.21667, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_3.h5\n",
      "Epoch 4/100\n",
      "64/64 - 1s - loss: 734.0748 - mse: 639.4714 - mae: 21.9451 - fp_mae: 21.4104 - val_loss: 788.4125 - val_mse: 704.6352 - val_mae: 23.3530 - val_fp_mae: 21.3330\n",
      "\n",
      "Epoch 00004: val_mae improved from 26.21667 to 23.35302, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_3.h5\n",
      "Epoch 5/100\n",
      "64/64 - 1s - loss: 632.6329 - mse: 555.6083 - mae: 20.3634 - fp_mae: 20.0508 - val_loss: 709.9019 - val_mse: 639.1088 - val_mae: 22.1419 - val_fp_mae: 20.0236\n",
      "\n",
      "Epoch 00005: val_mae improved from 23.35302 to 22.14188, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_3.h5\n",
      "Epoch 6/100\n",
      "64/64 - 1s - loss: 557.8514 - mse: 485.8417 - mae: 18.8816 - fp_mae: 18.6452 - val_loss: 551.0594 - val_mse: 477.6623 - val_mae: 18.0641 - val_fp_mae: 17.5659\n",
      "\n",
      "Epoch 00006: val_mae improved from 22.14188 to 18.06410, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_3.h5\n",
      "Epoch 7/100\n",
      "64/64 - 1s - loss: 500.7137 - mse: 426.7980 - mae: 17.4678 - fp_mae: 17.2619 - val_loss: 537.5486 - val_mse: 460.7811 - val_mae: 18.1908 - val_fp_mae: 16.2457\n",
      "\n",
      "Epoch 00007: val_mae did not improve from 18.06410\n",
      "Epoch 8/100\n",
      "64/64 - 1s - loss: 455.0600 - mse: 377.5016 - mae: 16.1730 - fp_mae: 15.9112 - val_loss: 495.7601 - val_mse: 418.1518 - val_mae: 16.6835 - val_fp_mae: 15.8569\n",
      "\n",
      "Epoch 00008: val_mae improved from 18.06410 to 16.68350, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_3.h5\n",
      "Epoch 9/100\n",
      "64/64 - 1s - loss: 419.1316 - mse: 336.8851 - mae: 15.0552 - fp_mae: 14.6750 - val_loss: 640.1063 - val_mse: 553.8613 - val_mae: 20.5402 - val_fp_mae: 20.4929\n",
      "\n",
      "Epoch 00009: val_mae did not improve from 16.68350\n",
      "Epoch 10/100\n",
      "64/64 - 1s - loss: 389.0936 - mse: 301.4277 - mae: 14.0040 - fp_mae: 13.5024 - val_loss: 796.9625 - val_mse: 706.3430 - val_mae: 23.0483 - val_fp_mae: 22.9508\n",
      "\n",
      "Epoch 00010: val_mae did not improve from 16.68350\n",
      "Epoch 11/100\n",
      "64/64 - 1s - loss: 361.8719 - mse: 268.3485 - mae: 12.9991 - fp_mae: 12.3363 - val_loss: 400.8852 - val_mse: 305.9411 - val_mae: 14.1708 - val_fp_mae: 11.9569\n",
      "\n",
      "Epoch 00011: val_mae improved from 16.68350 to 14.17076, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_3.h5\n",
      "Epoch 12/100\n",
      "64/64 - 1s - loss: 335.1153 - mse: 237.9676 - mae: 12.0947 - fp_mae: 11.2391 - val_loss: 373.0259 - val_mse: 273.1428 - val_mae: 13.1213 - val_fp_mae: 11.1568\n",
      "\n",
      "Epoch 00012: val_mae improved from 14.17076 to 13.12126, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_3.h5\n",
      "Epoch 13/100\n",
      "64/64 - 1s - loss: 318.4749 - mse: 216.7296 - mae: 11.4112 - fp_mae: 10.3204 - val_loss: 923.4446 - val_mse: 818.2443 - val_mae: 22.2540 - val_fp_mae: 21.7936\n",
      "\n",
      "Epoch 00013: val_mae did not improve from 13.12126\n",
      "Epoch 14/100\n",
      "64/64 - 1s - loss: 305.4712 - mse: 198.2190 - mae: 10.7889 - fp_mae: 9.4591 - val_loss: 396.7122 - val_mse: 286.6428 - val_mae: 13.5653 - val_fp_mae: 12.8768\n",
      "\n",
      "Epoch 00014: val_mae did not improve from 13.12126\n",
      "Epoch 15/100\n",
      "64/64 - 1s - loss: 291.7329 - mse: 182.7807 - mae: 10.3312 - fp_mae: 8.7791 - val_loss: 317.8255 - val_mse: 209.1361 - val_mae: 11.1578 - val_fp_mae: 9.0324\n",
      "\n",
      "Epoch 00015: val_mae improved from 13.12126 to 11.15785, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_3.h5\n",
      "Epoch 16/100\n",
      "64/64 - 1s - loss: 282.0974 - mse: 171.6670 - mae: 9.9233 - fp_mae: 8.1511 - val_loss: 320.7675 - val_mse: 209.3929 - val_mae: 11.0794 - val_fp_mae: 10.1478\n",
      "\n",
      "Epoch 00016: val_mae improved from 11.15785 to 11.07942, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_3.h5\n",
      "Epoch 17/100\n",
      "64/64 - 1s - loss: 272.9273 - mse: 163.3652 - mae: 9.6766 - fp_mae: 7.7442 - val_loss: 301.1079 - val_mse: 192.2804 - val_mae: 10.6882 - val_fp_mae: 7.8522\n",
      "\n",
      "Epoch 00017: val_mae improved from 11.07942 to 10.68816, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_3.h5\n",
      "Epoch 18/100\n",
      "64/64 - 1s - loss: 263.3355 - mse: 155.9904 - mae: 9.4717 - fp_mae: 7.3451 - val_loss: 276.8568 - val_mse: 170.4313 - val_mae: 9.6933 - val_fp_mae: 7.8523\n",
      "\n",
      "Epoch 00018: val_mae improved from 10.68816 to 9.69328, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_3.h5\n",
      "Epoch 19/100\n",
      "64/64 - 1s - loss: 259.0005 - mse: 153.3779 - mae: 9.3588 - fp_mae: 7.1329 - val_loss: 561.8054 - val_mse: 457.2203 - val_mae: 16.4230 - val_fp_mae: 15.4824\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 9.69328\n",
      "Epoch 20/100\n",
      "64/64 - 1s - loss: 254.3602 - mse: 151.2877 - mae: 9.3427 - fp_mae: 6.9543 - val_loss: 278.6950 - val_mse: 176.3370 - val_mae: 10.0375 - val_fp_mae: 8.6579\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 9.69328\n",
      "Epoch 21/100\n",
      "64/64 - 1s - loss: 250.0036 - mse: 147.0498 - mae: 9.2382 - fp_mae: 6.7424 - val_loss: 652.0583 - val_mse: 550.1776 - val_mae: 17.0434 - val_fp_mae: 15.3271\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 9.69328\n",
      "Epoch 22/100\n",
      "64/64 - 1s - loss: 244.4912 - mse: 145.8984 - mae: 9.1704 - fp_mae: 6.6679 - val_loss: 407.8223 - val_mse: 310.0767 - val_mae: 13.5619 - val_fp_mae: 12.1020\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 9.69328\n",
      "Epoch 23/100\n",
      "64/64 - 2s - loss: 240.6247 - mse: 143.5382 - mae: 9.1527 - fp_mae: 6.5709 - val_loss: 1009.8388 - val_mse: 912.7709 - val_mae: 21.0366 - val_fp_mae: 19.1223\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 9.69328\n",
      "Epoch 24/100\n",
      "64/64 - 1s - loss: 237.5120 - mse: 142.4709 - mae: 9.0794 - fp_mae: 6.4266 - val_loss: 337.4338 - val_mse: 245.7766 - val_mae: 12.1787 - val_fp_mae: 5.8641\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 9.69328\n",
      "Epoch 25/100\n",
      "64/64 - 1s - loss: 235.6429 - mse: 143.8380 - mae: 9.1074 - fp_mae: 6.4081 - val_loss: 371.8494 - val_mse: 281.3794 - val_mae: 13.1338 - val_fp_mae: 11.7376\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 9.69328\n",
      "Epoch 26/100\n",
      "64/64 - 1s - loss: 227.6187 - mse: 139.7430 - mae: 9.0062 - fp_mae: 6.2696 - val_loss: 239.7784 - val_mse: 152.8533 - val_mae: 9.2350 - val_fp_mae: 7.3822\n",
      "\n",
      "Epoch 00026: val_mae improved from 9.69328 to 9.23504, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_3.h5\n",
      "Epoch 27/100\n",
      "64/64 - 1s - loss: 226.8786 - mse: 140.6993 - mae: 9.0394 - fp_mae: 6.2475 - val_loss: 251.0824 - val_mse: 165.3305 - val_mae: 9.6211 - val_fp_mae: 7.9776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00027: val_mae did not improve from 9.23504\n",
      "Epoch 28/100\n",
      "64/64 - 1s - loss: 224.3574 - mse: 140.3532 - mae: 9.0187 - fp_mae: 6.2078 - val_loss: 293.3761 - val_mse: 210.8184 - val_mae: 11.3594 - val_fp_mae: 5.5182\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 9.23504\n",
      "Epoch 29/100\n",
      "64/64 - 1s - loss: 222.1231 - mse: 139.3528 - mae: 8.9889 - fp_mae: 6.1249 - val_loss: 219.3382 - val_mse: 134.6116 - val_mae: 8.7097 - val_fp_mae: 6.0647\n",
      "\n",
      "Epoch 00029: val_mae improved from 9.23504 to 8.70972, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_3.h5\n",
      "Epoch 30/100\n",
      "64/64 - 1s - loss: 220.7383 - mse: 137.6474 - mae: 8.9675 - fp_mae: 6.0418 - val_loss: 231.1714 - val_mse: 149.7855 - val_mae: 9.2898 - val_fp_mae: 7.0115\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 8.70972\n",
      "Epoch 31/100\n",
      "64/64 - 1s - loss: 219.4445 - mse: 139.5948 - mae: 8.9767 - fp_mae: 6.0015 - val_loss: 496.0793 - val_mse: 418.1929 - val_mae: 15.2697 - val_fp_mae: 13.4910\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 8.70972\n",
      "Epoch 32/100\n",
      "64/64 - 1s - loss: 217.3859 - mse: 138.1021 - mae: 8.9798 - fp_mae: 5.9838 - val_loss: 278.7999 - val_mse: 199.5761 - val_mae: 11.0588 - val_fp_mae: 5.3291\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 8.70972\n",
      "Epoch 33/100\n",
      "64/64 - 1s - loss: 222.7651 - mse: 144.4245 - mae: 9.0511 - fp_mae: 5.9246 - val_loss: 444.9618 - val_mse: 353.5271 - val_mae: 14.1660 - val_fp_mae: 5.1565\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 8.70972\n",
      "Epoch 34/100\n",
      "64/64 - 1s - loss: 265.9861 - mse: 146.4361 - mae: 9.1621 - fp_mae: 6.0302 - val_loss: 347.5430 - val_mse: 228.9152 - val_mae: 11.7533 - val_fp_mae: 9.7073\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 8.70972\n",
      "Epoch 35/100\n",
      "64/64 - 1s - loss: 249.5159 - mse: 136.4265 - mae: 8.9433 - fp_mae: 5.7655 - val_loss: 245.6458 - val_mse: 138.9431 - val_mae: 8.8074 - val_fp_mae: 6.2390\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 8.70972\n",
      "Epoch 36/100\n",
      "64/64 - 1s - loss: 238.8127 - mse: 136.5341 - mae: 8.9243 - fp_mae: 5.7556 - val_loss: 228.1625 - val_mse: 129.0537 - val_mae: 8.7057 - val_fp_mae: 5.3825\n",
      "\n",
      "Epoch 00036: val_mae improved from 8.70972 to 8.70569, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_3.h5\n",
      "Epoch 37/100\n",
      "64/64 - 1s - loss: 229.9572 - mse: 136.2849 - mae: 8.9274 - fp_mae: 5.8192 - val_loss: 230.9867 - val_mse: 140.7439 - val_mae: 9.1005 - val_fp_mae: 5.8056\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 8.70569\n",
      "Epoch 38/100\n",
      "64/64 - 1s - loss: 224.5268 - mse: 137.1102 - mae: 8.9429 - fp_mae: 5.7936 - val_loss: 358.8241 - val_mse: 273.8888 - val_mae: 12.8029 - val_fp_mae: 5.4386\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 8.70569\n",
      "Epoch 39/100\n",
      "64/64 - 1s - loss: 217.0515 - mse: 134.4144 - mae: 8.8702 - fp_mae: 5.6836 - val_loss: 361.3242 - val_mse: 280.2953 - val_mae: 12.8894 - val_fp_mae: 4.7610\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 8.70569\n",
      "Epoch 40/100\n",
      "64/64 - 1s - loss: 213.4625 - mse: 135.9671 - mae: 8.9426 - fp_mae: 5.7644 - val_loss: 310.4701 - val_mse: 234.0420 - val_mae: 11.9353 - val_fp_mae: 5.0864\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 8.70569\n",
      "Epoch 41/100\n",
      "64/64 - 1s - loss: 208.8255 - mse: 133.8500 - mae: 8.8625 - fp_mae: 5.5756 - val_loss: 215.9808 - val_mse: 143.7249 - val_mae: 9.3997 - val_fp_mae: 5.2390\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 8.70569\n",
      "Epoch 42/100\n",
      "64/64 - 1s - loss: 205.7693 - mse: 134.1235 - mae: 8.8724 - fp_mae: 5.5957 - val_loss: 203.0318 - val_mse: 132.5153 - val_mae: 8.6639 - val_fp_mae: 5.8303\n",
      "\n",
      "Epoch 00042: val_mae improved from 8.70569 to 8.66386, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_3.h5\n",
      "Epoch 43/100\n",
      "64/64 - 1s - loss: 201.3917 - mse: 132.4846 - mae: 8.8177 - fp_mae: 5.4933 - val_loss: 202.4425 - val_mse: 135.7111 - val_mae: 8.6903 - val_fp_mae: 6.1231\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 8.66386\n",
      "Epoch 44/100\n",
      "64/64 - 1s - loss: 200.1815 - mse: 134.4967 - mae: 8.9021 - fp_mae: 5.5855 - val_loss: 319.5739 - val_mse: 253.0201 - val_mae: 12.3334 - val_fp_mae: 10.2456\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 8.66386\n",
      "Epoch 45/100\n",
      "64/64 - 1s - loss: 197.2495 - mse: 132.1084 - mae: 8.8017 - fp_mae: 5.4605 - val_loss: 224.0117 - val_mse: 157.7883 - val_mae: 9.6015 - val_fp_mae: 7.5361\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 8.66386\n",
      "Epoch 46/100\n",
      "64/64 - 1s - loss: 196.4801 - mse: 131.9086 - mae: 8.8268 - fp_mae: 5.4014 - val_loss: 196.7747 - val_mse: 134.2000 - val_mae: 8.9175 - val_fp_mae: 5.3970\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 8.66386\n",
      "Epoch 47/100\n",
      "64/64 - 1s - loss: 192.6523 - mse: 130.9221 - mae: 8.8199 - fp_mae: 5.3600 - val_loss: 196.3091 - val_mse: 135.4869 - val_mae: 8.9303 - val_fp_mae: 5.3728\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 8.66386\n",
      "Epoch 48/100\n",
      "64/64 - 1s - loss: 192.1203 - mse: 131.7082 - mae: 8.8322 - fp_mae: 5.3874 - val_loss: 196.4435 - val_mse: 135.4293 - val_mae: 8.7404 - val_fp_mae: 5.9419\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 8.66386\n",
      "Epoch 49/100\n",
      "64/64 - 1s - loss: 190.7833 - mse: 131.4725 - mae: 8.8129 - fp_mae: 5.3703 - val_loss: 196.0935 - val_mse: 137.3864 - val_mae: 8.8239 - val_fp_mae: 6.0009\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 8.66386\n",
      "Epoch 50/100\n",
      "64/64 - 1s - loss: 188.7751 - mse: 130.8265 - mae: 8.8156 - fp_mae: 5.3378 - val_loss: 198.3536 - val_mse: 141.4277 - val_mae: 9.1934 - val_fp_mae: 5.3935\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 8.66386\n",
      "Epoch 51/100\n",
      "64/64 - 1s - loss: 186.5931 - mse: 130.2190 - mae: 8.7956 - fp_mae: 5.2819 - val_loss: 191.6983 - val_mse: 136.6031 - val_mae: 8.7562 - val_fp_mae: 6.2916\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 8.66386\n",
      "Epoch 52/100\n",
      "64/64 - 1s - loss: 185.0061 - mse: 129.8596 - mae: 8.7738 - fp_mae: 5.2181 - val_loss: 199.9592 - val_mse: 145.0614 - val_mae: 9.3910 - val_fp_mae: 5.1869\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 8.66386\n",
      "Epoch 53/100\n",
      "64/64 - 1s - loss: 184.6323 - mse: 130.6069 - mae: 8.8067 - fp_mae: 5.2663 - val_loss: 190.1968 - val_mse: 136.3443 - val_mae: 8.7309 - val_fp_mae: 6.1691\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 8.66386\n",
      "Epoch 54/100\n",
      "64/64 - 1s - loss: 184.1340 - mse: 130.5252 - mae: 8.7723 - fp_mae: 5.2575 - val_loss: 210.9037 - val_mse: 157.7818 - val_mae: 9.5729 - val_fp_mae: 7.5180\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 8.66386\n",
      "Epoch 55/100\n",
      "64/64 - 1s - loss: 182.3706 - mse: 129.0986 - mae: 8.7375 - fp_mae: 5.1909 - val_loss: 240.0747 - val_mse: 187.7353 - val_mae: 10.7393 - val_fp_mae: 5.2953\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 8.66386\n",
      "Epoch 56/100\n",
      "64/64 - 1s - loss: 184.0380 - mse: 130.1301 - mae: 8.8147 - fp_mae: 5.2075 - val_loss: 395.8483 - val_mse: 341.2173 - val_mae: 14.0756 - val_fp_mae: 11.8725\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 8.66386\n",
      "Epoch 57/100\n",
      "64/64 - 1s - loss: 181.4783 - mse: 129.3971 - mae: 8.7666 - fp_mae: 5.1914 - val_loss: 408.7270 - val_mse: 357.1983 - val_mae: 14.3139 - val_fp_mae: 12.2407\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 8.66386\n",
      "Epoch 58/100\n",
      "64/64 - 1s - loss: 179.9371 - mse: 128.6063 - mae: 8.7867 - fp_mae: 5.1003 - val_loss: 278.0625 - val_mse: 227.7739 - val_mae: 11.8453 - val_fp_mae: 9.5346\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 8.66386\n",
      "Epoch 59/100\n",
      "64/64 - 1s - loss: 178.7312 - mse: 128.1212 - mae: 8.7528 - fp_mae: 5.1092 - val_loss: 206.5499 - val_mse: 155.7183 - val_mae: 9.7250 - val_fp_mae: 7.1189\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 8.66386\n",
      "Epoch 60/100\n",
      "64/64 - 1s - loss: 178.9988 - mse: 129.3158 - mae: 8.7733 - fp_mae: 5.1040 - val_loss: 193.8239 - val_mse: 145.3815 - val_mae: 9.4393 - val_fp_mae: 5.0031\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 8.66386\n",
      "Epoch 61/100\n",
      "64/64 - 2s - loss: 177.9701 - mse: 129.1432 - mae: 8.7730 - fp_mae: 5.1119 - val_loss: 229.7489 - val_mse: 180.2806 - val_mae: 10.5179 - val_fp_mae: 5.5753\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 8.66386\n",
      "Epoch 62/100\n",
      "64/64 - 1s - loss: 178.1812 - mse: 128.8152 - mae: 8.7680 - fp_mae: 5.0448 - val_loss: 251.0166 - val_mse: 202.9625 - val_mae: 11.1405 - val_fp_mae: 9.0624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00062: val_mae did not improve from 8.66386\n",
      "Epoch 63/100\n",
      "64/64 - 1s - loss: 176.7268 - mse: 128.8526 - mae: 8.7691 - fp_mae: 5.1024 - val_loss: 217.0265 - val_mse: 169.6628 - val_mae: 10.2239 - val_fp_mae: 7.6173\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 8.66386\n",
      "Epoch 64/100\n",
      "64/64 - 1s - loss: 175.6657 - mse: 128.7264 - mae: 8.7882 - fp_mae: 5.1139 - val_loss: 215.1580 - val_mse: 168.9214 - val_mae: 10.1998 - val_fp_mae: 5.3888\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 8.66386\n",
      "Epoch 65/100\n",
      "64/64 - 1s - loss: 175.7562 - mse: 129.1747 - mae: 8.7988 - fp_mae: 5.0686 - val_loss: 214.5909 - val_mse: 167.3025 - val_mae: 10.1448 - val_fp_mae: 5.1345\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 8.66386\n",
      "Epoch 66/100\n",
      "64/64 - 1s - loss: 175.8653 - mse: 127.9610 - mae: 8.7652 - fp_mae: 4.9978 - val_loss: 230.6227 - val_mse: 183.5849 - val_mae: 10.6318 - val_fp_mae: 8.3579\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 8.66386\n",
      "Epoch 67/100\n",
      "64/64 - 1s - loss: 172.9792 - mse: 126.8484 - mae: 8.7553 - fp_mae: 4.9838 - val_loss: 256.3188 - val_mse: 211.2942 - val_mae: 11.4075 - val_fp_mae: 9.1797\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 8.66386\n",
      "Epoch 68/100\n",
      "64/64 - 1s - loss: 173.0719 - mse: 127.7564 - mae: 8.7394 - fp_mae: 5.0085 - val_loss: 391.0774 - val_mse: 345.2037 - val_mae: 14.2511 - val_fp_mae: 11.6342\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 8.66386\n",
      "Epoch 69/100\n",
      "64/64 - 1s - loss: 174.5379 - mse: 128.6878 - mae: 8.7750 - fp_mae: 5.0165 - val_loss: 230.7424 - val_mse: 185.7862 - val_mae: 10.6386 - val_fp_mae: 8.4387\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 8.66386\n",
      "Epoch 70/100\n",
      "64/64 - 1s - loss: 173.2450 - mse: 127.9336 - mae: 8.8056 - fp_mae: 4.9914 - val_loss: 275.6172 - val_mse: 231.1088 - val_mae: 11.8175 - val_fp_mae: 9.7900\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 8.66386\n",
      "Epoch 71/100\n",
      "64/64 - 1s - loss: 172.4024 - mse: 127.2270 - mae: 8.7597 - fp_mae: 4.9594 - val_loss: 182.6439 - val_mse: 138.6338 - val_mae: 8.9793 - val_fp_mae: 5.6117\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 8.66386\n",
      "Epoch 72/100\n",
      "64/64 - 1s - loss: 171.1952 - mse: 127.6868 - mae: 8.7181 - fp_mae: 5.0021 - val_loss: 218.5203 - val_mse: 175.2285 - val_mae: 10.2495 - val_fp_mae: 8.1105\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 8.66386\n",
      "Epoch 73/100\n",
      "64/64 - 1s - loss: 169.5823 - mse: 126.6292 - mae: 8.7538 - fp_mae: 4.9397 - val_loss: 177.2953 - val_mse: 134.0173 - val_mae: 8.8873 - val_fp_mae: 5.3882\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 8.66386\n",
      "Epoch 74/100\n",
      "64/64 - 1s - loss: 169.9159 - mse: 127.1154 - mae: 8.7364 - fp_mae: 4.9035 - val_loss: 186.2571 - val_mse: 143.8799 - val_mae: 9.0618 - val_fp_mae: 6.6614\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 8.66386\n",
      "Epoch 75/100\n",
      "64/64 - 1s - loss: 169.5784 - mse: 126.9321 - mae: 8.7246 - fp_mae: 4.9215 - val_loss: 370.5083 - val_mse: 328.1801 - val_mae: 13.8964 - val_fp_mae: 11.4094\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 8.66386\n",
      "Epoch 76/100\n",
      "64/64 - 1s - loss: 170.8101 - mse: 128.4969 - mae: 8.7865 - fp_mae: 5.0342 - val_loss: 200.0179 - val_mse: 156.1794 - val_mae: 9.9935 - val_fp_mae: 4.6329\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 8.66386\n",
      "Epoch 77/100\n",
      "64/64 - 1s - loss: 170.9270 - mse: 127.5979 - mae: 8.7365 - fp_mae: 4.8993 - val_loss: 284.1049 - val_mse: 241.5908 - val_mae: 12.1807 - val_fp_mae: 9.4999\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 8.66386\n",
      "Epoch 78/100\n",
      "64/64 - 1s - loss: 169.1453 - mse: 126.9338 - mae: 8.7341 - fp_mae: 4.8835 - val_loss: 184.2891 - val_mse: 142.2661 - val_mae: 9.4480 - val_fp_mae: 4.7305\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 8.66386\n",
      "Epoch 79/100\n",
      "64/64 - 1s - loss: 168.4240 - mse: 127.1434 - mae: 8.7280 - fp_mae: 4.9489 - val_loss: 173.6407 - val_mse: 133.0759 - val_mae: 8.7536 - val_fp_mae: 5.7434\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 8.66386\n",
      "Epoch 80/100\n",
      "64/64 - 1s - loss: 167.9972 - mse: 127.1024 - mae: 8.7246 - fp_mae: 4.8499 - val_loss: 181.9293 - val_mse: 139.9310 - val_mae: 8.9162 - val_fp_mae: 6.6141\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 8.66386\n",
      "Epoch 81/100\n",
      "64/64 - 1s - loss: 167.5421 - mse: 126.8007 - mae: 8.7478 - fp_mae: 4.8732 - val_loss: 188.3229 - val_mse: 146.6787 - val_mae: 9.5664 - val_fp_mae: 4.8886\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 8.66386\n",
      "Epoch 82/100\n",
      "64/64 - 1s - loss: 168.5084 - mse: 127.8069 - mae: 8.7639 - fp_mae: 4.9453 - val_loss: 199.2884 - val_mse: 158.0139 - val_mae: 9.8900 - val_fp_mae: 5.1355\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 8.66386\n",
      "Epoch 83/100\n",
      "64/64 - 1s - loss: 167.9633 - mse: 126.8303 - mae: 8.7562 - fp_mae: 4.8491 - val_loss: 535.8843 - val_mse: 495.4673 - val_mae: 16.1712 - val_fp_mae: 13.9647\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 8.66386\n",
      "Epoch 84/100\n",
      "64/64 - 1s - loss: 166.1288 - mse: 126.1087 - mae: 8.6881 - fp_mae: 4.8483 - val_loss: 171.0416 - val_mse: 130.8905 - val_mae: 8.6910 - val_fp_mae: 5.7564\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 8.66386\n",
      "Epoch 85/100\n",
      "64/64 - 1s - loss: 166.3592 - mse: 126.7441 - mae: 8.7245 - fp_mae: 4.8782 - val_loss: 200.0888 - val_mse: 160.7289 - val_mae: 9.6950 - val_fp_mae: 7.5086\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 8.66386\n",
      "Epoch 86/100\n",
      "64/64 - 1s - loss: 169.3493 - mse: 128.8164 - mae: 8.7999 - fp_mae: 4.8928 - val_loss: 297.5862 - val_mse: 255.5035 - val_mae: 12.4648 - val_fp_mae: 9.7490\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 8.66386\n",
      "Epoch 87/100\n",
      "64/64 - 2s - loss: 166.7228 - mse: 126.1133 - mae: 8.7362 - fp_mae: 4.7710 - val_loss: 798.2034 - val_mse: 758.7305 - val_mae: 19.1497 - val_fp_mae: 16.6925\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 8.66386\n",
      "Epoch 88/100\n",
      "64/64 - 1s - loss: 165.4515 - mse: 126.4133 - mae: 8.7321 - fp_mae: 4.7776 - val_loss: 331.1553 - val_mse: 292.5581 - val_mae: 13.3047 - val_fp_mae: 10.9445\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 8.66386\n",
      "Epoch 89/100\n",
      "64/64 - 1s - loss: 165.1411 - mse: 127.1405 - mae: 8.7311 - fp_mae: 4.9361 - val_loss: 186.0761 - val_mse: 148.1475 - val_mae: 9.2109 - val_fp_mae: 6.9495\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 8.66386\n",
      "Epoch 90/100\n",
      "64/64 - 1s - loss: 164.6377 - mse: 126.4120 - mae: 8.7178 - fp_mae: 4.8217 - val_loss: 200.9126 - val_mse: 162.9105 - val_mae: 9.9918 - val_fp_mae: 5.8100\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 8.66386\n",
      "Epoch 91/100\n",
      "64/64 - 1s - loss: 164.6495 - mse: 126.0225 - mae: 8.7455 - fp_mae: 4.7871 - val_loss: 1390.1288 - val_mse: 1352.5177 - val_mae: 24.3148 - val_fp_mae: 22.3971\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 8.66386\n",
      "Epoch 92/100\n",
      "64/64 - 1s - loss: 162.8632 - mse: 125.5441 - mae: 8.7059 - fp_mae: 4.7528 - val_loss: 179.4761 - val_mse: 142.2991 - val_mae: 9.1273 - val_fp_mae: 6.5368\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 8.66386\n",
      "Epoch 93/100\n",
      "64/64 - 1s - loss: 164.3091 - mse: 126.9626 - mae: 8.7333 - fp_mae: 4.8492 - val_loss: 161.0782 - val_mse: 122.5285 - val_mae: 8.6147 - val_fp_mae: 4.8572\n",
      "\n",
      "Epoch 00093: val_mae improved from 8.66386 to 8.61472, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/2048/best_model_lambda_3.h5\n",
      "Epoch 94/100\n",
      "64/64 - 1s - loss: 163.9490 - mse: 126.0097 - mae: 8.7378 - fp_mae: 4.8269 - val_loss: 221.8889 - val_mse: 185.0992 - val_mae: 10.7464 - val_fp_mae: 4.9491\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 8.61472\n",
      "Epoch 95/100\n",
      "64/64 - 1s - loss: 162.7206 - mse: 126.2717 - mae: 8.7497 - fp_mae: 4.7837 - val_loss: 175.6668 - val_mse: 139.6030 - val_mae: 9.2022 - val_fp_mae: 5.2411\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 8.61472\n",
      "Epoch 96/100\n",
      "64/64 - 1s - loss: 164.2237 - mse: 127.1505 - mae: 8.7925 - fp_mae: 4.8365 - val_loss: 164.0216 - val_mse: 127.0939 - val_mae: 8.7579 - val_fp_mae: 5.0247\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 8.61472\n",
      "Epoch 97/100\n",
      "64/64 - 1s - loss: 162.9072 - mse: 126.0641 - mae: 8.6999 - fp_mae: 4.8105 - val_loss: 164.6843 - val_mse: 127.2405 - val_mae: 8.6538 - val_fp_mae: 5.3195\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 8.61472\n",
      "Epoch 98/100\n",
      "64/64 - 1s - loss: 163.1407 - mse: 126.2232 - mae: 8.7288 - fp_mae: 4.8093 - val_loss: 306.2686 - val_mse: 269.3764 - val_mae: 12.7868 - val_fp_mae: 10.3393\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 8.61472\n",
      "Epoch 99/100\n",
      "64/64 - 1s - loss: 163.3322 - mse: 126.6369 - mae: 8.7614 - fp_mae: 4.7361 - val_loss: 439.2420 - val_mse: 403.2910 - val_mae: 15.1308 - val_fp_mae: 12.7494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00099: val_mae did not improve from 8.61472\n",
      "Epoch 100/100\n",
      "64/64 - 1s - loss: 161.1213 - mse: 126.3449 - mae: 8.7041 - fp_mae: 4.8107 - val_loss: 195.3804 - val_mse: 160.7701 - val_mae: 9.7692 - val_fp_mae: 7.6193\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 8.61472\n",
      "\n",
      "Lambda: 10 , Time: 0:02:23\n",
      "Train Error(all epochs): 8.688057899475098 \n",
      " [25.872, 24.932, 23.532, 21.945, 20.363, 18.882, 17.468, 16.173, 15.055, 14.004, 12.999, 12.095, 11.411, 10.789, 10.331, 9.923, 9.677, 9.472, 9.359, 9.343, 9.238, 9.17, 9.153, 9.079, 9.107, 9.006, 9.039, 9.019, 8.989, 8.967, 8.977, 8.98, 9.051, 9.162, 8.943, 8.924, 8.927, 8.943, 8.87, 8.943, 8.862, 8.872, 8.818, 8.902, 8.802, 8.827, 8.82, 8.832, 8.813, 8.816, 8.796, 8.774, 8.807, 8.772, 8.738, 8.815, 8.767, 8.787, 8.753, 8.773, 8.773, 8.768, 8.769, 8.788, 8.799, 8.765, 8.755, 8.739, 8.775, 8.806, 8.76, 8.718, 8.754, 8.736, 8.725, 8.787, 8.737, 8.734, 8.728, 8.725, 8.748, 8.764, 8.756, 8.688, 8.725, 8.8, 8.736, 8.732, 8.731, 8.718, 8.745, 8.706, 8.733, 8.738, 8.75, 8.792, 8.7, 8.729, 8.761, 8.704]\n",
      "Train FP Error(all epochs): 4.736123561859131 \n",
      " [24.256, 23.643, 22.654, 21.41, 20.051, 18.645, 17.262, 15.911, 14.675, 13.502, 12.336, 11.239, 10.32, 9.459, 8.779, 8.151, 7.744, 7.345, 7.133, 6.954, 6.742, 6.668, 6.571, 6.427, 6.408, 6.27, 6.247, 6.208, 6.125, 6.042, 6.002, 5.984, 5.925, 6.03, 5.765, 5.756, 5.819, 5.794, 5.684, 5.764, 5.576, 5.596, 5.493, 5.585, 5.461, 5.401, 5.36, 5.387, 5.37, 5.338, 5.282, 5.218, 5.266, 5.258, 5.191, 5.208, 5.191, 5.1, 5.109, 5.104, 5.112, 5.045, 5.102, 5.114, 5.069, 4.998, 4.984, 5.009, 5.017, 4.991, 4.959, 5.002, 4.94, 4.904, 4.922, 5.034, 4.899, 4.883, 4.949, 4.85, 4.873, 4.945, 4.849, 4.848, 4.878, 4.893, 4.771, 4.778, 4.936, 4.822, 4.787, 4.753, 4.849, 4.827, 4.784, 4.837, 4.811, 4.809, 4.736, 4.811]\n",
      "Val Error(all epochs): 8.614724159240723 \n",
      " [26.874, 26.902, 26.217, 23.353, 22.142, 18.064, 18.191, 16.684, 20.54, 23.048, 14.171, 13.121, 22.254, 13.565, 11.158, 11.079, 10.688, 9.693, 16.423, 10.037, 17.043, 13.562, 21.037, 12.179, 13.134, 9.235, 9.621, 11.359, 8.71, 9.29, 15.27, 11.059, 14.166, 11.753, 8.807, 8.706, 9.101, 12.803, 12.889, 11.935, 9.4, 8.664, 8.69, 12.333, 9.602, 8.918, 8.93, 8.74, 8.824, 9.193, 8.756, 9.391, 8.731, 9.573, 10.739, 14.076, 14.314, 11.845, 9.725, 9.439, 10.518, 11.141, 10.224, 10.2, 10.145, 10.632, 11.407, 14.251, 10.639, 11.818, 8.979, 10.249, 8.887, 9.062, 13.896, 9.993, 12.181, 9.448, 8.754, 8.916, 9.566, 9.89, 16.171, 8.691, 9.695, 12.465, 19.15, 13.305, 9.211, 9.992, 24.315, 9.127, 8.615, 10.746, 9.202, 8.758, 8.654, 12.787, 15.131, 9.769]\n",
      "Val FP Error(all epochs): 4.632877826690674 \n",
      " [25.42, 25.485, 24.779, 21.333, 20.024, 17.566, 16.246, 15.857, 20.493, 22.951, 11.957, 11.157, 21.794, 12.877, 9.032, 10.148, 7.852, 7.852, 15.482, 8.658, 15.327, 12.102, 19.122, 5.864, 11.738, 7.382, 7.978, 5.518, 6.065, 7.012, 13.491, 5.329, 5.156, 9.707, 6.239, 5.382, 5.806, 5.439, 4.761, 5.086, 5.239, 5.83, 6.123, 10.246, 7.536, 5.397, 5.373, 5.942, 6.001, 5.394, 6.292, 5.187, 6.169, 7.518, 5.295, 11.873, 12.241, 9.535, 7.119, 5.003, 5.575, 9.062, 7.617, 5.389, 5.135, 8.358, 9.18, 11.634, 8.439, 9.79, 5.612, 8.111, 5.388, 6.661, 11.409, 4.633, 9.5, 4.731, 5.743, 6.614, 4.889, 5.136, 13.965, 5.756, 7.509, 9.749, 16.692, 10.945, 6.95, 5.81, 22.397, 6.537, 4.857, 4.949, 5.241, 5.025, 5.319, 10.339, 12.749, 7.619]\n",
      "\n",
      "Trainig set size: 2048 , Time: 0:09:31 , best_lambda: 0 , min_  error: 8.14\n",
      "Test starts:  2458 , ends:  29999\n",
      "861/861 [==============================] - 12s 14ms/step - loss: 110.3135 - mse: 110.3135 - mae: 8.2286 - fp_mae: 4.6130\n",
      "average_error:  8.229 , fp_average_error:  4.613\n",
      "\n",
      "\n",
      "\n",
      "number_samples: 4096 , New samples: 4096\n",
      "Validation size: 820 , starts: 4096 , ends: 4915\n",
      "Epoch 1/100\n",
      "128/128 - 4s - loss: 807.9014 - mse: 807.9014 - mae: 25.1012 - fp_mae: 23.7325 - val_loss: 1191.9490 - val_mse: 1191.9490 - val_mae: 30.6591 - val_fp_mae: 30.1822\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 30.65908, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_0.h5\n",
      "Epoch 2/100\n",
      "128/128 - 3s - loss: 653.7248 - mse: 653.7248 - mae: 22.2721 - fp_mae: 21.6883 - val_loss: 559.8844 - val_mse: 559.8844 - val_mae: 20.0958 - val_fp_mae: 19.9811\n",
      "\n",
      "Epoch 00002: val_mae improved from 30.65908 to 20.09584, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_0.h5\n",
      "Epoch 3/100\n",
      "128/128 - 3s - loss: 495.1334 - mse: 495.1334 - mae: 19.1806 - fp_mae: 18.9764 - val_loss: 391.1920 - val_mse: 391.1920 - val_mae: 16.2911 - val_fp_mae: 16.1739\n",
      "\n",
      "Epoch 00003: val_mae improved from 20.09584 to 16.29111, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_0.h5\n",
      "Epoch 4/100\n",
      "128/128 - 3s - loss: 369.0281 - mse: 369.0281 - mae: 16.2204 - fp_mae: 16.0280 - val_loss: 323.0167 - val_mse: 323.0167 - val_mae: 14.2942 - val_fp_mae: 13.5594\n",
      "\n",
      "Epoch 00004: val_mae improved from 16.29111 to 14.29415, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_0.h5\n",
      "Epoch 5/100\n",
      "128/128 - 3s - loss: 269.9838 - mse: 269.9838 - mae: 13.4973 - fp_mae: 13.0568 - val_loss: 438.1232 - val_mse: 438.1232 - val_mae: 16.8267 - val_fp_mae: 16.3458\n",
      "\n",
      "Epoch 00005: val_mae did not improve from 14.29415\n",
      "Epoch 6/100\n",
      "128/128 - 3s - loss: 187.1851 - mse: 187.1851 - mae: 10.8908 - fp_mae: 10.0688 - val_loss: 182.5725 - val_mse: 182.5725 - val_mae: 10.7879 - val_fp_mae: 9.7076\n",
      "\n",
      "Epoch 00006: val_mae improved from 14.29415 to 10.78793, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_0.h5\n",
      "Epoch 7/100\n",
      "128/128 - 3s - loss: 132.3645 - mse: 132.3645 - mae: 9.0963 - fp_mae: 7.5518 - val_loss: 181.3168 - val_mse: 181.3168 - val_mae: 10.7384 - val_fp_mae: 9.3540\n",
      "\n",
      "Epoch 00007: val_mae improved from 10.78793 to 10.73842, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_0.h5\n",
      "Epoch 8/100\n",
      "128/128 - 3s - loss: 102.7891 - mse: 102.7891 - mae: 7.9492 - fp_mae: 5.7755 - val_loss: 127.0340 - val_mse: 127.0340 - val_mae: 8.8753 - val_fp_mae: 6.3157\n",
      "\n",
      "Epoch 00008: val_mae improved from 10.73842 to 8.87534, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_0.h5\n",
      "Epoch 9/100\n",
      "128/128 - 3s - loss: 90.4204 - mse: 90.4204 - mae: 7.5004 - fp_mae: 4.6480 - val_loss: 98.4074 - val_mse: 98.4074 - val_mae: 7.9231 - val_fp_mae: 3.6259\n",
      "\n",
      "Epoch 00009: val_mae improved from 8.87534 to 7.92306, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_0.h5\n",
      "Epoch 10/100\n",
      "128/128 - 3s - loss: 84.9038 - mse: 84.9038 - mae: 7.2613 - fp_mae: 4.0862 - val_loss: 98.2229 - val_mse: 98.2229 - val_mae: 7.8716 - val_fp_mae: 4.2480\n",
      "\n",
      "Epoch 00010: val_mae improved from 7.92306 to 7.87161, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_0.h5\n",
      "Epoch 11/100\n",
      "128/128 - 3s - loss: 83.6348 - mse: 83.6348 - mae: 7.2027 - fp_mae: 3.7666 - val_loss: 98.4321 - val_mse: 98.4321 - val_mae: 7.8684 - val_fp_mae: 3.7672\n",
      "\n",
      "Epoch 00011: val_mae improved from 7.87161 to 7.86836, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_0.h5\n",
      "Epoch 12/100\n",
      "128/128 - 3s - loss: 79.8252 - mse: 79.8252 - mae: 7.0313 - fp_mae: 3.6431 - val_loss: 105.7398 - val_mse: 105.7398 - val_mae: 8.0498 - val_fp_mae: 4.9950\n",
      "\n",
      "Epoch 00012: val_mae did not improve from 7.86836\n",
      "Epoch 13/100\n",
      "128/128 - 3s - loss: 77.4815 - mse: 77.4815 - mae: 6.9451 - fp_mae: 3.4852 - val_loss: 101.0090 - val_mse: 101.0090 - val_mae: 7.9434 - val_fp_mae: 3.1977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00013: val_mae did not improve from 7.86836\n",
      "Epoch 14/100\n",
      "128/128 - 3s - loss: 75.6697 - mse: 75.6697 - mae: 6.8561 - fp_mae: 3.4001 - val_loss: 103.6240 - val_mse: 103.6240 - val_mae: 7.9825 - val_fp_mae: 4.1899\n",
      "\n",
      "Epoch 00014: val_mae did not improve from 7.86836\n",
      "Epoch 15/100\n",
      "128/128 - 3s - loss: 73.7442 - mse: 73.7442 - mae: 6.8393 - fp_mae: 3.3899 - val_loss: 109.8678 - val_mse: 109.8678 - val_mae: 8.2269 - val_fp_mae: 3.3488\n",
      "\n",
      "Epoch 00015: val_mae did not improve from 7.86836\n",
      "Epoch 16/100\n",
      "128/128 - 3s - loss: 71.6296 - mse: 71.6296 - mae: 6.7058 - fp_mae: 3.3671 - val_loss: 102.0470 - val_mse: 102.0470 - val_mae: 7.9628 - val_fp_mae: 3.6388\n",
      "\n",
      "Epoch 00016: val_mae did not improve from 7.86836\n",
      "Epoch 17/100\n",
      "128/128 - 3s - loss: 68.5956 - mse: 68.5956 - mae: 6.5074 - fp_mae: 3.2345 - val_loss: 106.7535 - val_mse: 106.7535 - val_mae: 8.0672 - val_fp_mae: 4.3771\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 7.86836\n",
      "Epoch 18/100\n",
      "128/128 - 3s - loss: 68.8553 - mse: 68.8553 - mae: 6.5348 - fp_mae: 3.2466 - val_loss: 107.6035 - val_mse: 107.6035 - val_mae: 8.1530 - val_fp_mae: 4.8753\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 7.86836\n",
      "Epoch 19/100\n",
      "128/128 - 3s - loss: 67.8398 - mse: 67.8398 - mae: 6.4993 - fp_mae: 3.1840 - val_loss: 106.6340 - val_mse: 106.6340 - val_mae: 8.0925 - val_fp_mae: 4.5603\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 7.86836\n",
      "Epoch 20/100\n",
      "128/128 - 3s - loss: 65.7426 - mse: 65.7426 - mae: 6.4199 - fp_mae: 3.2294 - val_loss: 105.4732 - val_mse: 105.4732 - val_mae: 7.9803 - val_fp_mae: 3.7738\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 7.86836\n",
      "Epoch 21/100\n",
      "128/128 - 3s - loss: 64.8821 - mse: 64.8821 - mae: 6.3283 - fp_mae: 3.1178 - val_loss: 111.7861 - val_mse: 111.7861 - val_mae: 8.3279 - val_fp_mae: 4.6650\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 7.86836\n",
      "Epoch 22/100\n",
      "128/128 - 3s - loss: 63.0618 - mse: 63.0618 - mae: 6.2942 - fp_mae: 3.1382 - val_loss: 109.3122 - val_mse: 109.3122 - val_mae: 8.0912 - val_fp_mae: 3.9815\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 7.86836\n",
      "Epoch 23/100\n",
      "128/128 - 3s - loss: 62.5008 - mse: 62.5008 - mae: 6.2251 - fp_mae: 3.0740 - val_loss: 111.8920 - val_mse: 111.8920 - val_mae: 8.2301 - val_fp_mae: 3.7281\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 7.86836\n",
      "Epoch 24/100\n",
      "128/128 - 3s - loss: 59.8127 - mse: 59.8127 - mae: 6.1057 - fp_mae: 2.9966 - val_loss: 112.7934 - val_mse: 112.7934 - val_mae: 8.3508 - val_fp_mae: 5.0727\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 7.86836\n",
      "Epoch 25/100\n",
      "128/128 - 3s - loss: 57.9211 - mse: 57.9211 - mae: 6.0154 - fp_mae: 2.9769 - val_loss: 110.7424 - val_mse: 110.7424 - val_mae: 8.1579 - val_fp_mae: 4.1904\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 7.86836\n",
      "Epoch 26/100\n",
      "128/128 - 3s - loss: 57.3757 - mse: 57.3757 - mae: 5.9751 - fp_mae: 3.0002 - val_loss: 111.4384 - val_mse: 111.4384 - val_mae: 8.1640 - val_fp_mae: 4.6195\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 7.86836\n",
      "Epoch 27/100\n",
      "128/128 - 3s - loss: 55.2189 - mse: 55.2189 - mae: 5.8925 - fp_mae: 2.8728 - val_loss: 115.0137 - val_mse: 115.0137 - val_mae: 8.2858 - val_fp_mae: 4.1489\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 7.86836\n",
      "Epoch 28/100\n",
      "128/128 - 3s - loss: 54.3313 - mse: 54.3313 - mae: 5.8613 - fp_mae: 2.9138 - val_loss: 114.4280 - val_mse: 114.4280 - val_mae: 8.2495 - val_fp_mae: 4.2595\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 7.86836\n",
      "Epoch 29/100\n",
      "128/128 - 3s - loss: 54.1262 - mse: 54.1262 - mae: 5.8439 - fp_mae: 2.8893 - val_loss: 116.7249 - val_mse: 116.7249 - val_mae: 8.4011 - val_fp_mae: 4.7898\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 7.86836\n",
      "Epoch 30/100\n",
      "128/128 - 3s - loss: 52.4214 - mse: 52.4214 - mae: 5.7251 - fp_mae: 2.8856 - val_loss: 116.1980 - val_mse: 116.1980 - val_mae: 8.3707 - val_fp_mae: 4.2024\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 7.86836\n",
      "Epoch 31/100\n",
      "128/128 - 3s - loss: 51.9912 - mse: 51.9912 - mae: 5.7160 - fp_mae: 2.8476 - val_loss: 115.6470 - val_mse: 115.6470 - val_mae: 8.3706 - val_fp_mae: 4.2981\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 7.86836\n",
      "Epoch 32/100\n",
      "128/128 - 3s - loss: 50.5483 - mse: 50.5483 - mae: 5.6343 - fp_mae: 2.6953 - val_loss: 116.6771 - val_mse: 116.6771 - val_mae: 8.3808 - val_fp_mae: 4.1644\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 7.86836\n",
      "Epoch 33/100\n",
      "128/128 - 3s - loss: 48.8888 - mse: 48.8888 - mae: 5.5373 - fp_mae: 2.7230 - val_loss: 117.0745 - val_mse: 117.0745 - val_mae: 8.3585 - val_fp_mae: 4.7997\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 7.86836\n",
      "Epoch 34/100\n",
      "128/128 - 3s - loss: 50.2735 - mse: 50.2735 - mae: 5.6063 - fp_mae: 2.8873 - val_loss: 122.0345 - val_mse: 122.0345 - val_mae: 8.6178 - val_fp_mae: 5.1617\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 7.86836\n",
      "Epoch 35/100\n",
      "128/128 - 3s - loss: 48.3184 - mse: 48.3184 - mae: 5.5189 - fp_mae: 2.7026 - val_loss: 122.6434 - val_mse: 122.6434 - val_mae: 8.6322 - val_fp_mae: 4.6062\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 7.86836\n",
      "Epoch 36/100\n",
      "128/128 - 3s - loss: 48.1541 - mse: 48.1541 - mae: 5.4864 - fp_mae: 2.6501 - val_loss: 117.5458 - val_mse: 117.5458 - val_mae: 8.4587 - val_fp_mae: 4.6096\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 7.86836\n",
      "Epoch 37/100\n",
      "128/128 - 3s - loss: 46.2232 - mse: 46.2232 - mae: 5.3874 - fp_mae: 2.6372 - val_loss: 119.7888 - val_mse: 119.7888 - val_mae: 8.4907 - val_fp_mae: 4.7379\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 7.86836\n",
      "Epoch 38/100\n",
      "128/128 - 3s - loss: 45.9235 - mse: 45.9235 - mae: 5.3917 - fp_mae: 2.6531 - val_loss: 125.5997 - val_mse: 125.5997 - val_mae: 8.7141 - val_fp_mae: 4.5101\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 7.86836\n",
      "Epoch 39/100\n",
      "128/128 - 3s - loss: 44.9293 - mse: 44.9293 - mae: 5.3336 - fp_mae: 2.6888 - val_loss: 121.8397 - val_mse: 121.8397 - val_mae: 8.6350 - val_fp_mae: 5.2513\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 7.86836\n",
      "Epoch 40/100\n",
      "128/128 - 3s - loss: 44.0841 - mse: 44.0841 - mae: 5.2916 - fp_mae: 2.6374 - val_loss: 123.5179 - val_mse: 123.5179 - val_mae: 8.5911 - val_fp_mae: 4.0414\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 7.86836\n",
      "Epoch 41/100\n",
      "128/128 - 3s - loss: 45.0686 - mse: 45.0686 - mae: 5.3297 - fp_mae: 2.6928 - val_loss: 124.3341 - val_mse: 124.3341 - val_mae: 8.6805 - val_fp_mae: 5.0286\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 7.86836\n",
      "Epoch 42/100\n",
      "128/128 - 3s - loss: 43.8612 - mse: 43.8612 - mae: 5.2394 - fp_mae: 2.5384 - val_loss: 121.1569 - val_mse: 121.1569 - val_mae: 8.5573 - val_fp_mae: 4.4984\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 7.86836\n",
      "Epoch 43/100\n",
      "128/128 - 3s - loss: 43.5191 - mse: 43.5191 - mae: 5.2144 - fp_mae: 2.5930 - val_loss: 121.6369 - val_mse: 121.6369 - val_mae: 8.5254 - val_fp_mae: 4.1325\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 7.86836\n",
      "Epoch 44/100\n",
      "128/128 - 3s - loss: 41.0875 - mse: 41.0875 - mae: 5.1295 - fp_mae: 2.6038 - val_loss: 126.4409 - val_mse: 126.4409 - val_mae: 8.6904 - val_fp_mae: 3.9283\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 7.86836\n",
      "Epoch 45/100\n",
      "128/128 - 3s - loss: 41.1579 - mse: 41.1579 - mae: 5.0768 - fp_mae: 2.5371 - val_loss: 127.1920 - val_mse: 127.1920 - val_mae: 8.6796 - val_fp_mae: 3.9477\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 7.86836\n",
      "Epoch 46/100\n",
      "128/128 - 3s - loss: 40.6357 - mse: 40.6357 - mae: 5.0318 - fp_mae: 2.4577 - val_loss: 128.4586 - val_mse: 128.4586 - val_mae: 8.7898 - val_fp_mae: 5.5024\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 7.86836\n",
      "Epoch 47/100\n",
      "128/128 - 3s - loss: 41.3881 - mse: 41.3881 - mae: 5.0728 - fp_mae: 2.5188 - val_loss: 127.1823 - val_mse: 127.1823 - val_mae: 8.8548 - val_fp_mae: 5.3631\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 7.86836\n",
      "Epoch 48/100\n",
      "128/128 - 3s - loss: 40.2144 - mse: 40.2144 - mae: 5.0144 - fp_mae: 2.4779 - val_loss: 123.2014 - val_mse: 123.2014 - val_mae: 8.6079 - val_fp_mae: 4.4255\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 7.86836\n",
      "Epoch 49/100\n",
      "128/128 - 3s - loss: 38.8506 - mse: 38.8506 - mae: 4.9443 - fp_mae: 2.4528 - val_loss: 124.1842 - val_mse: 124.1842 - val_mae: 8.6866 - val_fp_mae: 5.0001\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 7.86836\n",
      "Epoch 50/100\n",
      "128/128 - 3s - loss: 38.4223 - mse: 38.4223 - mae: 4.9005 - fp_mae: 2.4755 - val_loss: 125.1236 - val_mse: 125.1236 - val_mae: 8.6794 - val_fp_mae: 4.8241\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 7.86836\n",
      "Epoch 51/100\n",
      "128/128 - 3s - loss: 38.3862 - mse: 38.3862 - mae: 4.9079 - fp_mae: 2.3759 - val_loss: 125.7073 - val_mse: 125.7073 - val_mae: 8.7135 - val_fp_mae: 4.6877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00051: val_mae did not improve from 7.86836\n",
      "Epoch 52/100\n",
      "128/128 - 3s - loss: 38.5794 - mse: 38.5794 - mae: 4.9476 - fp_mae: 2.4726 - val_loss: 120.3474 - val_mse: 120.3474 - val_mae: 8.5182 - val_fp_mae: 4.2998\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 7.86836\n",
      "Epoch 53/100\n",
      "128/128 - 3s - loss: 38.1763 - mse: 38.1763 - mae: 4.8986 - fp_mae: 2.4200 - val_loss: 125.2292 - val_mse: 125.2292 - val_mae: 8.6524 - val_fp_mae: 4.2488\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 7.86836\n",
      "Epoch 54/100\n",
      "128/128 - 3s - loss: 38.0093 - mse: 38.0093 - mae: 4.9010 - fp_mae: 2.4937 - val_loss: 123.3334 - val_mse: 123.3334 - val_mae: 8.6079 - val_fp_mae: 4.8392\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 7.86836\n",
      "Epoch 55/100\n",
      "128/128 - 3s - loss: 36.9612 - mse: 36.9612 - mae: 4.8032 - fp_mae: 2.3961 - val_loss: 123.7700 - val_mse: 123.7700 - val_mae: 8.6218 - val_fp_mae: 5.0856\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 7.86836\n",
      "Epoch 56/100\n",
      "128/128 - 3s - loss: 36.6730 - mse: 36.6730 - mae: 4.8106 - fp_mae: 2.3646 - val_loss: 126.5088 - val_mse: 126.5088 - val_mae: 8.5686 - val_fp_mae: 4.2378\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 7.86836\n",
      "Epoch 57/100\n",
      "128/128 - 3s - loss: 36.3567 - mse: 36.3567 - mae: 4.7595 - fp_mae: 2.3187 - val_loss: 130.5191 - val_mse: 130.5191 - val_mae: 8.7861 - val_fp_mae: 4.8614\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 7.86836\n",
      "Epoch 58/100\n",
      "128/128 - 3s - loss: 35.9729 - mse: 35.9729 - mae: 4.7451 - fp_mae: 2.4017 - val_loss: 132.5039 - val_mse: 132.5039 - val_mae: 8.8548 - val_fp_mae: 4.8304\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 7.86836\n",
      "Epoch 59/100\n",
      "128/128 - 3s - loss: 36.5702 - mse: 36.5702 - mae: 4.8166 - fp_mae: 2.3866 - val_loss: 130.1346 - val_mse: 130.1346 - val_mae: 8.7583 - val_fp_mae: 4.3216\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 7.86836\n",
      "Epoch 60/100\n",
      "128/128 - 3s - loss: 35.5966 - mse: 35.5966 - mae: 4.7538 - fp_mae: 2.3905 - val_loss: 125.9116 - val_mse: 125.9116 - val_mae: 8.6644 - val_fp_mae: 4.5025\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 7.86836\n",
      "Epoch 61/100\n",
      "128/128 - 3s - loss: 34.7799 - mse: 34.7799 - mae: 4.6502 - fp_mae: 2.2886 - val_loss: 125.9081 - val_mse: 125.9081 - val_mae: 8.7466 - val_fp_mae: 4.6460\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 7.86836\n",
      "Epoch 62/100\n",
      "128/128 - 3s - loss: 35.2881 - mse: 35.2881 - mae: 4.7078 - fp_mae: 2.3519 - val_loss: 126.0192 - val_mse: 126.0192 - val_mae: 8.6420 - val_fp_mae: 4.6935\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 7.86836\n",
      "Epoch 63/100\n",
      "128/128 - 3s - loss: 35.4886 - mse: 35.4886 - mae: 4.7186 - fp_mae: 2.3299 - val_loss: 124.9565 - val_mse: 124.9565 - val_mae: 8.6427 - val_fp_mae: 4.3997\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 7.86836\n",
      "Epoch 64/100\n",
      "128/128 - 3s - loss: 34.7402 - mse: 34.7402 - mae: 4.6870 - fp_mae: 2.3133 - val_loss: 125.9355 - val_mse: 125.9355 - val_mae: 8.6173 - val_fp_mae: 4.4391\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 7.86836\n",
      "Epoch 65/100\n",
      "128/128 - 3s - loss: 35.2058 - mse: 35.2058 - mae: 4.6971 - fp_mae: 2.3192 - val_loss: 126.6256 - val_mse: 126.6256 - val_mae: 8.7522 - val_fp_mae: 4.8462\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 7.86836\n",
      "Epoch 66/100\n",
      "128/128 - 3s - loss: 34.8738 - mse: 34.8738 - mae: 4.6770 - fp_mae: 2.3371 - val_loss: 127.8390 - val_mse: 127.8390 - val_mae: 8.7427 - val_fp_mae: 4.4264\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 7.86836\n",
      "Epoch 67/100\n",
      "128/128 - 3s - loss: 33.9667 - mse: 33.9667 - mae: 4.6065 - fp_mae: 2.3090 - val_loss: 131.5940 - val_mse: 131.5940 - val_mae: 8.8788 - val_fp_mae: 5.0240\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 7.86836\n",
      "Epoch 68/100\n",
      "128/128 - 3s - loss: 33.6950 - mse: 33.6950 - mae: 4.5765 - fp_mae: 2.3646 - val_loss: 126.2021 - val_mse: 126.2021 - val_mae: 8.7369 - val_fp_mae: 4.0493\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 7.86836\n",
      "Epoch 69/100\n",
      "128/128 - 3s - loss: 34.0136 - mse: 34.0136 - mae: 4.6297 - fp_mae: 2.2370 - val_loss: 129.9934 - val_mse: 129.9934 - val_mae: 8.8257 - val_fp_mae: 4.6025\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 7.86836\n",
      "Epoch 70/100\n",
      "128/128 - 3s - loss: 32.4120 - mse: 32.4120 - mae: 4.5126 - fp_mae: 2.3010 - val_loss: 131.8205 - val_mse: 131.8205 - val_mae: 8.9005 - val_fp_mae: 4.8093\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 7.86836\n",
      "Epoch 71/100\n",
      "128/128 - 3s - loss: 32.5776 - mse: 32.5776 - mae: 4.5214 - fp_mae: 2.2089 - val_loss: 130.7072 - val_mse: 130.7072 - val_mae: 8.8585 - val_fp_mae: 4.4046\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 7.86836\n",
      "Epoch 72/100\n",
      "128/128 - 3s - loss: 32.7247 - mse: 32.7247 - mae: 4.5499 - fp_mae: 2.2451 - val_loss: 130.3043 - val_mse: 130.3043 - val_mae: 8.8379 - val_fp_mae: 4.0070\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 7.86836\n",
      "Epoch 73/100\n",
      "128/128 - 3s - loss: 32.3339 - mse: 32.3339 - mae: 4.5478 - fp_mae: 2.2649 - val_loss: 129.2803 - val_mse: 129.2803 - val_mae: 8.7574 - val_fp_mae: 4.2307\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 7.86836\n",
      "Epoch 74/100\n",
      "128/128 - 3s - loss: 32.1832 - mse: 32.1832 - mae: 4.4828 - fp_mae: 2.2623 - val_loss: 128.6805 - val_mse: 128.6805 - val_mae: 8.7490 - val_fp_mae: 4.2970\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 7.86836\n",
      "Epoch 75/100\n",
      "128/128 - 3s - loss: 32.4047 - mse: 32.4047 - mae: 4.4836 - fp_mae: 2.2304 - val_loss: 128.7013 - val_mse: 128.7013 - val_mae: 8.7757 - val_fp_mae: 4.5979\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 7.86836\n",
      "Epoch 76/100\n",
      "128/128 - 3s - loss: 32.5292 - mse: 32.5292 - mae: 4.5164 - fp_mae: 2.2596 - val_loss: 127.2810 - val_mse: 127.2810 - val_mae: 8.8257 - val_fp_mae: 4.7172\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 7.86836\n",
      "Epoch 77/100\n",
      "128/128 - 3s - loss: 30.7043 - mse: 30.7043 - mae: 4.3623 - fp_mae: 2.1810 - val_loss: 131.5664 - val_mse: 131.5664 - val_mae: 8.8563 - val_fp_mae: 4.9974\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 7.86836\n",
      "Epoch 78/100\n",
      "128/128 - 3s - loss: 31.2597 - mse: 31.2597 - mae: 4.4375 - fp_mae: 2.2130 - val_loss: 130.1858 - val_mse: 130.1858 - val_mae: 8.8354 - val_fp_mae: 4.0676\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 7.86836\n",
      "Epoch 79/100\n",
      "128/128 - 3s - loss: 31.5139 - mse: 31.5139 - mae: 4.4671 - fp_mae: 2.1878 - val_loss: 129.5043 - val_mse: 129.5043 - val_mae: 8.8554 - val_fp_mae: 4.7550\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 7.86836\n",
      "Epoch 80/100\n",
      "128/128 - 3s - loss: 32.0287 - mse: 32.0287 - mae: 4.4989 - fp_mae: 2.2828 - val_loss: 128.0859 - val_mse: 128.0859 - val_mae: 8.8176 - val_fp_mae: 4.4756\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 7.86836\n",
      "Epoch 81/100\n",
      "128/128 - 3s - loss: 31.4787 - mse: 31.4787 - mae: 4.4264 - fp_mae: 2.2480 - val_loss: 130.6653 - val_mse: 130.6653 - val_mae: 8.8905 - val_fp_mae: 5.1869\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 7.86836\n",
      "Epoch 82/100\n",
      "128/128 - 3s - loss: 31.8184 - mse: 31.8184 - mae: 4.4390 - fp_mae: 2.1843 - val_loss: 127.5140 - val_mse: 127.5140 - val_mae: 8.7925 - val_fp_mae: 4.9555\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 7.86836\n",
      "Epoch 83/100\n",
      "128/128 - 3s - loss: 30.9890 - mse: 30.9890 - mae: 4.3926 - fp_mae: 2.1470 - val_loss: 126.6979 - val_mse: 126.6979 - val_mae: 8.7435 - val_fp_mae: 4.5259\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 7.86836\n",
      "Epoch 84/100\n",
      "128/128 - 3s - loss: 30.9773 - mse: 30.9773 - mae: 4.3772 - fp_mae: 2.1886 - val_loss: 126.3775 - val_mse: 126.3775 - val_mae: 8.7643 - val_fp_mae: 3.8967\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 7.86836\n",
      "Epoch 85/100\n",
      "128/128 - 3s - loss: 31.2202 - mse: 31.2202 - mae: 4.4100 - fp_mae: 2.2035 - val_loss: 131.8633 - val_mse: 131.8633 - val_mae: 8.9129 - val_fp_mae: 5.5193\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 7.86836\n",
      "Epoch 86/100\n",
      "128/128 - 3s - loss: 30.7262 - mse: 30.7262 - mae: 4.3954 - fp_mae: 2.2142 - val_loss: 134.1108 - val_mse: 134.1108 - val_mae: 9.0059 - val_fp_mae: 5.3615\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 7.86836\n",
      "Epoch 87/100\n",
      "128/128 - 3s - loss: 30.2898 - mse: 30.2898 - mae: 4.3759 - fp_mae: 2.2059 - val_loss: 127.2341 - val_mse: 127.2341 - val_mae: 8.7737 - val_fp_mae: 4.2965\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 7.86836\n",
      "Epoch 88/100\n",
      "128/128 - 3s - loss: 30.8767 - mse: 30.8767 - mae: 4.3712 - fp_mae: 2.1403 - val_loss: 132.8467 - val_mse: 132.8467 - val_mae: 8.9403 - val_fp_mae: 4.2589\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 7.86836\n",
      "Epoch 89/100\n",
      "128/128 - 3s - loss: 31.7521 - mse: 31.7521 - mae: 4.4527 - fp_mae: 2.1677 - val_loss: 128.6799 - val_mse: 128.6799 - val_mae: 8.8637 - val_fp_mae: 5.2667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00089: val_mae did not improve from 7.86836\n",
      "Epoch 90/100\n",
      "128/128 - 3s - loss: 29.8666 - mse: 29.8666 - mae: 4.2772 - fp_mae: 2.2045 - val_loss: 128.1565 - val_mse: 128.1565 - val_mae: 8.8572 - val_fp_mae: 4.8117\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 7.86836\n",
      "Epoch 91/100\n",
      "128/128 - 3s - loss: 30.5733 - mse: 30.5733 - mae: 4.3754 - fp_mae: 2.2097 - val_loss: 125.6034 - val_mse: 125.6034 - val_mae: 8.7172 - val_fp_mae: 4.6291\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 7.86836\n",
      "Epoch 92/100\n",
      "128/128 - 3s - loss: 30.5518 - mse: 30.5518 - mae: 4.3950 - fp_mae: 2.1718 - val_loss: 129.8255 - val_mse: 129.8255 - val_mae: 8.8349 - val_fp_mae: 4.7532\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 7.86836\n",
      "Epoch 93/100\n",
      "128/128 - 3s - loss: 30.6159 - mse: 30.6159 - mae: 4.3573 - fp_mae: 2.1824 - val_loss: 128.6854 - val_mse: 128.6854 - val_mae: 8.8163 - val_fp_mae: 4.2746\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 7.86836\n",
      "Epoch 94/100\n",
      "128/128 - 3s - loss: 29.6041 - mse: 29.6041 - mae: 4.2779 - fp_mae: 2.0391 - val_loss: 130.7536 - val_mse: 130.7536 - val_mae: 8.8625 - val_fp_mae: 5.0546\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 7.86836\n",
      "Epoch 95/100\n",
      "128/128 - 3s - loss: 29.8530 - mse: 29.8530 - mae: 4.2949 - fp_mae: 2.1867 - val_loss: 125.0697 - val_mse: 125.0697 - val_mae: 8.7420 - val_fp_mae: 4.8006\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 7.86836\n",
      "Epoch 96/100\n",
      "128/128 - 3s - loss: 30.5632 - mse: 30.5632 - mae: 4.3874 - fp_mae: 2.2330 - val_loss: 130.3585 - val_mse: 130.3585 - val_mae: 8.9013 - val_fp_mae: 4.9255\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 7.86836\n",
      "Epoch 97/100\n",
      "128/128 - 3s - loss: 29.6785 - mse: 29.6785 - mae: 4.3005 - fp_mae: 2.0896 - val_loss: 131.7657 - val_mse: 131.7657 - val_mae: 9.0006 - val_fp_mae: 4.3372\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 7.86836\n",
      "Epoch 98/100\n",
      "128/128 - 3s - loss: 29.8465 - mse: 29.8465 - mae: 4.3514 - fp_mae: 2.2383 - val_loss: 131.3804 - val_mse: 131.3804 - val_mae: 8.9191 - val_fp_mae: 5.1608\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 7.86836\n",
      "Epoch 99/100\n",
      "128/128 - 3s - loss: 28.8223 - mse: 28.8223 - mae: 4.2613 - fp_mae: 2.1330 - val_loss: 128.5061 - val_mse: 128.5061 - val_mae: 8.8026 - val_fp_mae: 4.5922\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 7.86836\n",
      "Epoch 100/100\n",
      "128/128 - 3s - loss: 28.7277 - mse: 28.7277 - mae: 4.2314 - fp_mae: 2.0512 - val_loss: 131.9703 - val_mse: 131.9703 - val_mae: 8.9712 - val_fp_mae: 4.6055\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 7.86836\n",
      "\n",
      "Lambda: 0 , Time: 0:04:27\n",
      "Train Error(all epochs): 4.231438159942627 \n",
      " [25.101, 22.272, 19.181, 16.22, 13.497, 10.891, 9.096, 7.949, 7.5, 7.261, 7.203, 7.031, 6.945, 6.856, 6.839, 6.706, 6.507, 6.535, 6.499, 6.42, 6.328, 6.294, 6.225, 6.106, 6.015, 5.975, 5.892, 5.861, 5.844, 5.725, 5.716, 5.634, 5.537, 5.606, 5.519, 5.486, 5.387, 5.392, 5.334, 5.292, 5.33, 5.239, 5.214, 5.13, 5.077, 5.032, 5.073, 5.014, 4.944, 4.9, 4.908, 4.948, 4.899, 4.901, 4.803, 4.811, 4.76, 4.745, 4.817, 4.754, 4.65, 4.708, 4.719, 4.687, 4.697, 4.677, 4.607, 4.576, 4.63, 4.513, 4.521, 4.55, 4.548, 4.483, 4.484, 4.516, 4.362, 4.437, 4.467, 4.499, 4.426, 4.439, 4.393, 4.377, 4.41, 4.395, 4.376, 4.371, 4.453, 4.277, 4.375, 4.395, 4.357, 4.278, 4.295, 4.387, 4.301, 4.351, 4.261, 4.231]\n",
      "Train FP Error(all epochs): 2.039116382598877 \n",
      " [23.732, 21.688, 18.976, 16.028, 13.057, 10.069, 7.552, 5.775, 4.648, 4.086, 3.767, 3.643, 3.485, 3.4, 3.39, 3.367, 3.234, 3.247, 3.184, 3.229, 3.118, 3.138, 3.074, 2.997, 2.977, 3.0, 2.873, 2.914, 2.889, 2.886, 2.848, 2.695, 2.723, 2.887, 2.703, 2.65, 2.637, 2.653, 2.689, 2.637, 2.693, 2.538, 2.593, 2.604, 2.537, 2.458, 2.519, 2.478, 2.453, 2.475, 2.376, 2.473, 2.42, 2.494, 2.396, 2.365, 2.319, 2.402, 2.387, 2.39, 2.289, 2.352, 2.33, 2.313, 2.319, 2.337, 2.309, 2.365, 2.237, 2.301, 2.209, 2.245, 2.265, 2.262, 2.23, 2.26, 2.181, 2.213, 2.188, 2.283, 2.248, 2.184, 2.147, 2.189, 2.204, 2.214, 2.206, 2.14, 2.168, 2.205, 2.21, 2.172, 2.182, 2.039, 2.187, 2.233, 2.09, 2.238, 2.133, 2.051]\n",
      "Val Error(all epochs): 7.8683576583862305 \n",
      " [30.659, 20.096, 16.291, 14.294, 16.827, 10.788, 10.738, 8.875, 7.923, 7.872, 7.868, 8.05, 7.943, 7.982, 8.227, 7.963, 8.067, 8.153, 8.092, 7.98, 8.328, 8.091, 8.23, 8.351, 8.158, 8.164, 8.286, 8.25, 8.401, 8.371, 8.371, 8.381, 8.359, 8.618, 8.632, 8.459, 8.491, 8.714, 8.635, 8.591, 8.68, 8.557, 8.525, 8.69, 8.68, 8.79, 8.855, 8.608, 8.687, 8.679, 8.714, 8.518, 8.652, 8.608, 8.622, 8.569, 8.786, 8.855, 8.758, 8.664, 8.747, 8.642, 8.643, 8.617, 8.752, 8.743, 8.879, 8.737, 8.826, 8.9, 8.859, 8.838, 8.757, 8.749, 8.776, 8.826, 8.856, 8.835, 8.855, 8.818, 8.89, 8.792, 8.744, 8.764, 8.913, 9.006, 8.774, 8.94, 8.864, 8.857, 8.717, 8.835, 8.816, 8.862, 8.742, 8.901, 9.001, 8.919, 8.803, 8.971]\n",
      "Val FP Error(all epochs): 3.197732448577881 \n",
      " [30.182, 19.981, 16.174, 13.559, 16.346, 9.708, 9.354, 6.316, 3.626, 4.248, 3.767, 4.995, 3.198, 4.19, 3.349, 3.639, 4.377, 4.875, 4.56, 3.774, 4.665, 3.982, 3.728, 5.073, 4.19, 4.62, 4.149, 4.26, 4.79, 4.202, 4.298, 4.164, 4.8, 5.162, 4.606, 4.61, 4.738, 4.51, 5.251, 4.041, 5.029, 4.498, 4.132, 3.928, 3.948, 5.502, 5.363, 4.425, 5.0, 4.824, 4.688, 4.3, 4.249, 4.839, 5.086, 4.238, 4.861, 4.83, 4.322, 4.503, 4.646, 4.693, 4.4, 4.439, 4.846, 4.426, 5.024, 4.049, 4.603, 4.809, 4.405, 4.007, 4.231, 4.297, 4.598, 4.717, 4.997, 4.068, 4.755, 4.476, 5.187, 4.955, 4.526, 3.897, 5.519, 5.362, 4.296, 4.259, 5.267, 4.812, 4.629, 4.753, 4.275, 5.055, 4.801, 4.926, 4.337, 5.161, 4.592, 4.605]\n",
      "Epoch 1/100\n",
      "128/128 - 4s - loss: 810.2203 - mse: 799.6863 - mae: 24.9409 - fp_mae: 23.6436 - val_loss: 698.9498 - val_mse: 688.9605 - val_mae: 22.9594 - val_fp_mae: 20.9887\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 22.95940, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_1.h5\n",
      "Epoch 2/100\n",
      "128/128 - 3s - loss: 658.0638 - mse: 648.3224 - mae: 22.1491 - fp_mae: 21.6142 - val_loss: 523.3460 - val_mse: 513.8007 - val_mae: 18.9900 - val_fp_mae: 18.0445\n",
      "\n",
      "Epoch 00002: val_mae improved from 22.95940 to 18.99002, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_1.h5\n",
      "Epoch 3/100\n",
      "128/128 - 3s - loss: 504.7403 - mse: 495.4754 - mae: 19.1873 - fp_mae: 18.9803 - val_loss: 452.8108 - val_mse: 443.7572 - val_mae: 17.5813 - val_fp_mae: 17.5211\n",
      "\n",
      "Epoch 00003: val_mae improved from 18.99002 to 17.58130, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_1.h5\n",
      "Epoch 4/100\n",
      "128/128 - 3s - loss: 387.0077 - mse: 378.0013 - mae: 16.2833 - fp_mae: 16.0977 - val_loss: 328.6660 - val_mse: 319.6468 - val_mae: 14.3493 - val_fp_mae: 14.0455\n",
      "\n",
      "Epoch 00004: val_mae improved from 17.58130 to 14.34934, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_1.h5\n",
      "Epoch 5/100\n",
      "128/128 - 3s - loss: 293.6764 - mse: 284.5318 - mae: 13.6274 - fp_mae: 13.1897 - val_loss: 323.6503 - val_mse: 314.4048 - val_mae: 14.3142 - val_fp_mae: 13.9495\n",
      "\n",
      "Epoch 00005: val_mae improved from 14.34934 to 14.31418, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_1.h5\n",
      "Epoch 6/100\n",
      "128/128 - 3s - loss: 214.2105 - mse: 204.7911 - mae: 11.2309 - fp_mae: 10.3253 - val_loss: 361.7618 - val_mse: 352.0378 - val_mae: 15.0170 - val_fp_mae: 14.3324\n",
      "\n",
      "Epoch 00006: val_mae did not improve from 14.31418\n",
      "Epoch 7/100\n",
      "128/128 - 3s - loss: 156.7085 - mse: 146.6826 - mae: 9.3744 - fp_mae: 7.8025 - val_loss: 207.9009 - val_mse: 197.6091 - val_mae: 11.1401 - val_fp_mae: 9.9047\n",
      "\n",
      "Epoch 00007: val_mae improved from 14.31418 to 11.14014, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_1.h5\n",
      "Epoch 8/100\n",
      "128/128 - 3s - loss: 123.3008 - mse: 112.7785 - mae: 8.2459 - fp_mae: 5.9756 - val_loss: 226.2924 - val_mse: 215.5291 - val_mae: 11.4288 - val_fp_mae: 9.7780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: val_mae did not improve from 11.14014\n",
      "Epoch 9/100\n",
      "128/128 - 3s - loss: 109.6442 - mse: 98.7158 - mae: 7.7836 - fp_mae: 4.8340 - val_loss: 131.2450 - val_mse: 120.1389 - val_mae: 8.6457 - val_fp_mae: 6.3359\n",
      "\n",
      "Epoch 00009: val_mae improved from 11.14014 to 8.64571, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_1.h5\n",
      "Epoch 10/100\n",
      "128/128 - 3s - loss: 103.2853 - mse: 92.0403 - mae: 7.5332 - fp_mae: 4.1989 - val_loss: 121.6913 - val_mse: 110.3098 - val_mae: 8.3794 - val_fp_mae: 5.4352\n",
      "\n",
      "Epoch 00010: val_mae improved from 8.64571 to 8.37940, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_1.h5\n",
      "Epoch 11/100\n",
      "128/128 - 3s - loss: 97.8986 - mse: 86.4549 - mae: 7.3467 - fp_mae: 3.8925 - val_loss: 119.6290 - val_mse: 108.0649 - val_mae: 8.2580 - val_fp_mae: 5.2622\n",
      "\n",
      "Epoch 00011: val_mae improved from 8.37940 to 8.25796, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_1.h5\n",
      "Epoch 12/100\n",
      "128/128 - 3s - loss: 94.9275 - mse: 83.2474 - mae: 7.2235 - fp_mae: 3.6905 - val_loss: 121.5752 - val_mse: 109.7705 - val_mae: 8.4675 - val_fp_mae: 3.0699\n",
      "\n",
      "Epoch 00012: val_mae did not improve from 8.25796\n",
      "Epoch 13/100\n",
      "128/128 - 3s - loss: 93.5666 - mse: 81.6666 - mae: 7.1731 - fp_mae: 3.6029 - val_loss: 117.5556 - val_mse: 105.5515 - val_mae: 8.1734 - val_fp_mae: 4.4273\n",
      "\n",
      "Epoch 00013: val_mae improved from 8.25796 to 8.17344, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_1.h5\n",
      "Epoch 14/100\n",
      "128/128 - 3s - loss: 90.5802 - mse: 78.5199 - mae: 7.0048 - fp_mae: 3.5530 - val_loss: 114.5628 - val_mse: 102.4123 - val_mae: 8.1032 - val_fp_mae: 3.8887\n",
      "\n",
      "Epoch 00014: val_mae improved from 8.17344 to 8.10321, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_1.h5\n",
      "Epoch 15/100\n",
      "128/128 - 3s - loss: 87.6560 - mse: 75.4428 - mae: 6.8532 - fp_mae: 3.4196 - val_loss: 114.6831 - val_mse: 102.3667 - val_mae: 8.1093 - val_fp_mae: 4.2390\n",
      "\n",
      "Epoch 00015: val_mae did not improve from 8.10321\n",
      "Epoch 16/100\n",
      "128/128 - 3s - loss: 85.0147 - mse: 72.6047 - mae: 6.7204 - fp_mae: 3.3073 - val_loss: 138.6887 - val_mse: 126.1482 - val_mae: 8.9839 - val_fp_mae: 6.1650\n",
      "\n",
      "Epoch 00016: val_mae did not improve from 8.10321\n",
      "Epoch 17/100\n",
      "128/128 - 3s - loss: 85.1492 - mse: 72.5188 - mae: 6.7242 - fp_mae: 3.3608 - val_loss: 123.6482 - val_mse: 110.8631 - val_mae: 8.4505 - val_fp_mae: 3.2136\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 8.10321\n",
      "Epoch 18/100\n",
      "128/128 - 3s - loss: 81.4728 - mse: 68.6350 - mae: 6.5798 - fp_mae: 3.2960 - val_loss: 125.7726 - val_mse: 112.8517 - val_mae: 8.4876 - val_fp_mae: 5.0523\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 8.10321\n",
      "Epoch 19/100\n",
      "128/128 - 3s - loss: 79.2131 - mse: 66.2229 - mae: 6.4845 - fp_mae: 3.1747 - val_loss: 128.9953 - val_mse: 115.9143 - val_mae: 8.5388 - val_fp_mae: 5.0912\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 8.10321\n",
      "Epoch 20/100\n",
      "128/128 - 3s - loss: 78.6269 - mse: 65.4485 - mae: 6.4109 - fp_mae: 3.1596 - val_loss: 125.8822 - val_mse: 112.6310 - val_mae: 8.5310 - val_fp_mae: 2.8559\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 8.10321\n",
      "Epoch 21/100\n",
      "128/128 - 3s - loss: 76.5170 - mse: 63.1918 - mae: 6.3441 - fp_mae: 3.1481 - val_loss: 129.2478 - val_mse: 115.8608 - val_mae: 8.4758 - val_fp_mae: 5.5128\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 8.10321\n",
      "Epoch 22/100\n",
      "128/128 - 3s - loss: 75.0398 - mse: 61.5758 - mae: 6.2486 - fp_mae: 3.1384 - val_loss: 132.5912 - val_mse: 119.0678 - val_mae: 8.6996 - val_fp_mae: 5.5328\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 8.10321\n",
      "Epoch 23/100\n",
      "128/128 - 3s - loss: 72.6355 - mse: 59.0467 - mae: 6.1115 - fp_mae: 3.0922 - val_loss: 123.9725 - val_mse: 110.3023 - val_mae: 8.3347 - val_fp_mae: 4.6669\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 8.10321\n",
      "Epoch 24/100\n",
      "128/128 - 3s - loss: 71.0453 - mse: 57.3107 - mae: 6.0394 - fp_mae: 2.9462 - val_loss: 125.9030 - val_mse: 112.1239 - val_mae: 8.4078 - val_fp_mae: 4.8690\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 8.10321\n",
      "Epoch 25/100\n",
      "128/128 - 3s - loss: 71.4233 - mse: 57.5815 - mae: 5.9981 - fp_mae: 3.0001 - val_loss: 127.6635 - val_mse: 113.7588 - val_mae: 8.5640 - val_fp_mae: 4.6641\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 8.10321\n",
      "Epoch 26/100\n",
      "128/128 - 3s - loss: 68.6266 - mse: 54.6714 - mae: 5.8708 - fp_mae: 2.9226 - val_loss: 128.4119 - val_mse: 114.4227 - val_mae: 8.4849 - val_fp_mae: 4.6770\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 8.10321\n",
      "Epoch 27/100\n",
      "128/128 - 3s - loss: 68.1839 - mse: 54.1417 - mae: 5.8380 - fp_mae: 2.9398 - val_loss: 125.1428 - val_mse: 111.0189 - val_mae: 8.3577 - val_fp_mae: 4.5466\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 8.10321\n",
      "Epoch 28/100\n",
      "128/128 - 3s - loss: 67.5335 - mse: 53.4018 - mae: 5.7813 - fp_mae: 2.9406 - val_loss: 128.8316 - val_mse: 114.6748 - val_mae: 8.4990 - val_fp_mae: 4.4922\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 8.10321\n",
      "Epoch 29/100\n",
      "128/128 - 3s - loss: 65.8936 - mse: 51.6737 - mae: 5.6973 - fp_mae: 2.7705 - val_loss: 131.2538 - val_mse: 117.0018 - val_mae: 8.5764 - val_fp_mae: 4.8728\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 8.10321\n",
      "Epoch 30/100\n",
      "128/128 - 3s - loss: 65.3989 - mse: 51.1394 - mae: 5.7062 - fp_mae: 2.8505 - val_loss: 130.6194 - val_mse: 116.3204 - val_mae: 8.4903 - val_fp_mae: 3.5823\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 8.10321\n",
      "Epoch 31/100\n",
      "128/128 - 3s - loss: 62.7870 - mse: 48.4465 - mae: 5.5241 - fp_mae: 2.7535 - val_loss: 130.5988 - val_mse: 116.2425 - val_mae: 8.4865 - val_fp_mae: 4.6537\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 8.10321\n",
      "Epoch 32/100\n",
      "128/128 - 3s - loss: 63.2384 - mse: 48.8501 - mae: 5.5193 - fp_mae: 2.7896 - val_loss: 132.0458 - val_mse: 117.6096 - val_mae: 8.6504 - val_fp_mae: 4.9862\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 8.10321\n",
      "Epoch 33/100\n",
      "128/128 - 3s - loss: 63.2671 - mse: 48.7823 - mae: 5.5204 - fp_mae: 2.7295 - val_loss: 130.5039 - val_mse: 115.9815 - val_mae: 8.4681 - val_fp_mae: 3.7864\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 8.10321\n",
      "Epoch 34/100\n",
      "128/128 - 3s - loss: 62.2749 - mse: 47.7206 - mae: 5.4674 - fp_mae: 2.7095 - val_loss: 129.4408 - val_mse: 114.8865 - val_mae: 8.4686 - val_fp_mae: 4.9196\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 8.10321\n",
      "Epoch 35/100\n",
      "128/128 - 3s - loss: 60.7312 - mse: 46.1371 - mae: 5.4019 - fp_mae: 2.6891 - val_loss: 131.4097 - val_mse: 116.8003 - val_mae: 8.4764 - val_fp_mae: 4.7743\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 8.10321\n",
      "Epoch 36/100\n",
      "128/128 - 3s - loss: 60.9331 - mse: 46.3143 - mae: 5.3796 - fp_mae: 2.6796 - val_loss: 146.2463 - val_mse: 131.5864 - val_mae: 9.1239 - val_fp_mae: 5.2628\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 8.10321\n",
      "Epoch 37/100\n",
      "128/128 - 3s - loss: 60.4633 - mse: 45.7861 - mae: 5.3606 - fp_mae: 2.7915 - val_loss: 139.3893 - val_mse: 124.6678 - val_mae: 8.8016 - val_fp_mae: 4.1546\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 8.10321\n",
      "Epoch 38/100\n",
      "128/128 - 3s - loss: 58.7981 - mse: 44.0400 - mae: 5.2634 - fp_mae: 2.4761 - val_loss: 134.8786 - val_mse: 120.1674 - val_mae: 8.6286 - val_fp_mae: 4.5875\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 8.10321\n",
      "Epoch 39/100\n",
      "128/128 - 3s - loss: 59.1576 - mse: 44.4124 - mae: 5.2914 - fp_mae: 2.7359 - val_loss: 134.9467 - val_mse: 120.1850 - val_mae: 8.5760 - val_fp_mae: 4.9944\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 8.10321\n",
      "Epoch 40/100\n",
      "128/128 - 3s - loss: 59.5296 - mse: 44.7496 - mae: 5.2787 - fp_mae: 2.7158 - val_loss: 135.4643 - val_mse: 120.6464 - val_mae: 8.6187 - val_fp_mae: 4.4819\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 8.10321\n",
      "Epoch 41/100\n",
      "128/128 - 3s - loss: 58.1980 - mse: 43.3684 - mae: 5.2205 - fp_mae: 2.5530 - val_loss: 141.7612 - val_mse: 126.9147 - val_mae: 8.8804 - val_fp_mae: 5.6074\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 8.10321\n",
      "Epoch 42/100\n",
      "128/128 - 3s - loss: 57.2628 - mse: 42.4230 - mae: 5.1564 - fp_mae: 2.5525 - val_loss: 128.4014 - val_mse: 113.5815 - val_mae: 8.3070 - val_fp_mae: 4.6954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00042: val_mae did not improve from 8.10321\n",
      "Epoch 43/100\n",
      "128/128 - 3s - loss: 56.8595 - mse: 42.0330 - mae: 5.1080 - fp_mae: 2.5314 - val_loss: 129.9163 - val_mse: 115.0989 - val_mae: 8.5097 - val_fp_mae: 4.5549\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 8.10321\n",
      "Epoch 44/100\n",
      "128/128 - 3s - loss: 55.6316 - mse: 40.8148 - mae: 5.0718 - fp_mae: 2.5876 - val_loss: 133.7121 - val_mse: 118.8668 - val_mae: 8.4847 - val_fp_mae: 4.8640\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 8.10321\n",
      "Epoch 45/100\n",
      "128/128 - 3s - loss: 57.3555 - mse: 42.4857 - mae: 5.1424 - fp_mae: 2.5313 - val_loss: 135.1465 - val_mse: 120.2693 - val_mae: 8.6342 - val_fp_mae: 4.1155\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 8.10321\n",
      "Epoch 46/100\n",
      "128/128 - 3s - loss: 56.3181 - mse: 41.4387 - mae: 5.0846 - fp_mae: 2.5723 - val_loss: 141.6677 - val_mse: 126.7706 - val_mae: 8.9175 - val_fp_mae: 4.2252\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 8.10321\n",
      "Epoch 47/100\n",
      "128/128 - 3s - loss: 56.3356 - mse: 41.4022 - mae: 5.0886 - fp_mae: 2.5315 - val_loss: 140.9627 - val_mse: 126.0258 - val_mae: 8.8750 - val_fp_mae: 4.0984\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 8.10321\n",
      "Epoch 48/100\n",
      "128/128 - 3s - loss: 54.8229 - mse: 39.8902 - mae: 5.0374 - fp_mae: 2.5654 - val_loss: 145.4504 - val_mse: 130.4885 - val_mae: 8.9300 - val_fp_mae: 5.8308\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 8.10321\n",
      "Epoch 49/100\n",
      "128/128 - 3s - loss: 54.0429 - mse: 39.1026 - mae: 4.9453 - fp_mae: 2.4830 - val_loss: 144.1551 - val_mse: 129.2144 - val_mae: 9.0044 - val_fp_mae: 5.5070\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 8.10321\n",
      "Epoch 50/100\n",
      "128/128 - 3s - loss: 54.7333 - mse: 39.8306 - mae: 5.0384 - fp_mae: 2.5047 - val_loss: 134.8685 - val_mse: 119.9625 - val_mae: 8.6664 - val_fp_mae: 4.2590\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 8.10321\n",
      "Epoch 51/100\n",
      "128/128 - 3s - loss: 54.3969 - mse: 39.4803 - mae: 4.9712 - fp_mae: 2.4647 - val_loss: 139.9986 - val_mse: 125.0500 - val_mae: 8.8783 - val_fp_mae: 5.1500\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 8.10321\n",
      "Epoch 52/100\n",
      "128/128 - 3s - loss: 55.3330 - mse: 40.4025 - mae: 4.9894 - fp_mae: 2.4897 - val_loss: 133.0898 - val_mse: 118.1579 - val_mae: 8.6375 - val_fp_mae: 4.0953\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 8.10321\n",
      "Epoch 53/100\n",
      "128/128 - 3s - loss: 54.4118 - mse: 39.4384 - mae: 4.9528 - fp_mae: 2.5076 - val_loss: 137.1948 - val_mse: 122.1692 - val_mae: 8.7961 - val_fp_mae: 4.7589\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 8.10321\n",
      "Epoch 54/100\n",
      "128/128 - 3s - loss: 54.8704 - mse: 39.8493 - mae: 4.9933 - fp_mae: 2.4463 - val_loss: 136.5444 - val_mse: 121.5353 - val_mae: 8.7007 - val_fp_mae: 4.8790\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 8.10321\n",
      "Epoch 55/100\n",
      "128/128 - 3s - loss: 52.3780 - mse: 37.4082 - mae: 4.8384 - fp_mae: 2.4976 - val_loss: 132.4173 - val_mse: 117.4472 - val_mae: 8.5198 - val_fp_mae: 3.9778\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 8.10321\n",
      "Epoch 56/100\n",
      "128/128 - 3s - loss: 51.7212 - mse: 36.7886 - mae: 4.8323 - fp_mae: 2.4024 - val_loss: 139.8781 - val_mse: 124.9635 - val_mae: 8.8466 - val_fp_mae: 4.4662\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 8.10321\n",
      "Epoch 57/100\n",
      "128/128 - 3s - loss: 54.7108 - mse: 39.7855 - mae: 4.9484 - fp_mae: 2.4814 - val_loss: 138.2831 - val_mse: 123.3560 - val_mae: 8.7262 - val_fp_mae: 4.9548\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 8.10321\n",
      "Epoch 58/100\n",
      "128/128 - 3s - loss: 53.9009 - mse: 38.9461 - mae: 4.9361 - fp_mae: 2.5108 - val_loss: 143.2262 - val_mse: 128.2185 - val_mae: 8.8898 - val_fp_mae: 4.4869\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 8.10321\n",
      "Epoch 59/100\n",
      "128/128 - 3s - loss: 53.4737 - mse: 38.4581 - mae: 4.8934 - fp_mae: 2.4236 - val_loss: 136.3038 - val_mse: 121.2438 - val_mae: 8.6686 - val_fp_mae: 4.5579\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 8.10321\n",
      "Epoch 60/100\n",
      "128/128 - 3s - loss: 52.9936 - mse: 37.9560 - mae: 4.8849 - fp_mae: 2.3876 - val_loss: 140.2141 - val_mse: 125.2176 - val_mae: 8.8962 - val_fp_mae: 5.0834\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 8.10321\n",
      "Epoch 61/100\n",
      "128/128 - 3s - loss: 51.5820 - mse: 36.5812 - mae: 4.7506 - fp_mae: 2.4531 - val_loss: 136.4879 - val_mse: 121.4768 - val_mae: 8.7500 - val_fp_mae: 5.1180\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 8.10321\n",
      "Epoch 62/100\n",
      "128/128 - 3s - loss: 51.4732 - mse: 36.4639 - mae: 4.7673 - fp_mae: 2.3217 - val_loss: 140.6221 - val_mse: 125.6379 - val_mae: 8.7960 - val_fp_mae: 4.9584\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 8.10321\n",
      "Epoch 63/100\n",
      "128/128 - 3s - loss: 50.8627 - mse: 35.8899 - mae: 4.7533 - fp_mae: 2.4621 - val_loss: 134.0696 - val_mse: 119.0877 - val_mae: 8.6466 - val_fp_mae: 4.5460\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 8.10321\n",
      "Epoch 64/100\n",
      "128/128 - 3s - loss: 51.2009 - mse: 36.2433 - mae: 4.7960 - fp_mae: 2.3938 - val_loss: 135.6745 - val_mse: 120.7449 - val_mae: 8.6354 - val_fp_mae: 4.5541\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 8.10321\n",
      "Epoch 65/100\n",
      "128/128 - 3s - loss: 52.8391 - mse: 37.8922 - mae: 4.8618 - fp_mae: 2.4241 - val_loss: 143.6086 - val_mse: 128.6300 - val_mae: 8.9462 - val_fp_mae: 5.3111\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 8.10321\n",
      "Epoch 66/100\n",
      "128/128 - 3s - loss: 51.0772 - mse: 36.1168 - mae: 4.7819 - fp_mae: 2.3832 - val_loss: 136.0283 - val_mse: 121.0976 - val_mae: 8.6559 - val_fp_mae: 4.7441\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 8.10321\n",
      "Epoch 67/100\n",
      "128/128 - 3s - loss: 51.0172 - mse: 36.1099 - mae: 4.7658 - fp_mae: 2.4180 - val_loss: 139.8705 - val_mse: 124.9666 - val_mae: 8.8233 - val_fp_mae: 4.8517\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 8.10321\n",
      "Epoch 68/100\n",
      "128/128 - 3s - loss: 50.0520 - mse: 35.1480 - mae: 4.6910 - fp_mae: 2.3311 - val_loss: 140.1546 - val_mse: 125.2660 - val_mae: 8.8014 - val_fp_mae: 4.9560\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 8.10321\n",
      "Epoch 69/100\n",
      "128/128 - 3s - loss: 49.7269 - mse: 34.8492 - mae: 4.6443 - fp_mae: 2.3066 - val_loss: 133.5568 - val_mse: 118.7037 - val_mae: 8.5544 - val_fp_mae: 3.7563\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 8.10321\n",
      "Epoch 70/100\n",
      "128/128 - 3s - loss: 50.1161 - mse: 35.2685 - mae: 4.6435 - fp_mae: 2.3309 - val_loss: 142.2011 - val_mse: 127.3787 - val_mae: 8.8914 - val_fp_mae: 4.8019\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 8.10321\n",
      "Epoch 71/100\n",
      "128/128 - 3s - loss: 50.5354 - mse: 35.7159 - mae: 4.6991 - fp_mae: 2.3568 - val_loss: 142.0074 - val_mse: 127.1804 - val_mae: 8.8708 - val_fp_mae: 4.6976\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 8.10321\n",
      "Epoch 72/100\n",
      "128/128 - 3s - loss: 49.5213 - mse: 34.7068 - mae: 4.6720 - fp_mae: 2.3584 - val_loss: 146.1726 - val_mse: 131.3663 - val_mae: 9.0073 - val_fp_mae: 4.7691\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 8.10321\n",
      "Epoch 73/100\n",
      "128/128 - 3s - loss: 50.7484 - mse: 35.9458 - mae: 4.7353 - fp_mae: 2.3510 - val_loss: 143.9601 - val_mse: 129.1802 - val_mae: 8.9777 - val_fp_mae: 4.4637\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 8.10321\n",
      "Epoch 74/100\n",
      "128/128 - 3s - loss: 49.9148 - mse: 35.1542 - mae: 4.6568 - fp_mae: 2.4013 - val_loss: 138.1456 - val_mse: 123.3601 - val_mae: 8.7840 - val_fp_mae: 4.5703\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 8.10321\n",
      "Epoch 75/100\n",
      "128/128 - 3s - loss: 49.7870 - mse: 35.0264 - mae: 4.6785 - fp_mae: 2.3344 - val_loss: 136.0994 - val_mse: 121.3544 - val_mae: 8.6359 - val_fp_mae: 4.7797\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 8.10321\n",
      "Epoch 76/100\n",
      "128/128 - 3s - loss: 50.0480 - mse: 35.3131 - mae: 4.7009 - fp_mae: 2.3390 - val_loss: 140.7300 - val_mse: 125.9843 - val_mae: 8.8120 - val_fp_mae: 4.7090\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 8.10321\n",
      "Epoch 77/100\n",
      "128/128 - 3s - loss: 50.3474 - mse: 35.5790 - mae: 4.6727 - fp_mae: 2.3060 - val_loss: 142.2944 - val_mse: 127.5416 - val_mae: 8.8877 - val_fp_mae: 4.3706\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 8.10321\n",
      "Epoch 78/100\n",
      "128/128 - 3s - loss: 49.2146 - mse: 34.4666 - mae: 4.6098 - fp_mae: 2.3958 - val_loss: 140.5143 - val_mse: 125.7344 - val_mae: 8.8098 - val_fp_mae: 5.0206\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 8.10321\n",
      "Epoch 79/100\n",
      "128/128 - 3s - loss: 49.0096 - mse: 34.2351 - mae: 4.6219 - fp_mae: 2.2538 - val_loss: 141.2944 - val_mse: 126.5237 - val_mae: 8.8863 - val_fp_mae: 4.2548\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 8.10321\n",
      "Epoch 80/100\n",
      "128/128 - 3s - loss: 49.1895 - mse: 34.4434 - mae: 4.6384 - fp_mae: 2.4127 - val_loss: 146.8966 - val_mse: 132.1243 - val_mae: 9.1254 - val_fp_mae: 5.6598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00080: val_mae did not improve from 8.10321\n",
      "Epoch 81/100\n",
      "128/128 - 3s - loss: 49.6583 - mse: 34.9055 - mae: 4.6411 - fp_mae: 2.3013 - val_loss: 137.7462 - val_mse: 122.9888 - val_mae: 8.7787 - val_fp_mae: 4.5331\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 8.10321\n",
      "Epoch 82/100\n",
      "128/128 - 3s - loss: 49.3888 - mse: 34.6267 - mae: 4.6411 - fp_mae: 2.3451 - val_loss: 140.1094 - val_mse: 125.3258 - val_mae: 8.8084 - val_fp_mae: 4.1580\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 8.10321\n",
      "Epoch 83/100\n",
      "128/128 - 3s - loss: 49.2156 - mse: 34.4598 - mae: 4.6685 - fp_mae: 2.3125 - val_loss: 137.5227 - val_mse: 122.7521 - val_mae: 8.7734 - val_fp_mae: 4.6923\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 8.10321\n",
      "Epoch 84/100\n",
      "128/128 - 3s - loss: 49.2741 - mse: 34.4956 - mae: 4.6029 - fp_mae: 2.2936 - val_loss: 143.4113 - val_mse: 128.6158 - val_mae: 8.9848 - val_fp_mae: 4.7542\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 8.10321\n",
      "Epoch 85/100\n",
      "128/128 - 3s - loss: 49.0000 - mse: 34.2278 - mae: 4.6347 - fp_mae: 2.3412 - val_loss: 145.7577 - val_mse: 130.9921 - val_mae: 8.9542 - val_fp_mae: 4.9133\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 8.10321\n",
      "Epoch 86/100\n",
      "128/128 - 3s - loss: 48.9307 - mse: 34.1754 - mae: 4.6408 - fp_mae: 2.3697 - val_loss: 135.7611 - val_mse: 120.9895 - val_mae: 8.6236 - val_fp_mae: 4.3471\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 8.10321\n",
      "Epoch 87/100\n",
      "128/128 - 3s - loss: 49.2604 - mse: 34.5235 - mae: 4.6625 - fp_mae: 2.3553 - val_loss: 138.2387 - val_mse: 123.4946 - val_mae: 8.6893 - val_fp_mae: 4.7148\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 8.10321\n",
      "Epoch 88/100\n",
      "128/128 - 3s - loss: 48.5038 - mse: 33.7698 - mae: 4.5824 - fp_mae: 2.3096 - val_loss: 135.8522 - val_mse: 121.0928 - val_mae: 8.6342 - val_fp_mae: 5.0008\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 8.10321\n",
      "Epoch 89/100\n",
      "128/128 - 3s - loss: 48.8441 - mse: 34.0863 - mae: 4.5890 - fp_mae: 2.2532 - val_loss: 137.1469 - val_mse: 122.4004 - val_mae: 8.7011 - val_fp_mae: 4.7055\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 8.10321\n",
      "Epoch 90/100\n",
      "128/128 - 3s - loss: 48.0704 - mse: 33.3305 - mae: 4.5621 - fp_mae: 2.2438 - val_loss: 143.2952 - val_mse: 128.5943 - val_mae: 8.8702 - val_fp_mae: 4.8076\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 8.10321\n",
      "Epoch 91/100\n",
      "128/128 - 3s - loss: 47.6429 - mse: 32.9534 - mae: 4.5648 - fp_mae: 2.3217 - val_loss: 133.0172 - val_mse: 118.3587 - val_mae: 8.4946 - val_fp_mae: 4.6337\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 8.10321\n",
      "Epoch 92/100\n",
      "128/128 - 3s - loss: 47.3316 - mse: 32.6877 - mae: 4.5075 - fp_mae: 2.2301 - val_loss: 146.3961 - val_mse: 131.7581 - val_mae: 9.0279 - val_fp_mae: 4.6936\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 8.10321\n",
      "Epoch 93/100\n",
      "128/128 - 3s - loss: 47.1608 - mse: 32.5481 - mae: 4.5225 - fp_mae: 2.3079 - val_loss: 139.3581 - val_mse: 124.7626 - val_mae: 8.7890 - val_fp_mae: 4.9846\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 8.10321\n",
      "Epoch 94/100\n",
      "128/128 - 3s - loss: 48.4960 - mse: 33.9097 - mae: 4.6071 - fp_mae: 2.3680 - val_loss: 135.3701 - val_mse: 120.7676 - val_mae: 8.6201 - val_fp_mae: 4.6684\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 8.10321\n",
      "Epoch 95/100\n",
      "128/128 - 3s - loss: 47.6705 - mse: 33.0842 - mae: 4.5380 - fp_mae: 2.2360 - val_loss: 140.4188 - val_mse: 125.8397 - val_mae: 8.8795 - val_fp_mae: 4.4541\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 8.10321\n",
      "Epoch 96/100\n",
      "128/128 - 3s - loss: 47.8964 - mse: 33.3465 - mae: 4.5562 - fp_mae: 2.3219 - val_loss: 148.0704 - val_mse: 133.4896 - val_mae: 9.0827 - val_fp_mae: 5.6912\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 8.10321\n",
      "Epoch 97/100\n",
      "128/128 - 3s - loss: 47.5619 - mse: 33.0000 - mae: 4.5300 - fp_mae: 2.2389 - val_loss: 140.8688 - val_mse: 126.3246 - val_mae: 8.8218 - val_fp_mae: 5.2036\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 8.10321\n",
      "Epoch 98/100\n",
      "128/128 - 3s - loss: 48.4918 - mse: 33.9542 - mae: 4.5931 - fp_mae: 2.3258 - val_loss: 137.4844 - val_mse: 122.9299 - val_mae: 8.7125 - val_fp_mae: 4.1868\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 8.10321\n",
      "Epoch 99/100\n",
      "128/128 - 3s - loss: 49.4961 - mse: 34.9272 - mae: 4.6680 - fp_mae: 2.3763 - val_loss: 137.3386 - val_mse: 122.7275 - val_mae: 8.6212 - val_fp_mae: 4.9438\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 8.10321\n",
      "Epoch 100/100\n",
      "128/128 - 3s - loss: 47.3866 - mse: 32.7774 - mae: 4.5402 - fp_mae: 2.2638 - val_loss: 143.3066 - val_mse: 128.7239 - val_mae: 8.8782 - val_fp_mae: 5.0955\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 8.10321\n",
      "\n",
      "Lambda: 0.1 , Time: 0:04:27\n",
      "Train Error(all epochs): 4.50748348236084 \n",
      " [24.941, 22.149, 19.187, 16.283, 13.627, 11.231, 9.374, 8.246, 7.784, 7.533, 7.347, 7.224, 7.173, 7.005, 6.853, 6.72, 6.724, 6.58, 6.484, 6.411, 6.344, 6.249, 6.112, 6.039, 5.998, 5.871, 5.838, 5.781, 5.697, 5.706, 5.524, 5.519, 5.52, 5.467, 5.402, 5.38, 5.361, 5.263, 5.291, 5.279, 5.22, 5.156, 5.108, 5.072, 5.142, 5.085, 5.089, 5.037, 4.945, 5.038, 4.971, 4.989, 4.953, 4.993, 4.838, 4.832, 4.948, 4.936, 4.893, 4.885, 4.751, 4.767, 4.753, 4.796, 4.862, 4.782, 4.766, 4.691, 4.644, 4.643, 4.699, 4.672, 4.735, 4.657, 4.679, 4.701, 4.673, 4.61, 4.622, 4.638, 4.641, 4.641, 4.669, 4.603, 4.635, 4.641, 4.662, 4.582, 4.589, 4.562, 4.565, 4.507, 4.523, 4.607, 4.538, 4.556, 4.53, 4.593, 4.668, 4.54]\n",
      "Train FP Error(all epochs): 2.2300734519958496 \n",
      " [23.644, 21.614, 18.98, 16.098, 13.19, 10.325, 7.802, 5.976, 4.834, 4.199, 3.893, 3.691, 3.603, 3.553, 3.42, 3.307, 3.361, 3.296, 3.175, 3.16, 3.148, 3.138, 3.092, 2.946, 3.0, 2.923, 2.94, 2.941, 2.771, 2.85, 2.753, 2.79, 2.73, 2.709, 2.689, 2.68, 2.791, 2.476, 2.736, 2.716, 2.553, 2.552, 2.531, 2.588, 2.531, 2.572, 2.531, 2.565, 2.483, 2.505, 2.465, 2.49, 2.508, 2.446, 2.498, 2.402, 2.481, 2.511, 2.424, 2.388, 2.453, 2.322, 2.462, 2.394, 2.424, 2.383, 2.418, 2.331, 2.307, 2.331, 2.357, 2.358, 2.351, 2.401, 2.334, 2.339, 2.306, 2.396, 2.254, 2.413, 2.301, 2.345, 2.313, 2.294, 2.341, 2.37, 2.355, 2.31, 2.253, 2.244, 2.322, 2.23, 2.308, 2.368, 2.236, 2.322, 2.239, 2.326, 2.376, 2.264]\n",
      "Val Error(all epochs): 8.103209495544434 \n",
      " [22.959, 18.99, 17.581, 14.349, 14.314, 15.017, 11.14, 11.429, 8.646, 8.379, 8.258, 8.467, 8.173, 8.103, 8.109, 8.984, 8.451, 8.488, 8.539, 8.531, 8.476, 8.7, 8.335, 8.408, 8.564, 8.485, 8.358, 8.499, 8.576, 8.49, 8.487, 8.65, 8.468, 8.469, 8.476, 9.124, 8.802, 8.629, 8.576, 8.619, 8.88, 8.307, 8.51, 8.485, 8.634, 8.917, 8.875, 8.93, 9.004, 8.666, 8.878, 8.637, 8.796, 8.701, 8.52, 8.847, 8.726, 8.89, 8.669, 8.896, 8.75, 8.796, 8.647, 8.635, 8.946, 8.656, 8.823, 8.801, 8.554, 8.891, 8.871, 9.007, 8.978, 8.784, 8.636, 8.812, 8.888, 8.81, 8.886, 9.125, 8.779, 8.808, 8.773, 8.985, 8.954, 8.624, 8.689, 8.634, 8.701, 8.87, 8.495, 9.028, 8.789, 8.62, 8.879, 9.083, 8.822, 8.712, 8.621, 8.878]\n",
      "Val FP Error(all epochs): 2.8558971881866455 \n",
      " [20.989, 18.044, 17.521, 14.046, 13.95, 14.332, 9.905, 9.778, 6.336, 5.435, 5.262, 3.07, 4.427, 3.889, 4.239, 6.165, 3.214, 5.052, 5.091, 2.856, 5.513, 5.533, 4.667, 4.869, 4.664, 4.677, 4.547, 4.492, 4.873, 3.582, 4.654, 4.986, 3.786, 4.92, 4.774, 5.263, 4.155, 4.587, 4.994, 4.482, 5.607, 4.695, 4.555, 4.864, 4.115, 4.225, 4.098, 5.831, 5.507, 4.259, 5.15, 4.095, 4.759, 4.879, 3.978, 4.466, 4.955, 4.487, 4.558, 5.083, 5.118, 4.958, 4.546, 4.554, 5.311, 4.744, 4.852, 4.956, 3.756, 4.802, 4.698, 4.769, 4.464, 4.57, 4.78, 4.709, 4.371, 5.021, 4.255, 5.66, 4.533, 4.158, 4.692, 4.754, 4.913, 4.347, 4.715, 5.001, 4.705, 4.808, 4.634, 4.694, 4.985, 4.668, 4.454, 5.691, 5.204, 4.187, 4.944, 5.095]\n",
      "Epoch 1/100\n",
      "128/128 - 4s - loss: 873.1002 - mse: 811.0132 - mae: 25.1618 - fp_mae: 23.7737 - val_loss: 1051.7069 - val_mse: 1014.8297 - val_mae: 28.0757 - val_fp_mae: 27.2012\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 28.07567, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_2.h5\n",
      "Epoch 2/100\n",
      "128/128 - 3s - loss: 690.4482 - mse: 660.3589 - mae: 22.3780 - fp_mae: 21.7726 - val_loss: 834.5798 - val_mse: 809.5068 - val_mae: 24.6727 - val_fp_mae: 24.3446\n",
      "\n",
      "Epoch 00002: val_mae improved from 28.07567 to 24.67273, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_2.h5\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 - 3s - loss: 526.7018 - mse: 503.4430 - mae: 19.2938 - fp_mae: 19.0690 - val_loss: 446.3383 - val_mse: 423.7961 - val_mae: 16.8386 - val_fp_mae: 16.3906\n",
      "\n",
      "Epoch 00003: val_mae improved from 24.67273 to 16.83858, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_2.h5\n",
      "Epoch 4/100\n",
      "128/128 - 3s - loss: 402.3650 - mse: 380.4041 - mae: 16.3178 - fp_mae: 16.1340 - val_loss: 344.8375 - val_mse: 323.0268 - val_mae: 14.3461 - val_fp_mae: 12.3121\n",
      "\n",
      "Epoch 00004: val_mae improved from 16.83858 to 14.34606, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_2.h5\n",
      "Epoch 5/100\n",
      "128/128 - 3s - loss: 310.8454 - mse: 288.5320 - mae: 13.6650 - fp_mae: 13.2200 - val_loss: 276.5050 - val_mse: 253.9327 - val_mae: 12.3911 - val_fp_mae: 11.6900\n",
      "\n",
      "Epoch 00005: val_mae improved from 14.34606 to 12.39113, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_2.h5\n",
      "Epoch 6/100\n",
      "128/128 - 3s - loss: 239.4829 - mse: 216.0060 - mae: 11.4031 - fp_mae: 10.4474 - val_loss: 257.8947 - val_mse: 233.6973 - val_mae: 11.8286 - val_fp_mae: 11.0184\n",
      "\n",
      "Epoch 00006: val_mae improved from 12.39113 to 11.82858, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_2.h5\n",
      "Epoch 7/100\n",
      "128/128 - 3s - loss: 189.3131 - mse: 164.5923 - mae: 9.7061 - fp_mae: 8.0463 - val_loss: 198.1020 - val_mse: 173.2486 - val_mae: 9.9376 - val_fp_mae: 8.2123\n",
      "\n",
      "Epoch 00007: val_mae improved from 11.82858 to 9.93756, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_2.h5\n",
      "Epoch 8/100\n",
      "128/128 - 3s - loss: 163.3503 - mse: 138.0147 - mae: 8.8983 - fp_mae: 6.4381 - val_loss: 171.8243 - val_mse: 146.0567 - val_mae: 9.1113 - val_fp_mae: 6.1560\n",
      "\n",
      "Epoch 00008: val_mae improved from 9.93756 to 9.11125, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_2.h5\n",
      "Epoch 9/100\n",
      "128/128 - 3s - loss: 152.2568 - mse: 125.7684 - mae: 8.6076 - fp_mae: 5.4395 - val_loss: 176.9007 - val_mse: 149.6735 - val_mae: 9.5354 - val_fp_mae: 6.3884\n",
      "\n",
      "Epoch 00009: val_mae did not improve from 9.11125\n",
      "Epoch 10/100\n",
      "128/128 - 3s - loss: 151.4015 - mse: 124.0302 - mae: 8.6121 - fp_mae: 5.0059 - val_loss: 155.5804 - val_mse: 127.8326 - val_mae: 8.9064 - val_fp_mae: 5.0444\n",
      "\n",
      "Epoch 00010: val_mae improved from 9.11125 to 8.90636, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_2.h5\n",
      "Epoch 11/100\n",
      "128/128 - 3s - loss: 146.2277 - mse: 118.3449 - mae: 8.5067 - fp_mae: 4.6502 - val_loss: 194.3144 - val_mse: 166.4076 - val_mae: 10.0894 - val_fp_mae: 7.3166\n",
      "\n",
      "Epoch 00011: val_mae did not improve from 8.90636\n",
      "Epoch 12/100\n",
      "128/128 - 3s - loss: 143.1302 - mse: 114.4052 - mae: 8.3199 - fp_mae: 4.3959 - val_loss: 156.6984 - val_mse: 127.5580 - val_mae: 9.1218 - val_fp_mae: 5.0434\n",
      "\n",
      "Epoch 00012: val_mae did not improve from 8.90636\n",
      "Epoch 13/100\n",
      "128/128 - 3s - loss: 138.1824 - mse: 108.1470 - mae: 8.1036 - fp_mae: 4.2086 - val_loss: 147.9912 - val_mse: 117.1957 - val_mae: 8.5738 - val_fp_mae: 4.8425\n",
      "\n",
      "Epoch 00013: val_mae improved from 8.90636 to 8.57375, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_2.h5\n",
      "Epoch 14/100\n",
      "128/128 - 3s - loss: 134.6133 - mse: 103.9749 - mae: 8.0094 - fp_mae: 4.2452 - val_loss: 219.1268 - val_mse: 187.9090 - val_mae: 10.8283 - val_fp_mae: 9.1770\n",
      "\n",
      "Epoch 00014: val_mae did not improve from 8.57375\n",
      "Epoch 15/100\n",
      "128/128 - 3s - loss: 131.7429 - mse: 100.5349 - mae: 7.8785 - fp_mae: 4.1641 - val_loss: 141.0489 - val_mse: 109.6817 - val_mae: 8.3487 - val_fp_mae: 4.9339\n",
      "\n",
      "Epoch 00015: val_mae improved from 8.57375 to 8.34867, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_2.h5\n",
      "Epoch 16/100\n",
      "128/128 - 3s - loss: 128.9521 - mse: 97.6196 - mae: 7.7641 - fp_mae: 4.0590 - val_loss: 200.8798 - val_mse: 169.1767 - val_mae: 10.2119 - val_fp_mae: 7.8286\n",
      "\n",
      "Epoch 00016: val_mae did not improve from 8.34867\n",
      "Epoch 17/100\n",
      "128/128 - 3s - loss: 128.3560 - mse: 96.7731 - mae: 7.7194 - fp_mae: 3.9862 - val_loss: 289.7957 - val_mse: 258.1888 - val_mae: 12.4283 - val_fp_mae: 10.7615\n",
      "\n",
      "Epoch 00017: val_mae did not improve from 8.34867\n",
      "Epoch 18/100\n",
      "128/128 - 3s - loss: 126.5350 - mse: 94.9269 - mae: 7.6782 - fp_mae: 4.0264 - val_loss: 151.8257 - val_mse: 120.3543 - val_mae: 8.7212 - val_fp_mae: 6.0593\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 8.34867\n",
      "Epoch 19/100\n",
      "128/128 - 3s - loss: 123.7506 - mse: 92.5533 - mae: 7.5997 - fp_mae: 3.9843 - val_loss: 142.6572 - val_mse: 111.4629 - val_mae: 8.5546 - val_fp_mae: 2.8830\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 8.34867\n",
      "Epoch 20/100\n",
      "128/128 - 3s - loss: 123.3182 - mse: 92.1097 - mae: 7.5394 - fp_mae: 3.9172 - val_loss: 133.7604 - val_mse: 102.4529 - val_mae: 7.9826 - val_fp_mae: 4.0520\n",
      "\n",
      "Epoch 00020: val_mae improved from 8.34867 to 7.98259, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_2.h5\n",
      "Epoch 21/100\n",
      "128/128 - 3s - loss: 121.6598 - mse: 90.2067 - mae: 7.5134 - fp_mae: 3.9409 - val_loss: 133.6327 - val_mse: 102.1929 - val_mae: 8.0551 - val_fp_mae: 4.2575\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 7.98259\n",
      "Epoch 22/100\n",
      "128/128 - 3s - loss: 121.6163 - mse: 90.3515 - mae: 7.5156 - fp_mae: 3.9453 - val_loss: 133.2233 - val_mse: 101.9460 - val_mae: 8.1250 - val_fp_mae: 3.9949\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 7.98259\n",
      "Epoch 23/100\n",
      "128/128 - 3s - loss: 120.6021 - mse: 89.1437 - mae: 7.4367 - fp_mae: 3.9144 - val_loss: 168.6385 - val_mse: 136.9722 - val_mae: 9.2103 - val_fp_mae: 7.4647\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 7.98259\n",
      "Epoch 24/100\n",
      "128/128 - 3s - loss: 119.5336 - mse: 87.8963 - mae: 7.3809 - fp_mae: 3.8188 - val_loss: 144.4691 - val_mse: 112.8228 - val_mae: 8.3175 - val_fp_mae: 5.6074\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 7.98259\n",
      "Epoch 25/100\n",
      "128/128 - 3s - loss: 118.8537 - mse: 87.4517 - mae: 7.3535 - fp_mae: 3.8519 - val_loss: 173.7265 - val_mse: 142.1301 - val_mae: 9.4432 - val_fp_mae: 6.7073\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 7.98259\n",
      "Epoch 26/100\n",
      "128/128 - 3s - loss: 116.3213 - mse: 85.0509 - mae: 7.3014 - fp_mae: 3.8207 - val_loss: 142.2845 - val_mse: 111.0198 - val_mae: 8.6471 - val_fp_mae: 3.0022\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 7.98259\n",
      "Epoch 27/100\n",
      "128/128 - 3s - loss: 117.2811 - mse: 85.8776 - mae: 7.3470 - fp_mae: 3.7822 - val_loss: 138.6223 - val_mse: 107.0547 - val_mae: 8.1733 - val_fp_mae: 3.6357\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 7.98259\n",
      "Epoch 28/100\n",
      "128/128 - 3s - loss: 114.5519 - mse: 83.1391 - mae: 7.1897 - fp_mae: 3.7627 - val_loss: 149.6869 - val_mse: 117.9429 - val_mae: 8.6820 - val_fp_mae: 4.2232\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 7.98259\n",
      "Epoch 29/100\n",
      "128/128 - 3s - loss: 115.9482 - mse: 84.1078 - mae: 7.2526 - fp_mae: 3.7601 - val_loss: 158.1870 - val_mse: 126.3010 - val_mae: 8.8277 - val_fp_mae: 5.7544\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 7.98259\n",
      "Epoch 30/100\n",
      "128/128 - 3s - loss: 112.7900 - mse: 81.0244 - mae: 7.1398 - fp_mae: 3.6709 - val_loss: 146.4254 - val_mse: 114.6618 - val_mae: 8.5755 - val_fp_mae: 2.8460\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 7.98259\n",
      "Epoch 31/100\n",
      "128/128 - 3s - loss: 113.9352 - mse: 82.1314 - mae: 7.1504 - fp_mae: 3.6928 - val_loss: 160.8182 - val_mse: 129.1050 - val_mae: 8.9712 - val_fp_mae: 6.5114\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 7.98259\n",
      "Epoch 32/100\n",
      "128/128 - 3s - loss: 111.0793 - mse: 79.4256 - mae: 7.0673 - fp_mae: 3.7146 - val_loss: 140.8986 - val_mse: 109.2611 - val_mae: 8.2305 - val_fp_mae: 4.1696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00032: val_mae did not improve from 7.98259\n",
      "Epoch 33/100\n",
      "128/128 - 3s - loss: 112.8104 - mse: 80.9878 - mae: 7.1308 - fp_mae: 3.7652 - val_loss: 249.7331 - val_mse: 217.3253 - val_mae: 11.4378 - val_fp_mae: 9.2560\n",
      "\n",
      "Epoch 00033: val_mae did not improve from 7.98259\n",
      "Epoch 34/100\n",
      "128/128 - 3s - loss: 111.7990 - mse: 79.7006 - mae: 7.0288 - fp_mae: 3.6424 - val_loss: 155.9419 - val_mse: 123.6673 - val_mae: 8.7758 - val_fp_mae: 6.4264\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 7.98259\n",
      "Epoch 35/100\n",
      "128/128 - 3s - loss: 110.8719 - mse: 78.6354 - mae: 6.9936 - fp_mae: 3.5675 - val_loss: 137.9874 - val_mse: 105.9331 - val_mae: 8.2171 - val_fp_mae: 4.2188\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 7.98259\n",
      "Epoch 36/100\n",
      "128/128 - 3s - loss: 109.0513 - mse: 77.0380 - mae: 6.9421 - fp_mae: 3.6196 - val_loss: 148.7013 - val_mse: 116.6757 - val_mae: 8.4591 - val_fp_mae: 4.8561\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 7.98259\n",
      "Epoch 37/100\n",
      "128/128 - 3s - loss: 108.4610 - mse: 76.4732 - mae: 6.9075 - fp_mae: 3.5503 - val_loss: 143.7905 - val_mse: 111.9355 - val_mae: 8.4768 - val_fp_mae: 4.2141\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 7.98259\n",
      "Epoch 38/100\n",
      "128/128 - 3s - loss: 107.8230 - mse: 75.7176 - mae: 6.8771 - fp_mae: 3.5503 - val_loss: 145.0762 - val_mse: 112.8773 - val_mae: 8.5520 - val_fp_mae: 3.6777\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 7.98259\n",
      "Epoch 39/100\n",
      "128/128 - 3s - loss: 108.6343 - mse: 76.3515 - mae: 6.9200 - fp_mae: 3.5562 - val_loss: 179.0123 - val_mse: 146.9297 - val_mae: 9.4800 - val_fp_mae: 6.9712\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 7.98259\n",
      "Epoch 40/100\n",
      "128/128 - 3s - loss: 106.6560 - mse: 74.5181 - mae: 6.8043 - fp_mae: 3.5430 - val_loss: 196.8942 - val_mse: 164.5255 - val_mae: 10.1290 - val_fp_mae: 7.6921\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 7.98259\n",
      "Epoch 41/100\n",
      "128/128 - 3s - loss: 107.0540 - mse: 74.7277 - mae: 6.8020 - fp_mae: 3.5696 - val_loss: 156.1759 - val_mse: 123.4228 - val_mae: 8.8270 - val_fp_mae: 4.9616\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 7.98259\n",
      "Epoch 42/100\n",
      "128/128 - 3s - loss: 105.8398 - mse: 73.3171 - mae: 6.7783 - fp_mae: 3.5383 - val_loss: 147.4995 - val_mse: 114.9035 - val_mae: 8.4236 - val_fp_mae: 4.7754\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 7.98259\n",
      "Epoch 43/100\n",
      "128/128 - 3s - loss: 106.3686 - mse: 73.6471 - mae: 6.7901 - fp_mae: 3.4089 - val_loss: 176.1863 - val_mse: 143.4356 - val_mae: 9.4244 - val_fp_mae: 7.1453\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 7.98259\n",
      "Epoch 44/100\n",
      "128/128 - 3s - loss: 104.7929 - mse: 72.1053 - mae: 6.6877 - fp_mae: 3.4696 - val_loss: 152.3813 - val_mse: 119.4688 - val_mae: 8.5564 - val_fp_mae: 5.6303\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 7.98259\n",
      "Epoch 45/100\n",
      "128/128 - 3s - loss: 103.4384 - mse: 70.3302 - mae: 6.6346 - fp_mae: 3.3277 - val_loss: 304.1920 - val_mse: 271.2884 - val_mae: 13.4043 - val_fp_mae: 12.5002\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 7.98259\n",
      "Epoch 46/100\n",
      "128/128 - 3s - loss: 103.5366 - mse: 70.7258 - mae: 6.6437 - fp_mae: 3.5079 - val_loss: 169.3301 - val_mse: 136.2042 - val_mae: 9.1768 - val_fp_mae: 6.0769\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 7.98259\n",
      "Epoch 47/100\n",
      "128/128 - 3s - loss: 102.8394 - mse: 69.9065 - mae: 6.6271 - fp_mae: 3.4492 - val_loss: 251.7939 - val_mse: 218.5475 - val_mae: 11.6433 - val_fp_mae: 10.2330\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 7.98259\n",
      "Epoch 48/100\n",
      "128/128 - 3s - loss: 102.5152 - mse: 69.4826 - mae: 6.5478 - fp_mae: 3.3321 - val_loss: 146.3368 - val_mse: 113.2385 - val_mae: 8.4637 - val_fp_mae: 5.1277\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 7.98259\n",
      "Epoch 49/100\n",
      "128/128 - 3s - loss: 101.8917 - mse: 69.0304 - mae: 6.5339 - fp_mae: 3.3730 - val_loss: 165.2217 - val_mse: 131.9285 - val_mae: 9.0722 - val_fp_mae: 6.7261\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 7.98259\n",
      "Epoch 50/100\n",
      "128/128 - 3s - loss: 101.7965 - mse: 68.8983 - mae: 6.5656 - fp_mae: 3.4327 - val_loss: 153.3193 - val_mse: 120.4354 - val_mae: 8.6213 - val_fp_mae: 5.5547\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 7.98259\n",
      "Epoch 51/100\n",
      "128/128 - 3s - loss: 101.5342 - mse: 68.6989 - mae: 6.5533 - fp_mae: 3.3170 - val_loss: 166.3486 - val_mse: 133.5230 - val_mae: 9.0039 - val_fp_mae: 6.6436\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 7.98259\n",
      "Epoch 52/100\n",
      "128/128 - 3s - loss: 100.7545 - mse: 67.7662 - mae: 6.5400 - fp_mae: 3.3463 - val_loss: 215.0529 - val_mse: 181.8927 - val_mae: 10.6095 - val_fp_mae: 8.5395\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 7.98259\n",
      "Epoch 53/100\n",
      "128/128 - 3s - loss: 98.4776 - mse: 65.5924 - mae: 6.3767 - fp_mae: 3.3160 - val_loss: 176.8411 - val_mse: 143.9640 - val_mae: 9.3529 - val_fp_mae: 6.7616\n",
      "\n",
      "Epoch 00053: val_mae did not improve from 7.98259\n",
      "Epoch 54/100\n",
      "128/128 - 3s - loss: 100.6218 - mse: 67.7675 - mae: 6.5355 - fp_mae: 3.2799 - val_loss: 152.9544 - val_mse: 120.0437 - val_mae: 8.5639 - val_fp_mae: 5.3239\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 7.98259\n",
      "Epoch 55/100\n",
      "128/128 - 3s - loss: 98.2079 - mse: 65.6063 - mae: 6.3866 - fp_mae: 3.3757 - val_loss: 144.6024 - val_mse: 111.8382 - val_mae: 8.2445 - val_fp_mae: 4.6699\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 7.98259\n",
      "Epoch 56/100\n",
      "128/128 - 3s - loss: 98.3096 - mse: 65.6142 - mae: 6.3801 - fp_mae: 3.2479 - val_loss: 141.1994 - val_mse: 108.3361 - val_mae: 8.1664 - val_fp_mae: 3.9932\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 7.98259\n",
      "Epoch 57/100\n",
      "128/128 - 3s - loss: 97.9944 - mse: 65.2540 - mae: 6.3793 - fp_mae: 3.2532 - val_loss: 149.0571 - val_mse: 116.2125 - val_mae: 8.5154 - val_fp_mae: 3.5851\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 7.98259\n",
      "Epoch 58/100\n",
      "128/128 - 3s - loss: 97.3833 - mse: 64.4525 - mae: 6.3465 - fp_mae: 3.2457 - val_loss: 144.1507 - val_mse: 111.0283 - val_mae: 8.3918 - val_fp_mae: 3.4209\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 7.98259\n",
      "Epoch 59/100\n",
      "128/128 - 3s - loss: 97.6829 - mse: 64.5538 - mae: 6.3430 - fp_mae: 3.1898 - val_loss: 152.1371 - val_mse: 119.1578 - val_mae: 8.5938 - val_fp_mae: 5.1763\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 7.98259\n",
      "Epoch 60/100\n",
      "128/128 - 3s - loss: 95.8456 - mse: 62.8341 - mae: 6.2950 - fp_mae: 3.2468 - val_loss: 156.3224 - val_mse: 123.2145 - val_mae: 8.6882 - val_fp_mae: 5.9501\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 7.98259\n",
      "Epoch 61/100\n",
      "128/128 - 3s - loss: 98.4106 - mse: 65.1979 - mae: 6.3741 - fp_mae: 3.2567 - val_loss: 151.8222 - val_mse: 118.4245 - val_mae: 8.6172 - val_fp_mae: 4.2570\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 7.98259\n",
      "Epoch 62/100\n",
      "128/128 - 3s - loss: 97.1976 - mse: 63.9064 - mae: 6.3139 - fp_mae: 3.2646 - val_loss: 191.9562 - val_mse: 158.6057 - val_mae: 9.8381 - val_fp_mae: 2.8326\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 7.98259\n",
      "Epoch 63/100\n",
      "128/128 - 3s - loss: 95.7724 - mse: 62.5222 - mae: 6.2553 - fp_mae: 3.2248 - val_loss: 158.5226 - val_mse: 125.4253 - val_mae: 8.6814 - val_fp_mae: 6.0057\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 7.98259\n",
      "Epoch 64/100\n",
      "128/128 - 3s - loss: 95.0668 - mse: 61.8463 - mae: 6.2404 - fp_mae: 3.1663 - val_loss: 213.4129 - val_mse: 180.0365 - val_mae: 10.4593 - val_fp_mae: 8.3321\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 7.98259\n",
      "Epoch 65/100\n",
      "128/128 - 3s - loss: 96.3583 - mse: 63.2555 - mae: 6.2924 - fp_mae: 3.2701 - val_loss: 158.1682 - val_mse: 124.9654 - val_mae: 8.7729 - val_fp_mae: 3.5993\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 7.98259\n",
      "Epoch 66/100\n",
      "128/128 - 3s - loss: 93.5918 - mse: 60.5611 - mae: 6.1346 - fp_mae: 3.1774 - val_loss: 177.1588 - val_mse: 143.9490 - val_mae: 9.4160 - val_fp_mae: 7.0754\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 7.98259\n",
      "Epoch 67/100\n",
      "128/128 - 3s - loss: 93.6751 - mse: 60.6654 - mae: 6.1583 - fp_mae: 3.0983 - val_loss: 153.7034 - val_mse: 120.7600 - val_mae: 8.6868 - val_fp_mae: 4.9388\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 7.98259\n",
      "Epoch 68/100\n",
      "128/128 - 3s - loss: 94.3086 - mse: 61.4784 - mae: 6.1827 - fp_mae: 3.1691 - val_loss: 159.9057 - val_mse: 126.9625 - val_mae: 8.8219 - val_fp_mae: 3.1638\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 7.98259\n",
      "Epoch 69/100\n",
      "128/128 - 3s - loss: 94.0136 - mse: 61.0117 - mae: 6.1529 - fp_mae: 3.1658 - val_loss: 146.9366 - val_mse: 113.7819 - val_mae: 8.2859 - val_fp_mae: 3.6360\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 7.98259\n",
      "Epoch 70/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 - 3s - loss: 95.2643 - mse: 62.0726 - mae: 6.2343 - fp_mae: 3.2433 - val_loss: 162.3253 - val_mse: 128.8402 - val_mae: 8.9348 - val_fp_mae: 6.2493\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 7.98259\n",
      "Epoch 71/100\n",
      "128/128 - 3s - loss: 94.9967 - mse: 61.6224 - mae: 6.2105 - fp_mae: 3.2074 - val_loss: 154.2980 - val_mse: 120.9495 - val_mae: 8.6255 - val_fp_mae: 5.7975\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 7.98259\n",
      "Epoch 72/100\n",
      "128/128 - 3s - loss: 94.0722 - mse: 60.8478 - mae: 6.1366 - fp_mae: 3.0877 - val_loss: 160.5562 - val_mse: 127.4892 - val_mae: 8.8158 - val_fp_mae: 6.2519\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 7.98259\n",
      "Epoch 73/100\n",
      "128/128 - 3s - loss: 92.6673 - mse: 59.7590 - mae: 6.0718 - fp_mae: 3.1379 - val_loss: 353.3578 - val_mse: 320.1889 - val_mae: 13.6890 - val_fp_mae: 2.0328\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 7.98259\n",
      "Epoch 74/100\n",
      "128/128 - 3s - loss: 94.6684 - mse: 61.5585 - mae: 6.1767 - fp_mae: 3.1635 - val_loss: 163.2325 - val_mse: 130.1069 - val_mae: 8.9555 - val_fp_mae: 5.4870\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 7.98259\n",
      "Epoch 75/100\n",
      "128/128 - 3s - loss: 90.3099 - mse: 57.3231 - mae: 5.9487 - fp_mae: 3.0309 - val_loss: 152.5559 - val_mse: 119.6855 - val_mae: 8.5845 - val_fp_mae: 3.9975\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 7.98259\n",
      "Epoch 76/100\n",
      "128/128 - 3s - loss: 90.8287 - mse: 58.0337 - mae: 6.0137 - fp_mae: 3.0929 - val_loss: 156.8172 - val_mse: 123.7902 - val_mae: 8.7188 - val_fp_mae: 4.2713\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 7.98259\n",
      "Epoch 77/100\n",
      "128/128 - 3s - loss: 91.8408 - mse: 58.8911 - mae: 6.0588 - fp_mae: 3.1458 - val_loss: 156.3493 - val_mse: 123.1731 - val_mae: 8.7572 - val_fp_mae: 3.5909\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 7.98259\n",
      "Epoch 78/100\n",
      "128/128 - 3s - loss: 90.8889 - mse: 57.8402 - mae: 6.0106 - fp_mae: 3.0485 - val_loss: 148.6024 - val_mse: 115.6255 - val_mae: 8.3631 - val_fp_mae: 5.1482\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 7.98259\n",
      "Epoch 79/100\n",
      "128/128 - 3s - loss: 92.7924 - mse: 59.9117 - mae: 6.0483 - fp_mae: 3.1640 - val_loss: 153.9124 - val_mse: 120.7387 - val_mae: 8.6472 - val_fp_mae: 4.7876\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 7.98259\n",
      "Epoch 80/100\n",
      "128/128 - 3s - loss: 91.9101 - mse: 58.8092 - mae: 6.0400 - fp_mae: 3.0426 - val_loss: 186.2615 - val_mse: 153.1013 - val_mae: 9.5941 - val_fp_mae: 7.1612\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 7.98259\n",
      "Epoch 81/100\n",
      "128/128 - 3s - loss: 92.3903 - mse: 59.1598 - mae: 6.0747 - fp_mae: 3.0618 - val_loss: 161.9201 - val_mse: 128.7383 - val_mae: 8.8130 - val_fp_mae: 6.2681\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 7.98259\n",
      "Epoch 82/100\n",
      "128/128 - 3s - loss: 90.4106 - mse: 57.5643 - mae: 5.9951 - fp_mae: 3.1572 - val_loss: 183.1495 - val_mse: 150.3917 - val_mae: 9.5064 - val_fp_mae: 6.9989\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 7.98259\n",
      "Epoch 83/100\n",
      "128/128 - 3s - loss: 89.8008 - mse: 57.1825 - mae: 6.0096 - fp_mae: 3.0626 - val_loss: 220.4934 - val_mse: 187.8269 - val_mae: 10.5495 - val_fp_mae: 8.7017\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 7.98259\n",
      "Epoch 84/100\n",
      "128/128 - 3s - loss: 90.4415 - mse: 57.8253 - mae: 5.9837 - fp_mae: 3.0631 - val_loss: 164.3266 - val_mse: 131.6923 - val_mae: 9.0773 - val_fp_mae: 3.3792\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 7.98259\n",
      "Epoch 85/100\n",
      "128/128 - 3s - loss: 94.9313 - mse: 61.8699 - mae: 6.2163 - fp_mae: 3.1506 - val_loss: 160.8682 - val_mse: 127.3686 - val_mae: 8.9093 - val_fp_mae: 5.5874\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 7.98259\n",
      "Epoch 86/100\n",
      "128/128 - 3s - loss: 90.3600 - mse: 56.9717 - mae: 5.9491 - fp_mae: 3.0190 - val_loss: 206.2875 - val_mse: 173.0536 - val_mae: 10.2293 - val_fp_mae: 8.4899\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 7.98259\n",
      "Epoch 87/100\n",
      "128/128 - 3s - loss: 89.9815 - mse: 56.8169 - mae: 5.9596 - fp_mae: 3.1633 - val_loss: 152.0860 - val_mse: 118.7665 - val_mae: 8.5137 - val_fp_mae: 4.7458\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 7.98259\n",
      "Epoch 88/100\n",
      "128/128 - 3s - loss: 90.7585 - mse: 57.4274 - mae: 5.9717 - fp_mae: 2.9778 - val_loss: 151.6141 - val_mse: 118.4394 - val_mae: 8.5587 - val_fp_mae: 4.2623\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 7.98259\n",
      "Epoch 89/100\n",
      "128/128 - 3s - loss: 88.9325 - mse: 55.8582 - mae: 5.8924 - fp_mae: 3.0429 - val_loss: 146.8893 - val_mse: 113.8063 - val_mae: 8.3661 - val_fp_mae: 4.3752\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 7.98259\n",
      "Epoch 90/100\n",
      "128/128 - 3s - loss: 91.0074 - mse: 57.8708 - mae: 5.9923 - fp_mae: 3.0613 - val_loss: 149.2126 - val_mse: 116.0914 - val_mae: 8.4384 - val_fp_mae: 4.3084\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 7.98259\n",
      "Epoch 91/100\n",
      "128/128 - 3s - loss: 90.6461 - mse: 57.5919 - mae: 5.9690 - fp_mae: 3.0067 - val_loss: 147.0353 - val_mse: 113.8504 - val_mae: 8.4652 - val_fp_mae: 4.6875\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 7.98259\n",
      "Epoch 92/100\n",
      "128/128 - 3s - loss: 87.7962 - mse: 54.8158 - mae: 5.9021 - fp_mae: 3.0854 - val_loss: 271.0204 - val_mse: 238.1758 - val_mae: 11.9798 - val_fp_mae: 10.6104\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 7.98259\n",
      "Epoch 93/100\n",
      "128/128 - 3s - loss: 87.8159 - mse: 54.9510 - mae: 5.8835 - fp_mae: 2.9903 - val_loss: 182.2246 - val_mse: 149.3473 - val_mae: 9.6184 - val_fp_mae: 6.9525\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 7.98259\n",
      "Epoch 94/100\n",
      "128/128 - 3s - loss: 87.5412 - mse: 54.9824 - mae: 5.8351 - fp_mae: 2.9753 - val_loss: 225.3842 - val_mse: 192.6834 - val_mae: 11.0441 - val_fp_mae: 2.0873\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 7.98259\n",
      "Epoch 95/100\n",
      "128/128 - 3s - loss: 86.8275 - mse: 54.2658 - mae: 5.8251 - fp_mae: 3.0148 - val_loss: 236.9955 - val_mse: 204.4241 - val_mae: 11.0038 - val_fp_mae: 4.1629\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 7.98259\n",
      "Epoch 96/100\n",
      "128/128 - 3s - loss: 88.1281 - mse: 55.6347 - mae: 5.8670 - fp_mae: 3.0092 - val_loss: 148.7393 - val_mse: 116.0385 - val_mae: 8.5074 - val_fp_mae: 3.4084\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 7.98259\n",
      "Epoch 97/100\n",
      "128/128 - 3s - loss: 87.9756 - mse: 55.1654 - mae: 5.8609 - fp_mae: 2.9545 - val_loss: 163.9203 - val_mse: 130.9994 - val_mae: 8.8801 - val_fp_mae: 5.5058\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 7.98259\n",
      "Epoch 98/100\n",
      "128/128 - 3s - loss: 87.4013 - mse: 54.6789 - mae: 5.8390 - fp_mae: 2.9960 - val_loss: 173.6164 - val_mse: 140.8662 - val_mae: 9.3053 - val_fp_mae: 6.3432\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 7.98259\n",
      "Epoch 99/100\n",
      "128/128 - 3s - loss: 87.2021 - mse: 54.5712 - mae: 5.8264 - fp_mae: 2.9691 - val_loss: 145.8104 - val_mse: 113.3149 - val_mae: 8.4115 - val_fp_mae: 3.5874\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 7.98259\n",
      "Epoch 100/100\n",
      "128/128 - 3s - loss: 86.8701 - mse: 54.4255 - mae: 5.8612 - fp_mae: 3.0496 - val_loss: 146.0826 - val_mse: 113.5616 - val_mae: 8.3494 - val_fp_mae: 3.5038\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 7.98259\n",
      "\n",
      "Lambda: 1 , Time: 0:04:25\n",
      "Train Error(all epochs): 5.8251051902771 \n",
      " [25.162, 22.378, 19.294, 16.318, 13.665, 11.403, 9.706, 8.898, 8.608, 8.612, 8.507, 8.32, 8.104, 8.009, 7.878, 7.764, 7.719, 7.678, 7.6, 7.539, 7.513, 7.516, 7.437, 7.381, 7.353, 7.301, 7.347, 7.19, 7.253, 7.14, 7.15, 7.067, 7.131, 7.029, 6.994, 6.942, 6.907, 6.877, 6.92, 6.804, 6.802, 6.778, 6.79, 6.688, 6.635, 6.644, 6.627, 6.548, 6.534, 6.566, 6.553, 6.54, 6.377, 6.536, 6.387, 6.38, 6.379, 6.347, 6.343, 6.295, 6.374, 6.314, 6.255, 6.24, 6.292, 6.135, 6.158, 6.183, 6.153, 6.234, 6.211, 6.137, 6.072, 6.177, 5.949, 6.014, 6.059, 6.011, 6.048, 6.04, 6.075, 5.995, 6.01, 5.984, 6.216, 5.949, 5.96, 5.972, 5.892, 5.992, 5.969, 5.902, 5.884, 5.835, 5.825, 5.867, 5.861, 5.839, 5.826, 5.861]\n",
      "Train FP Error(all epochs): 2.954521417617798 \n",
      " [23.774, 21.773, 19.069, 16.134, 13.22, 10.447, 8.046, 6.438, 5.44, 5.006, 4.65, 4.396, 4.209, 4.245, 4.164, 4.059, 3.986, 4.026, 3.984, 3.917, 3.941, 3.945, 3.914, 3.819, 3.852, 3.821, 3.782, 3.763, 3.76, 3.671, 3.693, 3.715, 3.765, 3.642, 3.568, 3.62, 3.55, 3.55, 3.556, 3.543, 3.57, 3.538, 3.409, 3.47, 3.328, 3.508, 3.449, 3.332, 3.373, 3.433, 3.317, 3.346, 3.316, 3.28, 3.376, 3.248, 3.253, 3.246, 3.19, 3.247, 3.257, 3.265, 3.225, 3.166, 3.27, 3.177, 3.098, 3.169, 3.166, 3.243, 3.207, 3.088, 3.138, 3.164, 3.031, 3.093, 3.146, 3.048, 3.164, 3.043, 3.062, 3.157, 3.063, 3.063, 3.151, 3.019, 3.163, 2.978, 3.043, 3.061, 3.007, 3.085, 2.99, 2.975, 3.015, 3.009, 2.955, 2.996, 2.969, 3.05]\n",
      "Val Error(all epochs): 7.982587814331055 \n",
      " [28.076, 24.673, 16.839, 14.346, 12.391, 11.829, 9.938, 9.111, 9.535, 8.906, 10.089, 9.122, 8.574, 10.828, 8.349, 10.212, 12.428, 8.721, 8.555, 7.983, 8.055, 8.125, 9.21, 8.317, 9.443, 8.647, 8.173, 8.682, 8.828, 8.576, 8.971, 8.231, 11.438, 8.776, 8.217, 8.459, 8.477, 8.552, 9.48, 10.129, 8.827, 8.424, 9.424, 8.556, 13.404, 9.177, 11.643, 8.464, 9.072, 8.621, 9.004, 10.609, 9.353, 8.564, 8.245, 8.166, 8.515, 8.392, 8.594, 8.688, 8.617, 9.838, 8.681, 10.459, 8.773, 9.416, 8.687, 8.822, 8.286, 8.935, 8.625, 8.816, 13.689, 8.956, 8.584, 8.719, 8.757, 8.363, 8.647, 9.594, 8.813, 9.506, 10.549, 9.077, 8.909, 10.229, 8.514, 8.559, 8.366, 8.438, 8.465, 11.98, 9.618, 11.044, 11.004, 8.507, 8.88, 9.305, 8.411, 8.349]\n",
      "Val FP Error(all epochs): 2.0327653884887695 \n",
      " [27.201, 24.345, 16.391, 12.312, 11.69, 11.018, 8.212, 6.156, 6.388, 5.044, 7.317, 5.043, 4.842, 9.177, 4.934, 7.829, 10.761, 6.059, 2.883, 4.052, 4.258, 3.995, 7.465, 5.607, 6.707, 3.002, 3.636, 4.223, 5.754, 2.846, 6.511, 4.17, 9.256, 6.426, 4.219, 4.856, 4.214, 3.678, 6.971, 7.692, 4.962, 4.775, 7.145, 5.63, 12.5, 6.077, 10.233, 5.128, 6.726, 5.555, 6.644, 8.54, 6.762, 5.324, 4.67, 3.993, 3.585, 3.421, 5.176, 5.95, 4.257, 2.833, 6.006, 8.332, 3.599, 7.075, 4.939, 3.164, 3.636, 6.249, 5.797, 6.252, 2.033, 5.487, 3.998, 4.271, 3.591, 5.148, 4.788, 7.161, 6.268, 6.999, 8.702, 3.379, 5.587, 8.49, 4.746, 4.262, 4.375, 4.308, 4.688, 10.61, 6.952, 2.087, 4.163, 3.408, 5.506, 6.343, 3.587, 3.504]\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 - 3s - loss: 1248.8580 - mse: 814.0836 - mae: 25.2115 - fp_mae: 23.7998 - val_loss: 1180.9291 - val_mse: 1020.3751 - val_mae: 28.2129 - val_fp_mae: 27.0812\n",
      "\n",
      "Epoch 00001: val_mae improved from inf to 28.21286, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_3.h5\n",
      "Epoch 2/100\n",
      "128/128 - 3s - loss: 775.3600 - mse: 672.2937 - mae: 22.5463 - fp_mae: 21.8919 - val_loss: 774.2659 - val_mse: 702.8586 - val_mae: 23.2663 - val_fp_mae: 21.2062\n",
      "\n",
      "Epoch 00002: val_mae improved from 28.21286 to 23.26632, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_3.h5\n",
      "Epoch 3/100\n",
      "128/128 - 3s - loss: 581.8358 - mse: 514.3160 - mae: 19.5039 - fp_mae: 19.2582 - val_loss: 641.7055 - val_mse: 577.3987 - val_mae: 20.3531 - val_fp_mae: 19.2672\n",
      "\n",
      "Epoch 00003: val_mae improved from 23.26632 to 20.35307, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_3.h5\n",
      "Epoch 4/100\n",
      "128/128 - 3s - loss: 466.7273 - mse: 397.1494 - mae: 16.6878 - fp_mae: 16.5055 - val_loss: 520.5859 - val_mse: 446.3363 - val_mae: 17.4223 - val_fp_mae: 17.1819\n",
      "\n",
      "Epoch 00004: val_mae improved from 20.35307 to 17.42228, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_3.h5\n",
      "Epoch 5/100\n",
      "128/128 - 3s - loss: 396.1545 - mse: 316.2717 - mae: 14.3911 - fp_mae: 14.0194 - val_loss: 418.7420 - val_mse: 334.5580 - val_mae: 14.7547 - val_fp_mae: 12.2463\n",
      "\n",
      "Epoch 00005: val_mae improved from 17.42228 to 14.75471, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_3.h5\n",
      "Epoch 6/100\n",
      "128/128 - 3s - loss: 343.7738 - mse: 252.2219 - mae: 12.5347 - fp_mae: 11.8006 - val_loss: 585.7209 - val_mse: 489.0703 - val_mae: 18.0265 - val_fp_mae: 17.7285\n",
      "\n",
      "Epoch 00006: val_mae did not improve from 14.75471\n",
      "Epoch 7/100\n",
      "128/128 - 3s - loss: 304.0700 - mse: 202.9322 - mae: 10.9253 - fp_mae: 9.7692 - val_loss: 315.0524 - val_mse: 210.9850 - val_mae: 11.0814 - val_fp_mae: 9.9418\n",
      "\n",
      "Epoch 00007: val_mae improved from 14.75471 to 11.08142, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_3.h5\n",
      "Epoch 8/100\n",
      "128/128 - 3s - loss: 277.0753 - mse: 173.6059 - mae: 9.9528 - fp_mae: 8.3678 - val_loss: 356.7810 - val_mse: 253.4660 - val_mae: 12.4626 - val_fp_mae: 7.5002\n",
      "\n",
      "Epoch 00008: val_mae did not improve from 11.08142\n",
      "Epoch 9/100\n",
      "128/128 - 3s - loss: 261.1752 - mse: 157.3961 - mae: 9.4629 - fp_mae: 7.4794 - val_loss: 337.8134 - val_mse: 233.8327 - val_mae: 11.8117 - val_fp_mae: 10.4437\n",
      "\n",
      "Epoch 00009: val_mae did not improve from 11.08142\n",
      "Epoch 10/100\n",
      "128/128 - 3s - loss: 250.6430 - mse: 148.3057 - mae: 9.1851 - fp_mae: 6.9343 - val_loss: 412.2496 - val_mse: 312.0880 - val_mae: 13.6452 - val_fp_mae: 12.2879\n",
      "\n",
      "Epoch 00010: val_mae did not improve from 11.08142\n",
      "Epoch 11/100\n",
      "128/128 - 3s - loss: 241.9619 - mse: 143.8624 - mae: 9.0541 - fp_mae: 6.5860 - val_loss: 280.4375 - val_mse: 185.5318 - val_mae: 10.4289 - val_fp_mae: 8.7226\n",
      "\n",
      "Epoch 00011: val_mae improved from 11.08142 to 10.42892, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_3.h5\n",
      "Epoch 12/100\n",
      "128/128 - 3s - loss: 233.7743 - mse: 140.9235 - mae: 8.9377 - fp_mae: 6.3441 - val_loss: 286.5557 - val_mse: 195.5937 - val_mae: 10.9179 - val_fp_mae: 5.5851\n",
      "\n",
      "Epoch 00012: val_mae did not improve from 10.42892\n",
      "Epoch 13/100\n",
      "128/128 - 3s - loss: 227.3735 - mse: 138.0900 - mae: 8.8800 - fp_mae: 6.1703 - val_loss: 249.0896 - val_mse: 161.9322 - val_mae: 9.5018 - val_fp_mae: 7.1434\n",
      "\n",
      "Epoch 00013: val_mae improved from 10.42892 to 9.50179, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_3.h5\n",
      "Epoch 14/100\n",
      "128/128 - 3s - loss: 219.2303 - mse: 134.8791 - mae: 8.7961 - fp_mae: 5.9218 - val_loss: 233.2950 - val_mse: 153.1304 - val_mae: 9.3498 - val_fp_mae: 6.9363\n",
      "\n",
      "Epoch 00014: val_mae improved from 9.50179 to 9.34978, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_3.h5\n",
      "Epoch 15/100\n",
      "128/128 - 3s - loss: 217.0002 - mse: 136.0653 - mae: 8.8301 - fp_mae: 5.8953 - val_loss: 727.8670 - val_mse: 647.9575 - val_mae: 18.6738 - val_fp_mae: 16.7872\n",
      "\n",
      "Epoch 00015: val_mae did not improve from 9.34978\n",
      "Epoch 16/100\n",
      "128/128 - 3s - loss: 212.7917 - mse: 134.5248 - mae: 8.8298 - fp_mae: 5.8448 - val_loss: 815.9069 - val_mse: 737.7943 - val_mae: 19.2402 - val_fp_mae: 17.4557\n",
      "\n",
      "Epoch 00016: val_mae did not improve from 9.34978\n",
      "Epoch 17/100\n",
      "128/128 - 3s - loss: 208.3430 - mse: 132.6921 - mae: 8.7929 - fp_mae: 5.7340 - val_loss: 227.0563 - val_mse: 152.8730 - val_mae: 9.3304 - val_fp_mae: 6.9628\n",
      "\n",
      "Epoch 00017: val_mae improved from 9.34978 to 9.33042, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_3.h5\n",
      "Epoch 18/100\n",
      "128/128 - 3s - loss: 204.4191 - mse: 133.1516 - mae: 8.7925 - fp_mae: 5.6946 - val_loss: 302.6350 - val_mse: 234.8785 - val_mae: 11.9555 - val_fp_mae: 5.2221\n",
      "\n",
      "Epoch 00018: val_mae did not improve from 9.33042\n",
      "Epoch 19/100\n",
      "128/128 - 3s - loss: 200.9599 - mse: 133.0322 - mae: 8.7886 - fp_mae: 5.7117 - val_loss: 978.5016 - val_mse: 909.4754 - val_mae: 20.6863 - val_fp_mae: 18.6655\n",
      "\n",
      "Epoch 00019: val_mae did not improve from 9.33042\n",
      "Epoch 20/100\n",
      "128/128 - 3s - loss: 199.0756 - mse: 132.4916 - mae: 8.7599 - fp_mae: 5.6396 - val_loss: 215.6656 - val_mse: 152.0097 - val_mae: 9.3981 - val_fp_mae: 5.7026\n",
      "\n",
      "Epoch 00020: val_mae did not improve from 9.33042\n",
      "Epoch 21/100\n",
      "128/128 - 3s - loss: 194.3012 - mse: 131.2372 - mae: 8.7393 - fp_mae: 5.5973 - val_loss: 249.2757 - val_mse: 188.6831 - val_mae: 10.6360 - val_fp_mae: 6.1697\n",
      "\n",
      "Epoch 00021: val_mae did not improve from 9.33042\n",
      "Epoch 22/100\n",
      "128/128 - 3s - loss: 192.5786 - mse: 131.9259 - mae: 8.7695 - fp_mae: 5.5386 - val_loss: 337.4462 - val_mse: 275.7820 - val_mae: 12.8714 - val_fp_mae: 11.0349\n",
      "\n",
      "Epoch 00022: val_mae did not improve from 9.33042\n",
      "Epoch 23/100\n",
      "128/128 - 3s - loss: 190.0144 - mse: 130.5250 - mae: 8.7656 - fp_mae: 5.4518 - val_loss: 234.9724 - val_mse: 178.6226 - val_mae: 10.5379 - val_fp_mae: 4.9815\n",
      "\n",
      "Epoch 00023: val_mae did not improve from 9.33042\n",
      "Epoch 24/100\n",
      "128/128 - 3s - loss: 186.4262 - mse: 130.1346 - mae: 8.7423 - fp_mae: 5.3939 - val_loss: 251.7128 - val_mse: 196.8414 - val_mae: 10.9564 - val_fp_mae: 5.8072\n",
      "\n",
      "Epoch 00024: val_mae did not improve from 9.33042\n",
      "Epoch 25/100\n",
      "128/128 - 3s - loss: 184.9680 - mse: 129.6215 - mae: 8.7365 - fp_mae: 5.3152 - val_loss: 265.3509 - val_mse: 211.5521 - val_mae: 11.4163 - val_fp_mae: 5.1571\n",
      "\n",
      "Epoch 00025: val_mae did not improve from 9.33042\n",
      "Epoch 26/100\n",
      "128/128 - 3s - loss: 180.8353 - mse: 128.3837 - mae: 8.7013 - fp_mae: 5.2314 - val_loss: 209.8943 - val_mse: 155.8256 - val_mae: 9.7221 - val_fp_mae: 5.2714\n",
      "\n",
      "Epoch 00026: val_mae did not improve from 9.33042\n",
      "Epoch 27/100\n",
      "128/128 - 3s - loss: 183.4931 - mse: 128.8872 - mae: 8.7168 - fp_mae: 5.1937 - val_loss: 312.7570 - val_mse: 260.6920 - val_mae: 12.5280 - val_fp_mae: 4.9338\n",
      "\n",
      "Epoch 00027: val_mae did not improve from 9.33042\n",
      "Epoch 28/100\n",
      "128/128 - 3s - loss: 179.5991 - mse: 128.7091 - mae: 8.7183 - fp_mae: 5.1525 - val_loss: 255.9398 - val_mse: 206.2568 - val_mae: 11.2730 - val_fp_mae: 5.2120\n",
      "\n",
      "Epoch 00028: val_mae did not improve from 9.33042\n",
      "Epoch 29/100\n",
      "128/128 - 3s - loss: 178.4211 - mse: 128.4719 - mae: 8.7154 - fp_mae: 5.1660 - val_loss: 202.3018 - val_mse: 149.8455 - val_mae: 9.6137 - val_fp_mae: 4.7227\n",
      "\n",
      "Epoch 00029: val_mae did not improve from 9.33042\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 - 3s - loss: 177.8044 - mse: 127.3910 - mae: 8.7018 - fp_mae: 5.0215 - val_loss: 226.2336 - val_mse: 178.7034 - val_mae: 10.4520 - val_fp_mae: 8.1034\n",
      "\n",
      "Epoch 00030: val_mae did not improve from 9.33042\n",
      "Epoch 31/100\n",
      "128/128 - 3s - loss: 174.6188 - mse: 127.8576 - mae: 8.6975 - fp_mae: 5.0733 - val_loss: 402.2065 - val_mse: 355.5881 - val_mae: 14.2314 - val_fp_mae: 11.5944\n",
      "\n",
      "Epoch 00031: val_mae did not improve from 9.33042\n",
      "Epoch 32/100\n",
      "128/128 - 3s - loss: 171.6407 - mse: 126.5402 - mae: 8.6742 - fp_mae: 5.0237 - val_loss: 194.7178 - val_mse: 148.3642 - val_mae: 9.5918 - val_fp_mae: 6.6825\n",
      "\n",
      "Epoch 00032: val_mae did not improve from 9.33042\n",
      "Epoch 33/100\n",
      "128/128 - 3s - loss: 171.6492 - mse: 125.9372 - mae: 8.6671 - fp_mae: 4.9088 - val_loss: 174.3882 - val_mse: 129.8250 - val_mae: 8.9770 - val_fp_mae: 5.2379\n",
      "\n",
      "Epoch 00033: val_mae improved from 9.33042 to 8.97704, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_3.h5\n",
      "Epoch 34/100\n",
      "128/128 - 3s - loss: 169.9607 - mse: 126.9245 - mae: 8.6899 - fp_mae: 4.9844 - val_loss: 231.6384 - val_mse: 189.1888 - val_mae: 10.8794 - val_fp_mae: 8.1560\n",
      "\n",
      "Epoch 00034: val_mae did not improve from 8.97704\n",
      "Epoch 35/100\n",
      "128/128 - 3s - loss: 169.6067 - mse: 126.4233 - mae: 8.6950 - fp_mae: 4.9109 - val_loss: 280.2094 - val_mse: 238.6090 - val_mae: 12.0316 - val_fp_mae: 9.6898\n",
      "\n",
      "Epoch 00035: val_mae did not improve from 8.97704\n",
      "Epoch 36/100\n",
      "128/128 - 3s - loss: 168.3730 - mse: 126.4418 - mae: 8.6877 - fp_mae: 4.9336 - val_loss: 176.5161 - val_mse: 135.2139 - val_mae: 8.9812 - val_fp_mae: 5.1685\n",
      "\n",
      "Epoch 00036: val_mae did not improve from 8.97704\n",
      "Epoch 37/100\n",
      "128/128 - 3s - loss: 168.0897 - mse: 126.4653 - mae: 8.7090 - fp_mae: 4.8907 - val_loss: 182.5147 - val_mse: 142.3044 - val_mae: 9.1728 - val_fp_mae: 6.3522\n",
      "\n",
      "Epoch 00037: val_mae did not improve from 8.97704\n",
      "Epoch 38/100\n",
      "128/128 - 3s - loss: 165.2427 - mse: 125.3209 - mae: 8.6661 - fp_mae: 4.9001 - val_loss: 290.7200 - val_mse: 251.8813 - val_mae: 12.3202 - val_fp_mae: 5.4542\n",
      "\n",
      "Epoch 00038: val_mae did not improve from 8.97704\n",
      "Epoch 39/100\n",
      "128/128 - 3s - loss: 166.0933 - mse: 126.0179 - mae: 8.7219 - fp_mae: 4.8273 - val_loss: 175.2855 - val_mse: 137.7374 - val_mae: 9.0015 - val_fp_mae: 5.4970\n",
      "\n",
      "Epoch 00039: val_mae did not improve from 8.97704\n",
      "Epoch 40/100\n",
      "128/128 - 3s - loss: 164.9949 - mse: 126.2340 - mae: 8.6864 - fp_mae: 4.8774 - val_loss: 1157.8628 - val_mse: 1118.1322 - val_mae: 21.6532 - val_fp_mae: 18.8795\n",
      "\n",
      "Epoch 00040: val_mae did not improve from 8.97704\n",
      "Epoch 41/100\n",
      "128/128 - 3s - loss: 164.2807 - mse: 125.9576 - mae: 8.7129 - fp_mae: 4.8427 - val_loss: 174.8003 - val_mse: 137.1340 - val_mae: 8.9839 - val_fp_mae: 5.3095\n",
      "\n",
      "Epoch 00041: val_mae did not improve from 8.97704\n",
      "Epoch 42/100\n",
      "128/128 - 3s - loss: 163.6086 - mse: 126.3571 - mae: 8.7087 - fp_mae: 4.8889 - val_loss: 577.9180 - val_mse: 540.5690 - val_mae: 16.7063 - val_fp_mae: 14.0136\n",
      "\n",
      "Epoch 00042: val_mae did not improve from 8.97704\n",
      "Epoch 43/100\n",
      "128/128 - 3s - loss: 162.7252 - mse: 125.7531 - mae: 8.6825 - fp_mae: 4.8003 - val_loss: 260.1724 - val_mse: 218.5611 - val_mae: 11.8447 - val_fp_mae: 4.0005\n",
      "\n",
      "Epoch 00043: val_mae did not improve from 8.97704\n",
      "Epoch 44/100\n",
      "128/128 - 3s - loss: 165.0789 - mse: 126.2284 - mae: 8.6956 - fp_mae: 4.7323 - val_loss: 228.4492 - val_mse: 193.5038 - val_mae: 11.0635 - val_fp_mae: 4.6159\n",
      "\n",
      "Epoch 00044: val_mae did not improve from 8.97704\n",
      "Epoch 45/100\n",
      "128/128 - 3s - loss: 160.0742 - mse: 125.9261 - mae: 8.6828 - fp_mae: 4.8194 - val_loss: 172.6492 - val_mse: 138.8677 - val_mae: 9.2127 - val_fp_mae: 5.9589\n",
      "\n",
      "Epoch 00045: val_mae did not improve from 8.97704\n",
      "Epoch 46/100\n",
      "128/128 - 3s - loss: 160.1900 - mse: 125.8060 - mae: 8.6860 - fp_mae: 4.7503 - val_loss: 245.9753 - val_mse: 212.0236 - val_mae: 11.4126 - val_fp_mae: 8.9194\n",
      "\n",
      "Epoch 00046: val_mae did not improve from 8.97704\n",
      "Epoch 47/100\n",
      "128/128 - 3s - loss: 158.8953 - mse: 125.6726 - mae: 8.6811 - fp_mae: 4.8028 - val_loss: 229.4701 - val_mse: 197.2066 - val_mae: 11.0208 - val_fp_mae: 5.4580\n",
      "\n",
      "Epoch 00047: val_mae did not improve from 8.97704\n",
      "Epoch 48/100\n",
      "128/128 - 3s - loss: 158.0399 - mse: 125.3975 - mae: 8.6785 - fp_mae: 4.7805 - val_loss: 359.2353 - val_mse: 326.2825 - val_mae: 13.8115 - val_fp_mae: 10.5857\n",
      "\n",
      "Epoch 00048: val_mae did not improve from 8.97704\n",
      "Epoch 49/100\n",
      "128/128 - 3s - loss: 156.3616 - mse: 124.6094 - mae: 8.6804 - fp_mae: 4.6575 - val_loss: 170.6580 - val_mse: 140.9580 - val_mae: 9.2566 - val_fp_mae: 4.8042\n",
      "\n",
      "Epoch 00049: val_mae did not improve from 8.97704\n",
      "Epoch 50/100\n",
      "128/128 - 3s - loss: 156.7562 - mse: 125.6378 - mae: 8.6873 - fp_mae: 4.7608 - val_loss: 357.1629 - val_mse: 324.9094 - val_mae: 13.7044 - val_fp_mae: 10.9474\n",
      "\n",
      "Epoch 00050: val_mae did not improve from 8.97704\n",
      "Epoch 51/100\n",
      "128/128 - 3s - loss: 156.7818 - mse: 124.8994 - mae: 8.6719 - fp_mae: 4.7187 - val_loss: 558.3224 - val_mse: 526.7283 - val_mae: 16.3988 - val_fp_mae: 13.5975\n",
      "\n",
      "Epoch 00051: val_mae did not improve from 8.97704\n",
      "Epoch 52/100\n",
      "128/128 - 3s - loss: 156.6652 - mse: 125.1700 - mae: 8.7104 - fp_mae: 4.6753 - val_loss: 168.6805 - val_mse: 138.4166 - val_mae: 9.0449 - val_fp_mae: 5.3059\n",
      "\n",
      "Epoch 00052: val_mae did not improve from 8.97704\n",
      "Epoch 53/100\n",
      "128/128 - 3s - loss: 156.5032 - mse: 125.5254 - mae: 8.6932 - fp_mae: 4.7525 - val_loss: 162.1043 - val_mse: 129.9711 - val_mae: 8.8487 - val_fp_mae: 5.2539\n",
      "\n",
      "Epoch 00053: val_mae improved from 8.97704 to 8.84871, saving model to ML/data/pictures_100_100_transfer/splat/pu_circle_su_circle_50/raw_power_min_max_norm/color/log_5/20pus_5sus_8channels/models/4096/best_model_lambda_3.h5\n",
      "Epoch 54/100\n",
      "128/128 - 3s - loss: 155.1513 - mse: 124.7450 - mae: 8.6730 - fp_mae: 4.6969 - val_loss: 204.5710 - val_mse: 174.6607 - val_mae: 10.3565 - val_fp_mae: 7.9872\n",
      "\n",
      "Epoch 00054: val_mae did not improve from 8.84871\n",
      "Epoch 55/100\n",
      "128/128 - 3s - loss: 154.5406 - mse: 125.0460 - mae: 8.7056 - fp_mae: 4.6724 - val_loss: 255.7489 - val_mse: 228.0120 - val_mae: 11.8337 - val_fp_mae: 5.0187\n",
      "\n",
      "Epoch 00055: val_mae did not improve from 8.84871\n",
      "Epoch 56/100\n",
      "128/128 - 3s - loss: 154.0727 - mse: 125.1995 - mae: 8.6721 - fp_mae: 4.7797 - val_loss: 189.3568 - val_mse: 159.7780 - val_mae: 10.0645 - val_fp_mae: 7.0790\n",
      "\n",
      "Epoch 00056: val_mae did not improve from 8.84871\n",
      "Epoch 57/100\n",
      "128/128 - 3s - loss: 154.2915 - mse: 124.7327 - mae: 8.6932 - fp_mae: 4.6532 - val_loss: 167.3508 - val_mse: 137.7231 - val_mae: 9.1526 - val_fp_mae: 6.1247\n",
      "\n",
      "Epoch 00057: val_mae did not improve from 8.84871\n",
      "Epoch 58/100\n",
      "128/128 - 3s - loss: 153.7349 - mse: 124.6924 - mae: 8.7081 - fp_mae: 4.6651 - val_loss: 202.9835 - val_mse: 174.0436 - val_mae: 10.6794 - val_fp_mae: 4.1105\n",
      "\n",
      "Epoch 00058: val_mae did not improve from 8.84871\n",
      "Epoch 59/100\n",
      "128/128 - 3s - loss: 153.1090 - mse: 125.2503 - mae: 8.7105 - fp_mae: 4.7049 - val_loss: 165.5766 - val_mse: 138.6315 - val_mae: 8.9844 - val_fp_mae: 5.6305\n",
      "\n",
      "Epoch 00059: val_mae did not improve from 8.84871\n",
      "Epoch 60/100\n",
      "128/128 - 3s - loss: 151.6346 - mse: 124.6656 - mae: 8.6740 - fp_mae: 4.6842 - val_loss: 157.2478 - val_mse: 129.6351 - val_mae: 8.9730 - val_fp_mae: 4.7323\n",
      "\n",
      "Epoch 00060: val_mae did not improve from 8.84871\n",
      "Epoch 61/100\n",
      "128/128 - 3s - loss: 151.2433 - mse: 124.5011 - mae: 8.7055 - fp_mae: 4.5624 - val_loss: 301.4218 - val_mse: 275.7920 - val_mae: 12.9398 - val_fp_mae: 9.7750\n",
      "\n",
      "Epoch 00061: val_mae did not improve from 8.84871\n",
      "Epoch 62/100\n",
      "128/128 - 3s - loss: 151.9718 - mse: 125.6693 - mae: 8.6943 - fp_mae: 4.6990 - val_loss: 158.3339 - val_mse: 132.8724 - val_mae: 9.0717 - val_fp_mae: 4.4905\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 8.84871\n",
      "Epoch 63/100\n",
      "128/128 - 3s - loss: 149.9685 - mse: 124.2878 - mae: 8.6951 - fp_mae: 4.6413 - val_loss: 247.6955 - val_mse: 221.1056 - val_mae: 11.8042 - val_fp_mae: 8.6935\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 8.84871\n",
      "Epoch 64/100\n",
      "128/128 - 3s - loss: 151.6928 - mse: 124.7185 - mae: 8.6945 - fp_mae: 4.6603 - val_loss: 209.4803 - val_mse: 183.4298 - val_mae: 10.5790 - val_fp_mae: 5.5113\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 8.84871\n",
      "Epoch 65/100\n",
      "128/128 - 3s - loss: 151.8471 - mse: 125.0383 - mae: 8.6927 - fp_mae: 4.6692 - val_loss: 167.8530 - val_mse: 140.8448 - val_mae: 9.3167 - val_fp_mae: 6.2351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00065: val_mae did not improve from 8.84871\n",
      "Epoch 66/100\n",
      "128/128 - 3s - loss: 151.0606 - mse: 124.5926 - mae: 8.6950 - fp_mae: 4.6270 - val_loss: 247.7171 - val_mse: 220.7025 - val_mae: 11.8196 - val_fp_mae: 4.3687\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 8.84871\n",
      "Epoch 67/100\n",
      "128/128 - 3s - loss: 150.2648 - mse: 124.1033 - mae: 8.6940 - fp_mae: 4.6106 - val_loss: 156.5278 - val_mse: 132.0978 - val_mae: 8.9435 - val_fp_mae: 5.2880\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 8.84871\n",
      "Epoch 68/100\n",
      "128/128 - 3s - loss: 149.3709 - mse: 124.8067 - mae: 8.6954 - fp_mae: 4.6579 - val_loss: 246.1666 - val_mse: 220.8831 - val_mae: 11.7551 - val_fp_mae: 8.6434\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 8.84871\n",
      "Epoch 69/100\n",
      "128/128 - 3s - loss: 149.0609 - mse: 124.2508 - mae: 8.6701 - fp_mae: 4.6370 - val_loss: 156.6243 - val_mse: 132.2960 - val_mae: 8.9808 - val_fp_mae: 4.9960\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 8.84871\n",
      "Epoch 70/100\n",
      "128/128 - 3s - loss: 148.8373 - mse: 124.2816 - mae: 8.6844 - fp_mae: 4.5826 - val_loss: 166.4362 - val_mse: 142.1264 - val_mae: 9.4346 - val_fp_mae: 6.1902\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 8.84871\n",
      "Epoch 71/100\n",
      "128/128 - 3s - loss: 148.9664 - mse: 124.7600 - mae: 8.7010 - fp_mae: 4.6303 - val_loss: 188.2979 - val_mse: 162.2985 - val_mae: 10.1773 - val_fp_mae: 4.3762\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 8.84871\n",
      "Epoch 72/100\n",
      "128/128 - 3s - loss: 150.3655 - mse: 124.6297 - mae: 8.6967 - fp_mae: 4.6099 - val_loss: 181.3035 - val_mse: 156.7113 - val_mae: 9.9328 - val_fp_mae: 4.6413\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 8.84871\n",
      "Epoch 73/100\n",
      "128/128 - 3s - loss: 148.1070 - mse: 124.6262 - mae: 8.6950 - fp_mae: 4.6164 - val_loss: 249.0707 - val_mse: 225.7061 - val_mae: 11.8725 - val_fp_mae: 4.5010\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 8.84871\n",
      "Epoch 74/100\n",
      "128/128 - 3s - loss: 146.5428 - mse: 124.2222 - mae: 8.6856 - fp_mae: 4.5882 - val_loss: 210.2883 - val_mse: 188.0654 - val_mae: 10.8810 - val_fp_mae: 4.7587\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 8.84871\n",
      "Epoch 75/100\n",
      "128/128 - 3s - loss: 148.9039 - mse: 125.1116 - mae: 8.7150 - fp_mae: 4.6101 - val_loss: 468.8303 - val_mse: 444.9859 - val_mae: 15.4905 - val_fp_mae: 12.5528\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 8.84871\n",
      "Epoch 76/100\n",
      "128/128 - 3s - loss: 146.8854 - mse: 124.2909 - mae: 8.6984 - fp_mae: 4.5962 - val_loss: 200.5453 - val_mse: 177.3211 - val_mae: 10.4413 - val_fp_mae: 7.8953\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 8.84871\n",
      "Epoch 77/100\n",
      "128/128 - 3s - loss: 148.6225 - mse: 124.8033 - mae: 8.7254 - fp_mae: 4.6154 - val_loss: 175.8086 - val_mse: 151.8547 - val_mae: 9.7349 - val_fp_mae: 6.6912\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 8.84871\n",
      "Epoch 78/100\n",
      "128/128 - 3s - loss: 147.6534 - mse: 124.2980 - mae: 8.6872 - fp_mae: 4.5850 - val_loss: 252.0060 - val_mse: 229.5699 - val_mae: 11.9247 - val_fp_mae: 4.5555\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 8.84871\n",
      "Epoch 79/100\n",
      "128/128 - 3s - loss: 146.3254 - mse: 123.8364 - mae: 8.6693 - fp_mae: 4.6016 - val_loss: 225.7008 - val_mse: 204.5398 - val_mae: 11.2593 - val_fp_mae: 5.2185\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 8.84871\n",
      "Epoch 80/100\n",
      "128/128 - 3s - loss: 146.3691 - mse: 123.8880 - mae: 8.6902 - fp_mae: 4.5402 - val_loss: 159.3121 - val_mse: 137.4357 - val_mae: 9.3711 - val_fp_mae: 5.7507\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 8.84871\n",
      "Epoch 81/100\n",
      "128/128 - 3s - loss: 145.3445 - mse: 124.2974 - mae: 8.6979 - fp_mae: 4.5904 - val_loss: 174.3903 - val_mse: 153.5668 - val_mae: 9.8813 - val_fp_mae: 4.3539\n",
      "\n",
      "Epoch 00081: val_mae did not improve from 8.84871\n",
      "Epoch 82/100\n",
      "128/128 - 3s - loss: 145.1592 - mse: 124.4361 - mae: 8.7063 - fp_mae: 4.5997 - val_loss: 164.9126 - val_mse: 143.8936 - val_mae: 9.5412 - val_fp_mae: 6.2778\n",
      "\n",
      "Epoch 00082: val_mae did not improve from 8.84871\n",
      "Epoch 83/100\n",
      "128/128 - 3s - loss: 145.8143 - mse: 124.6956 - mae: 8.7085 - fp_mae: 4.6067 - val_loss: 252.6128 - val_mse: 231.5363 - val_mae: 12.0844 - val_fp_mae: 8.7622\n",
      "\n",
      "Epoch 00083: val_mae did not improve from 8.84871\n",
      "Epoch 84/100\n",
      "128/128 - 3s - loss: 145.1405 - mse: 123.6084 - mae: 8.7032 - fp_mae: 4.5224 - val_loss: 193.8694 - val_mse: 173.6481 - val_mae: 10.3526 - val_fp_mae: 7.7062\n",
      "\n",
      "Epoch 00084: val_mae did not improve from 8.84871\n",
      "Epoch 85/100\n",
      "128/128 - 3s - loss: 144.7259 - mse: 124.0605 - mae: 8.6891 - fp_mae: 4.5641 - val_loss: 154.2218 - val_mse: 134.1763 - val_mae: 8.9939 - val_fp_mae: 5.6240\n",
      "\n",
      "Epoch 00085: val_mae did not improve from 8.84871\n",
      "Epoch 86/100\n",
      "128/128 - 3s - loss: 146.8047 - mse: 124.9514 - mae: 8.7096 - fp_mae: 4.6103 - val_loss: 152.1074 - val_mse: 130.7684 - val_mae: 8.9188 - val_fp_mae: 5.1641\n",
      "\n",
      "Epoch 00086: val_mae did not improve from 8.84871\n",
      "Epoch 87/100\n",
      "128/128 - 3s - loss: 145.7496 - mse: 124.3374 - mae: 8.7064 - fp_mae: 4.5723 - val_loss: 1058.2241 - val_mse: 1036.9150 - val_mae: 21.3509 - val_fp_mae: 18.7176\n",
      "\n",
      "Epoch 00087: val_mae did not improve from 8.84871\n",
      "Epoch 88/100\n",
      "128/128 - 3s - loss: 144.7937 - mse: 123.6590 - mae: 8.6822 - fp_mae: 4.6009 - val_loss: 173.7355 - val_mse: 152.0385 - val_mae: 9.5949 - val_fp_mae: 5.0585\n",
      "\n",
      "Epoch 00088: val_mae did not improve from 8.84871\n",
      "Epoch 89/100\n",
      "128/128 - 3s - loss: 145.4865 - mse: 124.4410 - mae: 8.7338 - fp_mae: 4.5634 - val_loss: 179.9658 - val_mse: 158.6736 - val_mae: 9.8624 - val_fp_mae: 5.0672\n",
      "\n",
      "Epoch 00089: val_mae did not improve from 8.84871\n",
      "Epoch 90/100\n",
      "128/128 - 3s - loss: 145.9166 - mse: 124.8072 - mae: 8.7302 - fp_mae: 4.5839 - val_loss: 194.4754 - val_mse: 174.4359 - val_mae: 10.5178 - val_fp_mae: 4.6020\n",
      "\n",
      "Epoch 00090: val_mae did not improve from 8.84871\n",
      "Epoch 91/100\n",
      "128/128 - 3s - loss: 146.3448 - mse: 125.0548 - mae: 8.7317 - fp_mae: 4.5552 - val_loss: 326.1967 - val_mse: 304.8438 - val_mae: 13.3914 - val_fp_mae: 4.5024\n",
      "\n",
      "Epoch 00091: val_mae did not improve from 8.84871\n",
      "Epoch 92/100\n",
      "128/128 - 3s - loss: 144.4827 - mse: 124.5439 - mae: 8.6860 - fp_mae: 4.6305 - val_loss: 160.9665 - val_mse: 141.0167 - val_mae: 9.4298 - val_fp_mae: 6.1792\n",
      "\n",
      "Epoch 00092: val_mae did not improve from 8.84871\n",
      "Epoch 93/100\n",
      "128/128 - 3s - loss: 146.5098 - mse: 125.5035 - mae: 8.7437 - fp_mae: 4.5494 - val_loss: 180.9442 - val_mse: 160.2796 - val_mae: 10.1362 - val_fp_mae: 4.3688\n",
      "\n",
      "Epoch 00093: val_mae did not improve from 8.84871\n",
      "Epoch 94/100\n",
      "128/128 - 3s - loss: 143.4974 - mse: 123.5912 - mae: 8.6404 - fp_mae: 4.5567 - val_loss: 270.4186 - val_mse: 249.8969 - val_mae: 12.3069 - val_fp_mae: 9.7680\n",
      "\n",
      "Epoch 00094: val_mae did not improve from 8.84871\n",
      "Epoch 95/100\n",
      "128/128 - 3s - loss: 145.1666 - mse: 124.3872 - mae: 8.7137 - fp_mae: 4.5230 - val_loss: 413.9410 - val_mse: 394.0840 - val_mae: 14.8223 - val_fp_mae: 12.0650\n",
      "\n",
      "Epoch 00095: val_mae did not improve from 8.84871\n",
      "Epoch 96/100\n",
      "128/128 - 3s - loss: 142.7977 - mse: 123.6690 - mae: 8.6873 - fp_mae: 4.4874 - val_loss: 163.1618 - val_mse: 145.1129 - val_mae: 9.1089 - val_fp_mae: 5.8276\n",
      "\n",
      "Epoch 00096: val_mae did not improve from 8.84871\n",
      "Epoch 97/100\n",
      "128/128 - 3s - loss: 142.3761 - mse: 123.6539 - mae: 8.6568 - fp_mae: 4.5968 - val_loss: 181.2543 - val_mse: 162.1666 - val_mae: 10.0304 - val_fp_mae: 4.9823\n",
      "\n",
      "Epoch 00097: val_mae did not improve from 8.84871\n",
      "Epoch 98/100\n",
      "128/128 - 3s - loss: 142.4227 - mse: 123.3348 - mae: 8.6913 - fp_mae: 4.4920 - val_loss: 171.8297 - val_mse: 154.2461 - val_mae: 9.9086 - val_fp_mae: 4.4394\n",
      "\n",
      "Epoch 00098: val_mae did not improve from 8.84871\n",
      "Epoch 99/100\n",
      "128/128 - 3s - loss: 142.6523 - mse: 124.1069 - mae: 8.6819 - fp_mae: 4.5695 - val_loss: 366.9551 - val_mse: 346.4866 - val_mae: 14.1100 - val_fp_mae: 10.8166\n",
      "\n",
      "Epoch 00099: val_mae did not improve from 8.84871\n",
      "Epoch 100/100\n",
      "128/128 - 3s - loss: 142.8918 - mse: 123.4633 - mae: 8.6938 - fp_mae: 4.4878 - val_loss: 529.5608 - val_mse: 511.1351 - val_mae: 16.5389 - val_fp_mae: 14.2158\n",
      "\n",
      "Epoch 00100: val_mae did not improve from 8.84871\n",
      "\n",
      "Lambda: 10 , Time: 0:04:26\n",
      "Train Error(all epochs): 8.640395164489746 \n",
      " [25.212, 22.546, 19.504, 16.688, 14.391, 12.535, 10.925, 9.953, 9.463, 9.185, 9.054, 8.938, 8.88, 8.796, 8.83, 8.83, 8.793, 8.793, 8.789, 8.76, 8.739, 8.769, 8.766, 8.742, 8.737, 8.701, 8.717, 8.718, 8.715, 8.702, 8.697, 8.674, 8.667, 8.69, 8.695, 8.688, 8.709, 8.666, 8.722, 8.686, 8.713, 8.709, 8.682, 8.696, 8.683, 8.686, 8.681, 8.679, 8.68, 8.687, 8.672, 8.71, 8.693, 8.673, 8.706, 8.672, 8.693, 8.708, 8.71, 8.674, 8.705, 8.694, 8.695, 8.694, 8.693, 8.695, 8.694, 8.695, 8.67, 8.684, 8.701, 8.697, 8.695, 8.686, 8.715, 8.698, 8.725, 8.687, 8.669, 8.69, 8.698, 8.706, 8.709, 8.703, 8.689, 8.71, 8.706, 8.682, 8.734, 8.73, 8.732, 8.686, 8.744, 8.64, 8.714, 8.687, 8.657, 8.691, 8.682, 8.694]\n",
      "Train FP Error(all epochs): 4.487368106842041 \n",
      " [23.8, 21.892, 19.258, 16.505, 14.019, 11.801, 9.769, 8.368, 7.479, 6.934, 6.586, 6.344, 6.17, 5.922, 5.895, 5.845, 5.734, 5.695, 5.712, 5.64, 5.597, 5.539, 5.452, 5.394, 5.315, 5.231, 5.194, 5.153, 5.166, 5.022, 5.073, 5.024, 4.909, 4.984, 4.911, 4.934, 4.891, 4.9, 4.827, 4.877, 4.843, 4.889, 4.8, 4.732, 4.819, 4.75, 4.803, 4.78, 4.658, 4.761, 4.719, 4.675, 4.752, 4.697, 4.672, 4.78, 4.653, 4.665, 4.705, 4.684, 4.562, 4.699, 4.641, 4.66, 4.669, 4.627, 4.611, 4.658, 4.637, 4.583, 4.63, 4.61, 4.616, 4.588, 4.61, 4.596, 4.615, 4.585, 4.602, 4.54, 4.59, 4.6, 4.607, 4.522, 4.564, 4.61, 4.572, 4.601, 4.563, 4.584, 4.555, 4.63, 4.549, 4.557, 4.523, 4.487, 4.597, 4.492, 4.569, 4.488]\n",
      "Val Error(all epochs): 8.848705291748047 \n",
      " [28.213, 23.266, 20.353, 17.422, 14.755, 18.027, 11.081, 12.463, 11.812, 13.645, 10.429, 10.918, 9.502, 9.35, 18.674, 19.24, 9.33, 11.956, 20.686, 9.398, 10.636, 12.871, 10.538, 10.956, 11.416, 9.722, 12.528, 11.273, 9.614, 10.452, 14.231, 9.592, 8.977, 10.879, 12.032, 8.981, 9.173, 12.32, 9.002, 21.653, 8.984, 16.706, 11.845, 11.064, 9.213, 11.413, 11.021, 13.811, 9.257, 13.704, 16.399, 9.045, 8.849, 10.356, 11.834, 10.064, 9.153, 10.679, 8.984, 8.973, 12.94, 9.072, 11.804, 10.579, 9.317, 11.82, 8.943, 11.755, 8.981, 9.435, 10.177, 9.933, 11.872, 10.881, 15.49, 10.441, 9.735, 11.925, 11.259, 9.371, 9.881, 9.541, 12.084, 10.353, 8.994, 8.919, 21.351, 9.595, 9.862, 10.518, 13.391, 9.43, 10.136, 12.307, 14.822, 9.109, 10.03, 9.909, 14.11, 16.539]\n",
      "Val FP Error(all epochs): 4.000529766082764 \n",
      " [27.081, 21.206, 19.267, 17.182, 12.246, 17.729, 9.942, 7.5, 10.444, 12.288, 8.723, 5.585, 7.143, 6.936, 16.787, 17.456, 6.963, 5.222, 18.666, 5.703, 6.17, 11.035, 4.981, 5.807, 5.157, 5.271, 4.934, 5.212, 4.723, 8.103, 11.594, 6.683, 5.238, 8.156, 9.69, 5.168, 6.352, 5.454, 5.497, 18.879, 5.31, 14.014, 4.001, 4.616, 5.959, 8.919, 5.458, 10.586, 4.804, 10.947, 13.597, 5.306, 5.254, 7.987, 5.019, 7.079, 6.125, 4.11, 5.63, 4.732, 9.775, 4.49, 8.694, 5.511, 6.235, 4.369, 5.288, 8.643, 4.996, 6.19, 4.376, 4.641, 4.501, 4.759, 12.553, 7.895, 6.691, 4.555, 5.218, 5.751, 4.354, 6.278, 8.762, 7.706, 5.624, 5.164, 18.718, 5.059, 5.067, 4.602, 4.502, 6.179, 4.369, 9.768, 12.065, 5.828, 4.982, 4.439, 10.817, 14.216]\n",
      "\n",
      "Trainig set size: 4096 , Time: 0:17:46 , best_lambda: 0 , min_  error: 7.868\n",
      "Test starts:  4916 , ends:  29999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784/784 [==============================] - 12s 15ms/step - loss: 92.7350 - mse: 92.7350 - mae: 7.5143 - fp_mae: 3.7459\n",
      "average_error:  7.514 , fp_average_error:  3.746\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CNN: support batching\n",
    "TEST, CONSERVE = True, False\n",
    "mini_batch = 16 if max(max_x, max_y) == 1000 else 32\n",
    "epochs = 35 if max(max_x, max_y) == 1000 else 100\n",
    "MAX_QUEUE_SIZE, WORKERS = 28, 4\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "hyper_metric, mode = \"val_mae\", 'min'  # the metric that hyper parameters are tuned with\n",
    "prev_sample = 0\n",
    "lambda_vec = [0, 0.1, 1, 10]  #0.001, 0.01, 0.1, \n",
    "# lambda_vec = [0.01, 0.1, 1]\n",
    "# lambda_vec = [10]\n",
    "# MODEL_PATH = 'models/'\n",
    "average_diff_power, fp_mean_power = [],[] #[7.177, 8.088, 8.183], [3.438, 3.506, 2.662]\n",
    "best_lambda = []\n",
    "average_diff_power_conserve, fp_mean_power_conserve = [], []\n",
    "all_cnns = []\n",
    "if CONSERVE: # for conservative\n",
    "    prev_number_samples = [0] + number_samples[:-1]\n",
    "\n",
    "for num_sample_idx, number_sample in enumerate(number_samples):\n",
    "#     if num_sample_idx < 3:\n",
    "#         continue\n",
    "#     if num_sample_idx == 0:\n",
    "    if CONSERVE:\n",
    "        data_reg[prev_number_samples[num_sample_idx]:number_sample, -1] = data_reg[\n",
    "            prev_number_samples[num_sample_idx]:number_sample, -1] - 1 # conserv value\n",
    "    MODEL_PATH = '/'.join(image_dir.split('/')[:-1]) + '/models/' + str(number_sample)\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "    MODEL_PATH += \"/best_model_lambda_\"\n",
    "    if True:\n",
    "        cnns = [cnn_model(10, lamb, 0) for lamb in lambda_vec]\n",
    "        for cnn in cnns:\n",
    "#             cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae', fp_mean])\n",
    "            cnn.compile(loss=custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                        optimizer='adam', \n",
    "                        metrics=['mse', 'mae', fp_mae])\n",
    "        checkpointers = [ModelCheckpoint(filepath=MODEL_PATH + str(lamb_idx)+ '.h5',\n",
    "                                         verbose=1, save_best_only=True, \n",
    "                                         monitor=hyper_metric,\n",
    "                                         mode=mode)\n",
    "                         for lamb_idx in range(len(lambda_vec))]\n",
    "    else:\n",
    "        cnns = []\n",
    "        cnns = [models.load_model(MODEL_PATH + str(lamb_idx) + '.h5', \n",
    "                                  custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                  'fp_mae': fp_mae }) \n",
    "                for lamb_idx in range(len(lambda_vec))]\n",
    "    number_start = time.time()\n",
    "    train_generator = DataBatchGenerator(dataset=data_reg[prev_sample:number_sample], batch_size=mini_batch,\n",
    "                                         start_idx=prev_sample, number_image_channels=number_image_channels,\n",
    "                                         max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "    \n",
    "\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "    val_generator = DataBatchGenerator(dataset=data_reg[number_sample:number_sample+val_size], \n",
    "                                       batch_size=mini_batch,\n",
    "                                       start_idx=number_sample,\n",
    "                                       number_image_channels=number_image_channels,\n",
    "                                       max_x=max_x, max_y=max_y, \n",
    "                                       float_memory_used=float_memory_used)\n",
    "  \n",
    "    print('number_samples:', number_sample, \", New samples:\", number_sample - prev_sample)\n",
    "    print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", number_sample + val_size - 1)\n",
    "    \n",
    "    for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "#     for lamb_idx, lamb in enumerate(lambda_vec[:len(lambda_vec) - num_sample_idx//2]):\n",
    "#         if num_sample_idx == 3 and lamb_idx < 4:\n",
    "#             continue\n",
    "        lambda_start = time.time()\n",
    "        cnns[lamb_idx].fit(train_generator, epochs=epochs, verbose=2,\n",
    "                           validation_data=val_generator, \n",
    "                           shuffle=True, callbacks=[checkpointers[lamb_idx]], \n",
    "                           workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                           use_multiprocessing=False)\n",
    "        \n",
    "        print(\"\\nLambda:\", lamb, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - lambda_start))))\n",
    "        print(\"Train Error(all epochs):\", min(cnns[lamb_idx].history.history['mae']), '\\n', \n",
    "              [round(val, 3) for val in cnns[lamb_idx].history.history['mae']])\n",
    "        print(\"Train FP Error(all epochs):\", min(cnns[lamb_idx].history.history['fp_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['fp_mae']])\n",
    "        print(\"Val Error(all epochs):\", min(cnns[lamb_idx].history.history['val_mae']), '\\n', \n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_mae']])\n",
    "        print(\"Val FP Error(all epochs):\", min(cnns[lamb_idx].history.history['val_fp_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_fp_mae']])\n",
    "#     if num_sample_idx == 3:    \n",
    "#         models_min_mae = [8.27781, 8.23545, 8.20838, 7.74743]\n",
    "#         models_min_mae += [min(cnns[lamb_idx].history.history[hyper_metric]) for lamb_idx in range(4,lamb_idx+1)]\n",
    "#     else:\n",
    "    models_min_mae = [min(cnns[lam_idx].history.history[hyper_metric]) for\n",
    "                      lam_idx,_ in enumerate(lambda_vec)]\n",
    "    best_lamb_idx = models_min_mae.index(min(models_min_mae))\n",
    "    best_lambda.append(lambda_vec[best_lamb_idx])\n",
    "    print(\"\\nTrainig set size:\", number_sample, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - \n",
    "                                                                                              number_start))),\n",
    "          \", best_lambda:\", lambda_vec[best_lamb_idx], \", min_\" , (\"fp_\" if hyper_metric == \"val_fp_mae\" else \"\"),\n",
    "          \"error:\", round(min(models_min_mae), 3))\n",
    "    all_cnns.append(cnns)\n",
    "    del cnns, train_generator, val_generator, checkpointers\n",
    "    \n",
    "    if TEST:\n",
    "        # evaluating test images\n",
    "        best_model = None\n",
    "        best_model = models.load_model(MODEL_PATH + str(best_lamb_idx) + '.h5', \n",
    "                                       custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                       'fp_mae': fp_mae,\n",
    "                                                      'mae':'mae', 'mse':'mse'})\n",
    "        test_generator = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                            batch_size=mini_batch,\n",
    "                                            start_idx=number_sample + val_size, \n",
    "                                            number_image_channels=number_image_channels,\n",
    "                                            max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "\n",
    "        print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "        time.sleep(1)\n",
    "        test_res = best_model.evaluate(test_generator, verbose=1, \n",
    "                                       workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)\n",
    "        \n",
    "        test_mae_idx, test_fp_mae_idx = [best_model.metrics_names.index(mtrc) \n",
    "                                         for mtrc in ['mae','fp_mae']]\n",
    "        test_mae, test_fp_mae = test_res[test_mae_idx], test_res[test_fp_mae_idx]\n",
    "        average_diff_power.append(round(test_mae, 3))\n",
    "        fp_mean_power.append(round(test_fp_mae, 3))\n",
    "        print('average_error: ', average_diff_power[-1], ', fp_average_error: ', \n",
    "              fp_mean_power[-1])\n",
    "        \n",
    "        if False:\n",
    "            test_generator_conserve = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                                         batch_size=mini_batch,\n",
    "                                                         start_idx=number_sample + val_size, \n",
    "                                                         number_image_channels=number_image_channels,\n",
    "                                                         max_x=max_x, max_y=max_y, \n",
    "                                                         float_memory_used=float_memory_used, \n",
    "                                                         conserve=1)\n",
    "            test_res_conserve = best_model.evaluate(test_generator_conserve, verbose=1, \n",
    "                                                    workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                                                    use_multiprocessing=False)\n",
    "            test_mae_cons, test_fp_mae_cons = test_res_conserve[test_mae_idx], test_res_conserve[test_fp_mae_idx]\n",
    "            average_diff_power_conserve.append(round(test_mae_cons, 3))\n",
    "            fp_mean_power_conserve.append(round(test_fp_mae_cons, 3))\n",
    "            print('Conserve, average_error: ', average_diff_power_conserve[-1], ', fp_average_error: ',\n",
    "                 fp_mean_power_conserve[-1])\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        \n",
    "        var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/' + intensity_degradation + '_' + str(slope) + '_' + \n",
    "                     dtime + \".dat\", \"wb\") # file for saving results\n",
    "        pickle.dump([average_diff_power, fp_mean_power, number_samples, best_lambda, \n",
    "                     dataset_name, max_dataset_name, average_diff_power_conserve, fp_mean_power_conserve],\n",
    "                    file=var_f)\n",
    "        var_f.close()\n",
    "        del best_model, test_generator\n",
    "#     prev_sample = number_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(number_samples)\n",
    "print(average_diff_power)\n",
    "print(fp_mean_power)\n",
    "# print(best_lambda)\n",
    "print(average_diff_power_conserve)\n",
    "print(fp_mean_power_conserve)\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnns[0].history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = all_cnns[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    checkpointers = ModelCheckpoint(filepath=MODEL_PATH + str(0)+ 'new.h5',\n",
    "                                         verbose=1, save_best_only=True, \n",
    "                                         monitor=hyper_metric,\n",
    "                                         mode=mode)\n",
    "    number_start = time.time()\n",
    "    train_generator = DataBatchGenerator(dataset=data_reg[prev_sample:number_sample], batch_size=mini_batch,\n",
    "                                         start_idx=prev_sample, number_image_channels=number_image_channels,\n",
    "                                         max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "    \n",
    "\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "    val_generator = DataBatchGenerator(dataset=data_reg[number_sample:number_sample+val_size], \n",
    "                                       batch_size=mini_batch,\n",
    "                                       start_idx=number_sample,\n",
    "                                       number_image_channels=number_image_channels,\n",
    "                                       max_x=max_x, max_y=max_y, \n",
    "                                       float_memory_used=float_memory_used)\n",
    "  \n",
    "    print('number_samples:', number_sample, \", New samples:\", number_sample - prev_sample)\n",
    "    print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", number_sample + val_size - 1)\n",
    "    best_model.fit(train_generator, epochs=80, verbose=0,\n",
    "                   validation_data=val_generator, shuffle=True, callbacks=[checkpointers], \n",
    "                   workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                   use_multiprocessing=False, initial_epoch=60)\n",
    "    print(\"Train Error(all epochs):\", min(best_model.history.history['mae']), '\\n',\n",
    "          [round(val, 3) for val in best_model.history.history['mae']])\n",
    "    print(\"Train FP Error(all epochs):\", min(best_model.history.history['fp_mae']), '\\n',\n",
    "          [round(val,3) for val in best_model.history.history['fp_mae']])\n",
    "    print(\"Val Error(all epochs):\", min(best_model.history.history['val_mae']), '\\n', \n",
    "          [round(val,3) for val in best_model.history.history['val_mae']])\n",
    "    print(\"Val FP Error(all epochs):\", min(best_model.history.history['val_fp_mae']), '\\n',\n",
    "          [round(val,3) for val in best_model.history.history['val_fp_mae']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_best_model = models.load_model(MODEL_PATH + str(0) + 'new.h5', \n",
    "                               custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                               'fp_mae': fp_mae,\n",
    "                                               'mae':'mae', 'mse':'mse'})\n",
    "test_generator = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                            batch_size=mini_batch,\n",
    "                                            start_idx=number_sample + val_size, \n",
    "                                            number_image_channels=number_image_channels,\n",
    "                                            max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "\n",
    "print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "time.sleep(1)\n",
    "test_res = best_best_model.evaluate(test_generator, verbose=1, \n",
    "                                    workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE,\n",
    "                                    use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(train_generator, epochs=15, verbose=0,\n",
    "               validation_data=val_generator, shuffle=True, callbacks=[checkpointers], \n",
    "               workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "               use_multiprocessing=False, initial_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_min_mae = [8.27781, 8.23545, 8.20838]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_diff_power = [8.166, 7.844, 7.592]\n",
    "fp_mean_power = [4.56, 4.42, 4.37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CNN: support batching\n",
    "TEST = True\n",
    "mini_batch, epochs = 16, 30\n",
    "batch_size = (batch_size // mini_batch) * mini_batch\n",
    "prev_sample = 0\n",
    "lambda_vec = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]  #, 0.3, 1, 3, 10\n",
    "average_diff_power, fp_mean_power = [], []\n",
    "cnns = [cnn_model(10, lamb, 0) for lamb in lambda_vec]\n",
    "for cnn in cnns:\n",
    "    cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "for number_sample in number_samples:\n",
    "    number_start = time.time()\n",
    "    current_sample = number_sample - prev_sample\n",
    "    train_samples = [batch_size] * (current_sample//batch_size) + ([current_sample%batch_size] if \n",
    "                                                                    current_sample%batch_size else [])\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "#     val_samples = [batch_size] * (val_size//batch_size) + ([val_size%batch_size] if \n",
    "#                                                                val_size%batch_size else [])\n",
    "    \n",
    "    print('number_samples:', number_sample)\n",
    "    print(\"Train batches:\", train_samples)\n",
    "    for i, train_sample in enumerate(train_samples):\n",
    "        print(\"Train batch#:\", i, \", batch size:\", train_sample, \", starts:\", prev_sample + i * batch_size,\n",
    "                      \", ends:\", prev_sample + i * batch_size + train_sample - 1)\n",
    "    print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", number_sample + val_size - 1)\n",
    "#     print(\"Validation Batches:\", val_samples)\n",
    "#     for i, val_sample in enumerate(val_samples):\n",
    "#         print(\"Validation batch#:\", i, \", batch size:\", val_sample, \", starts:\", number_sample + i * batch_size,\n",
    "#                       \", ends:\", number_sample + i * batch_size + val_sample - 1)\n",
    "        \n",
    "    min_error = float('inf')\n",
    "    best_model, best_lam = None, None\n",
    "    for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "        lambda_start = time.time()\n",
    "        \n",
    "#         cnn = cnn_model(10, lamb, 0)\n",
    "#         cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "        \n",
    "        # training on all batches\n",
    "        for i, train_sample in enumerate(train_samples):\n",
    "#             if lamb_idx == 0:\n",
    "#                 print(\"Train batch#:\", i, \", batch size:\", train_sample, \", starts:\", prev_sample + i * batch_size,\n",
    "#                       \", ends:\", prev_sample + i * batch_size + train_sample - 1)\n",
    "            x_train = np.empty((train_sample, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "            y_train = np.empty((train_sample), dtype=float_memory_used)\n",
    "            for image_num in range(prev_sample + i * batch_size, prev_sample + i * batch_size + train_sample):\n",
    "                x_train[(image_num - prev_sample) % batch_size] = read_image(image_num)\n",
    "                y_train[(image_num - prev_sample) % batch_size] = np.asarray(data_reg[image_num][-1], \n",
    "                                                                             dtype=float_memory_used)\n",
    "            cnns[lamb_idx].fit(x_train, y_train, epochs=epochs, verbose=2, batch_size=mini_batch,\n",
    "                               validation_split=0.2, \n",
    "                               shuffle=True)\n",
    "            del x_train, y_train\n",
    "#         if lamb_idx == 0:\n",
    "#             print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", \n",
    "#                   number_sample + val_size - 1)\n",
    "        print(\"\\nLambda:\", lamb)\n",
    "        print(\"Train Error(all epochs): \", cnns[lamb_idx].history.history['mae'])\n",
    "        \n",
    "        # validating\n",
    "        val_mae, val_fp_mae = 0.0, 0.0\n",
    "#         for i, val_sample in enumerate(val_samples):\n",
    "#             x_val = np.empty((val_sample, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "#             for image_num in range(val_sample):\n",
    "#                 x_val[image_num] = read_image(image_num + number_sample + i * batch_size)\n",
    "#             yp_val = cnns[lamb_idx].predict(x_val)\n",
    "        for image_num in range(val_size):\n",
    "            val_y = data_reg[image_num + number_sample][-1]\n",
    "            image = read_image(image_num + number_sample)\n",
    "            val_yp = cnns[lamb_idx].predict(image)[0][0]\n",
    "#             for image_num in range(val_sample):\n",
    "#                 val_yp = yp_val[image_num][0]\n",
    "#                 val_y = data_reg[image_num + number_sample + i * batch_size][-1]\n",
    "            val_mae += abs(val_y - val_yp)\n",
    "            if val_yp > val_y:\n",
    "                val_fp_mae += abs(val_yp - val_y)\n",
    "        val_mae /= val_size\n",
    "        val_fp_mae /= val_size\n",
    "        print(\"Val Error:\", round(val_mae, 3), \", Time:\", str(datetime.timedelta(seconds=int(time.time() - lambda_start))))\n",
    "        if val_mae < min_error:\n",
    "            min_error = val_mae\n",
    "            best_model = cnns[lamb_idx]\n",
    "            best_lam = lamb\n",
    "            best_lam_idx = lamb_idx\n",
    "    print(\"\\nTrainig set size:\", number_sample, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - number_start)))\n",
    "          ,\", best_lambda:\", best_lam, \", min_error:\", round(min_error, 3))\n",
    "    \n",
    "    \n",
    "    if TEST:\n",
    "        # evaluating test images\n",
    "        sum_mae, sum_fp_mae = 0, 0\n",
    "        test_size = 0\n",
    "\n",
    "        y_test_p = np.empty((data_reg.shape[0] - (number_sample + val_size)), dtype=float_memory_used)\n",
    "    #     test_size = data_reg.shape[0] - (number_sample + val_size)\n",
    "    #     test_samples = [batch_size] * (test_size//batch_size) + ([test_size%batch_size] if \n",
    "    #                                                              test_size%batch_size else [])\n",
    "        print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "        time.sleep(1)\n",
    "    #     for i, test_sample in tqdm.tqdm(enumerate(test_samples)):\n",
    "    #         x_test = np.empty((test_sample, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "    #         for image_num in range(test_sample):\n",
    "    #             x_test[image_num] = read_image(number_sample + val_size + i * batch_size)\n",
    "    #         yp_test = cnns[best_lam_idx].predict(x_test)\n",
    "    #         for image_num in range(test_sample):\n",
    "    #             test_y = data_reg[number_sample + val_size + i * batch_size][-1]\n",
    "    #             test_yp = yp_test[image_num][0]\n",
    "    #             sum_mae += abs(test_yp - test_y)\n",
    "    #             if test_yp > test_y:\n",
    "    #                 sum_fp_mae += abs(test_yp - test_y)\n",
    "\n",
    "        for test_num in tqdm.tqdm(range(number_sample + val_size, data_reg.shape[0])):\n",
    "            test_size += 1\n",
    "            test_image = read_image(test_num)\n",
    "            test_y = data_reg[test_num][-1]\n",
    "            test_yp = best_model.predict(test_image)[0][0]\n",
    "            y_test_p[test_num - (number_sample + val_size)] = test_yp\n",
    "            sum_mae += abs(test_yp - test_y)\n",
    "            if test_yp > test_y:\n",
    "                sum_fp_mae += abs(test_yp - test_y)\n",
    "        fp_mean_power.append(round(sum_fp_mae/ test_size, 3))\n",
    "        average_diff_power.append(round(sum_mae / test_size, 3))\n",
    "        print('average_error: ', average_diff_power[-1], ', fp_average_error: ', \n",
    "              fp_mean_power[-1])\n",
    "        print(\"\\n\\n\")\n",
    "        var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/' + intensity_degradation + '_' + str(slope) + '_' + \n",
    "                     dtime + \".dat\", \"wb\") # file for saving results\n",
    "        pickle.dump([average_diff_power, fp_mean_power, number_samples], file=var_f)\n",
    "        var_f.close()\n",
    "    prev_sample = number_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnns[1].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CNN: support batching\n",
    "prev_sample = 0\n",
    "# number_samples = [120, 200, 700]\n",
    "lambda_vec = [0, 0.001]  #0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10\n",
    "average_diff_power, fp_mean_power = [], []\n",
    "cnns = [cnn_model(10, lamb, 0) for lamb in lambda_vec]\n",
    "for cnn in cnns:\n",
    "    cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "for number_sample in number_samples:\n",
    "    current_sample = number_sample - prev_sample\n",
    "    print(\"prev: \", prev_sample, \", now: \", number_sample, \", size\", current_sample) \n",
    "    train_samples = [batch_size] * (current_sample//batch_size) + ([current_sample%batch_size] if \n",
    "                                                                    current_sample%batch_size else [])\n",
    "    print(train_samples)\n",
    "    \n",
    "    min_error = float('inf')\n",
    "    best_model, best_lam = None, None\n",
    "    for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "        print(\"Lambda:\", lamb)\n",
    "#         cnn = cnn_model(10, lamb, 0)\n",
    "#         cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "        \n",
    "        # training on all batches\n",
    "                                    \n",
    "        for i, train_sample in enumerate(train_samples):\n",
    "            for image_num in range(prev_sample + i * batch_size, prev_sample + i * batch_size + train_sample):\n",
    "                print(prev_sample + i * batch_size, prev_sample + i * batch_size + train_sample)\n",
    "                print((prev_sample + i * batch_size - prev_sample) % batch_size, \n",
    "                      (prev_sample + i * batch_size + train_sample - prev_sample)% batch_size)\n",
    "                break\n",
    "\n",
    "        \n",
    "        # validating\n",
    "        print(\"validating\")\n",
    "        val_size = math.ceil(number_sample * validation_size)\n",
    "        for image_num in range(val_size):\n",
    "            print(number_sample, val_size + number_sample)\n",
    "            break\n",
    "     \n",
    "    print(\"Test\") \n",
    "    \n",
    "    # evaluating test images\n",
    "\n",
    "    \n",
    "    for test_num in tqdm.tqdm(range(number_sample + val_size, data_reg.shape[0])):\n",
    "        print(number_sample + val_size, data_reg.shape[0])\n",
    "        break\n",
    "    prev_sample = number_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/' + 'best_cnn_4000samples' + intensity_degradation + '_' + str(slope) + '_' + \n",
    "                 dtime + \".dat\", \"wb\") # file for saving results\n",
    "pickle.dump(best_model, file=var_f)\n",
    "var_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use self-training\n",
    "unlabeled_train_samples = [batch_size] * (len(y_test_p)//batch_size) + ([len(y_test_p)%batch_size] if len(y_test_p)%batch_size else [])\n",
    "labeled_train_samples = [batch_size] * (number_sample//batch_size) + ([number_sample%batch_size] if number_sample%batch_size else [])   \n",
    "min_min_error = float('inf')\n",
    "best_best_model, best_best_lam = None, None\n",
    "for lamb in tqdm.tqdm(lambda_vec):\n",
    "    print(\"Lambda:\", lamb)\n",
    "    cnn = cnn_model(10, lamb, 0)\n",
    "    cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "#     cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "        \n",
    "    # training on all batches\n",
    "    # training on all batches\n",
    "    for i, train_sample in tqdm.tqdm(enumerate(labeled_train_samples)):\n",
    "        x_train = np.empty((train_sample, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "        y_train = np.empty((train_sample), dtype=float_memory_used)\n",
    "        for image_num in range(i * batch_size, i * batch_size + train_sample):\n",
    "            x_train[image_num % batch_size] = read_image(image_num)\n",
    "            y_train[image_num % batch_size] = np.asarray(data_reg[image_num][-1], dtype=float_memory_used)\n",
    "        cnn.fit(x_train, y_train, epochs=6, verbose=0, batch_size=1, validation_split=0.0)\n",
    "        del x_train, y_train\n",
    "            \n",
    "    for i, train_sample in tqdm.tqdm(enumerate(unlabeled_train_samples)):\n",
    "        x_train = np.empty((train_sample, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "        y_train = np.empty((train_sample), dtype=float_memory_used)\n",
    "        for image_num in range(i * batch_size + number_sample + val_size, i * batch_size + number_sample + val_size + train_sample):\n",
    "            x_train[(image_num-number_sample - val_size) % batch_size] = read_image(image_num)\n",
    "            y_train[(image_num-number_sample - val_size) % batch_size] = np.asarray(y_test_p[image_num-(number_sample + val_size)], dtype=float_memory_used)\n",
    "        cnn.fit(x_train, y_train, epochs=3, verbose=0, batch_size=1, validation_split=0.0)\n",
    "        del x_train, y_train\n",
    "        \n",
    "    # validating\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "    val_mae, val_fp_mae = 0.0, 0.0\n",
    "    for image_num in range(val_size):\n",
    "        val_y = data_reg[image_num + number_sample][-1]\n",
    "        image = read_image(image_num + number_sample)\n",
    "        val_yp = cnn.predict(image)[0][0]\n",
    "        val_mae += abs(val_y - val_yp)\n",
    "        if val_yp > val_y:\n",
    "            val_fp_mae += abs(val_yp - val_y)\n",
    "    val_mae /= val_size\n",
    "    val_fp_mae /= val_size\n",
    "    print(val_mae)\n",
    "    if val_mae < min_min_error:\n",
    "        min_min_error = val_mae\n",
    "        best_best_model = cnn\n",
    "        best_best_lam = lamb\n",
    "    sum_mae, sum_fp_mae = 0, 0\n",
    "    test_size = 0\n",
    "    \n",
    "for test_num in tqdm.tqdm(range(number_sample + val_size, data_reg.shape[0])):\n",
    "    test_size += 1\n",
    "    test_image = read_image(test_num)\n",
    "    test_y = data_reg[test_num][-1]\n",
    "    test_yp = best_best_model.predict(test_image)[0][0]\n",
    "#     y_test_p[test_num - (number_sample + val_size)] = test_yp\n",
    "    sum_mae += abs(test_yp - test_y)\n",
    "    if test_yp > test_y:\n",
    "        sum_fp_mae += abs(test_yp - test_y)\n",
    "fp_mean_power.append(round(sum_fp_mae/ test_size, 3))\n",
    "average_diff_power.append(round(sum_mae / test_size, 3))\n",
    "print('number_samples: ', number_sample, ', average_error: ', average_diff_power[-1], ' fp_average_error: ', \n",
    "      fp_mean_power[-1])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6.285, 6.366, 6.45, 6.454, 6.382, 6.26, 6.49, 6.224, 6.052, 5.87, 4.915, 4.836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "prev_sample = 0\n",
    "lambda_vec = [0, 0.001, 0.003, 0.01, 0.03, 0.1]\n",
    "max_train_samples = math.ceil(number_samples[-1] * (1 + validation_size))\n",
    "x_train = np.empty((max_train_samples, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "# x_train1 = np.empty((max_train_samples, 1, max_x, max_y), dtype=float_memory_used)\n",
    "# x_train2 = np.empty((max_train_samples, 1, max_x, max_y), dtype=float_memory_used)\n",
    "y_train = np.empty((max_train_samples), dtype=float_memory_used)\n",
    "average_diff_power, fp_mean_power = [], []\n",
    "for number_sample in number_samples:\n",
    "    sample = math.ceil(number_sample * (1 + validation_size))\n",
    "    for image_num in range(prev_sample, sample):\n",
    "        prev_sample = sample\n",
    "        if style == \"image_intensity\":\n",
    "            image = plt.imread(image_dir + '/image' + str(image_num)+'.png')\n",
    "            image = np.swapaxes(image, 0, 2)\n",
    "            x_train[image_num] = np.array(image[:number_image_channels], dtype=float_memory_used).reshape(1, number_image_channels, max_x, max_y)\n",
    "            del image\n",
    "        elif  style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\":\n",
    "            x_train[image_num] = np.load(image_dir + '/image' + str(image_num)+'.npy')\n",
    "#             image = np.load(image_dir + '/image' + str(image_num)+'.npy')\n",
    "#             x_train1[image_num][0] = image[0][0]\n",
    "#             x_train2[image_num][0] = image[0][1]\n",
    "        y_train[image_num] = np.asarray(data_reg[image_num][-1], dtype=float_memory_used)\n",
    "        if image_num + 1 % 100 == 0:\n",
    "            print(image_num)\n",
    "#     cnn = cnn_model(7, 0, 0)\n",
    "#     cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "#     cnn.fit(x_train[:sample], y_train[:sample], epochs=5, verbose=1, batch_size=1, validation_split=validation_size/\n",
    "#             (validation_size + 1))\n",
    "    \n",
    "    min_error = float('inf')\n",
    "    best_model, best_lam = None, None\n",
    "    for lamb in lambda_vec:\n",
    "        print(\"Lambda:\", lamb)\n",
    "        cnn = cnn_model(10, lamb, 0)\n",
    "        cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "#         cnn.fit([x_train1[:sample], x_train2[:sample]], y_train[:sample], epochs=6, verbose=1, batch_size=1, validation_split=validation_size/\n",
    "#                 (validation_size + 1))\n",
    "        cnn.fit(x_train[:sample], y_train[:sample], epochs=6, verbose=0, batch_size=1, validation_split=validation_size/\n",
    "                (validation_size + 1))\n",
    "        if cnn.history.history['val_mean_absolute_error'][-1] < min_error:\n",
    "            min_error = cnn.history.history['val_mean_absolute_error'][-1]\n",
    "            best_model = cnn\n",
    "            best_lam = lamb\n",
    "    print(\"best_lambda, \", best_lam, \"min_error\", min_error)    \n",
    "    # evaluating test images\n",
    "    sum_mae, sum_fp_mae = 0, 0\n",
    "    test_size = 0\n",
    "#     for test_num in range(max_train_samples, data_reg.shape[0]):\n",
    "    for test_num in range(sample, data_reg.shape[0]):\n",
    "        test_size += 1\n",
    "        if style == \"image_intensity\":\n",
    "            test_image = plt.imread(image_dir + '/image' + str(test_num) + '.png')\n",
    "            test_image = np.swapaxes(test_image, 0, 2)\n",
    "            test_image = np.array(test_image[:number_image_channels]).reshape(1, number_image_channels, max_x, max_y)\n",
    "        elif  style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\":\n",
    "            test_image = np.load(image_dir + '/image' + str(test_num)+'.npy')\n",
    "        test_y = data_reg[test_num][-1]\n",
    "        test_yp = best_model.predict(test_image)[0][0]\n",
    "        sum_mae += abs(test_yp - test_y)\n",
    "        if test_yp > test_y:\n",
    "            sum_fp_mae += abs(test_yp - test_y)\n",
    "        if test_num % 500 == 0:\n",
    "            print('test: ', test_num)\n",
    "    fp_mean_power.append(round(sum_fp_mae/ test_size, 3))\n",
    "    average_diff_power.append(round(sum_mae / test_size, 3))\n",
    "    print('number_samples: ', number_sample, ', average_error: ', average_diff_power[-1], ' fp_average_error: ', fp_mean_power[-1])\n",
    "    print(\"\\n\")\n",
    "    var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/' + intensity_degradation + '_' + str(slope) + '_' + dtime + \".dat\", \"wb\") # file for saving results\n",
    "    pickle.dump([average_diff_power, fp_mean_power, number_samples], file=var_f)\n",
    "    var_f.close()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_diff_power[8], average_diff_power[9] = average_diff_power[9], average_diff_power[8]\n",
    "# fp_mean_power = fp_mean_power[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shapee = Input(shape=(number_image_channels, max_x, max_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shapee[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_model(1, 0)\n",
    "cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cnn.history.history['val_mean_absolute_error'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_model(10, 0, 0)\n",
    "cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn.fit(x_train[:sample], y_train[:sample], epochs=5, verbose=1, batch_size=1, validation_split=validation_size/\n",
    "            (validation_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_vec = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3]\n",
    "min_error = float('inf')\n",
    "best_model, best_lam = None, None\n",
    "for lamb in lambda_vec:\n",
    "    print(\"Lambda:\", lamb)\n",
    "    cnn = cnn_model(15, lamb, 0)\n",
    "    cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "    cnn.fit(x_train[:sample], y_train[:sample], epochs=5, verbose=1, batch_size=1, validation_split=validation_size/\n",
    "            (validation_size + 1))\n",
    "    if cnn.history.history['val_mean_absolute_error'][-1] < min_error:\n",
    "        min_error = cnn.history.history['val_mean_absolute_error'][-1]\n",
    "        best_model = cnn\n",
    "        best_lam = lamb\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_lam)\n",
    "print(best_model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# just run to dispaly the image. First change return line from create_image\n",
    "aa = np.swapaxes(np.append(np.array(x_train[50]), np.zeros((2,max_x, max_y), dtype=float_memory_used), axis=0), 0, 2)\n",
    "plt.imshow(aa)\n",
    "# plt.imsave('image.png', aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to read saved variables\n",
    "var_ff = open('ML/data/pictures_1000_1000/log_201912_0705_37.txt', 'rb')\n",
    "[average_diff_power_1, fp_mean_power_1, number_samples_1] = pickle.load(var_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pydotplus\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "keras.utils.vis_utils.pydot = pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARALLEL CNN\n",
    "def cnn_model(num_filters, kernel_lam, bias_lam):\n",
    "#     num_filters, lam = 5, 5\n",
    "    data_format = 'channels_first'\n",
    "    convolution_filter, dense_filter = 'selu', 'linear' #softsign, sigmoid; relu, linear\n",
    "    filter_shape, pool_size = (3, 3), (2,2)\n",
    "    # CNN for PU image\n",
    "    input1  = layers.Input(shape=(number_image_channels - 1, max_x, max_y), name='pus_input')\n",
    "    x1 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(number_image_channels - 1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(input1)\n",
    "    x1 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x1)\n",
    "    \n",
    "    x1 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(number_image_channels - 1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(x1)\n",
    "    x1 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x1)\n",
    "    \n",
    "    x1 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(number_image_channels - 1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(x1)\n",
    "    x1 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x1)\n",
    "    \n",
    "    x1 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(number_image_channels - 1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(x1)\n",
    "    x1 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x1)\n",
    "    \n",
    "    x1 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(number_image_channels - 1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(x1)\n",
    "    x1 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x1)\n",
    "    \n",
    "    \n",
    "    # CNN for SU\n",
    "    input2  = layers.Input(shape=(1, max_x, max_y), name='su_input')\n",
    "    x2 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(input2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x2)\n",
    "    \n",
    "    x2 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(x2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x2)\n",
    "    \n",
    "    x2 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(x2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x2)\n",
    "    \n",
    "    x2 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(x2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x2)\n",
    "    \n",
    "    x2 = layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(1, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer='lecun_normal')(x2)\n",
    "    x2 = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(x2)\n",
    "    \n",
    "    \n",
    "    # concatanate two CNN outputs\n",
    "    x = layers.concatenate([x1, x2])\n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    x = layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer='lecun_normal')(x)\n",
    "    x = layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer='lecun_normal')(x)\n",
    "    out = layers.Dense(1, activation=dense_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer='lecun_normal')(x)\n",
    "    \n",
    "    model = models.Model(inputs=[input1, input2], outputs=out)\n",
    "#     plot_model(model, to_file='model.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_width = 5\n",
    "marker_size = 12\n",
    "reg_style = 'solid'\n",
    "class_reg = 'dashed'\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "plt.plot(number_samples, average_diff_power, color='r', linewidth=line_width, markersize=marker_size, linestyle=class_reg)\n",
    "plt.plot(number_samples, fp_mean_power, color='midnightblue', linewidth=line_width, markersize=marker_size, linestyle=class_reg)\n",
    "plt.xlabel('# of Training Samples', fontsize=47)\n",
    "plt.ylabel('Avg. Diff. wrt Opt. (dB)', fontsize=45)\n",
    "plt.title('Dynamic PUs(200m*200m)')\n",
    "plt.grid(True)\n",
    "\n",
    "ax.set_yticks(np.arange(0,14, 2))\n",
    "# ax.set_xticks(np.arange(100,7000, 1500))\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "ax.tick_params(axis='x', labelsize=46)\n",
    "ax.tick_params(axis='y', labelsize=45)\n",
    "\n",
    "# matplotlib.rcParams.update({'font.size': 22})\n",
    "\n",
    "ax.set_ylim([0, 14])\n",
    "ax.set_xlim([0, 8000])\n",
    "plt.legend(['Total', 'False-Positive'], ncol=2, loc='best', handletextpad=0.1,borderpad=0, columnspacing=0.2, borderaxespad=0.2)\n",
    "# plt.legend(handletextpad=0.1)\n",
    "plt.savefig('/'.join(image_dir.split('/')[:-1]) +  '/' + intensity_degradation + '_' + str(slope) + '_' + dtime + \".png\", \n",
    "            bbox_inches = 'tight', pad_inches = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load(image_dir + '/image10.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('/'.join(image_dir.split('/')[:-1]) + '/log_5__202006_2714_19.dat', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('/home/shahrokh/projects/research/MLSpectrumAllocation/ML/data/pictures_100_100/splat/' +\n",
    "            'pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/pus_1_4_sus_7_channels/log_5__202006_1302_40.dat', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[average_diff_power, fp_mean_power, number_samples, best_lambda, \n",
    " dataset_name, max_dataset_name, average_power_conserve, \n",
    " fp_mean_power_conserve] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(number_samples)\n",
    "print(average_diff_power)\n",
    "print(fp_mean_power)\n",
    "print(best_lambda)\n",
    "print(average_power_conserve)\n",
    "print(fp_mean_power_conserve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fp1, fp2, fp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples1, samples2, samples3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:research] *",
   "language": "python",
   "name": "conda-env-research-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
